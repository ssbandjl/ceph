diff --git a/src/msg/CMakeLists.txt b/src/msg/CMakeLists.txt
new file mode 100644
index 00000000000..fada39b457f
--- /dev/null
+++ b/src/msg/CMakeLists.txt
@@ -0,0 +1,65 @@
+set(msg_srcs
+  DispatchQueue.cc
+  Message.cc
+  Messenger.cc
+  QueueStrategy.cc
+  Connection.cc
+  msg_types.cc)
+
+list(APPEND msg_srcs
+  async/AsyncConnection.cc
+  async/AsyncMessenger.cc
+  async/Protocol.cc
+  async/ProtocolV1.cc
+  async/ProtocolV2.cc
+  async/Event.cc
+  async/EventSelect.cc
+  async/PosixStack.cc
+  async/Stack.cc
+  async/crypto_onwire.cc
+  async/frames_v2.cc
+  async/net_handler.cc)
+
+if(LINUX)
+  list(APPEND msg_srcs
+    async/EventEpoll.cc)
+elseif(FREEBSD OR APPLE)
+  list(APPEND msg_srcs
+    async/EventKqueue.cc)
+endif(LINUX)
+
+if(HAVE_RDMA)
+  list(APPEND msg_srcs
+    async/rdma/Infiniband.cc
+    async/rdma/RDMAConnectedSocketImpl.cc
+    async/rdma/RDMAIWARPConnectedSocketImpl.cc
+    async/rdma/RDMAServerSocketImpl.cc
+    async/rdma/RDMAIWARPServerSocketImpl.cc
+    async/rdma/RDMAStack.cc)
+endif()
+
+add_library(common-msg-objs OBJECT ${msg_srcs})
+target_include_directories(common-msg-objs PRIVATE ${OPENSSL_INCLUDE_DIR})
+
+if(WITH_DPDK)
+  set(async_dpdk_srcs
+    async/dpdk/ARP.cc
+    async/dpdk/DPDK.cc
+    async/dpdk/dpdk_rte.cc
+    async/dpdk/DPDKStack.cc
+    async/dpdk/EventDPDK.cc
+    async/dpdk/IP.cc
+    async/dpdk/net.cc
+    async/dpdk/IPChecksum.cc
+    async/dpdk/Packet.cc
+    async/dpdk/TCP.cc
+    async/dpdk/UserspaceEvent.cc
+    async/dpdk/ethernet.cc)
+  add_library(common_async_dpdk STATIC
+    ${async_dpdk_srcs})
+  target_link_libraries(common_async_dpdk PRIVATE
+    dpdk::dpdk)
+  # Stack.cc includes DPDKStack.h, which includes rte_config.h indirectly
+  target_include_directories(common-msg-objs PRIVATE
+    $<TARGET_PROPERTY:dpdk::dpdk,INTERFACE_INCLUDE_DIRECTORIES>)
+endif(WITH_DPDK)
diff --git a/src/msg/Connection.cc b/src/msg/Connection.cc
new file mode 100644
index 00000000000..21f147b8034
--- /dev/null
+++ b/src/msg/Connection.cc
@@ -0,0 +1,14 @@
+// -*- mode:C++; tab-width:8; c-basic-offset:2; indent-tabs-mode:t -*-
+// vim: ts=8 sw=2 smarttab
+
+#include "msg/Connection.h"
+#include "msg/Messenger.h"
+
+
+bool Connection::is_blackhole() const {
+  auto& conf = msgr->cct->_conf;
+  return ((conf->ms_blackhole_mon && peer_type == CEPH_ENTITY_TYPE_MON) ||
+      (conf->ms_blackhole_osd && peer_type == CEPH_ENTITY_TYPE_OSD) ||
+      (conf->ms_blackhole_mds && peer_type == CEPH_ENTITY_TYPE_MDS) ||
+      (conf->ms_blackhole_client && peer_type == CEPH_ENTITY_TYPE_CLIENT));
+}
diff --git a/src/msg/Connection.h b/src/msg/Connection.h
index 94e934c55f1..1532ce04919 100644
--- a/src/msg/Connection.h
+++ b/src/msg/Connection.h
@@ -18,78 +18,73 @@
 #include <stdlib.h>
 #include <ostream>
 
-#include <boost/intrusive_ptr.hpp>
-// Because intusive_ptr clobbers our assert...
-#include "include/assert.h"
-
-#include "include/types.h"
-#include "include/buffer.h"
-
+#include "auth/Auth.h"
 #include "common/RefCountedObj.h"
-
-#include "common/debug.h"
 #include "common/config.h"
-
+#include "common/debug.h"
+#include "common/ref.h"
+#include "common/ceph_mutex.h"
+#include "include/ceph_assert.h" // Because intusive_ptr clobbers our assert...
+#include "include/buffer.h"
+#include "include/types.h"
+#include "common/item_history.h"
+#include "msg/MessageRef.h"
 
 // ======================================================
 
 // abstract Connection, for keeping per-connection state
 
-class Message;
 class Messenger;
 
-struct Connection : public RefCountedObject {
-  mutable Mutex lock;
+#ifdef UNIT_TESTS_BUILT
+class Interceptor;
+#endif
+
+struct Connection : public RefCountedObjectSafe {
+  mutable ceph::mutex lock = ceph::make_mutex("Connection::lock");
   Messenger *msgr;
-  RefCountedObject *priv;
-  int peer_type;
-  entity_addr_t peer_addr;
+  RefCountedPtr priv;
+  int peer_type = -1;
+  int64_t peer_id = -1;  // [msgr2 only] the 0 of osd.0, 4567 or client.4567
+  safe_item_history<entity_addrvec_t> peer_addrs;
   utime_t last_keepalive, last_keepalive_ack;
+  bool anon = false;  ///< anonymous outgoing connection
 private:
-  uint64_t features;
+  uint64_t features = 0;
 public:
-  bool failed; // true if we are a lossy connection that has failed.
+  bool is_loopback = false;
+  bool failed = false; // true if we are a lossy connection that has failed.
 
-  int rx_buffers_version;
-  map<ceph_tid_t,pair<bufferlist,int> > rx_buffers;
+  int rx_buffers_version = 0;
+  std::map<ceph_tid_t,std::pair<ceph::buffer::list, int>> rx_buffers;
+
+  // authentication state
+  // FIXME make these private after ms_handle_authorizer is removed
+public:
+  AuthCapsInfo peer_caps_info;
+  EntityName peer_name;
+  uint64_t peer_global_id = 0;
+
+#ifdef UNIT_TESTS_BUILT
+  Interceptor *interceptor;
+#endif
 
-  friend class boost::intrusive_ptr<Connection>;
   friend class PipeConnection;
 
 public:
-  Connection(CephContext *cct, Messenger *m)
-    // we are managed exlusively by ConnectionRef; make it so you can
-    //   ConnectionRef foo = new Connection;
-    : RefCountedObject(cct, 0),
-      lock("Connection::lock"),
-      msgr(m),
-      priv(NULL),
-      peer_type(-1),
-      features(0),
-      failed(false),
-      rx_buffers_version(0) {
-  }
-
-  ~Connection() override {
-    //generic_dout(0) << "~Connection " << this << dendl;
-    if (priv) {
-      //generic_dout(0) << "~Connection " << this << " dropping priv " << priv << dendl;
-      priv->put();
-    }
+  void set_priv(const RefCountedPtr& o) {
+    std::lock_guard l{lock};
+    priv = o;
   }
 
-  void set_priv(RefCountedObject *o) {
-    Mutex::Locker l(lock);
-    if (priv)
-      priv->put();
-    priv = o;
+  RefCountedPtr get_priv() {
+    std::lock_guard l{lock};
+    return priv;
   }
 
-  RefCountedObject *get_priv() {
-    Mutex::Locker l(lock);
-    if (priv)
-      return priv->get();
-    return NULL;
+  void clear_priv() {
+    std::lock_guard l{lock};
+    priv.reset(nullptr);
   }
 
   /**
@@ -101,6 +96,14 @@ public:
    */
   virtual bool is_connected() = 0;
 
+  virtual bool is_msgr2() const {
+    return false;
+  }
+
+  bool is_anon() const {
+    return anon;
+  }
+
   Messenger *get_messenger() {
     return msgr;
   }
@@ -117,6 +120,12 @@ public:
    * @return 0 on success, or -errno on failure.
    */
   virtual int send_message(Message *m) = 0;
+
+  virtual int send_message2(MessageRef m)
+  {
+    return send_message(m.detach()); /* send_message(Message *m) consumes a reference */
+  }
+
   /**
    * Send a "keepalive" ping along the given Connection, if it's working.
    * If the underlying connection has broken, this function does nothing.
@@ -150,18 +159,43 @@ public:
    */
   virtual void mark_disposable() = 0;
 
+  // WARNING / FIXME: this is not populated for loopback connections
+  AuthCapsInfo& get_peer_caps_info() {
+    return peer_caps_info;
+  }
+  const EntityName& get_peer_entity_name() {
+    return peer_name;
+  }
+  uint64_t get_peer_global_id() {
+    return peer_global_id;
+  }
 
   int get_peer_type() const { return peer_type; }
   void set_peer_type(int t) { peer_type = t; }
 
+  // peer_id is only defined for msgr2
+  int64_t get_peer_id() const { return peer_id; }
+  void set_peer_id(int64_t t) { peer_id = t; }
+
   bool peer_is_mon() const { return peer_type == CEPH_ENTITY_TYPE_MON; }
   bool peer_is_mgr() const { return peer_type == CEPH_ENTITY_TYPE_MGR; }
   bool peer_is_mds() const { return peer_type == CEPH_ENTITY_TYPE_MDS; }
   bool peer_is_osd() const { return peer_type == CEPH_ENTITY_TYPE_OSD; }
   bool peer_is_client() const { return peer_type == CEPH_ENTITY_TYPE_CLIENT; }
 
-  const entity_addr_t& get_peer_addr() const { return peer_addr; }
-  void set_peer_addr(const entity_addr_t& a) { peer_addr = a; }
+  /// which of the peer's addrs is actually in use for this connection
+  virtual entity_addr_t get_peer_socket_addr() const = 0;
+
+  entity_addr_t get_peer_addr() const {
+    return peer_addrs->front();
+  }
+  const entity_addrvec_t& get_peer_addrs() const {
+    return *peer_addrs;
+  }
+  void set_peer_addr(const entity_addr_t& a) {
+    peer_addrs = entity_addrvec_t(a);
+  }
+  void set_peer_addrs(const entity_addrvec_t& av) { peer_addrs = av; }
 
   uint64_t get_features() const { return features; }
   bool has_feature(uint64_t f) const { return features & f; }
@@ -171,37 +205,54 @@ public:
   void set_features(uint64_t f) { features = f; }
   void set_feature(uint64_t f) { features |= f; }
 
-  void post_rx_buffer(ceph_tid_t tid, bufferlist& bl) {
-    Mutex::Locker l(lock);
+  virtual int get_con_mode() const {
+    return CEPH_CON_MODE_CRC;
+  }
+
+  void post_rx_buffer(ceph_tid_t tid, ceph::buffer::list& bl) {
+#if 0
+    std::lock_guard l{lock};
     ++rx_buffers_version;
     rx_buffers[tid] = pair<bufferlist,int>(bl, rx_buffers_version);
+#endif
   }
 
   void revoke_rx_buffer(ceph_tid_t tid) {
-    Mutex::Locker l(lock);
+#if 0
+    std::lock_guard l{lock};
     rx_buffers.erase(tid);
+#endif
   }
 
   utime_t get_last_keepalive() const {
-    Mutex::Locker l(lock);
+    std::lock_guard l{lock};
     return last_keepalive;
   }
   void set_last_keepalive(utime_t t) {
-    Mutex::Locker l(lock);
+    std::lock_guard l{lock};
     last_keepalive = t;
   }
   utime_t get_last_keepalive_ack() const {
-    Mutex::Locker l(lock);
+    std::lock_guard l{lock};
     return last_keepalive_ack;
   }
   void set_last_keepalive_ack(utime_t t) {
-    Mutex::Locker l(lock);
+    std::lock_guard l{lock};
     last_keepalive_ack = t;
   }
+  bool is_blackhole() const;
 
-};
+protected:
+  Connection(CephContext *cct, Messenger *m)
+    : RefCountedObjectSafe(cct),
+      msgr(m)
+  {}
 
-typedef boost::intrusive_ptr<Connection> ConnectionRef;
+  ~Connection() override {
+    //generic_dout(0) << "~Connection " << this << dendl;
+  }
+};
 
+using ConnectionRef = ceph::ref_t<Connection>;
 
 #endif /* CEPH_CONNECTION_H */
diff --git a/src/msg/DispatchQueue.cc b/src/msg/DispatchQueue.cc
index 263e81f85b4..5a081591a59 100644
--- a/src/msg/DispatchQueue.cc
+++ b/src/msg/DispatchQueue.cc
@@ -26,17 +26,17 @@
  */
 
 #undef dout_prefix
-#define dout_prefix *_dout << "-- " << msgr->get_myaddr() << " "
+#define dout_prefix *_dout << "-- " << msgr->get_myaddrs() << " "
 
 double DispatchQueue::get_max_age(utime_t now) const {
-  Mutex::Locker l(lock);
+  std::lock_guard l{lock};
   if (marrival.empty())
     return 0;
   else
     return (now - marrival.begin()->first);
 }
 
-uint64_t DispatchQueue::pre_dispatch(Message *m)
+uint64_t DispatchQueue::pre_dispatch(const ref_t<Message>& m)
 {
   ldout(cct,1) << "<== " << m->get_source_inst()
 	       << " " << m->get_seq()
@@ -44,7 +44,8 @@ uint64_t DispatchQueue::pre_dispatch(Message *m)
 	       << " ==== " << m->get_payload().length()
 	       << "+" << m->get_middle().length()
 	       << "+" << m->get_data().length()
-	       << " (" << m->get_footer().front_crc << " "
+	       << " (" << ceph_con_mode_name(m->get_connection()->get_con_mode())
+	       << " " << m->get_footer().front_crc << " "
 	       << m->get_footer().middle_crc
 	       << " " << m->get_footer().data_crc << ")"
 	       << " " << m << " con " << m->get_connection()
@@ -54,79 +55,81 @@ uint64_t DispatchQueue::pre_dispatch(Message *m)
   return msize;
 }
 
-void DispatchQueue::post_dispatch(Message *m, uint64_t msize)
+void DispatchQueue::post_dispatch(const ref_t<Message>& m, uint64_t msize)
 {
   dispatch_throttle_release(msize);
   ldout(cct,20) << "done calling dispatch on " << m << dendl;
 }
 
-bool DispatchQueue::can_fast_dispatch(const Message *m) const
+bool DispatchQueue::can_fast_dispatch(const cref_t<Message> &m) const
 {
   return msgr->ms_can_fast_dispatch(m);
 }
 
-void DispatchQueue::fast_dispatch(Message *m)
+void DispatchQueue::fast_dispatch(const ref_t<Message>& m)
 {
   uint64_t msize = pre_dispatch(m);
   msgr->ms_fast_dispatch(m);
   post_dispatch(m, msize);
 }
 
-void DispatchQueue::fast_preprocess(Message *m)
+void DispatchQueue::fast_preprocess(const ref_t<Message>& m)
 {
   msgr->ms_fast_preprocess(m);
 }
 
-void DispatchQueue::enqueue(Message *m, int priority, uint64_t id)
+void DispatchQueue::enqueue(const ref_t<Message>& m, int priority, uint64_t id)
 {
-
-  Mutex::Locker l(lock);
+  std::lock_guard l{lock};
+  if (stop) {
+    return;
+  }
   ldout(cct,20) << "queue " << m << " prio " << priority << dendl;
   add_arrival(m);
   if (priority >= CEPH_MSG_PRIO_LOW) {
-    mqueue.enqueue_strict(
-        id, priority, QueueItem(m));
+    mqueue.enqueue_strict(id, priority, QueueItem(m));
   } else {
-    mqueue.enqueue(
-        id, priority, m->get_cost(), QueueItem(m));
+    mqueue.enqueue(id, priority, m->get_cost(), QueueItem(m));
   }
-  cond.Signal();
+  cond.notify_all();
 }
 
-void DispatchQueue::local_delivery(Message *m, int priority)
+void DispatchQueue::local_delivery(const ref_t<Message>& m, int priority)
 {
-  m->set_recv_stamp(ceph_clock_now());
-  Mutex::Locker l(local_delivery_lock);
+  auto local_delivery_stamp = ceph_clock_now();
+  m->set_recv_stamp(local_delivery_stamp);
+  m->set_throttle_stamp(local_delivery_stamp);
+  m->set_recv_complete_stamp(local_delivery_stamp);
+  std::lock_guard l{local_delivery_lock};
   if (local_messages.empty())
-    local_delivery_cond.Signal();
-  local_messages.push_back(make_pair(m, priority));
+    local_delivery_cond.notify_all();
+  local_messages.emplace(m, priority);
   return;
 }
 
 void DispatchQueue::run_local_delivery()
 {
-  local_delivery_lock.Lock();
+  std::unique_lock l{local_delivery_lock};
   while (true) {
     if (stop_local_delivery)
       break;
     if (local_messages.empty()) {
-      local_delivery_cond.Wait(local_delivery_lock);
+      local_delivery_cond.wait(l);
       continue;
     }
-    pair<Message *, int> mp = local_messages.front();
-    local_messages.pop_front();
-    local_delivery_lock.Unlock();
-    Message *m = mp.first;
-    int priority = mp.second;
+    auto p = std::move(local_messages.front());
+    local_messages.pop();
+    l.unlock();
+    const ref_t<Message>& m = p.first;
+    int priority = p.second;
     fast_preprocess(m);
     if (can_fast_dispatch(m)) {
       fast_dispatch(m);
     } else {
       enqueue(m, priority, 0);
     }
-    local_delivery_lock.Lock();
+    l.lock();
   }
-  local_delivery_lock.Unlock();
 }
 
 void DispatchQueue::dispatch_throttle_release(uint64_t msize)
@@ -150,13 +153,13 @@ void DispatchQueue::dispatch_throttle_release(uint64_t msize)
  */
 void DispatchQueue::entry()
 {
-  lock.Lock();
+  std::unique_lock l{lock};
   while (true) {
     while (!mqueue.empty()) {
       QueueItem qitem = mqueue.dequeue();
       if (!qitem.is_code())
 	remove_arrival(qitem.get_message());
-      lock.Unlock();
+      l.unlock();
 
       if (qitem.is_code()) {
 	if (cct->_conf->ms_inject_internal_delays &&
@@ -188,10 +191,9 @@ void DispatchQueue::entry()
 	  ceph_abort();
 	}
       } else {
-	Message *m = qitem.get_message();
+	const ref_t<Message>& m = qitem.get_message();
 	if (stop) {
 	  ldout(cct,10) << " stop flag set, discarding " << m << " " << *m << dendl;
-	  m->put();
 	} else {
 	  uint64_t msize = pre_dispatch(m);
 	  msgr->ms_deliver_dispatch(m);
@@ -199,36 +201,34 @@ void DispatchQueue::entry()
 	}
       }
 
-      lock.Lock();
+      l.lock();
     }
     if (stop)
       break;
 
     // wait for something to be put on queue
-    cond.Wait(lock);
+    cond.wait(l);
   }
-  lock.Unlock();
 }
 
 void DispatchQueue::discard_queue(uint64_t id) {
-  Mutex::Locker l(lock);
+  std::lock_guard l{lock};
   list<QueueItem> removed;
   mqueue.remove_by_class(id, &removed);
   for (list<QueueItem>::iterator i = removed.begin();
        i != removed.end();
        ++i) {
-    assert(!(i->is_code())); // We don't discard id 0, ever!
-    Message *m = i->get_message();
+    ceph_assert(!(i->is_code())); // We don't discard id 0, ever!
+    const ref_t<Message>& m = i->get_message();
     remove_arrival(m);
     dispatch_throttle_release(m->get_dispatch_throttle_size());
-    m->put();
   }
 }
 
 void DispatchQueue::start()
 {
-  assert(!stop);
-  assert(!dispatch_thread.is_started());
+  ceph_assert(!stop);
+  ceph_assert(!dispatch_thread.is_started());
   dispatch_thread.create("ms_dispatch");
   local_delivery_thread.create("ms_local");
 }
@@ -241,26 +241,21 @@ void DispatchQueue::wait()
 
 void DispatchQueue::discard_local()
 {
-  for (list<pair<Message *, int> >::iterator p = local_messages.begin();
-       p != local_messages.end();
-       ++p) {
-    ldout(cct,20) << __func__ << " " << p->first << dendl;
-    p->first->put();
-  }
-  local_messages.clear();
+  decltype(local_messages)().swap(local_messages);
 }
 
 void DispatchQueue::shutdown()
 {
   // stop my local delivery thread
-  local_delivery_lock.Lock();
-  stop_local_delivery = true;
-  local_delivery_cond.Signal();
-  local_delivery_lock.Unlock();
-
+  {
+    std::scoped_lock l{local_delivery_lock};
+    stop_local_delivery = true;
+    local_delivery_cond.notify_all();
+  }
   // stop my dispatch thread
-  lock.Lock();
-  stop = true;
-  cond.Signal();
-  lock.Unlock();
+  {
+    std::scoped_lock l{lock};
+    stop = true;
+    cond.notify_all();
+  }
 }
diff --git a/src/msg/DispatchQueue.h b/src/msg/DispatchQueue.h
index 55698819361..243de2cba02 100644
--- a/src/msg/DispatchQueue.h
+++ b/src/msg/DispatchQueue.h
@@ -17,17 +17,18 @@
 
 #include <atomic>
 #include <map>
+#include <queue>
 #include <boost/intrusive_ptr.hpp>
-#include "include/assert.h"
-#include "include/xlist.h"
-#include "common/Mutex.h"
-#include "common/Cond.h"
+#include "include/ceph_assert.h"
+#include "include/common_fwd.h"
+#include "common/Throttle.h"
+#include "common/ceph_mutex.h"
 #include "common/Thread.h"
 #include "common/PrioritizedQueue.h"
 
-class CephContext;
+#include "Message.h"
+
 class Messenger;
-class Message;
 struct Connection;
 
 /**
@@ -40,37 +41,37 @@ class DispatchQueue {
   class QueueItem {
     int type;
     ConnectionRef con;
-    MessageRef m;
+    ref_t<Message> m;
   public:
-    explicit QueueItem(Message *m) : type(-1), con(0), m(m) {}
+    explicit QueueItem(const ref_t<Message>& m) : type(-1), con(0), m(m) {}
     QueueItem(int type, Connection *con) : type(type), con(con), m(0) {}
     bool is_code() const {
       return type != -1;
     }
     int get_code () const {
-      assert(is_code());
+      ceph_assert(is_code());
       return type;
     }
-    Message *get_message() {
-      assert(!is_code());
-      return m.get();
+    const ref_t<Message>& get_message() {
+      ceph_assert(!is_code());
+      return m;
     }
     Connection *get_connection() {
-      assert(is_code());
+      ceph_assert(is_code());
       return con.get();
     }
   };
     
   CephContext *cct;
   Messenger *msgr;
-  mutable Mutex lock;
-  Cond cond;
+  mutable ceph::mutex lock;
+  ceph::condition_variable cond;
 
   PrioritizedQueue<QueueItem, uint64_t> mqueue;
 
-  set<pair<double, Message*> > marrival;
-  map<Message *, set<pair<double, Message*> >::iterator> marrival_map;
-  void add_arrival(Message *m) {
+  std::set<pair<double, ref_t<Message>>> marrival;
+  map<ref_t<Message>, decltype(marrival)::iterator> marrival_map;
+  void add_arrival(const ref_t<Message>& m) {
     marrival_map.insert(
       make_pair(
 	m,
@@ -78,12 +79,11 @@ class DispatchQueue {
 	)
       );
   }
-  void remove_arrival(Message *m) {
-    map<Message *, set<pair<double, Message*> >::iterator>::iterator i =
-      marrival_map.find(m);
-    assert(i != marrival_map.end());
-    marrival.erase(i->second);
-    marrival_map.erase(i);
+  void remove_arrival(const ref_t<Message>& m) {
+    auto it = marrival_map.find(m);
+    ceph_assert(it != marrival_map.end());
+    marrival.erase(it->second);
+    marrival_map.erase(it);
   }
 
   std::atomic<uint64_t> next_id;
@@ -103,10 +103,10 @@ class DispatchQueue {
     }
   } dispatch_thread;
 
-  Mutex local_delivery_lock;
-  Cond local_delivery_cond;
+  ceph::mutex local_delivery_lock;
+  ceph::condition_variable local_delivery_cond;
   bool stop_local_delivery;
-  list<pair<Message *, int> > local_messages;
+  std::queue<pair<ref_t<Message>, int>> local_messages;
   class LocalDeliveryThread : public Thread {
     DispatchQueue *dq;
   public:
@@ -117,8 +117,8 @@ class DispatchQueue {
     }
   } local_delivery_thread;
 
-  uint64_t pre_dispatch(Message *m);
-  void post_dispatch(Message *m, uint64_t msize);
+  uint64_t pre_dispatch(const ref_t<Message>& m);
+  void post_dispatch(const ref_t<Message>& m, uint64_t msize);
 
  public:
 
@@ -126,13 +126,16 @@ class DispatchQueue {
   Throttle dispatch_throttler;
 
   bool stop;
-  void local_delivery(Message *m, int priority);
+  void local_delivery(const ref_t<Message>& m, int priority);
+  void local_delivery(Message* m, int priority) {
+    return local_delivery(ref_t<Message>(m, false), priority); /* consume ref */
+  }
   void run_local_delivery();
 
   double get_max_age(utime_t now) const;
 
   int get_queue_len() const {
-    Mutex::Locker l(lock);
+    std::lock_guard l{lock};
     return mqueue.length();
   }
 
@@ -144,60 +147,66 @@ class DispatchQueue {
   void dispatch_throttle_release(uint64_t msize);
 
   void queue_connect(Connection *con) {
-    Mutex::Locker l(lock);
+    std::lock_guard l{lock};
     if (stop)
       return;
     mqueue.enqueue_strict(
       0,
       CEPH_MSG_PRIO_HIGHEST,
       QueueItem(D_CONNECT, con));
-    cond.Signal();
+    cond.notify_all();
   }
   void queue_accept(Connection *con) {
-    Mutex::Locker l(lock);
+    std::lock_guard l{lock};
     if (stop)
       return;
     mqueue.enqueue_strict(
       0,
       CEPH_MSG_PRIO_HIGHEST,
       QueueItem(D_ACCEPT, con));
-    cond.Signal();
+    cond.notify_all();
   }
   void queue_remote_reset(Connection *con) {
-    Mutex::Locker l(lock);
+    std::lock_guard l{lock};
     if (stop)
       return;
     mqueue.enqueue_strict(
       0,
       CEPH_MSG_PRIO_HIGHEST,
       QueueItem(D_BAD_REMOTE_RESET, con));
-    cond.Signal();
+    cond.notify_all();
   }
   void queue_reset(Connection *con) {
-    Mutex::Locker l(lock);
+    std::lock_guard l{lock};
     if (stop)
       return;
     mqueue.enqueue_strict(
       0,
       CEPH_MSG_PRIO_HIGHEST,
       QueueItem(D_BAD_RESET, con));
-    cond.Signal();
+    cond.notify_all();
   }
   void queue_refused(Connection *con) {
-    Mutex::Locker l(lock);
+    std::lock_guard l{lock};
     if (stop)
       return;
     mqueue.enqueue_strict(
       0,
       CEPH_MSG_PRIO_HIGHEST,
       QueueItem(D_CONN_REFUSED, con));
-    cond.Signal();
+    cond.notify_all();
   }
 
-  bool can_fast_dispatch(const Message *m) const;
-  void fast_dispatch(Message *m);
-  void fast_preprocess(Message *m);
-  void enqueue(Message *m, int priority, uint64_t id);
+  bool can_fast_dispatch(const cref_t<Message> &m) const;
+  void fast_dispatch(const ref_t<Message>& m);
+  void fast_dispatch(Message* m) {
+    return fast_dispatch(ref_t<Message>(m, false)); /* consume ref */
+  }
+  void fast_preprocess(const ref_t<Message>& m);
+  void enqueue(const ref_t<Message>& m, int priority, uint64_t id);
+  void enqueue(Message* m, int priority, uint64_t id) {
+    return enqueue(ref_t<Message>(m, false), priority, id); /* consume ref */
+  }
   void discard_queue(uint64_t id);
   void discard_local();
   uint64_t get_id() {
@@ -211,12 +220,12 @@ class DispatchQueue {
 
   DispatchQueue(CephContext *cct, Messenger *msgr, string &name)
     : cct(cct), msgr(msgr),
-      lock("Messenger::DispatchQueue::lock" + name),
+      lock(ceph::make_mutex("Messenger::DispatchQueue::lock" + name)),
       mqueue(cct->_conf->ms_pq_max_tokens_per_priority,
 	     cct->_conf->ms_pq_min_cost),
       next_id(1),
       dispatch_thread(this),
-      local_delivery_lock("Messenger::DispatchQueue::local_delivery_lock" + name),
+      local_delivery_lock(ceph::make_mutex("Messenger::DispatchQueue::local_delivery_lock" + name)),
       stop_local_delivery(false),
       local_delivery_thread(this),
       dispatch_throttler(cct, string("msgr_dispatch_throttler-") + name,
@@ -224,9 +233,9 @@ class DispatchQueue {
       stop(false)
     {}
   ~DispatchQueue() {
-    assert(mqueue.empty());
-    assert(marrival.empty());
-    assert(local_messages.empty());
+    ceph_assert(mqueue.empty());
+    ceph_assert(marrival.empty());
+    ceph_assert(local_messages.empty());
   }
 };
 
diff --git a/src/msg/DispatchStrategy.h b/src/msg/DispatchStrategy.h
index 44d63d47368..4c9726ed635 100644
--- a/src/msg/DispatchStrategy.h
+++ b/src/msg/DispatchStrategy.h
@@ -22,7 +22,7 @@ class Messenger;
 class DispatchStrategy
 {
 protected:
-  Messenger *msgr;
+  Messenger *msgr = nullptr;
 public:
   DispatchStrategy() {}
   Messenger *get_messenger() { return msgr; }
diff --git a/src/msg/Dispatcher.h b/src/msg/Dispatcher.h
index 5af27ea17f3..5e025437b53 100644
--- a/src/msg/Dispatcher.h
+++ b/src/msg/Dispatcher.h
@@ -1,4 +1,4 @@
-// -*- mode:C++; tab-width:8; c-basic-offset:2; indent-tabs-mode:t -*- 
+// -*- mode:C++; tab-width:8; c-basic-offset:2; indent-tabs-mode:t -*-
 // vim: ts=8 sw=2 smarttab
 /*
  * Ceph - scalable distributed file system
@@ -7,25 +7,25 @@
  *
  * This is free software; you can redistribute it and/or
  * modify it under the terms of the GNU Lesser General Public
- * License version 2.1, as published by the Free Software 
+ * License version 2.1, as published by the Free Software
  * Foundation.  See file COPYING.
- * 
+ *
  */
 
 
 #ifndef CEPH_DISPATCHER_H
 #define CEPH_DISPATCHER_H
 
-#include "include/assert.h"
+#include <memory>
 #include "include/buffer_fwd.h"
-#include "include/assert.h"
+#include "include/ceph_assert.h"
+#include "include/common_fwd.h"
+#include "msg/MessageRef.h"
 
 class Messenger;
-class Message;
 class Connection;
-class AuthAuthorizer;
 class CryptoKey;
-class CephContext;
+class KeyStore;
 
 class Dispatcher {
 public:
@@ -59,7 +59,10 @@ public:
    * @param m The message we want to fast dispatch.
    * @returns True if the message can be fast dispatched; false otherwise.
    */
-  virtual bool ms_can_fast_dispatch(const Message *m) const { return false;}
+  virtual bool ms_can_fast_dispatch(const Message *m) const { return false; }
+  virtual bool ms_can_fast_dispatch2(const MessageConstRef& m) const {
+    return ms_can_fast_dispatch(m.get());
+  }
   /**
    * This function determines if a dispatcher is included in the
    * list of fast-dispatch capable Dispatchers.
@@ -74,6 +77,13 @@ public:
    * @param m The Message to fast dispatch.
    */
   virtual void ms_fast_dispatch(Message *m) { ceph_abort(); }
+
+  /* ms_fast_dispatch2 because otherwise the child must define both */
+  virtual void ms_fast_dispatch2(const MessageRef &m) {
+    /* allow old style dispatch handling that expects a Message * with a floating ref */
+    return ms_fast_dispatch(MessageRef(m).detach()); /* XXX N.B. always consumes ref */
+  }
+
   /**
    * Let the Dispatcher preview a Message before it is dispatched. This
    * function is called on *every* Message, prior to the fast/regular dispatch
@@ -90,13 +100,33 @@ public:
    * @param m A message which has been received
    */
   virtual void ms_fast_preprocess(Message *m) {}
+
+  /* ms_fast_preprocess2 because otherwise the child must define both */
+  virtual void ms_fast_preprocess2(const MessageRef &m) {
+    /* allow old style dispatch handling that expects a Message* */
+    return ms_fast_preprocess(m.get());
+  }
+
   /**
    * The Messenger calls this function to deliver a single message.
    *
    * @param m The message being delivered. You (the Dispatcher)
    * are given a single reference count on it.
    */
-  virtual bool ms_dispatch(Message *m) = 0;
+  virtual bool ms_dispatch(Message *m) {
+    ceph_abort();
+  }
+
+  /* ms_dispatch2 because otherwise the child must define both */
+  virtual bool ms_dispatch2(const MessageRef &m) {
+    /* allow old style dispatch handling that expects a Message * with a floating ref */
+    MessageRef mr(m);
+    if (ms_dispatch(mr.get())) {
+      mr.detach(); /* dispatcher consumed ref */
+      return true;
+    }
+    return false;
+  }
 
   /**
    * This function will be called whenever a Connection is newly-created
@@ -135,7 +165,7 @@ public:
   virtual void ms_handle_fast_accept(Connection *con) {}
 
   /*
-   * this indicates that the ordered+reliable delivery semantics have 
+   * this indicates that the ordered+reliable delivery semantics have
    * been violated.  Messages may have been lost due to a fault
    * in the network connection.
    * Only called on lossy Connections.
@@ -155,7 +185,7 @@ public:
    * a reference to it.
    */
   virtual void ms_handle_remote_reset(Connection *con) = 0;
-  
+
   /**
    * This indicates that the connection is both broken and further
    * connection attempts are failing because other side refuses
@@ -170,44 +200,24 @@ public:
    * @defgroup Authentication
    * @{
    */
+
   /**
-   * Retrieve the AuthAuthorizer for the given peer type. It might not
-   * provide one if it knows there is no AuthAuthorizer for that type.
-   *
-   * @param dest_type The peer type we want the authorizer for.
-   * @param a Double pointer to an AuthAuthorizer. The Dispatcher will fill
-   * in *a with the correct AuthAuthorizer, if it can. Make sure that you have
-   * set *a to NULL before calling in.
-   * @param force_new Force the Dispatcher to wait for a new set of keys before
-   * returning the authorizer.
-   *
-   * @return True if this function call properly filled in *a, false otherwise.
-   */
-  virtual bool ms_get_authorizer(int dest_type, AuthAuthorizer **a, bool force_new) { return false; }
-  /**
-   * Verify the authorizer for a new incoming Connection.
+   * handle successful authentication (msgr2)
    *
-   * @param con The new incoming Connection
-   * @param peer_type The type of the endpoint which initiated this Connection
-   * @param protocol The ID of the protocol in use (at time of writing, cephx or none)
-   * @param authorizer The authorization string supplied by the remote
-   * @param authorizer_reply Output param: The string we should send back to
-   * the remote to authorize ourselves. Only filled in if isvalid
-   * @param isvalid Output param: True if authorizer is valid, false otherwise
+   * Authenticated result/state will be attached to the Connection.
    *
-   * @return True if we were able to prove or disprove correctness of
-   * authorizer, false otherwise.
+   * return 1 for success
+   * return 0 for no action (let another Dispatcher handle it)
+   * return <0 for failure (failure to parse caps, for instance)
    */
-  virtual bool ms_verify_authorizer(Connection *con,
-				    int peer_type,
-				    int protocol,
-				    ceph::bufferlist& authorizer,
-				    ceph::bufferlist& authorizer_reply,
-				    bool& isvalid,
-				    CryptoKey& session_key) { return false; }
+  virtual int ms_handle_authentication(Connection *con) {
+    return 0;
+  }
+
   /**
    * @} //Authentication
    */
+
 protected:
   CephContext *cct;
 private:
diff --git a/src/msg/Message.cc b/src/msg/Message.cc
index 7763caf77cd..c57bc3bc1bd 100644
--- a/src/msg/Message.cc
+++ b/src/msg/Message.cc
@@ -7,7 +7,6 @@
 #endif
 
 #include <iostream>
-using namespace std;
 
 #include "include/types.h"
 
@@ -35,6 +34,8 @@ using namespace std;
 #include "messages/MMonCommand.h"
 #include "messages/MMonCommandAck.h"
 #include "messages/MMonPaxos.h"
+#include "messages/MConfig.h"
+#include "messages/MGetConfig.h"
 
 #include "messages/MMonProbe.h"
 #include "messages/MMonJoin.h"
@@ -61,26 +62,34 @@ using namespace std;
 #include "messages/MOSDPGTemp.h"
 #include "messages/MOSDFailure.h"
 #include "messages/MOSDMarkMeDown.h"
+#include "messages/MOSDMarkMeDead.h"
 #include "messages/MOSDFull.h"
 #include "messages/MOSDPing.h"
 #include "messages/MOSDOp.h"
 #include "messages/MOSDOpReply.h"
-#include "messages/MOSDSubOp.h"
-#include "messages/MOSDSubOpReply.h"
 #include "messages/MOSDRepOp.h"
 #include "messages/MOSDRepOpReply.h"
 #include "messages/MOSDMap.h"
 #include "messages/MMonGetOSDMap.h"
+#include "messages/MMonGetPurgedSnaps.h"
+#include "messages/MMonGetPurgedSnapsReply.h"
 
 #include "messages/MOSDPGCreated.h"
 #include "messages/MOSDPGNotify.h"
+#include "messages/MOSDPGNotify2.h"
 #include "messages/MOSDPGQuery.h"
+#include "messages/MOSDPGQuery2.h"
 #include "messages/MOSDPGLog.h"
 #include "messages/MOSDPGRemove.h"
 #include "messages/MOSDPGInfo.h"
+#include "messages/MOSDPGInfo2.h"
 #include "messages/MOSDPGCreate.h"
+#include "messages/MOSDPGCreate2.h"
 #include "messages/MOSDPGTrim.h"
+#include "messages/MOSDPGLease.h"
+#include "messages/MOSDPGLeaseAck.h"
 #include "messages/MOSDScrub.h"
+#include "messages/MOSDScrub2.h"
 #include "messages/MOSDScrubReserve.h"
 #include "messages/MOSDRepScrub.h"
 #include "messages/MOSDRepScrubMap.h"
@@ -91,6 +100,7 @@ using namespace std;
 #include "messages/MOSDPGBackfillRemove.h"
 #include "messages/MOSDPGRecoveryDelete.h"
 #include "messages/MOSDPGRecoveryDeleteReply.h"
+#include "messages/MOSDPGReadyToMerge.h"
 
 #include "messages/MRemoveSnaps.h"
 
@@ -101,7 +111,6 @@ using namespace std;
 #include "messages/MMonHealth.h"
 #include "messages/MMonHealthChecks.h"
 #include "messages/MMonMetadata.h"
-#include "messages/MDataPing.h"
 #include "messages/MAuth.h"
 #include "messages/MAuthReply.h"
 #include "messages/MMonSubscribe.h"
@@ -112,6 +121,8 @@ using namespace std;
 #include "messages/MClientRequest.h"
 #include "messages/MClientRequestForward.h"
 #include "messages/MClientReply.h"
+#include "messages/MClientReclaim.h"
+#include "messages/MClientReclaimReply.h"
 #include "messages/MClientCaps.h"
 #include "messages/MClientCapRelease.h"
 #include "messages/MClientLease.h"
@@ -132,12 +143,14 @@ using namespace std;
 #include "messages/MMDSFindInoReply.h"
 #include "messages/MMDSOpenIno.h"
 #include "messages/MMDSOpenInoReply.h"
+#include "messages/MMDSSnapUpdate.h"
 
 #include "messages/MDirUpdate.h"
 #include "messages/MDiscover.h"
 #include "messages/MDiscoverReply.h"
 
 #include "messages/MMDSFragmentNotify.h"
+#include "messages/MMDSFragmentNotifyAck.h"
 
 #include "messages/MExportDirDiscover.h"
 #include "messages/MExportDirDiscoverAck.h"
@@ -171,14 +184,18 @@ using namespace std;
 #include "messages/MMgrDigest.h"
 #include "messages/MMgrReport.h"
 #include "messages/MMgrOpen.h"
+#include "messages/MMgrClose.h"
 #include "messages/MMgrConfigure.h"
 #include "messages/MMonMgrReport.h"
+#include "messages/MMgrCommand.h"
+#include "messages/MMgrCommandReply.h"
 #include "messages/MServiceMap.h"
 
 #include "messages/MLock.h"
 
 #include "messages/MWatchNotify.h"
 #include "messages/MTimeCheck.h"
+#include "messages/MTimeCheck2.h"
 
 #include "common/config.h"
 
@@ -198,11 +215,11 @@ using namespace std;
 
 #define dout_subsys ceph_subsys_ms
 
-void Message::encode(uint64_t features, int crcflags)
+void Message::encode(uint64_t features, int crcflags, bool skip_header_crc)
 {
   // encode and copy out of *m
   if (empty_payload()) {
-    assert(middle.length() == 0);
+    ceph_assert(middle.length() == 0);
     encode_payload(features);
 
     if (byte_throttler) {
@@ -221,7 +238,7 @@ void Message::encode(uint64_t features, int crcflags)
   header.front_len = get_payload().length();
   header.middle_len = get_middle().length();
   header.data_len = get_data().length();
-  if (crcflags & MSG_CRC_HEADER)
+  if (!skip_header_crc && (crcflags & MSG_CRC_HEADER))
     calc_header_crc();
 
   footer.flags = CEPH_MSG_FOOTER_COMPLETE;
@@ -231,7 +248,7 @@ void Message::encode(uint64_t features, int crcflags)
 
 #ifdef ENCODE_DUMP
     bufferlist bl;
-    ::encode(get_header(), bl);
+    encode(get_header(), bl);
 
     // dump the old footer format
     ceph_msg_footer_old old_footer;
@@ -239,11 +256,11 @@ void Message::encode(uint64_t features, int crcflags)
     old_footer.middle_crc = footer.middle_crc;
     old_footer.data_crc = footer.data_crc;
     old_footer.flags = footer.flags;
-    ::encode(old_footer, bl);
+    encode(old_footer, bl);
 
-    ::encode(get_payload(), bl);
-    ::encode(get_middle(), bl);
-    ::encode(get_data(), bl);
+    encode(get_payload(), bl);
+    encode(get_middle(), bl);
+    encode(get_data(), bl);
 
     // this is almost an exponential backoff, except because we count
     // bits we tend to sample things we encode later, which should be
@@ -259,7 +276,7 @@ void Message::encode(uint64_t features, int crcflags)
       snprintf(fn, sizeof(fn), ENCODE_STRINGIFY(ENCODE_DUMP) "/%s__%d.%x",
 	       abi::__cxa_demangle(typeid(*this).name(), 0, 0, &status),
 	       getpid(), i++);
-      int fd = ::open(fn, O_WRONLY|O_TRUNC|O_CREAT, 0644);
+      int fd = ::open(fn, O_WRONLY|O_TRUNC|O_CREAT|O_CLOEXEC, 0644);
       if (fd >= 0) {
 	bl.write_fd(fd);
 	::close(fd);
@@ -278,11 +295,14 @@ void Message::dump(Formatter *f) const
   f->dump_string("summary", ss.str());
 }
 
-Message *decode_message(CephContext *cct, int crcflags,
-			ceph_msg_header& header,
-			ceph_msg_footer& footer,
-			bufferlist& front, bufferlist& middle,
-			bufferlist& data, Connection* conn)
+Message *decode_message(CephContext *cct,
+                        int crcflags,
+                        ceph_msg_header& header,
+                        ceph_msg_footer& footer,
+                        ceph::bufferlist& front,
+                        ceph::bufferlist& middle,
+                        ceph::bufferlist& data,
+                        Message::ConnectionRef conn)
 {
   // verify crc
   if (crcflags & MSG_CRC_HEADER) {
@@ -291,7 +311,8 @@ Message *decode_message(CephContext *cct, int crcflags,
 
     if (front_crc != footer.front_crc) {
       if (cct) {
-	ldout(cct, 0) << "bad crc in front " << front_crc << " != exp " << footer.front_crc << dendl;
+	ldout(cct, 0) << "bad crc in front " << front_crc << " != exp " << footer.front_crc
+		      << " from " << conn->get_peer_addr() << dendl;
 	ldout(cct, 20) << " ";
 	front.hexdump(*_dout);
 	*_dout << dendl;
@@ -300,7 +321,8 @@ Message *decode_message(CephContext *cct, int crcflags,
     }
     if (middle_crc != footer.middle_crc) {
       if (cct) {
-	ldout(cct, 0) << "bad crc in middle " << middle_crc << " != exp " << footer.middle_crc << dendl;
+	ldout(cct, 0) << "bad crc in middle " << middle_crc << " != exp " << footer.middle_crc
+		      << " from " << conn->get_peer_addr() << dendl;
 	ldout(cct, 20) << " ";
 	middle.hexdump(*_dout);
 	*_dout << dendl;
@@ -313,7 +335,8 @@ Message *decode_message(CephContext *cct, int crcflags,
       __u32 data_crc = data.crc32c(0);
       if (data_crc != footer.data_crc) {
 	if (cct) {
-	  ldout(cct, 0) << "bad crc in data " << data_crc << " != exp " << footer.data_crc << dendl;
+	  ldout(cct, 0) << "bad crc in data " << data_crc << " != exp " << footer.data_crc
+			<< " from " << conn->get_peer_addr() << dendl;
 	  ldout(cct, 20) << " ";
 	  data.hexdump(*_dout);
 	  *_dout << dendl;
@@ -324,492 +347,549 @@ Message *decode_message(CephContext *cct, int crcflags,
   }
 
   // make message
-  Message *m = 0;
+  ref_t<Message> m;
   int type = header.type;
   switch (type) {
 
     // -- with payload --
 
   case MSG_PGSTATS:
-    m = new MPGStats;
+    m = make_message<MPGStats>();
     break;
   case MSG_PGSTATSACK:
-    m = new MPGStatsAck;
+    m = make_message<MPGStatsAck>();
     break;
 
   case CEPH_MSG_STATFS:
-    m = new MStatfs;
+    m = make_message<MStatfs>();
     break;
   case CEPH_MSG_STATFS_REPLY:
-    m = new MStatfsReply;
+    m = make_message<MStatfsReply>();
     break;
   case MSG_GETPOOLSTATS:
-    m = new MGetPoolStats;
+    m = make_message<MGetPoolStats>();
     break;
   case MSG_GETPOOLSTATSREPLY:
-    m = new MGetPoolStatsReply;
+    m = make_message<MGetPoolStatsReply>();
     break;
   case CEPH_MSG_POOLOP:
-    m = new MPoolOp;
+    m = make_message<MPoolOp>();
     break;
   case CEPH_MSG_POOLOP_REPLY:
-    m = new MPoolOpReply;
+    m = make_message<MPoolOpReply>();
     break;
   case MSG_MON_COMMAND:
-    m = new MMonCommand;
+    m = make_message<MMonCommand>();
     break;
   case MSG_MON_COMMAND_ACK:
-    m = new MMonCommandAck;
+    m = make_message<MMonCommandAck>();
     break;
   case MSG_MON_PAXOS:
-    m = new MMonPaxos;
+    m = make_message<MMonPaxos>();
+    break;
+  case MSG_CONFIG:
+    m = make_message<MConfig>();
+    break;
+  case MSG_GET_CONFIG:
+    m = make_message<MGetConfig>();
     break;
 
   case MSG_MON_PROBE:
-    m = new MMonProbe;
+    m = make_message<MMonProbe>();
     break;
   case MSG_MON_JOIN:
-    m = new MMonJoin;
+    m = make_message<MMonJoin>();
     break;
   case MSG_MON_ELECTION:
-    m = new MMonElection;
+    m = make_message<MMonElection>();
     break;
   case MSG_MON_SYNC:
-    m = new MMonSync;
+    m = make_message<MMonSync>();
     break;
   case MSG_MON_SCRUB:
-    m = new MMonScrub;
+    m = make_message<MMonScrub>();
     break;
 
   case MSG_LOG:
-    m = new MLog;
+    m = make_message<MLog>();
     break;
   case MSG_LOGACK:
-    m = new MLogAck;
+    m = make_message<MLogAck>();
     break;
 
   case CEPH_MSG_PING:
-    m = new MPing();
+    m = make_message<MPing>();
     break;
   case MSG_COMMAND:
-    m = new MCommand;
+    m = make_message<MCommand>();
     break;
   case MSG_COMMAND_REPLY:
-    m = new MCommandReply;
+    m = make_message<MCommandReply>();
     break;
   case MSG_OSD_BACKFILL_RESERVE:
-    m = new MBackfillReserve;
+    m = make_message<MBackfillReserve>();
     break;
   case MSG_OSD_RECOVERY_RESERVE:
-    m = new MRecoveryReserve;
+    m = make_message<MRecoveryReserve>();
     break;
   case MSG_OSD_FORCE_RECOVERY:
-    m = new MOSDForceRecovery;
+    m = make_message<MOSDForceRecovery>();
     break;
 
   case MSG_ROUTE:
-    m = new MRoute;
+    m = make_message<MRoute>();
     break;
   case MSG_FORWARD:
-    m = new MForward;
+    m = make_message<MForward>();
     break;
     
   case CEPH_MSG_MON_MAP:
-    m = new MMonMap;
+    m = make_message<MMonMap>();
     break;
   case CEPH_MSG_MON_GET_MAP:
-    m = new MMonGetMap;
+    m = make_message<MMonGetMap>();
     break;
   case CEPH_MSG_MON_GET_OSDMAP:
-    m = new MMonGetOSDMap;
+    m = make_message<MMonGetOSDMap>();
+    break;
+  case MSG_MON_GET_PURGED_SNAPS:
+    m = make_message<MMonGetPurgedSnaps>();
+    break;
+  case MSG_MON_GET_PURGED_SNAPS_REPLY:
+    m = make_message<MMonGetPurgedSnapsReply>();
     break;
   case CEPH_MSG_MON_GET_VERSION:
-    m = new MMonGetVersion();
+    m = make_message<MMonGetVersion>();
     break;
   case CEPH_MSG_MON_GET_VERSION_REPLY:
-    m = new MMonGetVersionReply();
+    m = make_message<MMonGetVersionReply>();
     break;
   case CEPH_MSG_MON_METADATA:
-    m = new MMonMetadata();
+    m = make_message<MMonMetadata>();
     break;
 
   case MSG_OSD_BOOT:
-    m = new MOSDBoot();
+    m = make_message<MOSDBoot>();
     break;
   case MSG_OSD_ALIVE:
-    m = new MOSDAlive();
+    m = make_message<MOSDAlive>();
     break;
   case MSG_OSD_BEACON:
-    m = new MOSDBeacon();
+    m = make_message<MOSDBeacon>();
     break;
   case MSG_OSD_PGTEMP:
-    m = new MOSDPGTemp;
+    m = make_message<MOSDPGTemp>();
     break;
   case MSG_OSD_FAILURE:
-    m = new MOSDFailure();
+    m = make_message<MOSDFailure>();
     break;
   case MSG_OSD_MARK_ME_DOWN:
-    m = new MOSDMarkMeDown();
+    m = make_message<MOSDMarkMeDown>();
+    break;
+  case MSG_OSD_MARK_ME_DEAD:
+    m = make_message<MOSDMarkMeDead>();
     break;
   case MSG_OSD_FULL:
-    m = new MOSDFull();
+    m = make_message<MOSDFull>();
     break;
   case MSG_OSD_PING:
-    m = new MOSDPing();
+    m = make_message<MOSDPing>();
     break;
   case CEPH_MSG_OSD_OP:
-    m = new MOSDOp();
+    m = make_message<MOSDOp>();
     break;
   case CEPH_MSG_OSD_OPREPLY:
-    m = new MOSDOpReply();
-    break;
-  case MSG_OSD_SUBOP:
-    m = new MOSDSubOp();
-    break;
-  case MSG_OSD_SUBOPREPLY:
-    m = new MOSDSubOpReply();
+    m = make_message<MOSDOpReply>();
     break;
   case MSG_OSD_REPOP:
-    m = new MOSDRepOp();
+    m = make_message<MOSDRepOp>();
     break;
   case MSG_OSD_REPOPREPLY:
-    m = new MOSDRepOpReply();
+    m = make_message<MOSDRepOpReply>();
     break;
   case MSG_OSD_PG_CREATED:
-    m = new MOSDPGCreated();
+    m = make_message<MOSDPGCreated>();
     break;
   case MSG_OSD_PG_UPDATE_LOG_MISSING:
-    m = new MOSDPGUpdateLogMissing();
+    m = make_message<MOSDPGUpdateLogMissing>();
     break;
   case MSG_OSD_PG_UPDATE_LOG_MISSING_REPLY:
-    m = new MOSDPGUpdateLogMissingReply();
+    m = make_message<MOSDPGUpdateLogMissingReply>();
     break;
   case CEPH_MSG_OSD_BACKOFF:
-    m = new MOSDBackoff;
+    m = make_message<MOSDBackoff>();
     break;
 
   case CEPH_MSG_OSD_MAP:
-    m = new MOSDMap;
+    m = make_message<MOSDMap>();
     break;
 
   case CEPH_MSG_WATCH_NOTIFY:
-    m = new MWatchNotify;
+    m = make_message<MWatchNotify>();
     break;
 
   case MSG_OSD_PG_NOTIFY:
-    m = new MOSDPGNotify;
+    m = make_message<MOSDPGNotify>();
+    break;
+  case MSG_OSD_PG_NOTIFY2:
+    m = make_message<MOSDPGNotify2>();
     break;
   case MSG_OSD_PG_QUERY:
-    m = new MOSDPGQuery;
+    m = make_message<MOSDPGQuery>();
+    break;
+  case MSG_OSD_PG_QUERY2:
+    m = make_message<MOSDPGQuery2>();
     break;
   case MSG_OSD_PG_LOG:
-    m = new MOSDPGLog;
+    m = make_message<MOSDPGLog>();
     break;
   case MSG_OSD_PG_REMOVE:
-    m = new MOSDPGRemove;
+    m = make_message<MOSDPGRemove>();
     break;
   case MSG_OSD_PG_INFO:
-    m = new MOSDPGInfo;
+    m = make_message<MOSDPGInfo>();
+    break;
+  case MSG_OSD_PG_INFO2:
+    m = make_message<MOSDPGInfo2>();
     break;
   case MSG_OSD_PG_CREATE:
-    m = new MOSDPGCreate;
+    m = make_message<MOSDPGCreate>();
+    break;
+  case MSG_OSD_PG_CREATE2:
+    m = make_message<MOSDPGCreate2>();
     break;
   case MSG_OSD_PG_TRIM:
-    m = new MOSDPGTrim;
+    m = make_message<MOSDPGTrim>();
+    break;
+  case MSG_OSD_PG_LEASE:
+    m = make_message<MOSDPGLease>();
+    break;
+  case MSG_OSD_PG_LEASE_ACK:
+    m = make_message<MOSDPGLeaseAck>();
     break;
 
   case MSG_OSD_SCRUB:
-    m = new MOSDScrub;
+    m = make_message<MOSDScrub>();
+    break;
+  case MSG_OSD_SCRUB2:
+    m = make_message<MOSDScrub2>();
     break;
   case MSG_OSD_SCRUB_RESERVE:
-    m = new MOSDScrubReserve;
+    m = make_message<MOSDScrubReserve>();
     break;
   case MSG_REMOVE_SNAPS:
-    m = new MRemoveSnaps;
+    m = make_message<MRemoveSnaps>();
     break;
   case MSG_OSD_REP_SCRUB:
-    m = new MOSDRepScrub;
+    m = make_message<MOSDRepScrub>();
     break;
   case MSG_OSD_REP_SCRUBMAP:
-    m = new MOSDRepScrubMap;
+    m = make_message<MOSDRepScrubMap>();
     break;
   case MSG_OSD_PG_SCAN:
-    m = new MOSDPGScan;
+    m = make_message<MOSDPGScan>();
     break;
   case MSG_OSD_PG_BACKFILL:
-    m = new MOSDPGBackfill;
+    m = make_message<MOSDPGBackfill>();
     break;
   case MSG_OSD_PG_BACKFILL_REMOVE:
-    m = new MOSDPGBackfillRemove;
+    m = make_message<MOSDPGBackfillRemove>();
     break;
   case MSG_OSD_PG_PUSH:
-    m = new MOSDPGPush;
+    m = make_message<MOSDPGPush>();
     break;
   case MSG_OSD_PG_PULL:
-    m = new MOSDPGPull;
+    m = make_message<MOSDPGPull>();
     break;
   case MSG_OSD_PG_PUSH_REPLY:
-    m = new MOSDPGPushReply;
+    m = make_message<MOSDPGPushReply>();
     break;
   case MSG_OSD_PG_RECOVERY_DELETE:
-    m = new MOSDPGRecoveryDelete;
+    m = make_message<MOSDPGRecoveryDelete>();
     break;
   case MSG_OSD_PG_RECOVERY_DELETE_REPLY:
-    m = new MOSDPGRecoveryDeleteReply;
+    m = make_message<MOSDPGRecoveryDeleteReply>();
+    break;
+  case MSG_OSD_PG_READY_TO_MERGE:
+    m = make_message<MOSDPGReadyToMerge>();
     break;
   case MSG_OSD_EC_WRITE:
-    m = new MOSDECSubOpWrite;
+    m = make_message<MOSDECSubOpWrite>();
     break;
   case MSG_OSD_EC_WRITE_REPLY:
-    m = new MOSDECSubOpWriteReply;
+    m = make_message<MOSDECSubOpWriteReply>();
     break;
   case MSG_OSD_EC_READ:
-    m = new MOSDECSubOpRead;
+    m = make_message<MOSDECSubOpRead>();
     break;
   case MSG_OSD_EC_READ_REPLY:
-    m = new MOSDECSubOpReadReply;
+    m = make_message<MOSDECSubOpReadReply>();
     break;
    // auth
   case CEPH_MSG_AUTH:
-    m = new MAuth;
+    m = make_message<MAuth>();
     break;
   case CEPH_MSG_AUTH_REPLY:
-    m = new MAuthReply;
+    m = make_message<MAuthReply>();
     break;
 
   case MSG_MON_GLOBAL_ID:
-    m = new MMonGlobalID;
+    m = make_message<MMonGlobalID>();
     break; 
 
     // clients
   case CEPH_MSG_MON_SUBSCRIBE:
-    m = new MMonSubscribe;
+    m = make_message<MMonSubscribe>();
     break;
   case CEPH_MSG_MON_SUBSCRIBE_ACK:
-    m = new MMonSubscribeAck;
+    m = make_message<MMonSubscribeAck>();
     break;
   case CEPH_MSG_CLIENT_SESSION:
-    m = new MClientSession;
+    m = make_message<MClientSession>();
     break;
   case CEPH_MSG_CLIENT_RECONNECT:
-    m = new MClientReconnect;
+    m = make_message<MClientReconnect>();
     break;
   case CEPH_MSG_CLIENT_REQUEST:
-    m = new MClientRequest;
+    m = make_message<MClientRequest>();
     break;
   case CEPH_MSG_CLIENT_REQUEST_FORWARD:
-    m = new MClientRequestForward;
+    m = make_message<MClientRequestForward>();
     break;
   case CEPH_MSG_CLIENT_REPLY:
-    m = new MClientReply;
+    m = make_message<MClientReply>();
+    break;
+  case CEPH_MSG_CLIENT_RECLAIM:
+    m = make_message<MClientReclaim>();
+    break;
+  case CEPH_MSG_CLIENT_RECLAIM_REPLY:
+    m = make_message<MClientReclaimReply>();
     break;
   case CEPH_MSG_CLIENT_CAPS:
-    m = new MClientCaps;
+    m = make_message<MClientCaps>();
     break;
   case CEPH_MSG_CLIENT_CAPRELEASE:
-    m = new MClientCapRelease;
+    m = make_message<MClientCapRelease>();
     break;
   case CEPH_MSG_CLIENT_LEASE:
-    m = new MClientLease;
+    m = make_message<MClientLease>();
     break;
   case CEPH_MSG_CLIENT_SNAP:
-    m = new MClientSnap;
+    m = make_message<MClientSnap>();
     break;
   case CEPH_MSG_CLIENT_QUOTA:
-    m = new MClientQuota;
+    m = make_message<MClientQuota>();
     break;
 
     // mds
   case MSG_MDS_SLAVE_REQUEST:
-    m = new MMDSSlaveRequest;
+    m = make_message<MMDSSlaveRequest>();
     break;
 
   case CEPH_MSG_MDS_MAP:
-    m = new MMDSMap;
+    m = make_message<MMDSMap>();
     break;
   case CEPH_MSG_FS_MAP:
-    m = new MFSMap;
+    m = make_message<MFSMap>();
     break;
   case CEPH_MSG_FS_MAP_USER:
-    m = new MFSMapUser;
+    m = make_message<MFSMapUser>();
     break;
   case MSG_MDS_BEACON:
-    m = new MMDSBeacon;
+    m = make_message<MMDSBeacon>();
     break;
   case MSG_MDS_OFFLOAD_TARGETS:
-    m = new MMDSLoadTargets;
+    m = make_message<MMDSLoadTargets>();
     break;
   case MSG_MDS_RESOLVE:
-    m = new MMDSResolve;
+    m = make_message<MMDSResolve>();
     break;
   case MSG_MDS_RESOLVEACK:
-    m = new MMDSResolveAck;
+    m = make_message<MMDSResolveAck>();
     break;
   case MSG_MDS_CACHEREJOIN:
-    m = new MMDSCacheRejoin;
+    m = make_message<MMDSCacheRejoin>();
 	break;
   
   case MSG_MDS_DIRUPDATE:
-    m = new MDirUpdate();
+    m = make_message<MDirUpdate>();
     break;
 
   case MSG_MDS_DISCOVER:
-    m = new MDiscover();
+    m = make_message<MDiscover>();
     break;
   case MSG_MDS_DISCOVERREPLY:
-    m = new MDiscoverReply();
+    m = make_message<MDiscoverReply>();
     break;
 
   case MSG_MDS_FINDINO:
-    m = new MMDSFindIno;
+    m = make_message<MMDSFindIno>();
     break;
   case MSG_MDS_FINDINOREPLY:
-    m = new MMDSFindInoReply;
+    m = make_message<MMDSFindInoReply>();
     break;
 
   case MSG_MDS_OPENINO:
-    m = new MMDSOpenIno;
+    m = make_message<MMDSOpenIno>();
     break;
   case MSG_MDS_OPENINOREPLY:
-    m = new MMDSOpenInoReply;
+    m = make_message<MMDSOpenInoReply>();
+    break;
+
+  case MSG_MDS_SNAPUPDATE:
+    m = make_message<MMDSSnapUpdate>();
     break;
 
   case MSG_MDS_FRAGMENTNOTIFY:
-    m = new MMDSFragmentNotify;
+    m = make_message<MMDSFragmentNotify>();
+    break;
+
+  case MSG_MDS_FRAGMENTNOTIFYACK:
+    m = make_message<MMDSFragmentNotifyAck>();
     break;
 
   case MSG_MDS_EXPORTDIRDISCOVER:
-    m = new MExportDirDiscover();
+    m = make_message<MExportDirDiscover>();
     break;
   case MSG_MDS_EXPORTDIRDISCOVERACK:
-    m = new MExportDirDiscoverAck();
+    m = make_message<MExportDirDiscoverAck>();
     break;
   case MSG_MDS_EXPORTDIRCANCEL:
-    m = new MExportDirCancel();
+    m = make_message<MExportDirCancel>();
     break;
 
   case MSG_MDS_EXPORTDIR:
-    m = new MExportDir;
+    m = make_message<MExportDir>();
     break;
   case MSG_MDS_EXPORTDIRACK:
-    m = new MExportDirAck;
+    m = make_message<MExportDirAck>();
     break;
   case MSG_MDS_EXPORTDIRFINISH:
-    m = new MExportDirFinish;
+    m = make_message<MExportDirFinish>();
     break;
 
   case MSG_MDS_EXPORTDIRNOTIFY:
-    m = new MExportDirNotify();
+    m = make_message<MExportDirNotify>();
     break;
 
   case MSG_MDS_EXPORTDIRNOTIFYACK:
-    m = new MExportDirNotifyAck();
+    m = make_message<MExportDirNotifyAck>();
     break;
 
   case MSG_MDS_EXPORTDIRPREP:
-    m = new MExportDirPrep();
+    m = make_message<MExportDirPrep>();
     break;
 
   case MSG_MDS_EXPORTDIRPREPACK:
-    m = new MExportDirPrepAck();
+    m = make_message<MExportDirPrepAck>();
     break;
 
   case MSG_MDS_EXPORTCAPS:
-    m = new MExportCaps;
+    m = make_message<MExportCaps>();
     break;
   case MSG_MDS_EXPORTCAPSACK:
-    m = new MExportCapsAck;
+    m = make_message<MExportCapsAck>();
     break;
   case MSG_MDS_GATHERCAPS:
-    m = new MGatherCaps;
+    m = make_message<MGatherCaps>();
     break;
 
 
   case MSG_MDS_DENTRYUNLINK:
-    m = new MDentryUnlink;
+    m = make_message<MDentryUnlink>();
     break;
   case MSG_MDS_DENTRYLINK:
-    m = new MDentryLink;
+    m = make_message<MDentryLink>();
     break;
 
   case MSG_MDS_HEARTBEAT:
-    m = new MHeartbeat();
+    m = make_message<MHeartbeat>();
     break;
 
   case MSG_MDS_CACHEEXPIRE:
-    m = new MCacheExpire();
+    m = make_message<MCacheExpire>();
     break;
 
   case MSG_MDS_TABLE_REQUEST:
-    m = new MMDSTableRequest;
+    m = make_message<MMDSTableRequest>();
     break;
 
 	/*  case MSG_MDS_INODEUPDATE:
-    m = new MInodeUpdate();
+    m = make_message<MInodeUpdate>();
     break;
 	*/
 
   case MSG_MDS_INODEFILECAPS:
-    m = new MInodeFileCaps();
+    m = make_message<MInodeFileCaps>();
     break;
 
   case MSG_MDS_LOCK:
-    m = new MLock();
+    m = make_message<MLock>();
     break;
 
   case MSG_MGR_BEACON:
-    m = new MMgrBeacon();
+    m = make_message<MMgrBeacon>();
     break;
 
   case MSG_MON_MGR_REPORT:
-    m = new MMonMgrReport();
+    m = make_message<MMonMgrReport>();
     break;
 
   case MSG_SERVICE_MAP:
-    m = new MServiceMap();
+    m = make_message<MServiceMap>();
     break;
 
   case MSG_MGR_MAP:
-    m = new MMgrMap();
+    m = make_message<MMgrMap>();
     break;
 
   case MSG_MGR_DIGEST:
-    m = new MMgrDigest();
+    m = make_message<MMgrDigest>();
+    break;
+
+  case MSG_MGR_COMMAND:
+    m = make_message<MMgrCommand>();
+    break;
+
+  case MSG_MGR_COMMAND_REPLY:
+    m = make_message<MMgrCommandReply>();
     break;
 
   case MSG_MGR_OPEN:
-    m = new MMgrOpen();
+    m = make_message<MMgrOpen>();
+    break;
+
+  case MSG_MGR_CLOSE:
+    m = make_message<MMgrClose>();
     break;
 
   case MSG_MGR_REPORT:
-    m = new MMgrReport();
+    m = make_message<MMgrReport>();
     break;
 
   case MSG_MGR_CONFIGURE:
-    m = new MMgrConfigure();
+    m = make_message<MMgrConfigure>();
     break;
 
   case MSG_TIMECHECK:
-    m = new MTimeCheck();
+    m = make_message<MTimeCheck>();
+    break;
+  case MSG_TIMECHECK2:
+    m = make_message<MTimeCheck2>();
     break;
 
   case MSG_MON_HEALTH:
-    m = new MMonHealth();
+    m = make_message<MMonHealth>();
     break;
 
   case MSG_MON_HEALTH_CHECKS:
-    m = new MMonHealthChecks();
+    m = make_message<MMonHealthChecks>();
     break;
 
-#if defined(HAVE_XIO)
-  case MSG_DATA_PING:
-    m = new MDataPing();
-    break;
-#endif
     // -- simple messages without payload --
 
   case CEPH_MSG_SHUTDOWN:
-    m = new MGenericMessage(type);
+    m = make_message<MGenericMessage>(type);
     break;
 
   default:
@@ -836,11 +916,10 @@ Message *decode_message(CephContext *cct, int crcflags,
       if (cct->_conf->ms_die_on_bad_msg)
 	ceph_abort();
     }
-    m->put();
     return 0;
   }
 
-  m->set_connection(conn);
+  m->set_connection(std::move(conn));
   m->set_header(header);
   m->set_footer(footer);
   m->set_payload(front);
@@ -855,34 +934,35 @@ Message *decode_message(CephContext *cct, int crcflags,
       lderr(cct) << "failed to decode message of type " << type
 		 << " v" << header.version
 		 << ": " << e.what() << dendl;
-      ldout(cct, cct->_conf->ms_dump_corrupt_message_level) << "dump: \n";
+      ldout(cct, ceph::dout::need_dynamic(
+	cct->_conf->ms_dump_corrupt_message_level)) << "dump: \n";
       m->get_payload().hexdump(*_dout);
       *_dout << dendl;
       if (cct->_conf->ms_die_on_bad_msg)
 	ceph_abort();
     }
-    m->put();
     return 0;
   }
 
   // done!
-  return m;
+  return m.detach();
 }
 
 void Message::encode_trace(bufferlist &bl, uint64_t features) const
 {
+  using ceph::encode;
   auto p = trace.get_info();
   static const blkin_trace_info empty = { 0, 0, 0 };
   if (!p) {
     p = &empty;
   }
-  ::encode(*p, bl);
+  encode(*p, bl);
 }
 
-void Message::decode_trace(bufferlist::iterator &p, bool create)
+void Message::decode_trace(bufferlist::const_iterator &p, bool create)
 {
   blkin_trace_info info = {};
-  ::decode(info, p);
+  decode(info, p);
 
 #ifdef WITH_BLKIN
   if (!connection)
@@ -914,24 +994,21 @@ void Message::decode_trace(bufferlist::iterator &p, bool create)
 
 void encode_message(Message *msg, uint64_t features, bufferlist& payload)
 {
-  bufferlist front, middle, data;
   ceph_msg_footer_old old_footer;
-  ceph_msg_footer footer;
   msg->encode(features, MSG_CRC_ALL);
-  ::encode(msg->get_header(), payload);
+  encode(msg->get_header(), payload);
 
   // Here's where we switch to the old footer format.  PLR
-
-  footer = msg->get_footer();
+  ceph_msg_footer footer = msg->get_footer();
   old_footer.front_crc = footer.front_crc;   
   old_footer.middle_crc = footer.middle_crc;   
   old_footer.data_crc = footer.data_crc;   
   old_footer.flags = footer.flags;   
-  ::encode(old_footer, payload);
+  encode(old_footer, payload);
 
-  ::encode(msg->get_payload(), payload);
-  ::encode(msg->get_middle(), payload);
-  ::encode(msg->get_data(), payload);
+  encode(msg->get_payload(), payload);
+  encode(msg->get_middle(), payload);
+  encode(msg->get_data(), payload);
 }
 
 // See above for somewhat bogus use of the old message footer.  We switch to the current footer
@@ -939,22 +1016,21 @@ void encode_message(Message *msg, uint64_t features, bufferlist& payload)
 // We've slipped in a 0 signature at this point, so any signature checking after this will
 // fail.  PLR
 
-Message *decode_message(CephContext *cct, int crcflags, bufferlist::iterator& p)
+Message *decode_message(CephContext *cct, int crcflags, bufferlist::const_iterator& p)
 {
   ceph_msg_header h;
   ceph_msg_footer_old fo;
   ceph_msg_footer f;
   bufferlist fr, mi, da;
-  ::decode(h, p);
-  ::decode(fo, p);
+  decode(h, p);
+  decode(fo, p);
   f.front_crc = fo.front_crc;
   f.middle_crc = fo.middle_crc;
   f.data_crc = fo.data_crc;
   f.flags = fo.flags;
   f.sig = 0;
-  ::decode(fr, p);
-  ::decode(mi, p);
-  ::decode(da, p);
+  decode(fr, p);
+  decode(mi, p);
+  decode(da, p);
   return decode_message(cct, crcflags, h, f, fr, mi, da, nullptr);
 }
-
diff --git a/src/msg/Message.h b/src/msg/Message.h
index 10a7004f7d1..b223aa6ebe7 100644
--- a/src/msg/Message.h
+++ b/src/msg/Message.h
@@ -14,26 +14,30 @@
 
 #ifndef CEPH_MESSAGE_H
 #define CEPH_MESSAGE_H
- 
-#include <stdlib.h>
+
+#include <cstdlib>
 #include <ostream>
+#include <string_view>
 
-#include <boost/intrusive_ptr.hpp>
 #include <boost/intrusive/list.hpp>
-// Because intrusive_ptr clobbers our assert...
-#include "include/assert.h"
-
-#include "include/types.h"
-#include "include/buffer.h"
-#include "common/Throttle.h"
-#include "common/zipkin_trace.h"
-#include "msg_types.h"
 
+#include "include/Context.h"
 #include "common/RefCountedObj.h"
+#include "common/ThrottleInterface.h"
+#include "common/config.h"
+#include "common/ref.h"
+#include "common/debug.h"
+#include "common/zipkin_trace.h"
+#include "include/ceph_assert.h" // Because intrusive_ptr clobbers our assert...
+#include "include/buffer.h"
+#include "include/types.h"
 #include "msg/Connection.h"
+#include "msg/MessageRef.h"
+#include "msg_types.h"
 
-#include "common/debug.h"
-#include "common/config.h"
+#ifdef WITH_SEASTAR
+#  include "crimson/net/SocketConnection.h"
+#endif // WITH_SEASTAR
 
 // monitor internal
 #define MSG_MON_SCRUB              64
@@ -59,6 +63,11 @@
 
 #define MSG_PAXOS                  40
 
+#define MSG_CONFIG           62
+#define MSG_GET_CONFIG       63
+
+#define MSG_MON_GET_PURGED_SNAPS 76
+#define MSG_MON_GET_PURGED_SNAPS_REPLY 77
 
 // osd internal
 #define MSG_OSD_PING         70
@@ -67,19 +76,24 @@
 #define MSG_OSD_ALIVE        73
 #define MSG_OSD_MARK_ME_DOWN 74
 #define MSG_OSD_FULL         75
+#define MSG_OSD_MARK_ME_DEAD 123
 
-#define MSG_OSD_SUBOP        76
-#define MSG_OSD_SUBOPREPLY   77
+// removed right after luminous
+//#define MSG_OSD_SUBOP        76
+//#define MSG_OSD_SUBOPREPLY   77
 
 #define MSG_OSD_PGTEMP       78
 
 #define MSG_OSD_BEACON       79
 
 #define MSG_OSD_PG_NOTIFY      80
+#define MSG_OSD_PG_NOTIFY2    130
 #define MSG_OSD_PG_QUERY       81
+#define MSG_OSD_PG_QUERY2     131
 #define MSG_OSD_PG_LOG         83
 #define MSG_OSD_PG_REMOVE      84
 #define MSG_OSD_PG_INFO        85
+#define MSG_OSD_PG_INFO2      132
 #define MSG_OSD_PG_TRIM        86
 
 #define MSG_PGSTATS            87
@@ -121,6 +135,13 @@
 #define MSG_OSD_REP_SCRUBMAP    117
 #define MSG_OSD_PG_RECOVERY_DELETE 118
 #define MSG_OSD_PG_RECOVERY_DELETE_REPLY 119
+#define MSG_OSD_PG_CREATE2      120
+#define MSG_OSD_SCRUB2          121
+
+#define MSG_OSD_PG_READY_TO_MERGE 122
+
+#define MSG_OSD_PG_LEASE        133
+#define MSG_OSD_PG_LEASE_ACK    134
 
 // *** MDS ***
 
@@ -146,7 +167,8 @@
 #define MSG_MDS_FINDINOREPLY       0x20e
 #define MSG_MDS_OPENINO            0x20f
 #define MSG_MDS_OPENINOREPLY       0x210
-
+#define MSG_MDS_SNAPUPDATE         0x211
+#define MSG_MDS_FRAGMENTNOTIFYACK  0x212
 #define MSG_MDS_LOCK               0x300
 #define MSG_MDS_INODEFILECAPS      0x301
 
@@ -178,15 +200,12 @@
 #define MSG_CRC_HEADER         (1 << 1)
 #define MSG_CRC_ALL            (MSG_CRC_DATA | MSG_CRC_HEADER)
 
-// Xio Testing
-#define MSG_DATA_PING		  0x602
-
-// Xio intends to define messages 0x603..0x606
 
 // Special
 #define MSG_NOP                   0x607
 
 #define MSG_MON_HEALTH_CHECKS     0x608
+#define MSG_TIMECHECK2            0x609
 
 // *** ceph-mgr <-> OSD/MDS daemons ***
 #define MSG_MGR_OPEN              0x700
@@ -205,31 +224,28 @@
 #define MSG_MON_MGR_REPORT        0x706
 #define MSG_SERVICE_MAP           0x707
 
+#define MSG_MGR_CLOSE             0x708
+#define MSG_MGR_COMMAND           0x709
+#define MSG_MGR_COMMAND_REPLY     0x70a
+
 // ======================================================
 
 // abstract Message class
 
-namespace bi = boost::intrusive;
-
-// XioMessenger conditional trace flags
-#define MSG_MAGIC_XIO          0x0002
-#define MSG_MAGIC_TRACE_XCON   0x0004
-#define MSG_MAGIC_TRACE_DTOR   0x0008
-#define MSG_MAGIC_TRACE_HDR    0x0010
-#define MSG_MAGIC_TRACE_XIO    0x0020
-#define MSG_MAGIC_TRACE_XMSGR  0x0040
-#define MSG_MAGIC_TRACE_CTR    0x0080
-
-// XioMessenger diagnostic "ping pong" flag (resend msg when send completes)
-#define MSG_MAGIC_REDUPE       0x0100
-
 class Message : public RefCountedObject {
+public:
+#ifdef WITH_SEASTAR
+  using ConnectionRef = crimson::net::ConnectionRef;
+#else
+  using ConnectionRef = ::ConnectionRef;
+#endif // WITH_SEASTAR
+
 protected:
   ceph_msg_header  header;      // headerelope
   ceph_msg_footer  footer;
-  bufferlist       payload;  // "front" unaligned blob
-  bufferlist       middle;   // "middle" unaligned blob
-  bufferlist       data;     // data payload (page-alignment will be preserved where possible)
+  ceph::buffer::list       payload;  // "front" unaligned blob
+  ceph::buffer::list       middle;   // "middle" unaligned blob
+  ceph::buffer::list       data;     // data payload (page-alignment will be preserved where possible)
 
   /* recv_stamp is set when the Messenger starts reading the
    * Message off the wire */
@@ -246,13 +262,13 @@ protected:
 
   uint32_t magic = 0;
 
-  bi::list_member_hook<> dispatch_q;
+  boost::intrusive::list_member_hook<> dispatch_q;
 
 public:
   // zipkin tracing
   ZTracer::Trace trace;
-  void encode_trace(bufferlist &bl, uint64_t features) const;
-  void decode_trace(bufferlist::iterator &p, bool create = false);
+  void encode_trace(ceph::buffer::list &bl, uint64_t features) const;
+  void decode_trace(ceph::buffer::list::const_iterator &p, bool create = false);
 
   class CompletionHook : public Context {
   protected:
@@ -263,20 +279,22 @@ public:
     virtual void set_message(Message *_m) { m = _m; }
   };
 
-  typedef bi::list< Message,
-		    bi::member_hook< Message,
-				     bi::list_member_hook<>,
-				     &Message::dispatch_q > > Queue;
+  typedef boost::intrusive::list<Message,
+				 boost::intrusive::member_hook<
+				   Message,
+				   boost::intrusive::list_member_hook<>,
+				   &Message::dispatch_q>> Queue;
 
+  ceph::mono_time queue_start;
 protected:
   CompletionHook* completion_hook = nullptr; // owned by Messenger
 
   // release our size in bytes back to this throttler when our payload
   // is adjusted or when we are destroyed.
-  Throttle *byte_throttler = nullptr;
+  ThrottleInterface *byte_throttler = nullptr;
 
   // release a count back to this throttler when we are destroyed
-  Throttle *msg_throttler = nullptr;
+  ThrottleInterface *msg_throttler = nullptr;
 
   // keep track of how big this message was when we reserved space in
   // the msgr dispatch_throttler, so that we can properly release it
@@ -318,15 +336,17 @@ protected:
   }
 public:
   const ConnectionRef& get_connection() const { return connection; }
-  void set_connection(const ConnectionRef& c) {
-    connection = c;
+  void set_connection(ConnectionRef c) {
+    connection = std::move(c);
   }
   CompletionHook* get_completion_hook() { return completion_hook; }
   void set_completion_hook(CompletionHook *hook) { completion_hook = hook; }
-  void set_byte_throttler(Throttle *t) { byte_throttler = t; }
-  Throttle *get_byte_throttler() { return byte_throttler; }
-  void set_message_throttler(Throttle *t) { msg_throttler = t; }
-  Throttle *get_message_throttler() { return msg_throttler; }
+  void set_byte_throttler(ThrottleInterface *t) {
+    byte_throttler = t;
+  }
+  void set_message_throttler(ThrottleInterface *t) {
+    msg_throttler = t;
+  }
 
   void set_dispatch_throttle_size(uint64_t s) { dispatch_throttle_size = s; }
   uint64_t get_dispatch_throttle_size() const { return dispatch_throttle_size; }
@@ -344,7 +364,7 @@ public:
 
   /*
    * If you use get_[data, middle, payload] you shouldn't
-   * use it to change those bufferlists unless you KNOW
+   * use it to change those ceph::buffer::lists unless you KNOW
    * there is no throttle being used. The other
    * functions are throttling-aware as appropriate.
    */
@@ -371,25 +391,26 @@ public:
   }
 
   bool empty_payload() const { return payload.length() == 0; }
-  bufferlist& get_payload() { return payload; }
-  void set_payload(bufferlist& bl) {
+  ceph::buffer::list& get_payload() { return payload; }
+  const ceph::buffer::list& get_payload() const { return payload; }
+  void set_payload(ceph::buffer::list& bl) {
     if (byte_throttler)
       byte_throttler->put(payload.length());
-    payload.claim(bl, buffer::list::CLAIM_ALLOW_NONSHAREABLE);
+    payload.claim(bl);
     if (byte_throttler)
       byte_throttler->take(payload.length());
   }
 
-  void set_middle(bufferlist& bl) {
+  void set_middle(ceph::buffer::list& bl) {
     if (byte_throttler)
       byte_throttler->put(middle.length());
-    middle.claim(bl, buffer::list::CLAIM_ALLOW_NONSHAREABLE);
+    middle.claim(bl);
     if (byte_throttler)
       byte_throttler->take(middle.length());
   }
-  bufferlist& get_middle() { return middle; }
+  ceph::buffer::list& get_middle() { return middle; }
 
-  void set_data(const bufferlist &bl) {
+  void set_data(const ceph::buffer::list &bl) {
     if (byte_throttler)
       byte_throttler->put(data.length());
     data.share(bl);
@@ -397,13 +418,12 @@ public:
       byte_throttler->take(data.length());
   }
 
-  const bufferlist& get_data() const { return data; }
-  bufferlist& get_data() { return data; }
-  void claim_data(bufferlist& bl,
-		  unsigned int flags = buffer::list::CLAIM_DEFAULT) {
+  const ceph::buffer::list& get_data() const { return data; }
+  ceph::buffer::list& get_data() { return data; }
+  void claim_data(ceph::buffer::list& bl) {
     if (byte_throttler)
       byte_throttler->put(data.length());
-    bl.claim(data, flags);
+    bl.claim(data);
   }
   off_t get_data_len() const { return data.length(); }
 
@@ -457,6 +477,11 @@ public:
       return connection->get_peer_addr();
     return entity_addr_t();
   }
+  entity_addrvec_t get_source_addrs() const {
+    if (connection)
+      return connection->get_peer_addrs();
+    return entity_addrvec_t();
+  }
 
   // forwarded?
   entity_inst_t get_orig_source_inst() const {
@@ -468,35 +493,59 @@ public:
   entity_addr_t get_orig_source_addr() const {
     return get_source_addr();
   }
+  entity_addrvec_t get_orig_source_addrs() const {
+    return get_source_addrs();
+  }
 
   // virtual bits
   virtual void decode_payload() = 0;
   virtual void encode_payload(uint64_t features) = 0;
-  virtual const char *get_type_name() const = 0;
-  virtual void print(ostream& out) const {
+  virtual std::string_view get_type_name() const = 0;
+  virtual void print(std::ostream& out) const {
     out << get_type_name() << " magic: " << magic;
   }
 
-  virtual void dump(Formatter *f) const;
+  virtual void dump(ceph::Formatter *f) const;
 
-  void encode(uint64_t features, int crcflags);
+  void encode(uint64_t features, int crcflags, bool skip_header_crc = false);
 };
-typedef boost::intrusive_ptr<Message> MessageRef;
 
-extern Message *decode_message(CephContext *cct, int crcflags,
-			       ceph_msg_header &header,
-			       ceph_msg_footer& footer, bufferlist& front,
-			       bufferlist& middle, bufferlist& data,
-			       Connection* conn);
-inline ostream& operator<<(ostream& out, const Message& m) {
+extern Message *decode_message(CephContext *cct,
+                               int crcflags,
+                               ceph_msg_header& header,
+                               ceph_msg_footer& footer,
+                               ceph::buffer::list& front,
+                               ceph::buffer::list& middle,
+                               ceph::buffer::list& data,
+                               Message::ConnectionRef conn);
+inline std::ostream& operator<<(std::ostream& out, const Message& m) {
   m.print(out);
   if (m.get_header().version)
     out << " v" << m.get_header().version;
   return out;
 }
 
-extern void encode_message(Message *m, uint64_t features, bufferlist& bl);
+extern void encode_message(Message *m, uint64_t features, ceph::buffer::list& bl);
 extern Message *decode_message(CephContext *cct, int crcflags,
-                               bufferlist::iterator& bl);
+                               ceph::buffer::list::const_iterator& bl);
+
+/// this is a "safe" version of Message. it does not allow calling get/put
+/// methods on its derived classes. This is intended to prevent some accidental
+/// reference leaks by forcing . Instead, you must either cast the derived class to a
+/// RefCountedObject to do the get/put or detach a temporary reference.
+class SafeMessage : public Message {
+public:
+  using Message::Message;
+private:
+  using RefCountedObject::get;
+  using RefCountedObject::put;
+};
+
+namespace ceph {
+template<class T, typename... Args>
+ceph::ref_t<T> make_message(Args&&... args) {
+  return {new T(std::forward<Args>(args)...), false};
+}
+}
 
 #endif
diff --git a/src/msg/MessageRef.h b/src/msg/MessageRef.h
new file mode 100644
index 00000000000..5eb3655cb84
--- /dev/null
+++ b/src/msg/MessageRef.h
@@ -0,0 +1,184 @@
+// -*- mode:C++; tab-width:8; c-basic-offset:2; indent-tabs-mode:t -*- 
+// vim: ts=8 sw=2 smarttab
+/*
+ * Ceph - scalable distributed file system
+ *
+ * Copyright (C) 2018 Red Hat, Inc. <contact@redhat.com>
+ *
+ * This is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License version 2.1, as published by the Free Software 
+ * Foundation.  See file COPYING.
+ * 
+ */
+
+#ifndef CEPH_MESSAGEREF_H
+#define CEPH_MESSAGEREF_H
+ 
+#include <boost/intrusive_ptr.hpp>
+
+template<typename T>
+using MRef = boost::intrusive_ptr<T>;
+template<typename T>
+using MConstRef = boost::intrusive_ptr<T const>;
+
+using MessageRef = MRef<class Message>;
+using MessageConstRef = MConstRef<class Message>;
+
+/* cd src/messages/ && for f in *; do printf 'class '; basename "$f" .h | tr -d '\n'; printf ';\n'; done >> ../msg/MessageRef.h */
+
+class MAuth;
+class MAuthReply;
+class MBackfillReserve;
+class MCacheExpire;
+class MClientCapRelease;
+class MClientCaps;
+class MClientLease;
+class MClientQuota;
+class MClientReclaim;
+class MClientReclaimReply;
+class MClientReconnect;
+class MClientReply;
+class MClientRequestForward;
+class MClientRequest;
+class MClientSession;
+class MClientSnap;
+class MCommand;
+class MCommandReply;
+class MConfig;
+class MDentryLink;
+class MDentryUnlink;
+class MDirUpdate;
+class MDiscover;
+class MDiscoverReply;
+class MExportCapsAck;
+class MExportCaps;
+class MExportDirAck;
+class MExportDirCancel;
+class MExportDirDiscoverAck;
+class MExportDirDiscover;
+class MExportDirFinish;
+class MExportDir;
+class MExportDirNotifyAck;
+class MExportDirNotify;
+class MExportDirPrepAck;
+class MExportDirPrep;
+class MForward;
+class MFSMap;
+class MFSMapUser;
+class MGatherCaps;
+class MGenericMessage;
+class MGetConfig;
+class MGetPoolStats;
+class MGetPoolStatsReply;
+class MHeartbeat;
+class MInodeFileCaps;
+class MLock;
+class MLogAck;
+class MLog;
+class MMDSBeacon;
+class MMDSCacheRejoin;
+class MMDSFindIno;
+class MMDSFindInoReply;
+class MMDSFragmentNotifyAck;
+class MMDSFragmentNotify;
+class MMDSLoadTargets;
+class MMDSMap;
+class MMDSOpenIno;
+class MMDSOpenInoReply;
+class MMDSResolveAck;
+class MMDSResolve;
+class MMDSSlaveRequest;
+class MMDSSnapUpdate;
+class MMDSTableRequest;
+class MMgrBeacon;
+class MMgrClose;
+class MMgrConfigure;
+class MMgrDigest;
+class MMgrMap;
+class MMgrOpen;
+class MMgrReport;
+class MMonCommandAck;
+class MMonCommand;
+class MMonElection;
+class MMonGetMap;
+class MMonGetOSDMap;
+class MMonGetVersion;
+class MMonGetVersionReply;
+class MMonGlobalID;
+class MMonHealthChecks;
+class MMonHealth;
+class MMonJoin;
+class MMonMap;
+class MMonMetadata;
+class MMonMgrReport;
+class MMonPaxos;
+class MMonProbe;
+class MMonQuorumService;
+class MMonScrub;
+class MMonSubscribeAck;
+class MMonSubscribe;
+class MMonSync;
+class MOSDAlive;
+class MOSDBackoff;
+class MOSDBeacon;
+class MOSDBoot;
+class MOSDECSubOpRead;
+class MOSDECSubOpReadReply;
+class MOSDECSubOpWrite;
+class MOSDECSubOpWriteReply;
+class MOSDFailure;
+class MOSDFastDispatchOp;
+class MOSDForceRecovery;
+class MOSDFull;
+class MOSDMap;
+class MOSDMarkMeDown;
+class MOSDOp;
+class MOSDOpReply;
+class MOSDPeeringOp;
+class MOSDPGBackfill;
+class MOSDPGBackfillRemove;
+class MOSDPGCreate2;
+class MOSDPGCreated;
+class MOSDPGCreate;
+class MOSDPGInfo;
+class MOSDPGLog;
+class MOSDPGNotify;
+class MOSDPGPull;
+class MOSDPGPush;
+class MOSDPGPushReply;
+class MOSDPGQuery;
+class MOSDPGReadyToMerge;
+class MOSDPGRecoveryDelete;
+class MOSDPGRecoveryDeleteReply;
+class MOSDPGRemove;
+class MOSDPGScan;
+class MOSDPGTemp;
+class MOSDPGTrim;
+class MOSDPGUpdateLogMissing;
+class MOSDPGUpdateLogMissingReply;
+class MOSDPing;
+class MOSDRepOp;
+class MOSDRepOpReply;
+class MOSDRepScrub;
+class MOSDRepScrubMap;
+class MOSDScrub2;
+class MOSDScrub;
+class MOSDScrubReserve;
+class MPGStatsAck;
+class MPGStats;
+class MPing;
+class MPoolOp;
+class MPoolOpReply;
+class MRecoveryReserve;
+class MRemoveSnaps;
+class MRoute;
+class MServiceMap;
+class MStatfs;
+class MStatfsReply;
+class MTimeCheck2;
+class MTimeCheck;
+class MWatchNotify;
+class PaxosServiceMessage;
+
+#endif
diff --git a/src/msg/Messenger.cc b/src/msg/Messenger.cc
index 6ab2862dc41..0b57891bd2d 100644
--- a/src/msg/Messenger.cc
+++ b/src/msg/Messenger.cc
@@ -1,55 +1,73 @@
 // -*- mode:C++; tab-width:8; c-basic-offset:2; indent-tabs-mode:t -*-
 // vim: ts=8 sw=2 smarttab
 
-#include <random>
 #include <netdb.h>
-#include "include/Spinlock.h"
 
 #include "include/types.h"
+#include "include/random.h"
+
 #include "Messenger.h"
 
-#include "msg/simple/SimpleMessenger.h"
 #include "msg/async/AsyncMessenger.h"
-#ifdef HAVE_XIO
-#include "msg/xio/XioMessenger.h"
-#endif
 
 Messenger *Messenger::create_client_messenger(CephContext *cct, string lname)
 {
-  std::string public_msgr_type = cct->_conf->ms_public_type.empty() ? cct->_conf->get_val<std::string>("ms_type") : cct->_conf->ms_public_type;
-  uint64_t nonce = 0;
-  get_random_bytes((char*)&nonce, sizeof(nonce));
+  std::string public_msgr_type = cct->_conf->ms_public_type.empty() ? cct->_conf.get_val<std::string>("ms_type") : cct->_conf->ms_public_type;
+  auto nonce = get_random_nonce();
   return Messenger::create(cct, public_msgr_type, entity_name_t::CLIENT(),
 			   std::move(lname), nonce, 0);
 }
 
+uint64_t Messenger::get_pid_nonce()
+{
+  uint64_t nonce = getpid();
+  if (nonce == 1 || getenv("CEPH_USE_RANDOM_NONCE")) {
+    // we're running in a container; use a random number instead!
+    nonce = ceph::util::generate_random_number<uint64_t>();
+  }
+  return nonce;
+}
+
+uint64_t Messenger::get_random_nonce()
+{
+  return ceph::util::generate_random_number<uint64_t>();
+}
+
 Messenger *Messenger::create(CephContext *cct, const string &type,
 			     entity_name_t name, string lname,
 			     uint64_t nonce, uint64_t cflags)
 {
   int r = -1;
   if (type == "random") {
-    static std::random_device seed;
-    static std::default_random_engine random_engine(seed());
-    static Spinlock random_lock;
-
-    std::lock_guard<Spinlock> lock(random_lock);
-    std::uniform_int_distribution<> dis(0, 1);
-    r = dis(random_engine);
+    r = 0;
+    //r = ceph::util::generate_random_number(0, 1);
   }
-  if (r == 0 || type == "simple")
-    return new SimpleMessenger(cct, name, std::move(lname), nonce);
-  else if (r == 1 || type.find("async") != std::string::npos)
+  if (r == 0 || type.find("async") != std::string::npos)
     return new AsyncMessenger(cct, name, type, std::move(lname), nonce);
-#ifdef HAVE_XIO
-  else if ((type == "xio") &&
-	   cct->check_experimental_feature_enabled("ms-type-xio"))
-    return new XioMessenger(cct, name, std::move(lname), nonce, cflags);
-#endif
   lderr(cct) << "unrecognized ms_type '" << type << "'" << dendl;
   return nullptr;
 }
 
+/**
+ * Get the default crc flags for this messenger.
+ * but not yet dispatched.
+ */
+static int get_default_crc_flags(const ConfigProxy&);
+
+Messenger::Messenger(CephContext *cct_, entity_name_t w)
+  : trace_endpoint("0.0.0.0", 0, "Messenger"),
+    my_name(w),
+    default_send_priority(CEPH_MSG_PRIO_DEFAULT),
+    started(false),
+    magic(0),
+    socket_priority(-1),
+    cct(cct_),
+    crcflags(get_default_crc_flags(cct->_conf)),
+    auth_registry(cct)
+{
+  auth_registry.refresh_config();
+}
+
 void Messenger::set_endpoint_addr(const entity_addr_t& a,
                                   const entity_name_t &name)
 {
@@ -71,12 +89,15 @@ void Messenger::set_endpoint_addr(const entity_addr_t& a,
   trace_endpoint.set_port(a.get_port());
 }
 
-/*
+/**
+ * Get the default crc flags for this messenger.
+ * but not yet dispatched.
+ *
  * Pre-calculate desired software CRC settings.  CRC computation may
  * be disabled by default for some transports (e.g., those with strong
  * hardware checksum support).
  */
-int Messenger::get_default_crc_flags(md_config_t * conf)
+int get_default_crc_flags(const ConfigProxy& conf)
 {
   int r = 0;
   if (conf->ms_crc_data)
@@ -85,3 +106,9 @@ int Messenger::get_default_crc_flags(md_config_t * conf)
     r |= MSG_CRC_HEADER;
   return r;
 }
+
+int Messenger::bindv(const entity_addrvec_t& addrs)
+{
+  return bind(addrs.legacy_addr());
+}
+
diff --git a/src/msg/Messenger.h b/src/msg/Messenger.h
index 7c1a0d1fad5..3620a7e5ae0 100644
--- a/src/msg/Messenger.h
+++ b/src/msg/Messenger.h
@@ -1,4 +1,4 @@
-// -*- mode:C++; tab-width:8; c-basic-offset:2; indent-tabs-mode:t -*- 
+// -*- mode:C++; tab-width:8; c-basic-offset:2; indent-tabs-mode:t -*-
 // vim: ts=8 sw=2 smarttab
 /*
  * Ceph - scalable distributed file system
@@ -7,9 +7,9 @@
  *
  * This is free software; you can redistribute it and/or
  * modify it under the terms of the GNU Lesser General Public
- * License version 2.1, as published by the Free Software 
+ * License version 2.1, as published by the Free Software
  * Foundation.  See file COPYING.
- * 
+ *
  */
 
 
@@ -18,44 +18,84 @@
 #define CEPH_MESSENGER_H
 
 #include <map>
-using namespace std;
+#include <deque>
+
+#include <errno.h>
+#include <sstream>
+#include <memory>
 
 #include "Message.h"
 #include "Dispatcher.h"
-#include "common/Mutex.h"
-#include "common/Cond.h"
+#include "Policy.h"
+#include "common/Throttle.h"
 #include "include/Context.h"
 #include "include/types.h"
 #include "include/ceph_features.h"
 #include "auth/Crypto.h"
+#include "common/item_history.h"
+#include "auth/AuthRegistry.h"
+#include "include/ceph_assert.h"
 
 #include <errno.h>
 #include <sstream>
+#include <signal.h>
 
 #define SOCKET_PRIORITY_MIN_DELAY 6
 
 class Timer;
 
+class AuthClient;
+class AuthServer;
+
+#ifdef UNIT_TESTS_BUILT
+
+struct Interceptor {
+  std::mutex lock;
+  std::condition_variable cond_var;
+
+  enum ACTION : uint32_t {
+    CONTINUE = 0,
+    FAIL,
+    STOP
+  };
+
+  virtual ~Interceptor() {}
+  virtual ACTION intercept(Connection *conn, uint32_t step) = 0;
+};
+
+#endif
 
 class Messenger {
 private:
-  list<Dispatcher*> dispatchers;
-  list <Dispatcher*> fast_dispatchers;
+  std::deque<Dispatcher*> dispatchers;
+  std::deque<Dispatcher*> fast_dispatchers;
   ZTracer::Endpoint trace_endpoint;
 
+protected:
   void set_endpoint_addr(const entity_addr_t& a,
                          const entity_name_t &name);
 
 protected:
   /// the "name" of the local daemon. eg client.99
-  entity_inst_t my_inst;
+  entity_name_t my_name;
+
+  /// my addr
+  safe_item_history<entity_addrvec_t> my_addrs;
+
   int default_send_priority;
-  /// set to true once the Messenger has started, and set to false on shutdown
+  /// std::set to true once the Messenger has started, and std::set to false on shutdown
   bool started;
   uint32_t magic;
   int socket_priority;
 
 public:
+  AuthClient *auth_client = 0;
+  AuthServer *auth_server = 0;
+
+#ifdef UNIT_TESTS_BUILT
+  Interceptor *interceptor = nullptr;
+#endif
+
   /**
    * Various Messenger conditional config/type flags to allow
    * different "transport" Messengers to tune themselves
@@ -71,85 +111,25 @@ public:
   CephContext *cct;
   int crcflags;
 
-  /**
-   * A Policy describes the rules of a Connection. Is there a limit on how
-   * much data this Connection can have locally? When the underlying connection
-   * experiences an error, does the Connection disappear? Can this Messenger
-   * re-establish the underlying connection?
-   */
-  struct Policy {
-    /// If true, the Connection is tossed out on errors.
-    bool lossy;
-    /// If true, the underlying connection can't be re-established from this end.
-    bool server;
-    /// If true, we will standby when idle
-    bool standby;
-    /// If true, we will try to detect session resets
-    bool resetcheck;
-    /**
-     *  The throttler is used to limit how much data is held by Messages from
-     *  the associated Connection(s). When reading in a new Message, the Messenger
-     *  will call throttler->throttle() for the size of the new Message.
-     */
-    Throttle *throttler_bytes;
-    Throttle *throttler_messages;
-
-    /// Specify features supported locally by the endpoint.
-    uint64_t features_supported;
-    /// Specify features any remotes must have to talk to this endpoint.
-    uint64_t features_required;
-
-    Policy()
-      : lossy(false), server(false), standby(false), resetcheck(true),
-	throttler_bytes(NULL),
-	throttler_messages(NULL),
-	features_supported(CEPH_FEATURES_SUPPORTED_DEFAULT),
-	features_required(0) {}
-  private:
-    Policy(bool l, bool s, bool st, bool r, uint64_t req)
-      : lossy(l), server(s), standby(st), resetcheck(r),
-	throttler_bytes(NULL),
-	throttler_messages(NULL),
-	features_supported(CEPH_FEATURES_SUPPORTED_DEFAULT),
-	features_required(req) {}
-
-  public:
-    static Policy stateful_server(uint64_t req) {
-      return Policy(false, true, true, true, req);
-    }
-    static Policy stateless_server(uint64_t req) {
-      return Policy(true, true, false, false, req);
-    }
-    static Policy lossless_peer(uint64_t req) {
-      return Policy(false, false, true, false, req);
-    }
-    static Policy lossless_peer_reuse(uint64_t req) {
-      return Policy(false, false, true, true, req);
-    }
-    static Policy lossy_client(uint64_t req) {
-      return Policy(true, false, false, false, req);
-    }
-    static Policy lossless_client(uint64_t req) {
-      return Policy(false, false, false, true, req);
-    }
-  };
+  using Policy = ceph::net::Policy<Throttle>;
+
+public:
+  // allow unauthenticated connections.  This is needed for
+  // compatibility with pre-nautilus OSDs, which do not authenticate
+  // the heartbeat sessions.
+  bool require_authorizer = true;
+
+protected:
+  // for authentication
+  AuthRegistry auth_registry;
 
+public:
   /**
    * Messenger constructor. Call this from your implementation.
    * Messenger users should construct full implementations directly,
    * or use the create() function.
    */
-  Messenger(CephContext *cct_, entity_name_t w)
-    : trace_endpoint("0.0.0.0", 0, "Messenger"),
-      my_inst(),
-      default_send_priority(CEPH_MSG_PRIO_DEFAULT), started(false),
-      magic(0),
-      socket_priority(-1),
-      cct(cct_),
-      crcflags(get_default_crc_flags(cct->_conf))
-  {
-    my_inst.name = w;
-  }
+  Messenger(CephContext *cct_, entity_name_t w);
   virtual ~Messenger() {}
 
   /**
@@ -164,15 +144,18 @@ public:
    * @param lname logical name of the messenger in this process (e.g., "client")
    * @param nonce nonce value to uniquely identify this instance on the current host
    * @param features bits for the local connection
-   * @param cflags general set of flags to configure transport resources
+   * @param cflags general std::set of flags to configure transport resources
    */
   static Messenger *create(CephContext *cct,
-                           const string &type,
+                           const std::string &type,
                            entity_name_t name,
-			   string lname,
+			   std::string lname,
                            uint64_t nonce,
 			   uint64_t cflags);
 
+  static uint64_t get_random_nonce();
+  static uint64_t get_pid_nonce();
+
   /**
    * create a new messenger
    *
@@ -186,26 +169,21 @@ public:
    * @param cct context
    * @param lname logical name of the messenger in this process (e.g., "client")
    */
-  static Messenger *create_client_messenger(CephContext *cct, string lname);
+  static Messenger *create_client_messenger(CephContext *cct, std::string lname);
 
   /**
    * @defgroup Accessors
    * @{
    */
+  int get_mytype() const { return my_name.type(); }
+
   /**
-   * Retrieve the Messenger's instance.
+   * Retrieve the Messenger's name
    *
-   * @return A const reference to the instance this Messenger
+   * @return A const reference to the name this Messenger
    * currently believes to be its own.
    */
-  const entity_inst_t& get_myinst() { return my_inst; }
-  /**
-   * set messenger's instance
-   */
-  void set_myinst(entity_inst_t i) { my_inst = i; }
-
-  uint32_t get_magic() { return magic; }
-  void set_magic(int _magic) { magic = _magic; }
+  const entity_name_t& get_myname() { return my_name; }
 
   /**
    * Retrieve the Messenger's address.
@@ -213,14 +191,41 @@ public:
    * @return A const reference to the address this Messenger
    * currently believes to be its own.
    */
-  const entity_addr_t& get_myaddr() { return my_inst.addr; }
+  const entity_addrvec_t& get_myaddrs() {
+    return *my_addrs;
+  }
+
+  /**
+   * get legacy addr for myself, suitable for protocol v1
+   *
+   * Note that myaddrs might be a proper addrvec with v1 in it, or it might be an
+   * ANY addr (if i am a pure client).
+   */
+  entity_addr_t get_myaddr_legacy() {
+    return my_addrs->as_legacy_addr();
+  }
+
+
+  /**
+   * std::set messenger's instance
+   */
+  uint32_t get_magic() { return magic; }
+  void set_magic(int _magic) { magic = _magic; }
+
+  void set_auth_client(AuthClient *ac) {
+    auth_client = ac;
+  }
+  void set_auth_server(AuthServer *as) {
+    auth_server = as;
+  }
+
 protected:
   /**
-   * set messenger's address
+   * std::set messenger's address
    */
-  virtual void set_myaddr(const entity_addr_t& a) {
-    my_inst.addr = a;
-    set_endpoint_addr(a, my_inst.name);
+  virtual void set_myaddrs(const entity_addrvec_t& a) {
+    my_addrs = a;
+    set_endpoint_addr(a.front(), my_name);
   }
 public:
   /**
@@ -231,22 +236,16 @@ public:
   }
 
   /**
-   * Retrieve the Messenger's name.
-   *
-   * @return A const reference to the name this Messenger
-   * currently believes to be its own.
-   */
-  const entity_name_t& get_myname() { return my_inst.name; }
-  /**
-   * Set the name of the local entity. The name is reported to others and
+   * set the name of the local entity. The name is reported to others and
    * can be changed while the system is running, but doing so at incorrect
    * times may have bad results.
    *
-   * @param m The name to set.
+   * @param m The name to std::set.
    */
-  void set_myname(const entity_name_t& m) { my_inst.name = m; }
+  void set_myname(const entity_name_t& m) { my_name = m; }
+
   /**
-   * Set the unknown address components for this Messenger.
+   * set the unknown address components for this Messenger.
    * This is useful if the Messenger doesn't know its full address just by
    * binding, but another Messenger on the same interface has already learned
    * its full address. This function does not fill in known address elements,
@@ -254,15 +253,15 @@ public:
    *
    * @param addr The address to use as a template.
    */
-  virtual void set_addr_unknowns(const entity_addr_t &addr) = 0;
+  virtual bool set_addr_unknowns(const entity_addrvec_t &addrs) = 0;
   /**
-   * Set the address for this Messenger. This is useful if the Messenger
+   * set the address for this Messenger. This is useful if the Messenger
    * binds to a specific address but advertises a different address on the
    * the network.
    *
    * @param addr The address to use.
    */
-  virtual void set_addr(const entity_addr_t &addr) = 0;
+  virtual void set_addrs(const entity_addrvec_t &addr) = 0;
   /// Get the default send priority.
   int get_default_send_priority() { return default_send_priority; }
   /**
@@ -276,11 +275,6 @@ public:
    * (0 if the queue is empty)
    */
   virtual double get_dispatch_queue_max_age(utime_t now) = 0;
-  /**
-   * Get the default crc flags for this messenger.
-   * but not yet dispatched.
-   */
-  static int get_default_crc_flags(md_config_t *);
 
   /**
    * @} // Accessors
@@ -291,7 +285,7 @@ public:
    * @{
    */
   /**
-   * Set the cluster protocol in use by this daemon.
+   * set the cluster protocol in use by this daemon.
    * This is an init-time function and cannot be called after calling
    * start() or bind().
    *
@@ -299,7 +293,7 @@ public:
    */
   virtual void set_cluster_protocol(int p) = 0;
   /**
-   * Set a policy which is applied to all peers who do not have a type-specific
+   * set a policy which is applied to all peers who do not have a type-specific
    * Policy.
    * This is an init-time function and cannot be called after calling
    * start() or bind().
@@ -308,7 +302,7 @@ public:
    */
   virtual void set_default_policy(Policy p) = 0;
   /**
-   * Set a policy which is applied to all peers of the given type.
+   * set a policy which is applied to all peers of the given type.
    * This is an init-time function and cannot be called after calling
    * start() or bind().
    *
@@ -317,7 +311,7 @@ public:
    */
   virtual void set_policy(int type, Policy p) = 0;
   /**
-   * Set the Policy associated with a type of peer.
+   * set the Policy associated with a type of peer.
    *
    * This can be called either on initial setup, or after connections
    * are already established.  However, the policies for existing
@@ -335,7 +329,7 @@ public:
    */
   virtual Policy get_default_policy() = 0;
   /**
-   * Set Throttlers applied to all Messages from the given type of peer
+   * set Throttlers applied to all Messages from the given type of peer
    *
    * This is an init-time function and cannot be called after calling
    * start() or bind().
@@ -348,7 +342,7 @@ public:
    */
   virtual void set_policy_throttlers(int type, Throttle *bytes, Throttle *msgs=NULL) = 0;
   /**
-   * Set the default send priority
+   * set the default send priority
    *
    * This is an init-time function and must be called *before* calling
    * start().
@@ -356,11 +350,11 @@ public:
    * @param p The cluster protocol to use. Defined externally.
    */
   void set_default_send_priority(int p) {
-    assert(!started);
+    ceph_assert(!started);
     default_send_priority = p;
   }
   /**
-   * Set the priority(SO_PRIORITY) for all packets to be sent on this socket.
+   * set the priority(SO_PRIORITY) for all packets to be sent on this socket.
    *
    * Linux uses this value to order the networking queues: packets with a higher
    * priority may be processed first depending on the selected device queueing
@@ -387,7 +381,7 @@ public:
    *
    * @param d The Dispatcher to insert into the list.
    */
-  void add_dispatcher_head(Dispatcher *d) { 
+  void add_dispatcher_head(Dispatcher *d) {
     bool first = dispatchers.empty();
     dispatchers.push_front(d);
     if (d->ms_can_fast_dispatch_any())
@@ -402,7 +396,7 @@ public:
    *
    * @param d The Dispatcher to insert into the list.
    */
-  void add_dispatcher_tail(Dispatcher *d) { 
+  void add_dispatcher_tail(Dispatcher *d) {
     bool first = dispatchers.empty();
     dispatchers.push_back(d);
     if (d->ms_can_fast_dispatch_any())
@@ -421,6 +415,9 @@ public:
    * we can be more specific about the failure.
    */
   virtual int bind(const entity_addr_t& bind_addr) = 0;
+
+  virtual int bindv(const entity_addrvec_t& addrs);
+
   /**
    * This function performs a full restart of the Messenger component,
    * whatever that means.  Other entities who connect to this
@@ -430,15 +427,28 @@ public:
    *
    * @param avoid_ports Additional port to avoid binding to.
    */
-  virtual int rebind(const set<int>& avoid_ports) { return -EOPNOTSUPP; }
+  virtual int rebind(const std::set<int>& avoid_ports) { return -EOPNOTSUPP; }
   /**
    * Bind the 'client' Messenger to a specific address.Messenger will bind
    * the address before connect to others when option ms_bind_before_connect
    * is true.
    * @param bind_addr The address to bind to.
    * @return 0 on success, or -1 on error, or -errno if
+   * we can be more specific about the failure.
    */
   virtual int client_bind(const entity_addr_t& bind_addr) = 0;
+
+  /**
+   * reset the 'client' Messenger. Mark all the existing Connections down
+   * and update 'nonce'.
+   */
+  virtual int client_reset() = 0;
+
+
+  virtual bool should_use_msgr2() {
+    return false;
+  }
+
   /**
    * @} // Configuration
    */
@@ -494,7 +504,18 @@ public:
    *
    * @return 0 on success, or -errno on failure.
    */
-  virtual int send_message(Message *m, const entity_inst_t& dest) = 0;
+  virtual int send_to(
+    Message *m,
+    int type,
+    const entity_addrvec_t& addr) = 0;
+  int send_to_mon(
+    Message *m, const entity_addrvec_t& addrs) {
+    return send_to(m, CEPH_ENTITY_TYPE_MON, addrs);
+  }
+  int send_to_mds(
+    Message *m, const entity_addrvec_t& addrs) {
+    return send_to(m, CEPH_ENTITY_TYPE_MDS, addrs);
+  }
 
   /**
    * @} // Messaging
@@ -511,7 +532,26 @@ public:
    *
    * @param dest The entity to get a connection for.
    */
-  virtual ConnectionRef get_connection(const entity_inst_t& dest) = 0;
+  virtual ConnectionRef connect_to(
+    int type, const entity_addrvec_t& dest,
+    bool anon=false, bool not_local_dest=false) = 0;
+  ConnectionRef connect_to_mon(const entity_addrvec_t& dest,
+      bool anon=false, bool not_local_dest=false) {
+	return connect_to(CEPH_ENTITY_TYPE_MON, dest, anon, not_local_dest);
+  }
+  ConnectionRef connect_to_mds(const entity_addrvec_t& dest,
+      bool anon=false, bool not_local_dest=false) {
+	return connect_to(CEPH_ENTITY_TYPE_MDS, dest, anon, not_local_dest);
+  }
+  ConnectionRef connect_to_osd(const entity_addrvec_t& dest,
+      bool anon=false, bool not_local_dest=false) {
+	return connect_to(CEPH_ENTITY_TYPE_OSD, dest, anon, not_local_dest);
+  }
+  ConnectionRef connect_to_mgr(const entity_addrvec_t& dest,
+      bool anon=false, bool not_local_dest=false) {
+	return connect_to(CEPH_ENTITY_TYPE_MGR, dest, anon, not_local_dest);
+  }
+
   /**
    * Get the Connection object associated with ourselves.
    */
@@ -534,6 +574,9 @@ public:
    * @param a The address to mark down.
    */
   virtual void mark_down(const entity_addr_t& a) = 0;
+  virtual void mark_down_addrs(const entity_addrvec_t& a) {
+    mark_down(a.legacy_addr());
+  }
   /**
    * Mark all the existing Connections down. This is equivalent
    * to iterating over all Connections and calling mark_down()
@@ -558,11 +601,53 @@ protected:
   /**
    * @} // Subclass Interfacing
    */
+public:
+#ifdef CEPH_USE_SIGPIPE_BLOCKER
+  /**
+   * We need to disable SIGPIPE on all platforms, and if they
+   * don't give us a better mechanism (read: are on Solaris) that
+   * means blocking the signal whenever we do a send or sendmsg...
+   * That means any implementations must invoke MSGR_SIGPIPE_STOPPER in-scope
+   * whenever doing so. On most systems that's blank, but on systems where
+   * it's needed we construct an RAII object to plug and un-plug the SIGPIPE.
+   * See http://www.microhowto.info/howto/ignore_sigpipe_without_affecting_other_threads_in_a_process.html
+   */
+  struct sigpipe_stopper {
+    bool blocked;
+    sigset_t existing_mask;
+    sigset_t pipe_mask;
+    sigpipe_stopper() {
+      sigemptyset(&pipe_mask);
+      sigaddset(&pipe_mask, SIGPIPE);
+      sigset_t signals;
+      sigemptyset(&signals);
+      sigpending(&signals);
+      if (sigismember(&signals, SIGPIPE)) {
+	blocked = false;
+      } else {
+	blocked = true;
+	int r = pthread_sigmask(SIG_BLOCK, &pipe_mask, &existing_mask);
+	ceph_assert(r == 0);
+      }
+    }
+    ~sigpipe_stopper() {
+      if (blocked) {
+	struct timespec nowait{0};
+	int r = sigtimedwait(&pipe_mask, 0, &nowait);
+	ceph_assert(r == EAGAIN || r == 0);
+	r = pthread_sigmask(SIG_SETMASK, &existing_mask, 0);
+	ceph_assert(r == 0);
+      }
+    }
+  };
+#  define MSGR_SIGPIPE_STOPPER Messenger::sigpipe_stopper stopper();
+#else
+#  define MSGR_SIGPIPE_STOPPER
+#endif
   /**
    * @defgroup Dispatcher Interfacing
    * @{
    */
-public:
   /**
    * Determine whether a message can be fast-dispatched. We will
    * query each Dispatcher in sequence to determine if they are
@@ -570,11 +655,9 @@ public:
    *
    * @param m The Message we are testing.
    */
-  bool ms_can_fast_dispatch(const Message *m) {
-    for (list<Dispatcher*>::iterator p = fast_dispatchers.begin();
-	 p != fast_dispatchers.end();
-	 ++p) {
-      if ((*p)->ms_can_fast_dispatch(m))
+  bool ms_can_fast_dispatch(const cref_t<Message>& m) {
+    for (const auto &dispatcher : fast_dispatchers) {
+      if (dispatcher->ms_can_fast_dispatch2(m))
 	return true;
     }
     return false;
@@ -583,52 +666,49 @@ public:
   /**
    * Deliver a single Message via "fast dispatch".
    *
-   * @param m The Message we are fast dispatching. We take ownership
-   * of one reference to it.
+   * @param m The Message we are fast dispatching.
    * If none of our Dispatchers can handle it, ceph_abort().
    */
-  void ms_fast_dispatch(Message *m) {
+  void ms_fast_dispatch(const ref_t<Message> &m) {
     m->set_dispatch_stamp(ceph_clock_now());
-    for (list<Dispatcher*>::iterator p = fast_dispatchers.begin();
-	 p != fast_dispatchers.end();
-	 ++p) {
-      if ((*p)->ms_can_fast_dispatch(m)) {
-	(*p)->ms_fast_dispatch(m);
+    for (const auto &dispatcher : fast_dispatchers) {
+      if (dispatcher->ms_can_fast_dispatch2(m)) {
+	dispatcher->ms_fast_dispatch2(m);
 	return;
       }
     }
     ceph_abort();
   }
+  void ms_fast_dispatch(Message *m) {
+    return ms_fast_dispatch(ref_t<Message>(m, false)); /* consume ref */
+  }
   /**
    *
    */
-  void ms_fast_preprocess(Message *m) {
-    for (list<Dispatcher*>::iterator p = fast_dispatchers.begin();
-	 p != fast_dispatchers.end();
-	 ++p) {
-      (*p)->ms_fast_preprocess(m);
+  void ms_fast_preprocess(const ref_t<Message> &m) {
+    for (const auto &dispatcher : fast_dispatchers) {
+      dispatcher->ms_fast_preprocess2(m);
     }
   }
   /**
    *  Deliver a single Message. Send it to each Dispatcher
    *  in sequence until one of them handles it.
-   *  If none of our Dispatchers can handle it, assert(0).
+   *  If none of our Dispatchers can handle it, ceph_abort().
    *
-   *  @param m The Message to deliver. We take ownership of
-   *  one reference to it.
+   *  @param m The Message to deliver.
    */
-  void ms_deliver_dispatch(Message *m) {
+  void ms_deliver_dispatch(const ref_t<Message> &m) {
     m->set_dispatch_stamp(ceph_clock_now());
-    for (list<Dispatcher*>::iterator p = dispatchers.begin();
-	 p != dispatchers.end();
-	 ++p) {
-      if ((*p)->ms_dispatch(m))
+    for (const auto &dispatcher : dispatchers) {
+      if (dispatcher->ms_dispatch2(m))
 	return;
     }
     lsubdout(cct, ms, 0) << "ms_deliver_dispatch: unhandled message " << m << " " << *m << " from "
 			 << m->get_source_inst() << dendl;
-    assert(!cct->_conf->ms_die_on_unhandled_msg);
-    m->put();
+    ceph_assert(!cct->_conf->ms_die_on_unhandled_msg);
+  }
+  void ms_deliver_dispatch(Message *m) {
+    return ms_deliver_dispatch(ref_t<Message>(m, false)); /* consume ref */
   }
   /**
    * Notify each Dispatcher of a new Connection. Call
@@ -638,10 +718,9 @@ public:
    * @param con Pointer to the new Connection.
    */
   void ms_deliver_handle_connect(Connection *con) {
-    for (list<Dispatcher*>::iterator p = dispatchers.begin();
-	 p != dispatchers.end();
-	 ++p)
-      (*p)->ms_handle_connect(con);
+    for (const auto& dispatcher : dispatchers) {
+      dispatcher->ms_handle_connect(con);
+    }
   }
 
   /**
@@ -652,23 +731,21 @@ public:
    * @param con Pointer to the new Connection.
    */
   void ms_deliver_handle_fast_connect(Connection *con) {
-    for (list<Dispatcher*>::iterator p = fast_dispatchers.begin();
-         p != fast_dispatchers.end();
-         ++p)
-      (*p)->ms_handle_fast_connect(con);
+    for (const auto& dispatcher : fast_dispatchers) {
+      dispatcher->ms_handle_fast_connect(con);
+    }
   }
 
   /**
-   * Notify each Dispatcher of a new incomming Connection. Call
+   * Notify each Dispatcher of a new incoming Connection. Call
    * this function whenever a new Connection is accepted.
    *
    * @param con Pointer to the new Connection.
    */
   void ms_deliver_handle_accept(Connection *con) {
-    for (list<Dispatcher*>::iterator p = dispatchers.begin();
-	 p != dispatchers.end();
-	 ++p)
-      (*p)->ms_handle_accept(con);
+    for (const auto& dispatcher : dispatchers) {
+      dispatcher->ms_handle_accept(con);
+    }
   }
 
   /**
@@ -678,10 +755,9 @@ public:
    * @param con Pointer to the new Connection.
    */
   void ms_deliver_handle_fast_accept(Connection *con) {
-    for (list<Dispatcher*>::iterator p = fast_dispatchers.begin();
-         p != fast_dispatchers.end();
-         ++p)
-      (*p)->ms_handle_fast_accept(con);
+    for (const auto& dispatcher : fast_dispatchers) {
+      dispatcher->ms_handle_fast_accept(con);
+    }
   }
 
   /**
@@ -692,10 +768,8 @@ public:
    * @param con Pointer to the broken Connection.
    */
   void ms_deliver_handle_reset(Connection *con) {
-    for (list<Dispatcher*>::iterator p = dispatchers.begin();
-	 p != dispatchers.end();
-	 ++p) {
-      if ((*p)->ms_handle_reset(con))
+    for (const auto& dispatcher : dispatchers) {
+      if (dispatcher->ms_handle_reset(con))
 	return;
     }
   }
@@ -707,10 +781,9 @@ public:
    * @param con Pointer to the broken Connection.
    */
   void ms_deliver_handle_remote_reset(Connection *con) {
-    for (list<Dispatcher*>::iterator p = dispatchers.begin();
-	 p != dispatchers.end();
-	 ++p)
-      (*p)->ms_handle_remote_reset(con);
+    for (const auto& dispatcher : dispatchers) {
+      dispatcher->ms_handle_remote_reset(con);
+    }
   }
 
   /**
@@ -722,55 +795,14 @@ public:
    * @param con Pointer to the broken Connection.
    */
   void ms_deliver_handle_refused(Connection *con) {
-    for (list<Dispatcher*>::iterator p = dispatchers.begin();
-         p != dispatchers.end();
-         ++p) {
-      if ((*p)->ms_handle_refused(con))
+    for (const auto& dispatcher : dispatchers) {
+      if (dispatcher->ms_handle_refused(con))
         return;
     }
   }
 
-  /**
-   * Get the AuthAuthorizer for a new outgoing Connection.
-   *
-   * @param peer_type The peer type for the new Connection
-   * @param force_new True if we want to wait for new keys, false otherwise.
-   * @return A pointer to the AuthAuthorizer, if we have one; NULL otherwise
-   */
-  AuthAuthorizer *ms_deliver_get_authorizer(int peer_type, bool force_new) {
-    AuthAuthorizer *a = 0;
-    for (list<Dispatcher*>::iterator p = dispatchers.begin();
-	 p != dispatchers.end();
-	 ++p) {
-      if ((*p)->ms_get_authorizer(peer_type, &a, force_new))
-	return a;
-    }
-    return NULL;
-  }
-  /**
-   * Verify that the authorizer on a new incoming Connection is correct.
-   *
-   * @param con The new incoming Connection
-   * @param peer_type The type of the endpoint on the new Connection
-   * @param protocol The ID of the protocol in use (at time of writing, cephx or none)
-   * @param authorizer The authorization string supplied by the remote
-   * @param authorizer_reply Output param: The string we should send back to
-   * the remote to authorize ourselves. Only filled in if isvalid
-   * @param isvalid Output param: True if authorizer is valid, false otherwise
-   *
-   * @return True if we were able to prove or disprove correctness of
-   * authorizer, false otherwise.
-   */
-  bool ms_deliver_verify_authorizer(Connection *con, int peer_type,
-				    int protocol, bufferlist& authorizer, bufferlist& authorizer_reply,
-				    bool& isvalid, CryptoKey& session_key) {
-    for (list<Dispatcher*>::iterator p = dispatchers.begin();
-	 p != dispatchers.end();
-	 ++p) {
-      if ((*p)->ms_verify_authorizer(con, peer_type, protocol, authorizer, authorizer_reply, isvalid, session_key))
-	return true;
-    }
-    return false;
+  void set_require_authorizer(bool b) {
+    require_authorizer = b;
   }
 
   /**
diff --git a/src/msg/Policy.h b/src/msg/Policy.h
new file mode 100644
index 00000000000..10a426f2f46
--- /dev/null
+++ b/src/msg/Policy.h
@@ -0,0 +1,129 @@
+// -*- mode:C++; tab-width:8; c-basic-offset:2; indent-tabs-mode:t -*-
+// vim: ts=8 sw=2 smarttab
+
+#pragma once
+
+#include "include/ceph_features.h"
+
+namespace ceph::net {
+
+using peer_type_t = int;
+
+/**
+ * A Policy describes the rules of a Connection. Is there a limit on how
+ * much data this Connection can have locally? When the underlying connection
+ * experiences an error, does the Connection disappear? Can this Messenger
+ * re-establish the underlying connection?
+ */
+template<class ThrottleType>
+struct Policy {
+  /// If true, the Connection is tossed out on errors.
+  bool lossy;
+  /// If true, the underlying connection can't be re-established from this end.
+  bool server;
+  /// If true, we will standby when idle
+  bool standby;
+  /// If true, we will try to detect session resets
+  bool resetcheck;
+
+  /// Server: register lossy client connections.
+  bool register_lossy_clients = true;
+  // The net result of this is that a given client can only have one
+  // open connection with the server.  If a new connection is made,
+  // the old (registered) one is closed by the messenger during the accept
+  // process.
+  
+  /**
+   *  The throttler is used to limit how much data is held by Messages from
+   *  the associated Connection(s). When reading in a new Message, the Messenger
+   *  will call throttler->throttle() for the size of the new Message.
+   */
+  ThrottleType* throttler_bytes;
+  ThrottleType* throttler_messages;
+  
+  /// Specify features supported locally by the endpoint.
+  uint64_t features_supported;
+  /// Specify features any remotes must have to talk to this endpoint.
+  uint64_t features_required;
+  
+  Policy()
+    : lossy(false), server(false), standby(false), resetcheck(true),
+      throttler_bytes(NULL),
+      throttler_messages(NULL),
+      features_supported(CEPH_FEATURES_SUPPORTED_DEFAULT),
+      features_required(0) {}
+private:
+  Policy(bool l, bool s, bool st, bool r, bool rlc, uint64_t req)
+    : lossy(l), server(s), standby(st), resetcheck(r),
+      register_lossy_clients(rlc),
+      throttler_bytes(NULL),
+      throttler_messages(NULL),
+      features_supported(CEPH_FEATURES_SUPPORTED_DEFAULT),
+      features_required(req) {}
+  
+public:
+  static Policy stateful_server(uint64_t req) {
+    return Policy(false, true, true, true, true, req);
+  }
+  static Policy stateless_registered_server(uint64_t req) {
+    return Policy(true, true, false, false, true, req);
+  }
+  static Policy stateless_server(uint64_t req) {
+    return Policy(true, true, false, false, false, req);
+  }
+  static Policy lossless_peer(uint64_t req) {
+    return Policy(false, false, true, false, true, req);
+  }
+  static Policy lossless_peer_reuse(uint64_t req) {
+    return Policy(false, false, true, true, true, req);
+  }
+  static Policy lossy_client(uint64_t req) {
+    return Policy(true, false, false, false, true, req);
+  }
+  static Policy lossless_client(uint64_t req) {
+    return Policy(false, false, false, true, true, req);
+  }
+};
+
+template<class ThrottleType>
+class PolicySet {
+  using policy_t = Policy<ThrottleType> ;
+  /// the default Policy we use for Pipes
+  policy_t default_policy;
+  /// map specifying different Policies for specific peer types
+  std::map<int, policy_t> policy_map; // entity_name_t::type -> Policy
+
+public:
+  const policy_t& get(peer_type_t peer_type) const {
+    if (auto found = policy_map.find(peer_type); found != policy_map.end()) {
+      return found->second;
+    } else {
+      return default_policy;
+    }
+  }
+  policy_t& get(peer_type_t peer_type) {
+    if (auto found = policy_map.find(peer_type); found != policy_map.end()) {
+      return found->second;
+    } else {
+      return default_policy;
+    }
+  }
+  void set(peer_type_t peer_type, const policy_t& p) {
+    policy_map[peer_type] = p;
+  }
+  const policy_t& get_default() const {
+    return default_policy;
+  }
+  void set_default(const policy_t& p) {
+    default_policy = p;
+  }
+  void set_throttlers(peer_type_t peer_type,
+                      ThrottleType* byte_throttle,
+                      ThrottleType* msg_throttle) {
+    auto& policy = get(peer_type);
+    policy.throttler_bytes = byte_throttle;
+    policy.throttler_messages = msg_throttle;
+  }
+};
+
+}
diff --git a/src/msg/QueueStrategy.cc b/src/msg/QueueStrategy.cc
index e41ab79fdeb..85b0a11e602 100644
--- a/src/msg/QueueStrategy.cc
+++ b/src/msg/QueueStrategy.cc
@@ -15,11 +15,9 @@
 #include "QueueStrategy.h"
 #define dout_subsys ceph_subsys_ms
 #include "common/debug.h"
-#include "common/backport14.h"
 
 QueueStrategy::QueueStrategy(int _n_threads)
-  : lock("QueueStrategy::lock"),
-    n_threads(_n_threads),
+  : n_threads(_n_threads),
     stop(false),
     mqueue(),
     disp_threads()
@@ -32,39 +30,36 @@ void QueueStrategy::ds_dispatch(Message *m) {
     msgr->ms_fast_dispatch(m);
     return;
   }
-  lock.Lock();
+  std::lock_guard l{lock};
   mqueue.push_back(*m);
   if (disp_threads.size()) {
     if (! disp_threads.empty()) {
       QSThread *thrd = &disp_threads.front();
       disp_threads.pop_front();
-      thrd->cond.Signal();
+      thrd->cond.notify_all();
     }
   }
-  lock.Unlock();
 }
 
 void QueueStrategy::entry(QSThread *thrd)
 {
-  Message *m = NULL;
   for (;;) {
-    lock.Lock();
+    ref_t<Message> m;
+    std::unique_lock l{lock};
     for (;;) {
       if (! mqueue.empty()) {
-	m = &(mqueue.front());
+	m = ref_t<Message>(&mqueue.front(), false);
 	mqueue.pop_front();
 	break;
       }
-      m = NULL;
       if (stop)
 	break;
       disp_threads.push_front(*thrd);
-      thrd->cond.Wait(lock);
+      thrd->cond.wait(l);
     }
-    lock.Unlock();
+    l.unlock();
     if (stop) {
 	if (!m) break;
-	m->put();
 	continue;
     }
     get_messenger()->ms_deliver_dispatch(m);
@@ -74,42 +69,39 @@ void QueueStrategy::entry(QSThread *thrd)
 void QueueStrategy::shutdown()
 {
   QSThread *thrd;
-  lock.Lock();
+  std::lock_guard l{lock};
   stop = true;
   while (disp_threads.size()) {
     thrd = &(disp_threads.front());
     disp_threads.pop_front();
-    thrd->cond.Signal();
+    thrd->cond.notify_all();
   }
-  lock.Unlock();
 }
 
 void QueueStrategy::wait()
 {
-  lock.Lock();
-  assert(stop);
+  std::unique_lock l{lock};
+  ceph_assert(stop);
   for (auto& thread : threads) {
-    lock.Unlock();
+    l.unlock();
 
     // join outside of lock
     thread->join();
 
-    lock.Lock();
+    l.lock();
   }
-  lock.Unlock();
 }
 
 void QueueStrategy::start()
 {
-  assert(!stop);
-  lock.Lock();
+  ceph_assert(!stop);
+  std::lock_guard l{lock};
   threads.reserve(n_threads);
   for (int ix = 0; ix < n_threads; ++ix) {
-    string thread_name = "ms_xio_qs_";
+    string thread_name = "ms_qs_";
     thread_name.append(std::to_string(ix));
-    auto thrd = ceph::make_unique<QSThread>(this);
+    auto thrd = std::make_unique<QSThread>(this);
     thrd->create(thread_name.c_str());
     threads.emplace_back(std::move(thrd));
   }
-  lock.Unlock();
 }
diff --git a/src/msg/QueueStrategy.h b/src/msg/QueueStrategy.h
index a531cd77743..b7f6df85d7c 100644
--- a/src/msg/QueueStrategy.h
+++ b/src/msg/QueueStrategy.h
@@ -25,7 +25,7 @@
 namespace bi = boost::intrusive;
 
 class QueueStrategy : public DispatchStrategy {
-  Mutex lock;
+  ceph::mutex lock = ceph::make_mutex("QueueStrategy::lock");
   const int n_threads;
   bool stop;
 
@@ -35,8 +35,8 @@ class QueueStrategy : public DispatchStrategy {
   public:
     bi::list_member_hook<> thread_q;
     QueueStrategy *dq;
-    Cond cond;
-    explicit QSThread(QueueStrategy *dq) : thread_q(), dq(dq), cond() {}
+    ceph::condition_variable cond;
+    explicit QSThread(QueueStrategy *dq) : thread_q(), dq(dq) {}
     void* entry() {
       dq->entry(this);
       return NULL;
diff --git a/src/msg/SimplePolicyMessenger.h b/src/msg/SimplePolicyMessenger.h
index 466eb1d34c4..3b25983a62b 100644
--- a/src/msg/SimplePolicyMessenger.h
+++ b/src/msg/SimplePolicyMessenger.h
@@ -17,23 +17,21 @@
 #define SIMPLE_POLICY_MESSENGER_H
 
 #include "Messenger.h"
+#include "Policy.h"
 
 class SimplePolicyMessenger : public Messenger
 {
 private:
   /// lock protecting policy
-  Mutex policy_lock;
-  /// the default Policy we use for Pipes
-  Policy default_policy;
-  /// map specifying different Policies for specific peer types
-  map<int, Policy> policy_map; // entity_name_t::type -> Policy
+  ceph::mutex policy_lock =
+    ceph::make_mutex("SimplePolicyMessenger::policy_lock");
+  // entity_name_t::type -> Policy
+  ceph::net::PolicySet<Throttle> policy_set;
 
 public:
 
-  SimplePolicyMessenger(CephContext *cct, entity_name_t name,
-			string mname, uint64_t _nonce)
-    : Messenger(cct, name),
-      policy_lock("SimplePolicyMessenger::policy_lock")
+  SimplePolicyMessenger(CephContext *cct, entity_name_t name)
+    : Messenger(cct, name)
     {
     }
 
@@ -44,18 +42,13 @@ public:
    * @return A const Policy reference.
    */
   Policy get_policy(int t) override {
-    Mutex::Locker l(policy_lock);
-    map<int, Policy>::iterator iter =
-      policy_map.find(t);
-    if (iter != policy_map.end())
-      return iter->second;
-    else
-      return default_policy;
+    std::lock_guard l{policy_lock};
+    return policy_set.get(t);
   }
 
   Policy get_default_policy() override {
-    Mutex::Locker l(policy_lock);
-    return default_policy;
+    std::lock_guard l{policy_lock};
+    return policy_set.get_default();
   }
 
   /**
@@ -67,8 +60,8 @@ public:
    * @param p The Policy to apply.
    */
   void set_default_policy(Policy p) override {
-    Mutex::Locker l(policy_lock);
-    default_policy = p;
+    std::lock_guard l{policy_lock};
+    policy_set.set_default(p);
   }
   /**
    * Set a policy which is applied to all peers of the given type.
@@ -79,8 +72,8 @@ public:
    * @param p The policy to apply.
    */
   void set_policy(int type, Policy p) override {
-    Mutex::Locker l(policy_lock);
-    policy_map[type] = p;
+    std::lock_guard l{policy_lock};
+    policy_set.set(type, p);
   }
 
   /**
@@ -90,23 +83,15 @@ public:
    * start() or bind().
    *
    * @param type The peer type this Throttler will apply to.
-   * @param t The Throttler to apply. SimpleMessenger does not take
+   * @param t The Throttler to apply. The messenger does not take
    * ownership of this pointer, but you must not destroy it before
-   * you destroy SimpleMessenger.
+   * you destroy messenger.
    */
   void set_policy_throttlers(int type,
-			     Throttle *byte_throttle,
-			     Throttle *msg_throttle) override {
-    Mutex::Locker l(policy_lock);
-    map<int, Policy>::iterator iter =
-      policy_map.find(type);
-    if (iter != policy_map.end()) {
-      iter->second.throttler_bytes = byte_throttle;
-      iter->second.throttler_messages = msg_throttle;
-    } else {
-      default_policy.throttler_bytes = byte_throttle;
-      default_policy.throttler_messages = msg_throttle;
-    }
+			     Throttle* byte_throttle,
+			     Throttle* msg_throttle) override {
+    std::lock_guard l{policy_lock};
+    policy_set.set_throttlers(type, byte_throttle, msg_throttle);
   }
 
 }; /* SimplePolicyMessenger */
diff --git a/src/msg/async/AsyncConnection.cc b/src/msg/async/AsyncConnection.cc
index e7895ddca75..82ef1389734 100644
--- a/src/msg/async/AsyncConnection.cc
+++ b/src/msg/async/AsyncConnection.cc
@@ -1,4 +1,4 @@
-// -*- mode:C++; tab-width:8; c-basic-offset:2; indent-tabs-mode:t -*- 
+// -*- mode:C++; tab-width:8; c-basic-offset:2; indent-tabs-mode:t -*-
 // vim: ts=8 sw=2 smarttab
 /*
  * Ceph - scalable distributed file system
@@ -17,26 +17,32 @@
 #include <unistd.h>
 
 #include "include/Context.h"
+#include "include/random.h"
 #include "common/errno.h"
 #include "AsyncMessenger.h"
 #include "AsyncConnection.h"
 
+#include "ProtocolV1.h"
+#include "ProtocolV2.h"
+
 #include "messages/MOSDOp.h"
 #include "messages/MOSDOpReply.h"
 #include "common/EventTrace.h"
 
 // Constant to limit starting sequence number to 2^31.  Nothing special about it, just a big number.  PLR
-#define SEQ_MASK  0x7fffffff 
+#define SEQ_MASK  0x7fffffff
 
 #define dout_subsys ceph_subsys_ms
 #undef dout_prefix
 #define dout_prefix _conn_prefix(_dout)
 ostream& AsyncConnection::_conn_prefix(std::ostream *_dout) {
-  return *_dout << "-- " << async_msgr->get_myinst().addr << " >> " << peer_addr << " conn(" << this
+  return *_dout << "-- " << async_msgr->get_myaddrs() << " >> "
+		<< *peer_addrs << " conn(" << this
+		<< (msgr2 ? " msgr2=" : " legacy=")
+		<< protocol.get()
+		<< " " << ceph_con_mode_name(protocol->auth_meta->con_mode)
                 << " :" << port
                 << " s=" << get_state_name(state)
-                << " pgs=" << peer_global_seq
-                << " cs=" << connect_seq
                 << " l=" << policy.lossy
                 << ").";
 }
@@ -44,15 +50,14 @@ ostream& AsyncConnection::_conn_prefix(std::ostream *_dout) {
 // Notes:
 // 1. Don't dispatch any event when closed! It may cause AsyncConnection alive even if AsyncMessenger dead
 
-const int AsyncConnection::TCP_PREFETCH_MIN_SIZE = 512;
-const int ASYNC_COALESCE_THRESHOLD = 256;
+const uint32_t AsyncConnection::TCP_PREFETCH_MIN_SIZE = 512;
 
 class C_time_wakeup : public EventCallback {
   AsyncConnectionRef conn;
 
  public:
   explicit C_time_wakeup(AsyncConnectionRef c): conn(c) {}
-  void do_request(int fd_or_id) override {
+  void do_request(uint64_t fd_or_id) override {
     conn->wakeup_from(fd_or_id);
   }
 };
@@ -62,7 +67,7 @@ class C_handle_read : public EventCallback {
 
  public:
   explicit C_handle_read(AsyncConnectionRef c): conn(c) {}
-  void do_request(int fd_or_id) override {
+  void do_request(uint64_t fd_or_id) override {
     conn->process();
   }
 };
@@ -72,16 +77,24 @@ class C_handle_write : public EventCallback {
 
  public:
   explicit C_handle_write(AsyncConnectionRef c): conn(c) {}
-  void do_request(int fd) override {
+  void do_request(uint64_t fd) override {
     conn->handle_write();
   }
 };
 
+class C_handle_write_callback : public EventCallback {
+  AsyncConnectionRef conn;
+
+public:
+  explicit C_handle_write_callback(AsyncConnectionRef c) : conn(c) {}
+  void do_request(uint64_t fd) override { conn->handle_write_callback(); }
+};
+
 class C_clean_handler : public EventCallback {
   AsyncConnectionRef conn;
  public:
   explicit C_clean_handler(AsyncConnectionRef c): conn(c) {}
-  void do_request(int id) override {
+  void do_request(uint64_t id) override {
     conn->cleanup();
     delete this;
   }
@@ -92,140 +105,93 @@ class C_tick_wakeup : public EventCallback {
 
  public:
   explicit C_tick_wakeup(AsyncConnectionRef c): conn(c) {}
-  void do_request(int fd_or_id) override {
+  void do_request(uint64_t fd_or_id) override {
     conn->tick(fd_or_id);
   }
 };
 
-static void alloc_aligned_buffer(bufferlist& data, unsigned len, unsigned off)
-{
-  // create a buffer to read into that matches the data alignment
-  unsigned left = len;
-  if (off & ~CEPH_PAGE_MASK) {
-    // head
-    unsigned head = 0;
-    head = MIN(CEPH_PAGE_SIZE - (off & ~CEPH_PAGE_MASK), left);
-    data.push_back(buffer::create(head));
-    left -= head;
-  }
-  unsigned middle = left & CEPH_PAGE_MASK;
-  if (middle > 0) {
-    data.push_back(buffer::create_page_aligned(middle));
-    left -= middle;
-  }
-  if (left) {
-    data.push_back(buffer::create(left));
-  }
-}
 
 AsyncConnection::AsyncConnection(CephContext *cct, AsyncMessenger *m, DispatchQueue *q,
-                                 Worker *w)
-  : Connection(cct, m), delay_state(NULL), async_msgr(m), conn_id(q->get_id()),
-    logger(w->get_perf_counter()), global_seq(0), connect_seq(0), peer_global_seq(0),
-    state(STATE_NONE), state_after_send(STATE_NONE), port(-1),
-    dispatch_queue(q), can_write(WriteStatus::NOWRITE),
-    keepalive(false), recv_buf(NULL),
-    recv_max_prefetch(MAX(msgr->cct->_conf->ms_tcp_prefetch_max_size, TCP_PREFETCH_MIN_SIZE)),
+                                 Worker *w, bool m2, bool local)
+  : Connection(cct, m),
+    delay_state(NULL), async_msgr(m), conn_id(q->get_id()),
+    logger(w->get_perf_counter()),
+    state(STATE_NONE), port(-1),
+    dispatch_queue(q), recv_buf(NULL),
+    recv_max_prefetch(std::max<int64_t>(msgr->cct->_conf->ms_tcp_prefetch_max_size, TCP_PREFETCH_MIN_SIZE)),
     recv_start(0), recv_end(0),
     last_active(ceph::coarse_mono_clock::now()),
-    inactive_timeout_us(cct->_conf->ms_tcp_read_timeout*1000*1000),
-    got_bad_auth(false), authorizer(NULL), replacing(false),
-    is_reset_from_peer(false), once_ready(false), state_buffer(NULL), state_offset(0),
-    worker(w), center(&w->center)
+    connect_timeout_us(cct->_conf->ms_connection_ready_timeout*1000*1000),
+    inactive_timeout_us(cct->_conf->ms_connection_idle_timeout*1000*1000),
+    msgr2(m2), state_offset(0),
+    worker(w), center(&w->center),read_buffer(nullptr)
 {
+#ifdef UNIT_TESTS_BUILT
+  this->interceptor = m->interceptor;
+#endif
   read_handler = new C_handle_read(this);
   write_handler = new C_handle_write(this);
+  write_callback_handler = new C_handle_write_callback(this);
   wakeup_handler = new C_time_wakeup(this);
   tick_handler = new C_tick_wakeup(this);
-  memset(msgvec, 0, sizeof(msgvec));
   // double recv_max_prefetch see "read_until"
   recv_buf = new char[2*recv_max_prefetch];
-  state_buffer = new char[4096];
+  if (local) {
+    protocol = std::unique_ptr<Protocol>(new LoopbackProtocolV1(this));
+  } else if (m2) {
+    protocol = std::unique_ptr<Protocol>(new ProtocolV2(this));
+  } else {
+    protocol = std::unique_ptr<Protocol>(new ProtocolV1(this));
+  }
   logger->inc(l_msgr_created_connections);
 }
 
 AsyncConnection::~AsyncConnection()
 {
-  assert(out_q.empty());
-  assert(sent.empty());
-  delete authorizer;
   if (recv_buf)
     delete[] recv_buf;
-  if (state_buffer)
-    delete[] state_buffer;
-  assert(!delay_state);
+  ceph_assert(!delay_state);
 }
 
-void AsyncConnection::maybe_start_delay_thread()
+int AsyncConnection::get_con_mode() const
 {
-  if (!delay_state) {
-    auto pos = async_msgr->cct->_conf->get_val<std::string>("ms_inject_delay_type").find(ceph_entity_type_name(peer_type));
-    if (pos != string::npos) {
-      ldout(msgr->cct, 1) << __func__ << " setting up a delay queue" << dendl;
-      delay_state = new DelayedDelivery(async_msgr, center, dispatch_queue, conn_id);
-    }
-  }
+  return protocol->get_con_mode();
 }
 
-/* return -1 means `fd` occurs error or closed, it should be closed
- * return 0 means EAGAIN or EINTR */
-ssize_t AsyncConnection::read_bulk(char *buf, unsigned len)
+bool AsyncConnection::is_msgr2() const
 {
-  ssize_t nread;
- again:
-  nread = cs.read(buf, len);
-  if (nread < 0) {
-    if (nread == -EAGAIN) {
-      nread = 0;
-    } else if (nread == -EINTR) {
-      goto again;
-    } else {
-      ldout(async_msgr->cct, 1) << __func__ << " reading from fd=" << cs.fd()
-                          << " : "<< strerror(nread) << dendl;
-      return -1;
-    }
-  } else if (nread == 0) {
-    ldout(async_msgr->cct, 1) << __func__ << " peer close file descriptor "
-                              << cs.fd() << dendl;
-    return -1;
-  }
-  return nread;
+  return protocol->proto_type == 2;
 }
 
-// return the remaining bytes, it may larger than the length of ptr
-// else return < 0 means error
-ssize_t AsyncConnection::_try_send(bool more)
+void AsyncConnection::maybe_start_delay_thread()
 {
-  if (async_msgr->cct->_conf->ms_inject_socket_failures && cs) {
-    if (rand() % async_msgr->cct->_conf->ms_inject_socket_failures == 0) {
-      ldout(async_msgr->cct, 0) << __func__ << " injecting socket failure" << dendl;
-      cs.shutdown();
-    }
-  }
-
-  assert(center->in_thread());
-  ssize_t r = cs.send(outcoming_bl, more);
-  if (r < 0) {
-    ldout(async_msgr->cct, 1) << __func__ << " send error: " << cpp_strerror(r) << dendl;
-    return r;
+  if (!delay_state) {
+    async_msgr->cct->_conf.with_val<std::string>(
+      "ms_inject_delay_type",
+      [this](const string& s) {
+	if (s.find(ceph_entity_type_name(peer_type)) != string::npos) {
+	  ldout(msgr->cct, 1) << __func__ << " setting up a delay queue"
+			      << dendl;
+	  delay_state = new DelayedDelivery(async_msgr, center, dispatch_queue,
+					    conn_id);
+	}
+      });
   }
+}
 
-  ldout(async_msgr->cct, 10) << __func__ << " sent bytes " << r
-                             << " remaining bytes " << outcoming_bl.length() << dendl;
 
-  if (!open_write && is_queued()) {
-    center->create_file_event(cs.fd(), EVENT_WRITABLE, write_handler);
-    open_write = true;
+ssize_t AsyncConnection::read(unsigned len, char *buffer,
+                              std::function<void(char *, ssize_t)> callback) {
+  ldout(async_msgr->cct, 20) << __func__
+                             << (pendingReadLen ? " continue" : " start")
+                             << " len=" << len << dendl;
+  ssize_t r = read_until(len, buffer);
+  if (r > 0) {
+    readCallback = callback;
+    pendingReadLen = len;
+    read_buffer = buffer;
   }
-
-  if (open_write && !is_queued()) {
-    center->delete_file_event(cs.fd(), EVENT_WRITABLE);
-    open_write = false;
-    if (state_after_send != STATE_NONE)
-      center->dispatch_event_external(read_handler);
-  }
-
-  return outcoming_bl.length();
+  return r;
 }
 
 // Because this func will be called multi times to populate
@@ -252,7 +218,7 @@ ssize_t AsyncConnection::read_until(unsigned len, char *p)
   ssize_t r = 0;
   uint64_t left = len - state_offset;
   if (recv_end > recv_start) {
-    uint64_t to_read = MIN(recv_end - recv_start, left);
+    uint64_t to_read = std::min<uint64_t>(recv_end - recv_start, left);
     memcpy(p, recv_buf+recv_start, to_read);
     recv_start += to_read;
     left -= to_read;
@@ -267,7 +233,7 @@ ssize_t AsyncConnection::read_until(unsigned len, char *p)
 
   recv_end = recv_start = 0;
   /* nothing left in the prefetch buffer */
-  if (len > recv_max_prefetch) {
+  if (left > (uint64_t)recv_max_prefetch) {
     /* this was a large read, we don't prefetch for these */
     do {
       r = read_bulk(p+state_offset, left);
@@ -309,1594 +275,263 @@ ssize_t AsyncConnection::read_until(unsigned len, char *p)
   return len - state_offset;
 }
 
-void AsyncConnection::inject_delay() {
-  if (async_msgr->cct->_conf->ms_inject_internal_delays) {
-    ldout(async_msgr->cct, 10) << __func__ << " sleep for " << 
-      async_msgr->cct->_conf->ms_inject_internal_delays << dendl;
-    utime_t t;
-    t.set_from_double(async_msgr->cct->_conf->ms_inject_internal_delays);
-    t.sleep();
-  }
-}
-
-void AsyncConnection::process()
+/* return -1 means `fd` occurs error or closed, it should be closed
+ * return 0 means EAGAIN or EINTR */
+ssize_t AsyncConnection::read_bulk(char *buf, unsigned len)
 {
-  ssize_t r = 0;
-  int prev_state = state;
-#if defined(WITH_LTTNG) && defined(WITH_EVENTTRACE)
-  utime_t ltt_recv_stamp = ceph_clock_now();
-#endif
-  bool need_dispatch_writer = false;
-  std::lock_guard<std::mutex> l(lock);
-  last_active = ceph::coarse_mono_clock::now();
-  auto recv_start_time = ceph::mono_clock::now();
-  do {
-    ldout(async_msgr->cct, 20) << __func__ << " prev state is " << get_state_name(prev_state) << dendl;
-    prev_state = state;
-    switch (state) {
-      case STATE_OPEN:
-        {
-          char tag = -1;
-          r = read_until(sizeof(tag), &tag);
-          if (r < 0) {
-            ldout(async_msgr->cct, 1) << __func__ << " read tag failed" << dendl;
-            goto fail;
-          } else if (r > 0) {
-            break;
-          }
-
-          if (tag == CEPH_MSGR_TAG_KEEPALIVE) {
-            ldout(async_msgr->cct, 20) << __func__ << " got KEEPALIVE" << dendl;
-	    set_last_keepalive(ceph_clock_now());
-          } else if (tag == CEPH_MSGR_TAG_KEEPALIVE2) {
-            state = STATE_OPEN_KEEPALIVE2;
-          } else if (tag == CEPH_MSGR_TAG_KEEPALIVE2_ACK) {
-            state = STATE_OPEN_KEEPALIVE2_ACK;
-          } else if (tag == CEPH_MSGR_TAG_ACK) {
-            state = STATE_OPEN_TAG_ACK;
-          } else if (tag == CEPH_MSGR_TAG_MSG) {
-            state = STATE_OPEN_MESSAGE_HEADER;
-          } else if (tag == CEPH_MSGR_TAG_CLOSE) {
-            state = STATE_OPEN_TAG_CLOSE;
-          } else {
-            ldout(async_msgr->cct, 0) << __func__ << " bad tag " << (int)tag << dendl;
-            goto fail;
-          }
-
-          break;
-        }
-
-      case STATE_OPEN_KEEPALIVE2:
-        {
-          ceph_timespec *t;
-          r = read_until(sizeof(*t), state_buffer);
-          if (r < 0) {
-            ldout(async_msgr->cct, 1) << __func__ << " read keeplive timespec failed" << dendl;
-            goto fail;
-          } else if (r > 0) {
-            break;
-          }
-
-          ldout(async_msgr->cct, 30) << __func__ << " got KEEPALIVE2 tag ..." << dendl;
-          t = (ceph_timespec*)state_buffer;
-          utime_t kp_t = utime_t(*t);
-          write_lock.lock();
-          _append_keepalive_or_ack(true, &kp_t);
-	  write_lock.unlock();
-          ldout(async_msgr->cct, 20) << __func__ << " got KEEPALIVE2 " << kp_t << dendl;
-	  set_last_keepalive(ceph_clock_now());
-          need_dispatch_writer = true;
-          state = STATE_OPEN;
-          break;
-        }
-
-      case STATE_OPEN_KEEPALIVE2_ACK:
-        {
-          ceph_timespec *t;
-          r = read_until(sizeof(*t), state_buffer);
-          if (r < 0) {
-            ldout(async_msgr->cct, 1) << __func__ << " read keeplive timespec failed" << dendl;
-            goto fail;
-          } else if (r > 0) {
-            break;
-          }
-
-          t = (ceph_timespec*)state_buffer;
-          set_last_keepalive_ack(utime_t(*t));
-          ldout(async_msgr->cct, 20) << __func__ << " got KEEPALIVE_ACK" << dendl;
-          state = STATE_OPEN;
-          break;
-        }
-
-      case STATE_OPEN_TAG_ACK:
-        {
-          ceph_le64 *seq;
-          r = read_until(sizeof(*seq), state_buffer);
-          if (r < 0) {
-            ldout(async_msgr->cct, 1) << __func__ << " read ack seq failed" << dendl;
-            goto fail;
-          } else if (r > 0) {
-            break;
-          }
-
-          seq = (ceph_le64*)state_buffer;
-          ldout(async_msgr->cct, 20) << __func__ << " got ACK" << dendl;
-          handle_ack(*seq);
-          state = STATE_OPEN;
-          break;
-        }
-
-      case STATE_OPEN_MESSAGE_HEADER:
-        {
-#if defined(WITH_LTTNG) && defined(WITH_EVENTTRACE)
-          ltt_recv_stamp = ceph_clock_now();
-#endif
-          recv_stamp = ceph_clock_now();
-          ldout(async_msgr->cct, 20) << __func__ << " begin MSG" << dendl;
-          ceph_msg_header header;
-          ceph_msg_header_old oldheader;
-          __u32 header_crc = 0;
-          unsigned len;
-          if (has_feature(CEPH_FEATURE_NOSRCADDR))
-            len = sizeof(header);
-          else
-            len = sizeof(oldheader);
-
-          r = read_until(len, state_buffer);
-          if (r < 0) {
-            ldout(async_msgr->cct, 1) << __func__ << " read message header failed" << dendl;
-            goto fail;
-          } else if (r > 0) {
-            break;
-          }
-
-          ldout(async_msgr->cct, 20) << __func__ << " got MSG header" << dendl;
-
-          if (has_feature(CEPH_FEATURE_NOSRCADDR)) {
-            header = *((ceph_msg_header*)state_buffer);
-            if (msgr->crcflags & MSG_CRC_HEADER)
-              header_crc = ceph_crc32c(0, (unsigned char *)&header,
-                                       sizeof(header) - sizeof(header.crc));
-          } else {
-            oldheader = *((ceph_msg_header_old*)state_buffer);
-            // this is fugly
-            memcpy(&header, &oldheader, sizeof(header));
-            header.src = oldheader.src.name;
-            header.reserved = oldheader.reserved;
-            if (msgr->crcflags & MSG_CRC_HEADER) {
-              header.crc = oldheader.crc;
-              header_crc = ceph_crc32c(0, (unsigned char *)&oldheader, sizeof(oldheader) - sizeof(oldheader.crc));
-            }
-          }
-
-          ldout(async_msgr->cct, 20) << __func__ << " got envelope type=" << header.type
-                              << " src " << entity_name_t(header.src)
-                              << " front=" << header.front_len
-                              << " data=" << header.data_len
-                              << " off " << header.data_off << dendl;
-
-          // verify header crc
-          if (msgr->crcflags & MSG_CRC_HEADER && header_crc != header.crc) {
-            ldout(async_msgr->cct,0) << __func__ << " got bad header crc "
-                                     << header_crc << " != " << header.crc << dendl;
-            goto fail;
-          }
-
-          // Reset state
-          data_buf.clear();
-          front.clear();
-          middle.clear();
-          data.clear();
-          current_header = header;
-          state = STATE_OPEN_MESSAGE_THROTTLE_MESSAGE;
-          break;
-        }
-
-      case STATE_OPEN_MESSAGE_THROTTLE_MESSAGE:
-        {
-          if (policy.throttler_messages) {
-            ldout(async_msgr->cct, 10) << __func__ << " wants " << 1 << " message from policy throttler "
-                                       << policy.throttler_messages->get_current() << "/"
-                                       << policy.throttler_messages->get_max() << dendl;
-            if (!policy.throttler_messages->get_or_fail()) {
-              ldout(async_msgr->cct, 10) << __func__ << " wants 1 message from policy throttle "
-					 << policy.throttler_messages->get_current() << "/"
-					 << policy.throttler_messages->get_max() << " failed, just wait." << dendl;
-              // following thread pool deal with th full message queue isn't a
-              // short time, so we can wait a ms.
-              if (register_time_events.empty())
-                register_time_events.insert(center->create_time_event(1000, wakeup_handler));
-              break;
-            }
-          }
-
-          state = STATE_OPEN_MESSAGE_THROTTLE_BYTES;
-          break;
-        }
-
-      case STATE_OPEN_MESSAGE_THROTTLE_BYTES:
-        {
-          cur_msg_size = current_header.front_len + current_header.middle_len + current_header.data_len;
-          if (cur_msg_size) {
-            if (policy.throttler_bytes) {
-              ldout(async_msgr->cct, 10) << __func__ << " wants " << cur_msg_size << " bytes from policy throttler "
-                                         << policy.throttler_bytes->get_current() << "/"
-                                         << policy.throttler_bytes->get_max() << dendl;
-              if (!policy.throttler_bytes->get_or_fail(cur_msg_size)) {
-                ldout(async_msgr->cct, 10) << __func__ << " wants " << cur_msg_size << " bytes from policy throttler "
-                                           << policy.throttler_bytes->get_current() << "/"
-                                           << policy.throttler_bytes->get_max() << " failed, just wait." << dendl;
-                // following thread pool deal with th full message queue isn't a
-                // short time, so we can wait a ms.
-                if (register_time_events.empty())
-                  register_time_events.insert(center->create_time_event(1000, wakeup_handler));
-                break;
-              }
-            }
-          }
-
-          state = STATE_OPEN_MESSAGE_THROTTLE_DISPATCH_QUEUE;
-          break;
-        }
-
-      case STATE_OPEN_MESSAGE_THROTTLE_DISPATCH_QUEUE:
-        {
-          if (cur_msg_size) {
-            if (!dispatch_queue->dispatch_throttler.get_or_fail(cur_msg_size)) {
-              ldout(async_msgr->cct, 10) << __func__ << " wants " << cur_msg_size << " bytes from dispatch throttle "
-                                         << dispatch_queue->dispatch_throttler.get_current() << "/"
-                                         << dispatch_queue->dispatch_throttler.get_max() << " failed, just wait." << dendl;
-              // following thread pool deal with th full message queue isn't a
-              // short time, so we can wait a ms.
-              if (register_time_events.empty())
-                register_time_events.insert(center->create_time_event(1000, wakeup_handler));
-              break;
-            }
-          }
-
-          throttle_stamp = ceph_clock_now();
-          state = STATE_OPEN_MESSAGE_READ_FRONT;
-          break;
-        }
-
-      case STATE_OPEN_MESSAGE_READ_FRONT:
-        {
-          // read front
-          unsigned front_len = current_header.front_len;
-          if (front_len) {
-            if (!front.length())
-              front.push_back(buffer::create(front_len));
-
-            r = read_until(front_len, front.c_str());
-            if (r < 0) {
-              ldout(async_msgr->cct, 1) << __func__ << " read message front failed" << dendl;
-              goto fail;
-            } else if (r > 0) {
-              break;
-            }
-
-            ldout(async_msgr->cct, 20) << __func__ << " got front " << front.length() << dendl;
-          }
-          state = STATE_OPEN_MESSAGE_READ_MIDDLE;
-        }
-
-      case STATE_OPEN_MESSAGE_READ_MIDDLE:
-        {
-          // read middle
-          unsigned middle_len = current_header.middle_len;
-          if (middle_len) {
-            if (!middle.length())
-              middle.push_back(buffer::create(middle_len));
-
-            r = read_until(middle_len, middle.c_str());
-            if (r < 0) {
-              ldout(async_msgr->cct, 1) << __func__ << " read message middle failed" << dendl;
-              goto fail;
-            } else if (r > 0) {
-              break;
-            }
-            ldout(async_msgr->cct, 20) << __func__ << " got middle " << middle.length() << dendl;
-          }
-
-          state = STATE_OPEN_MESSAGE_READ_DATA_PREPARE;
-        }
-
-      case STATE_OPEN_MESSAGE_READ_DATA_PREPARE:
-        {
-          // read data
-          unsigned data_len = le32_to_cpu(current_header.data_len);
-          unsigned data_off = le32_to_cpu(current_header.data_off);
-          if (data_len) {
-            // get a buffer
-            map<ceph_tid_t,pair<bufferlist,int> >::iterator p = rx_buffers.find(current_header.tid);
-            if (p != rx_buffers.end()) {
-              ldout(async_msgr->cct,10) << __func__ << " seleting rx buffer v " << p->second.second
-                                  << " at offset " << data_off
-                                  << " len " << p->second.first.length() << dendl;
-              data_buf = p->second.first;
-              // make sure it's big enough
-              if (data_buf.length() < data_len)
-                data_buf.push_back(buffer::create(data_len - data_buf.length()));
-              data_blp = data_buf.begin();
-            } else {
-              ldout(async_msgr->cct,20) << __func__ << " allocating new rx buffer at offset " << data_off << dendl;
-              alloc_aligned_buffer(data_buf, data_len, data_off);
-              data_blp = data_buf.begin();
-            }
-          }
-
-          msg_left = data_len;
-          state = STATE_OPEN_MESSAGE_READ_DATA;
-        }
-
-      case STATE_OPEN_MESSAGE_READ_DATA:
-        {
-          while (msg_left > 0) {
-            bufferptr bp = data_blp.get_current_ptr();
-            unsigned read = MIN(bp.length(), msg_left);
-            r = read_until(read, bp.c_str());
-            if (r < 0) {
-              ldout(async_msgr->cct, 1) << __func__ << " read data error " << dendl;
-              goto fail;
-            } else if (r > 0) {
-              break;
-            }
-
-            data_blp.advance(read);
-            data.append(bp, 0, read);
-            msg_left -= read;
-          }
-
-          if (msg_left > 0)
-            break;
-
-          state = STATE_OPEN_MESSAGE_READ_FOOTER_AND_DISPATCH;
-        }
-
-      case STATE_OPEN_MESSAGE_READ_FOOTER_AND_DISPATCH:
-        {
-          ceph_msg_footer footer;
-          ceph_msg_footer_old old_footer;
-          unsigned len;
-          // footer
-          if (has_feature(CEPH_FEATURE_MSG_AUTH))
-            len = sizeof(footer);
-          else
-            len = sizeof(old_footer);
-
-          r = read_until(len, state_buffer);
-          if (r < 0) {
-            ldout(async_msgr->cct, 1) << __func__ << " read footer data error " << dendl;
-            goto fail;
-          } else if (r > 0) {
-            break;
-          }
-
-          if (has_feature(CEPH_FEATURE_MSG_AUTH)) {
-            footer = *((ceph_msg_footer*)state_buffer);
-          } else {
-            old_footer = *((ceph_msg_footer_old*)state_buffer);
-            footer.front_crc = old_footer.front_crc;
-            footer.middle_crc = old_footer.middle_crc;
-            footer.data_crc = old_footer.data_crc;
-            footer.sig = 0;
-            footer.flags = old_footer.flags;
-          }
-          int aborted = (footer.flags & CEPH_MSG_FOOTER_COMPLETE) == 0;
-          ldout(async_msgr->cct, 10) << __func__ << " aborted = " << aborted << dendl;
-          if (aborted) {
-            ldout(async_msgr->cct, 0) << __func__ << " got " << front.length() << " + " << middle.length() << " + " << data.length()
-                                << " byte message.. ABORTED" << dendl;
-            goto fail;
-          }
-
-          ldout(async_msgr->cct, 20) << __func__ << " got " << front.length() << " + " << middle.length()
-                              << " + " << data.length() << " byte message" << dendl;
-          Message *message = decode_message(async_msgr->cct, async_msgr->crcflags, current_header, footer,
-                                            front, middle, data, this);
-          if (!message) {
-            ldout(async_msgr->cct, 1) << __func__ << " decode message failed " << dendl;
-            goto fail;
-          }
-
-          //
-          //  Check the signature if one should be present.  A zero return indicates success. PLR
-          //
-
-          if (session_security.get() == NULL) {
-            ldout(async_msgr->cct, 10) << __func__ << " no session security set" << dendl;
-          } else {
-            if (session_security->check_message_signature(message)) {
-              ldout(async_msgr->cct, 0) << __func__ << " Signature check failed" << dendl;
-              message->put();
-              goto fail;
-            }
-          }
-          message->set_byte_throttler(policy.throttler_bytes);
-          message->set_message_throttler(policy.throttler_messages);
-
-          // store reservation size in message, so we don't get confused
-          // by messages entering the dispatch queue through other paths.
-          message->set_dispatch_throttle_size(cur_msg_size);
-
-          message->set_recv_stamp(recv_stamp);
-          message->set_throttle_stamp(throttle_stamp);
-          message->set_recv_complete_stamp(ceph_clock_now());
-
-          // check received seq#.  if it is old, drop the message.  
-          // note that incoming messages may skip ahead.  this is convenient for the client
-          // side queueing because messages can't be renumbered, but the (kernel) client will
-          // occasionally pull a message out of the sent queue to send elsewhere.  in that case
-          // it doesn't matter if we "got" it or not.
-          uint64_t cur_seq = in_seq;
-          if (message->get_seq() <= cur_seq) {
-            ldout(async_msgr->cct,0) << __func__ << " got old message "
-                    << message->get_seq() << " <= " << cur_seq << " " << message << " " << *message
-                    << ", discarding" << dendl;
-            message->put();
-            if (has_feature(CEPH_FEATURE_RECONNECT_SEQ) && async_msgr->cct->_conf->ms_die_on_old_message)
-              assert(0 == "old msgs despite reconnect_seq feature");
-            break;
-          }
-          if (message->get_seq() > cur_seq + 1) {
-            ldout(async_msgr->cct, 0) << __func__ << " missed message?  skipped from seq "
-                                      << cur_seq << " to " << message->get_seq() << dendl;
-            if (async_msgr->cct->_conf->ms_die_on_skipped_message)
-              assert(0 == "skipped incoming seq");
-          }
-
-          message->set_connection(this);
-
-#if defined(WITH_LTTNG) && defined(WITH_EVENTTRACE)
-          if (message->get_type() == CEPH_MSG_OSD_OP || message->get_type() == CEPH_MSG_OSD_OPREPLY) {
-            utime_t ltt_processed_stamp = ceph_clock_now();
-            double usecs_elapsed = (ltt_processed_stamp.to_nsec()-ltt_recv_stamp.to_nsec())/1000;
-            ostringstream buf;
-            if (message->get_type() == CEPH_MSG_OSD_OP)
-              OID_ELAPSED_WITH_MSG(message, usecs_elapsed, "TIME_TO_DECODE_OSD_OP", false);
-            else
-              OID_ELAPSED_WITH_MSG(message, usecs_elapsed, "TIME_TO_DECODE_OSD_OPREPLY", false);
-          }
-#endif
-
-          // note last received message.
-          in_seq = message->get_seq();
-	  ldout(async_msgr->cct, 5) << " rx " << message->get_source() << " seq "
-                                    << message->get_seq() << " " << message
-				    << " " << *message << dendl;
-
-          if (!policy.lossy) {
-            ack_left++;
-            need_dispatch_writer = true;
-          }
-          state = STATE_OPEN;
-
-          logger->inc(l_msgr_recv_messages);
-          logger->inc(l_msgr_recv_bytes, cur_msg_size + sizeof(ceph_msg_header) + sizeof(ceph_msg_footer));
-
-          async_msgr->ms_fast_preprocess(message);
-          auto fast_dispatch_time = ceph::mono_clock::now();
-          logger->tinc(l_msgr_running_recv_time, fast_dispatch_time - recv_start_time);
-          if (delay_state) {
-            utime_t release = message->get_recv_stamp();
-            double delay_period = 0;
-            if (rand() % 10000 < async_msgr->cct->_conf->ms_inject_delay_probability * 10000.0) {
-              delay_period = async_msgr->cct->_conf->ms_inject_delay_max * (double)(rand() % 10000) / 10000.0;
-              release += delay_period;
-              ldout(async_msgr->cct, 1) << "queue_received will delay until " << release << " on "
-                                        << message << " " << *message << dendl;
-            }
-            delay_state->queue(delay_period, release, message);
-          } else if (async_msgr->ms_can_fast_dispatch(message)) {
-            lock.unlock();
-            dispatch_queue->fast_dispatch(message);
-            recv_start_time = ceph::mono_clock::now();
-            logger->tinc(l_msgr_running_fast_dispatch_time,
-                         recv_start_time - fast_dispatch_time);
-            lock.lock();
-          } else {
-            dispatch_queue->enqueue(message, message->get_priority(), conn_id);
-          }
-
-          break;
-        }
-
-      case STATE_OPEN_TAG_CLOSE:
-        {
-          ldout(async_msgr->cct, 20) << __func__ << " got CLOSE" << dendl;
-          _stop();
-          return ;
-        }
-
-      case STATE_STANDBY:
-        {
-          ldout(async_msgr->cct, 20) << __func__ << " enter STANDY" << dendl;
-
-          break;
-        }
-
-      case STATE_NONE:
-        {
-          ldout(async_msgr->cct, 20) << __func__ << " enter none state" << dendl;
-          break;
-        }
-
-      case STATE_CLOSED:
-        {
-          ldout(async_msgr->cct, 20) << __func__ << " socket closed" << dendl;
-          break;
-        }
-
-      case STATE_WAIT:
-        {
-          ldout(async_msgr->cct, 1) << __func__ << " enter wait state, failing" << dendl;
-          goto fail;
-        }
-
-      default:
-        {
-          if (_process_connection() < 0)
-            goto fail;
-          break;
-        }
+  ssize_t nread;
+ again:
+  nread = cs.read(buf, len);
+  if (nread < 0) {
+    if (nread == -EAGAIN) {
+      nread = 0;
+    } else if (nread == -EINTR) {
+      goto again;
+    } else {
+      ldout(async_msgr->cct, 1) << __func__ << " reading from fd=" << cs.fd()
+                          << " : "<< strerror(nread) << dendl;
+      return -1;
     }
-  } while (prev_state != state);
-
-  if (need_dispatch_writer && is_connected())
-    center->dispatch_event_external(write_handler);
-
-  logger->tinc(l_msgr_running_recv_time, ceph::mono_clock::now() - recv_start_time);
-  return;
-
- fail:
-  fault();
-}
-
-ssize_t AsyncConnection::_process_connection()
-{
-  ssize_t r = 0;
-
-  switch(state) {
-    case STATE_WAIT_SEND:
-      {
-        std::lock_guard<std::mutex> l(write_lock);
-        if (!outcoming_bl.length()) {
-          assert(state_after_send);
-          state = state_after_send;
-          state_after_send = STATE_NONE;
-        }
-        break;
-      }
-
-    case STATE_CONNECTING:
-      {
-        assert(!policy.server);
-
-        // reset connect state variables
-        got_bad_auth = false;
-        delete authorizer;
-        authorizer = NULL;
-        authorizer_buf.clear();
-        memset(&connect_msg, 0, sizeof(connect_msg));
-        memset(&connect_reply, 0, sizeof(connect_reply));
-
-        global_seq = async_msgr->get_global_seq();
-        // close old socket.  this is safe because we stopped the reader thread above.
-        if (cs) {
-          center->delete_file_event(cs.fd(), EVENT_READABLE|EVENT_WRITABLE);
-          cs.close();
-        }
-
-        SocketOptions opts;
-        opts.priority = async_msgr->get_socket_priority();
-        opts.connect_bind_addr = msgr->get_myaddr();
-        r = worker->connect(get_peer_addr(), opts, &cs);
-        if (r < 0)
-          goto fail;
-
-        center->create_file_event(cs.fd(), EVENT_READABLE, read_handler);
-        state = STATE_CONNECTING_RE;
-        break;
-      }
-
-    case STATE_CONNECTING_RE:
-      {
-        r = cs.is_connected();
-        if (r < 0) {
-          ldout(async_msgr->cct, 1) << __func__ << " reconnect failed " << dendl;
-          if (r == -ECONNREFUSED) {
-            ldout(async_msgr->cct, 2) << __func__ << " connection refused!" << dendl;
-            dispatch_queue->queue_refused(this);
-          }
-          goto fail;
-        } else if (r == 0) {
-          ldout(async_msgr->cct, 10) << __func__ << " nonblock connect inprogress" << dendl;
-          if (async_msgr->get_stack()->nonblock_connect_need_writable_event())
-            center->create_file_event(cs.fd(), EVENT_WRITABLE, read_handler);
-          break;
-        }
-
-        center->delete_file_event(cs.fd(), EVENT_WRITABLE);
-        ldout(async_msgr->cct, 10) << __func__ << " connect successfully, ready to send banner" << dendl;
-
-        bufferlist bl;
-        bl.append(CEPH_BANNER, strlen(CEPH_BANNER));
-        r = try_send(bl);
-        if (r == 0) {
-          state = STATE_CONNECTING_WAIT_BANNER_AND_IDENTIFY;
-          ldout(async_msgr->cct, 10) << __func__ << " connect write banner done: "
-                                     << get_peer_addr() << dendl;
-        } else if (r > 0) {
-          state = STATE_WAIT_SEND;
-          state_after_send = STATE_CONNECTING_WAIT_BANNER_AND_IDENTIFY;
-          ldout(async_msgr->cct, 10) << __func__ << " connect wait for write banner: "
-                               << get_peer_addr() << dendl;
-        } else {
-          goto fail;
-        }
-
-        break;
-      }
-
-    case STATE_CONNECTING_WAIT_BANNER_AND_IDENTIFY:
-      {
-        entity_addr_t paddr, peer_addr_for_me;
-        bufferlist myaddrbl;
-        unsigned banner_len = strlen(CEPH_BANNER);
-        unsigned need_len = banner_len + sizeof(ceph_entity_addr)*2;
-        r = read_until(need_len, state_buffer);
-        if (r < 0) {
-          ldout(async_msgr->cct, 1) << __func__ << " read banner and identify addresses failed" << dendl;
-          goto fail;
-        } else if (r > 0) {
-          break;
-        }
-
-        if (memcmp(state_buffer, CEPH_BANNER, banner_len)) {
-          ldout(async_msgr->cct, 0) << __func__ << " connect protocol error (bad banner) on peer "
-                                    << get_peer_addr() << dendl;
-          goto fail;
-        }
-
-        bufferlist bl;
-        bl.append(state_buffer+banner_len, sizeof(ceph_entity_addr)*2);
-        bufferlist::iterator p = bl.begin();
-        try {
-          ::decode(paddr, p);
-          ::decode(peer_addr_for_me, p);
-        } catch (const buffer::error& e) {
-          lderr(async_msgr->cct) << __func__ <<  " decode peer addr failed " << dendl;
-          goto fail;
-        }
-        ldout(async_msgr->cct, 20) << __func__ <<  " connect read peer addr "
-                             << paddr << " on socket " << cs.fd() << dendl;
-        if (peer_addr != paddr) {
-          if (paddr.is_blank_ip() && peer_addr.get_port() == paddr.get_port() &&
-              peer_addr.get_nonce() == paddr.get_nonce()) {
-            ldout(async_msgr->cct, 0) << __func__ <<  " connect claims to be " << paddr
-                                << " not " << peer_addr
-                                << " - presumably this is the same node!" << dendl;
-          } else {
-            ldout(async_msgr->cct, 10) << __func__ << " connect claims to be "
-				       << paddr << " not " << peer_addr << dendl;
-	    goto fail;
-          }
-        }
-
-        ldout(async_msgr->cct, 20) << __func__ << " connect peer addr for me is " << peer_addr_for_me << dendl;
-        lock.unlock();
-        async_msgr->learned_addr(peer_addr_for_me);
-        if (async_msgr->cct->_conf->ms_inject_internal_delays) {
-          if (rand() % async_msgr->cct->_conf->ms_inject_socket_failures == 0) {
-            ldout(msgr->cct, 10) << __func__ << " sleep for "
-                                 << async_msgr->cct->_conf->ms_inject_internal_delays << dendl;
-            utime_t t;
-            t.set_from_double(async_msgr->cct->_conf->ms_inject_internal_delays);
-            t.sleep();
-          }
-        }
-
-        lock.lock();
-        if (state != STATE_CONNECTING_WAIT_BANNER_AND_IDENTIFY) {
-          ldout(async_msgr->cct, 1) << __func__ << " state changed while learned_addr, mark_down or "
-                                    << " replacing must be happened just now" << dendl;
-          return 0;
-        }
-
-        ::encode(async_msgr->get_myaddr(), myaddrbl, 0); // legacy
-        r = try_send(myaddrbl);
-        if (r == 0) {
-          state = STATE_CONNECTING_SEND_CONNECT_MSG;
-          ldout(async_msgr->cct, 10) << __func__ << " connect sent my addr "
-              << async_msgr->get_myaddr() << dendl;
-        } else if (r > 0) {
-          state = STATE_WAIT_SEND;
-          state_after_send = STATE_CONNECTING_SEND_CONNECT_MSG;
-          ldout(async_msgr->cct, 10) << __func__ << " connect send my addr done: "
-              << async_msgr->get_myaddr() << dendl;
-        } else {
-          ldout(async_msgr->cct, 2) << __func__ << " connect couldn't write my addr, "
-              << cpp_strerror(r) << dendl;
-          goto fail;
-        }
-
-        break;
-      }
-
-    case STATE_CONNECTING_SEND_CONNECT_MSG:
-      {
-        if (!got_bad_auth) {
-          delete authorizer;
-          authorizer = async_msgr->get_authorizer(peer_type, false);
-        }
-        bufferlist bl;
-
-        connect_msg.features = policy.features_supported;
-        connect_msg.host_type = async_msgr->get_myinst().name.type();
-        connect_msg.global_seq = global_seq;
-        connect_msg.connect_seq = connect_seq;
-        connect_msg.protocol_version = async_msgr->get_proto_version(peer_type, true);
-        connect_msg.authorizer_protocol = authorizer ? authorizer->protocol : 0;
-        connect_msg.authorizer_len = authorizer ? authorizer->bl.length() : 0;
-        if (authorizer)
-          ldout(async_msgr->cct, 10) << __func__ <<  " connect_msg.authorizer_len="
-                                     << connect_msg.authorizer_len << " protocol="
-                                     << connect_msg.authorizer_protocol << dendl;
-        connect_msg.flags = 0;
-        if (policy.lossy)
-          connect_msg.flags |= CEPH_MSG_CONNECT_LOSSY;  // this is fyi, actually, server decides!
-        bl.append((char*)&connect_msg, sizeof(connect_msg));
-        if (authorizer) {
-          bl.append(authorizer->bl.c_str(), authorizer->bl.length());
-        }
-        ldout(async_msgr->cct, 10) << __func__ << " connect sending gseq=" << global_seq << " cseq="
-            << connect_seq << " proto=" << connect_msg.protocol_version << dendl;
-
-        r = try_send(bl);
-        if (r == 0) {
-          state = STATE_CONNECTING_WAIT_CONNECT_REPLY;
-          ldout(async_msgr->cct,20) << __func__ << " connect wrote (self +) cseq, waiting for reply" << dendl;
-        } else if (r > 0) {
-          state = STATE_WAIT_SEND;
-          state_after_send = STATE_CONNECTING_WAIT_CONNECT_REPLY;
-          ldout(async_msgr->cct, 10) << __func__ << " continue send reply " << dendl;
-        } else {
-          ldout(async_msgr->cct, 2) << __func__ << " connect couldn't send reply "
-              << cpp_strerror(r) << dendl;
-          goto fail;
-        }
-
-        break;
-      }
-
-    case STATE_CONNECTING_WAIT_CONNECT_REPLY:
-      {
-        r = read_until(sizeof(connect_reply), state_buffer);
-        if (r < 0) {
-          ldout(async_msgr->cct, 1) << __func__ << " read connect reply failed" << dendl;
-          goto fail;
-        } else if (r > 0) {
-          break;
-        }
-
-        connect_reply = *((ceph_msg_connect_reply*)state_buffer);
-
-        ldout(async_msgr->cct, 20) << __func__ << " connect got reply tag " << (int)connect_reply.tag
-                             << " connect_seq " << connect_reply.connect_seq << " global_seq "
-                             << connect_reply.global_seq << " proto " << connect_reply.protocol_version
-                             << " flags " << (int)connect_reply.flags << " features "
-                             << connect_reply.features << dendl;
-        state = STATE_CONNECTING_WAIT_CONNECT_REPLY_AUTH;
-
-        break;
-      }
-
-    case STATE_CONNECTING_WAIT_CONNECT_REPLY_AUTH:
-      {
-        bufferlist authorizer_reply;
-        if (connect_reply.authorizer_len) {
-          ldout(async_msgr->cct, 10) << __func__ << " reply.authorizer_len=" << connect_reply.authorizer_len << dendl;
-          assert(connect_reply.authorizer_len < 4096);
-          r = read_until(connect_reply.authorizer_len, state_buffer);
-          if (r < 0) {
-            ldout(async_msgr->cct, 1) << __func__ << " read connect reply authorizer failed" << dendl;
-            goto fail;
-          } else if (r > 0) {
-            break;
-          }
-
-          authorizer_reply.append(state_buffer, connect_reply.authorizer_len);
-          bufferlist::iterator iter = authorizer_reply.begin();
-          if (authorizer && !authorizer->verify_reply(iter)) {
-            ldout(async_msgr->cct, 0) << __func__ << " failed verifying authorize reply" << dendl;
-            goto fail;
-          }
-        }
-        r = handle_connect_reply(connect_msg, connect_reply);
-        if (r < 0)
-          goto fail;
-
-        // state must be changed!
-        assert(state != STATE_CONNECTING_WAIT_CONNECT_REPLY_AUTH);
-        break;
-      }
-
-    case STATE_CONNECTING_WAIT_ACK_SEQ:
-      {
-        uint64_t newly_acked_seq = 0;
-
-        r = read_until(sizeof(newly_acked_seq), state_buffer);
-        if (r < 0) {
-          ldout(async_msgr->cct, 1) << __func__ << " read connect ack seq failed" << dendl;
-          goto fail;
-        } else if (r > 0) {
-          break;
-        }
-
-        newly_acked_seq = *((uint64_t*)state_buffer);
-        ldout(async_msgr->cct, 2) << __func__ << " got newly_acked_seq " << newly_acked_seq
-                            << " vs out_seq " << out_seq << dendl;
-        discard_requeued_up_to(newly_acked_seq);
-        //while (newly_acked_seq > out_seq.read()) {
-        //  Message *m = _get_next_outgoing(NULL);
-        //  assert(m);
-        //  ldout(async_msgr->cct, 2) << __func__ << " discarding previously sent " << m->get_seq()
-        //                      << " " << *m << dendl;
-        //  assert(m->get_seq() <= newly_acked_seq);
-        //  m->put();
-        //  out_seq.inc();
-        //}
-
-        bufferlist bl;
-        uint64_t s = in_seq;
-        bl.append((char*)&s, sizeof(s));
-        r = try_send(bl);
-        if (r == 0) {
-          state = STATE_CONNECTING_READY;
-          ldout(async_msgr->cct, 10) << __func__ << " send in_seq done " << dendl;
-        } else if (r > 0) {
-          state_after_send = STATE_CONNECTING_READY;
-          state = STATE_WAIT_SEND;
-          ldout(async_msgr->cct, 10) << __func__ << " continue send in_seq " << dendl;
-        } else {
-          goto fail;
-        }
-        break;
-      }
-
-    case STATE_CONNECTING_READY:
-      {
-        // hooray!
-        peer_global_seq = connect_reply.global_seq;
-        policy.lossy = connect_reply.flags & CEPH_MSG_CONNECT_LOSSY;
-        state = STATE_OPEN;
-        once_ready = true;
-        connect_seq += 1;
-        assert(connect_seq == connect_reply.connect_seq);
-        backoff = utime_t();
-        set_features((uint64_t)connect_reply.features & (uint64_t)connect_msg.features);
-        ldout(async_msgr->cct, 10) << __func__ << " connect success " << connect_seq
-                                   << ", lossy = " << policy.lossy << ", features "
-                                   << get_features() << dendl;
-
-        // If we have an authorizer, get a new AuthSessionHandler to deal with ongoing security of the
-        // connection.  PLR
-        if (authorizer != NULL) {
-          session_security.reset(
-              get_auth_session_handler(async_msgr->cct,
-                                       authorizer->protocol,
-                                       authorizer->session_key,
-                                       get_features()));
-        } else {
-          // We have no authorizer, so we shouldn't be applying security to messages in this AsyncConnection.  PLR
-          session_security.reset();
-        }
-
-        if (delay_state)
-          assert(delay_state->ready());
-        dispatch_queue->queue_connect(this);
-        async_msgr->ms_deliver_handle_fast_connect(this);
-
-        // make sure no pending tick timer
-        if (last_tick_id)
-          center->delete_time_event(last_tick_id);
-        last_tick_id = center->create_time_event(inactive_timeout_us, tick_handler);
-
-        // message may in queue between last _try_send and connection ready
-        // write event may already notify and we need to force scheduler again
-        write_lock.lock();
-        can_write = WriteStatus::CANWRITE;
-        if (is_queued())
-          center->dispatch_event_external(write_handler);
-        write_lock.unlock();
-        maybe_start_delay_thread();
-        break;
-      }
-
-    case STATE_ACCEPTING:
-      {
-        bufferlist bl;
-        center->create_file_event(cs.fd(), EVENT_READABLE, read_handler);
-
-        bl.append(CEPH_BANNER, strlen(CEPH_BANNER));
-
-        ::encode(async_msgr->get_myaddr(), bl, 0); // legacy
-        port = async_msgr->get_myaddr().get_port();
-        ::encode(socket_addr, bl, 0); // legacy
-        ldout(async_msgr->cct, 1) << __func__ << " sd=" << cs.fd() << " " << socket_addr << dendl;
-
-        r = try_send(bl);
-        if (r == 0) {
-          state = STATE_ACCEPTING_WAIT_BANNER_ADDR;
-          ldout(async_msgr->cct, 10) << __func__ << " write banner and addr done: "
-            << get_peer_addr() << dendl;
-        } else if (r > 0) {
-          state = STATE_WAIT_SEND;
-          state_after_send = STATE_ACCEPTING_WAIT_BANNER_ADDR;
-          ldout(async_msgr->cct, 10) << __func__ << " wait for write banner and addr: "
-                              << get_peer_addr() << dendl;
-        } else {
-          goto fail;
-        }
-
-        break;
-      }
-    case STATE_ACCEPTING_WAIT_BANNER_ADDR:
-      {
-        bufferlist addr_bl;
-        entity_addr_t peer_addr;
-
-        r = read_until(strlen(CEPH_BANNER) + sizeof(ceph_entity_addr), state_buffer);
-        if (r < 0) {
-          ldout(async_msgr->cct, 1) << __func__ << " read peer banner and addr failed" << dendl;
-          goto fail;
-        } else if (r > 0) {
-          break;
-        }
-
-        if (memcmp(state_buffer, CEPH_BANNER, strlen(CEPH_BANNER))) {
-          ldout(async_msgr->cct, 1) << __func__ << " accept peer sent bad banner '" << state_buffer
-                                    << "' (should be '" << CEPH_BANNER << "')" << dendl;
-          goto fail;
-        }
-
-        addr_bl.append(state_buffer+strlen(CEPH_BANNER), sizeof(ceph_entity_addr));
-        {
-          bufferlist::iterator ti = addr_bl.begin();
-          ::decode(peer_addr, ti);
-        }
-
-        ldout(async_msgr->cct, 10) << __func__ << " accept peer addr is " << peer_addr << dendl;
-        if (peer_addr.is_blank_ip()) {
-          // peer apparently doesn't know what ip they have; figure it out for them.
-          int port = peer_addr.get_port();
-          peer_addr.u = socket_addr.u;
-          peer_addr.set_port(port);
-          ldout(async_msgr->cct, 0) << __func__ << " accept peer addr is really " << peer_addr
-                             << " (socket is " << socket_addr << ")" << dendl;
-        }
-        set_peer_addr(peer_addr);  // so that connection_state gets set up
-        state = STATE_ACCEPTING_WAIT_CONNECT_MSG;
-        break;
-      }
-
-    case STATE_ACCEPTING_WAIT_CONNECT_MSG:
-      {
-        r = read_until(sizeof(connect_msg), state_buffer);
-        if (r < 0) {
-          ldout(async_msgr->cct, 1) << __func__ << " read connect msg failed" << dendl;
-          goto fail;
-        } else if (r > 0) {
-          break;
-        }
-
-        connect_msg = *((ceph_msg_connect*)state_buffer);
-        state = STATE_ACCEPTING_WAIT_CONNECT_MSG_AUTH;
-        break;
-      }
-
-    case STATE_ACCEPTING_WAIT_CONNECT_MSG_AUTH:
-      {
-        bufferlist authorizer_reply;
-
-        if (connect_msg.authorizer_len) {
-          if (!authorizer_buf.length())
-            authorizer_buf.push_back(buffer::create(connect_msg.authorizer_len));
-
-          r = read_until(connect_msg.authorizer_len, authorizer_buf.c_str());
-          if (r < 0) {
-            ldout(async_msgr->cct, 1) << __func__ << " read connect authorizer failed" << dendl;
-            goto fail;
-          } else if (r > 0) {
-            break;
-          }
-        }
-
-        ldout(async_msgr->cct, 20) << __func__ << " accept got peer connect_seq "
-                             << connect_msg.connect_seq << " global_seq "
-                             << connect_msg.global_seq << dendl;
-        set_peer_type(connect_msg.host_type);
-        policy = async_msgr->get_policy(connect_msg.host_type);
-        ldout(async_msgr->cct, 10) << __func__ << " accept of host_type " << connect_msg.host_type
-                                   << ", policy.lossy=" << policy.lossy << " policy.server="
-                                   << policy.server << " policy.standby=" << policy.standby
-                                   << " policy.resetcheck=" << policy.resetcheck << dendl;
-
-        r = handle_connect_msg(connect_msg, authorizer_buf, authorizer_reply);
-        if (r < 0)
-          goto fail;
-
-        // state is changed by "handle_connect_msg"
-        assert(state != STATE_ACCEPTING_WAIT_CONNECT_MSG_AUTH);
-        break;
-      }
-
-    case STATE_ACCEPTING_WAIT_SEQ:
-      {
-        uint64_t newly_acked_seq;
-        r = read_until(sizeof(newly_acked_seq), state_buffer);
-        if (r < 0) {
-          ldout(async_msgr->cct, 1) << __func__ << " read ack seq failed" << dendl;
-          goto fail_registered;
-        } else if (r > 0) {
-          break;
-        }
-
-        newly_acked_seq = *((uint64_t*)state_buffer);
-        ldout(async_msgr->cct, 2) << __func__ << " accept get newly_acked_seq " << newly_acked_seq << dendl;
-        discard_requeued_up_to(newly_acked_seq);
-        state = STATE_ACCEPTING_READY;
-        break;
-      }
-
-    case STATE_ACCEPTING_READY:
-      {
-        ldout(async_msgr->cct, 20) << __func__ << " accept done" << dendl;
-        state = STATE_OPEN;
-        memset(&connect_msg, 0, sizeof(connect_msg));
-
-        if (delay_state)
-          assert(delay_state->ready());
-        // make sure no pending tick timer
-        if (last_tick_id)
-          center->delete_time_event(last_tick_id);
-        last_tick_id = center->create_time_event(inactive_timeout_us, tick_handler);
-
-        write_lock.lock();
-        can_write = WriteStatus::CANWRITE;
-        if (is_queued())
-          center->dispatch_event_external(write_handler);
-        write_lock.unlock();
-        maybe_start_delay_thread();
-        break;
-      }
-
-    default:
-      {
-        lderr(async_msgr->cct) << __func__ << " bad state: " << state << dendl;
-        ceph_abort();
-      }
+  } else if (nread == 0) {
+    ldout(async_msgr->cct, 1) << __func__ << " peer close file descriptor "
+                              << cs.fd() << dendl;
+    return -1;
   }
+  return nread;
+}
 
-  return 0;
-
-fail_registered:
-  ldout(async_msgr->cct, 10) << "accept fault after register" << dendl;
-  inject_delay();
+ssize_t AsyncConnection::write(bufferlist &bl,
+                               std::function<void(ssize_t)> callback,
+                               bool more) {
 
-fail:
-  return -1;
+    std::unique_lock<std::mutex> l(write_lock);
+    outgoing_bl.claim_append(bl);
+    ssize_t r = _try_send(more);
+    if (r > 0) {
+      writeCallback = callback;
+    }
+    return r;
 }
 
-int AsyncConnection::handle_connect_reply(ceph_msg_connect &connect, ceph_msg_connect_reply &reply)
+// return the remaining bytes, it may larger than the length of ptr
+// else return < 0 means error
+ssize_t AsyncConnection::_try_send(bool more)
 {
-  uint64_t feat_missing;
-  if (reply.tag == CEPH_MSGR_TAG_FEATURES) {
-    ldout(async_msgr->cct, 0) << __func__ << " connect protocol feature mismatch, my "
-                        << std::hex << connect.features << " < peer "
-                        << reply.features << " missing "
-                        << (reply.features & ~policy.features_supported)
-                        << std::dec << dendl;
-    goto fail;
-  }
-
-  if (reply.tag == CEPH_MSGR_TAG_BADPROTOVER) {
-    ldout(async_msgr->cct, 0) << __func__ << " connect protocol version mismatch, my "
-                        << connect.protocol_version << " != " << reply.protocol_version
-                        << dendl;
-    goto fail;
+  if (async_msgr->cct->_conf->ms_inject_socket_failures && cs) {
+    if (rand() % async_msgr->cct->_conf->ms_inject_socket_failures == 0) {
+      ldout(async_msgr->cct, 0) << __func__ << " injecting socket failure" << dendl;
+      cs.shutdown();
+    }
   }
 
-  if (reply.tag == CEPH_MSGR_TAG_BADAUTHORIZER) {
-    ldout(async_msgr->cct,0) << __func__ << " connect got BADAUTHORIZER" << dendl;
-    if (got_bad_auth)
-      goto fail;
-    got_bad_auth = true;
-    delete authorizer;
-    authorizer = async_msgr->get_authorizer(peer_type, true);  // try harder
-    state = STATE_CONNECTING_SEND_CONNECT_MSG;
-  }
-  if (reply.tag == CEPH_MSGR_TAG_RESETSESSION) {
-    ldout(async_msgr->cct, 0) << __func__ << " connect got RESETSESSION" << dendl;
-    was_session_reset();
-    // see was_session_reset
-    outcoming_bl.clear();
-    state = STATE_CONNECTING_SEND_CONNECT_MSG;
-  }
-  if (reply.tag == CEPH_MSGR_TAG_RETRY_GLOBAL) {
-    global_seq = async_msgr->get_global_seq(reply.global_seq);
-    ldout(async_msgr->cct, 5) << __func__ << " connect got RETRY_GLOBAL "
-                              << reply.global_seq << " chose new "
-                              << global_seq << dendl;
-    state = STATE_CONNECTING_SEND_CONNECT_MSG;
-  }
-  if (reply.tag == CEPH_MSGR_TAG_RETRY_SESSION) {
-    assert(reply.connect_seq > connect_seq);
-    ldout(async_msgr->cct, 5) << __func__ << " connect got RETRY_SESSION "
-                              << connect_seq << " -> "
-                              << reply.connect_seq << dendl;
-    connect_seq = reply.connect_seq;
-    state = STATE_CONNECTING_SEND_CONNECT_MSG;
-  }
-  if (reply.tag == CEPH_MSGR_TAG_WAIT) {
-    ldout(async_msgr->cct, 1) << __func__ << " connect got WAIT (connection race)" << dendl;
-    state = STATE_WAIT;
+  ceph_assert(center->in_thread());
+  ldout(async_msgr->cct, 25) << __func__ << " cs.send " << outgoing_bl.length()
+                             << " bytes" << dendl;
+  ssize_t r = cs.send(outgoing_bl, more);
+  if (r < 0) {
+    ldout(async_msgr->cct, 1) << __func__ << " send error: " << cpp_strerror(r) << dendl;
+    return r;
   }
 
-  feat_missing = policy.features_required & ~(uint64_t)connect_reply.features;
-  if (feat_missing) {
-    ldout(async_msgr->cct, 1) << __func__ << " missing required features " << std::hex
-                              << feat_missing << std::dec << dendl;
-    goto fail;
-  }
+  ldout(async_msgr->cct, 10) << __func__ << " sent bytes " << r
+                             << " remaining bytes " << outgoing_bl.length() << dendl;
 
-  if (reply.tag == CEPH_MSGR_TAG_SEQ) {
-    ldout(async_msgr->cct, 10) << __func__ << " got CEPH_MSGR_TAG_SEQ, reading acked_seq and writing in_seq" << dendl;
-    state = STATE_CONNECTING_WAIT_ACK_SEQ;
-  }
-  if (reply.tag == CEPH_MSGR_TAG_READY) {
-    ldout(async_msgr->cct, 10) << __func__ << " got CEPH_MSGR_TAG_READY " << dendl;
-    state = STATE_CONNECTING_READY;
+  if (!open_write && is_queued()) {
+    center->create_file_event(cs.fd(), EVENT_WRITABLE, write_handler);
+    open_write = true;
   }
 
-  return 0;
-
- fail:
-  return -1;
-}
-
-ssize_t AsyncConnection::handle_connect_msg(ceph_msg_connect &connect, bufferlist &authorizer_bl,
-                                            bufferlist &authorizer_reply)
-{
-  ssize_t r = 0;
-  ceph_msg_connect_reply reply;
-  bufferlist reply_bl;
-
-  memset(&reply, 0, sizeof(reply));
-  reply.protocol_version = async_msgr->get_proto_version(peer_type, false);
-
-  // mismatch?
-  ldout(async_msgr->cct, 10) << __func__ << " accept my proto " << reply.protocol_version
-                      << ", their proto " << connect.protocol_version << dendl;
-  if (connect.protocol_version != reply.protocol_version) {
-    return _reply_accept(CEPH_MSGR_TAG_BADPROTOVER, connect, reply, authorizer_reply);
-  }
-  // require signatures for cephx?
-  if (connect.authorizer_protocol == CEPH_AUTH_CEPHX) {
-    if (peer_type == CEPH_ENTITY_TYPE_OSD ||
-        peer_type == CEPH_ENTITY_TYPE_MDS) {
-      if (async_msgr->cct->_conf->cephx_require_signatures ||
-          async_msgr->cct->_conf->cephx_cluster_require_signatures) {
-        ldout(async_msgr->cct, 10) << __func__ << " using cephx, requiring MSG_AUTH feature bit for cluster" << dendl;
-        policy.features_required |= CEPH_FEATURE_MSG_AUTH;
-      }
-    } else {
-      if (async_msgr->cct->_conf->cephx_require_signatures ||
-          async_msgr->cct->_conf->cephx_service_require_signatures) {
-        ldout(async_msgr->cct, 10) << __func__ << " using cephx, requiring MSG_AUTH feature bit for service" << dendl;
-        policy.features_required |= CEPH_FEATURE_MSG_AUTH;
-      }
+  if (open_write && !is_queued()) {
+    center->delete_file_event(cs.fd(), EVENT_WRITABLE);
+    open_write = false;
+    if (writeCallback) {
+      center->dispatch_event_external(write_callback_handler);
     }
   }
-  uint64_t feat_missing = policy.features_required & ~(uint64_t)connect.features;
-  if (feat_missing) {
-    ldout(async_msgr->cct, 1) << __func__ << " peer missing required features "
-                        << std::hex << feat_missing << std::dec << dendl;
-    return _reply_accept(CEPH_MSGR_TAG_FEATURES, connect, reply, authorizer_reply);
-  }
-
-  lock.unlock();
-
-  bool authorizer_valid;
-  if (!async_msgr->verify_authorizer(this, peer_type, connect.authorizer_protocol, authorizer_bl,
-                               authorizer_reply, authorizer_valid, session_key) || !authorizer_valid) {
-    lock.lock();
-    ldout(async_msgr->cct,0) << __func__ << ": got bad authorizer" << dendl;
-    session_security.reset();
-    return _reply_accept(CEPH_MSGR_TAG_BADAUTHORIZER, connect, reply, authorizer_reply);
-  }
-
-  // We've verified the authorizer for this AsyncConnection, so set up the session security structure.  PLR
-  ldout(async_msgr->cct, 10) << __func__ << " accept setting up session_security." << dendl;
 
-  // existing?
-  AsyncConnectionRef existing = async_msgr->lookup_conn(peer_addr);
-
-  inject_delay();
+  return outgoing_bl.length();
+}
 
-  lock.lock();
-  if (state != STATE_ACCEPTING_WAIT_CONNECT_MSG_AUTH) {
-    ldout(async_msgr->cct, 1) << __func__ << " state changed while accept, it must be mark_down" << dendl;
-    assert(state == STATE_CLOSED);
-    goto fail;
+void AsyncConnection::inject_delay() {
+  if (async_msgr->cct->_conf->ms_inject_internal_delays) {
+    ldout(async_msgr->cct, 10) << __func__ << " sleep for " <<
+      async_msgr->cct->_conf->ms_inject_internal_delays << dendl;
+    utime_t t;
+    t.set_from_double(async_msgr->cct->_conf->ms_inject_internal_delays);
+    t.sleep();
   }
+}
 
-  if (existing == this)
-    existing = NULL;
-  if (existing) {
-    // There is no possible that existing connection will acquire this
-    // connection's lock
-    existing->lock.lock();  // skip lockdep check (we are locking a second AsyncConnection here)
-
-    if (existing->state == STATE_CLOSED) {
-      ldout(async_msgr->cct, 1) << __func__ << " existing already closed." << dendl;
-      existing->lock.unlock();
-      existing = NULL;
-      goto open;
-    }
-
-    if (existing->replacing) {
-      ldout(async_msgr->cct, 1) << __func__ << " existing racing replace happened while replacing."
-                                << " existing_state=" << get_state_name(existing->state) << dendl;
-      reply.global_seq = existing->peer_global_seq;
-      r = _reply_accept(CEPH_MSGR_TAG_RETRY_GLOBAL, connect, reply, authorizer_reply);
-      existing->lock.unlock();
-      if (r < 0)
-        goto fail;
-      return 0;
-    }
+void AsyncConnection::process() {
+  std::lock_guard<std::mutex> l(lock);
+  last_active = ceph::coarse_mono_clock::now();
+  recv_start_time = ceph::mono_clock::now();
 
-    if (connect.global_seq < existing->peer_global_seq) {
-      ldout(async_msgr->cct, 10) << __func__ << " accept existing " << existing
-                           << ".gseq " << existing->peer_global_seq << " > "
-                           << connect.global_seq << ", RETRY_GLOBAL" << dendl;
-      reply.global_seq = existing->peer_global_seq;  // so we can send it below..
-      existing->lock.unlock();
-      return _reply_accept(CEPH_MSGR_TAG_RETRY_GLOBAL, connect, reply, authorizer_reply);
-    } else {
-      ldout(async_msgr->cct, 10) << __func__ << " accept existing " << existing
-                           << ".gseq " << existing->peer_global_seq
-                           << " <= " << connect.global_seq << ", looks ok" << dendl;
-    }
+  ldout(async_msgr->cct, 20) << __func__ << dendl;
 
-    if (existing->policy.lossy) {
-      ldout(async_msgr->cct, 0) << __func__ << " accept replacing existing (lossy) channel (new one lossy="
-                          << policy.lossy << ")" << dendl;
-      existing->was_session_reset();
-      goto replace;
+  switch (state) {
+    case STATE_NONE: {
+      ldout(async_msgr->cct, 20) << __func__ << " enter none state" << dendl;
+      return;
     }
-
-    ldout(async_msgr->cct, 0) << __func__ << " accept connect_seq " << connect.connect_seq
-                              << " vs existing csq=" << existing->connect_seq << " existing_state="
-                              << get_state_name(existing->state) << dendl;
-
-    if (connect.connect_seq == 0 && existing->connect_seq > 0) {
-      ldout(async_msgr->cct,0) << __func__ << " accept peer reset, then tried to connect to us, replacing" << dendl;
-      // this is a hard reset from peer
-      is_reset_from_peer = true;
-      if (policy.resetcheck)
-        existing->was_session_reset(); // this resets out_queue, msg_ and connect_seq #'s
-      goto replace;
+    case STATE_CLOSED: {
+      ldout(async_msgr->cct, 20) << __func__ << " socket closed" << dendl;
+      return;
     }
+    case STATE_CONNECTING: {
+      ceph_assert(!policy.server);
 
-    if (connect.connect_seq < existing->connect_seq) {
-      // old attempt, or we sent READY but they didn't get it.
-      ldout(async_msgr->cct, 10) << __func__ << " accept existing " << existing << ".cseq "
-                           << existing->connect_seq << " > " << connect.connect_seq
-                           << ", RETRY_SESSION" << dendl;
-      reply.connect_seq = existing->connect_seq + 1;
-      existing->lock.unlock();
-      return _reply_accept(CEPH_MSGR_TAG_RETRY_SESSION, connect, reply, authorizer_reply);
-    }
+      // clear timer (if any) since we are connecting/re-connecting
+      if (last_tick_id) {
+        center->delete_time_event(last_tick_id);
+        last_tick_id = 0;
+      }
 
-    if (connect.connect_seq == existing->connect_seq) {
-      // if the existing connection successfully opened, and/or
-      // subsequently went to standby, then the peer should bump
-      // their connect_seq and retry: this is not a connection race
-      // we need to resolve here.
-      if (existing->state == STATE_OPEN ||
-          existing->state == STATE_STANDBY) {
-        ldout(async_msgr->cct, 10) << __func__ << " accept connection race, existing " << existing
-                             << ".cseq " << existing->connect_seq << " == "
-                             << connect.connect_seq << ", OPEN|STANDBY, RETRY_SESSION" << dendl;
-        reply.connect_seq = existing->connect_seq + 1;
-        existing->lock.unlock();
-        return _reply_accept(CEPH_MSGR_TAG_RETRY_SESSION, connect, reply, authorizer_reply);
+      if (cs) {
+        center->delete_file_event(cs.fd(), EVENT_READABLE | EVENT_WRITABLE);
+        cs.close();
       }
 
-      // connection race?
-      if (peer_addr < async_msgr->get_myaddr() || existing->policy.server) {
-        // incoming wins
-        ldout(async_msgr->cct, 10) << __func__ << " accept connection race, existing " << existing
-                             << ".cseq " << existing->connect_seq << " == " << connect.connect_seq
-                             << ", or we are server, replacing my attempt" << dendl;
-        goto replace;
-      } else {
-        // our existing outgoing wins
-        ldout(async_msgr->cct,10) << __func__ << " accept connection race, existing "
-                            << existing << ".cseq " << existing->connect_seq
-                            << " == " << connect.connect_seq << ", sending WAIT" << dendl;
-        assert(peer_addr > async_msgr->get_myaddr());
-        existing->lock.unlock();
-        return _reply_accept(CEPH_MSGR_TAG_WAIT, connect, reply, authorizer_reply);
+      SocketOptions opts;
+      opts.priority = async_msgr->get_socket_priority();
+      opts.connect_bind_addr = msgr->get_myaddrs().front();
+      ssize_t r = worker->connect(target_addr, opts, &cs);
+      if (r < 0) {
+        protocol->fault();
+        return;
       }
-    }
 
-    assert(connect.connect_seq > existing->connect_seq);
-    assert(connect.global_seq >= existing->peer_global_seq);
-    if (policy.resetcheck &&   // RESETSESSION only used by servers; peers do not reset each other
-        existing->connect_seq == 0) {
-      ldout(async_msgr->cct, 0) << __func__ << " accept we reset (peer sent cseq "
-                          << connect.connect_seq << ", " << existing << ".cseq = "
-                          << existing->connect_seq << "), sending RESETSESSION" << dendl;
-      existing->lock.unlock();
-      return _reply_accept(CEPH_MSGR_TAG_RESETSESSION, connect, reply, authorizer_reply);
+      center->create_file_event(cs.fd(), EVENT_READABLE, read_handler);
+      state = STATE_CONNECTING_RE;
     }
+    case STATE_CONNECTING_RE: {
+      ssize_t r = cs.is_connected();
+      if (r < 0) {
+        ldout(async_msgr->cct, 1) << __func__ << " reconnect failed to "
+                                  << target_addr << dendl;
+        if (r == -ECONNREFUSED) {
+          ldout(async_msgr->cct, 2)
+              << __func__ << " connection refused!" << dendl;
+          dispatch_queue->queue_refused(this);
+        }
+        protocol->fault();
+        return;
+      } else if (r == 0) {
+        ldout(async_msgr->cct, 10)
+            << __func__ << " nonblock connect inprogress" << dendl;
+        if (async_msgr->get_stack()->nonblock_connect_need_writable_event()) {
+          center->create_file_event(cs.fd(), EVENT_WRITABLE,
+                                    read_handler);
+        }
+        logger->tinc(l_msgr_running_recv_time,
+               ceph::mono_clock::now() - recv_start_time);
+        return;
+      }
 
-    // reconnect
-    ldout(async_msgr->cct, 10) << __func__ << " accept peer sent cseq " << connect.connect_seq
-                         << " > " << existing->connect_seq << dendl;
-    goto replace;
-  } // existing
-  else if (!replacing && connect.connect_seq > 0) {
-    // we reset, and they are opening a new session
-    ldout(async_msgr->cct, 0) << __func__ << " accept we reset (peer sent cseq "
-                        << connect.connect_seq << "), sending RESETSESSION" << dendl;
-    return _reply_accept(CEPH_MSGR_TAG_RESETSESSION, connect, reply, authorizer_reply);
-  } else {
-    // new session
-    ldout(async_msgr->cct, 10) << __func__ << " accept new session" << dendl;
-    existing = NULL;
-    goto open;
-  }
-  ceph_abort();
-
- replace:
-  ldout(async_msgr->cct, 10) << __func__ << " accept replacing " << existing << dendl;
-
-  inject_delay();
-  if (existing->policy.lossy) {
-    // disconnect from the Connection
-    ldout(async_msgr->cct, 1) << __func__ << " replacing on lossy channel, failing existing" << dendl;
-    existing->_stop();
-    existing->dispatch_queue->queue_reset(existing.get());
-  } else {
-    assert(can_write == WriteStatus::NOWRITE);
-    existing->write_lock.lock();
-
-    // reset the in_seq if this is a hard reset from peer,
-    // otherwise we respect our original connection's value
-    if (is_reset_from_peer) {
-      existing->is_reset_from_peer = true;
+      center->delete_file_event(cs.fd(), EVENT_WRITABLE);
+      ldout(async_msgr->cct, 10)
+          << __func__ << " connect successfully, ready to send banner" << dendl;
+      state = STATE_CONNECTION_ESTABLISHED;
+      ceph_assert(last_tick_id == 0);
+      // exclude TCP nonblock connect time
+      last_connect_started = ceph::coarse_mono_clock::now();
+      last_tick_id = center->create_time_event(
+        connect_timeout_us, tick_handler);
+      break;
     }
 
-    center->delete_file_event(cs.fd(), EVENT_READABLE|EVENT_WRITABLE);
+    case STATE_ACCEPTING: {
+      center->create_file_event(cs.fd(), EVENT_READABLE, read_handler);
+      state = STATE_CONNECTION_ESTABLISHED;
 
-    if (existing->delay_state) {
-      existing->delay_state->flush();
-      assert(!delay_state);
+      break;
     }
-    existing->reset_recv_state();
-
-    auto temp_cs = std::move(cs);
-    EventCenter *new_center = center;
-    Worker *new_worker = worker;
-    // avoid _stop shutdown replacing socket
-    // queue a reset on the new connection, which we're dumping for the old
-    _stop();
-
-    dispatch_queue->queue_reset(this);
-    ldout(async_msgr->cct, 1) << __func__ << " stop myself to swap existing" << dendl;
-    existing->can_write = WriteStatus::REPLACING;
-    existing->replacing = true;
-    existing->state_offset = 0;
-    // avoid previous thread modify event
-    existing->state = STATE_NONE;
-    // Discard existing prefetch buffer in `recv_buf`
-    existing->recv_start = existing->recv_end = 0;
-    // there shouldn't exist any buffer
-    assert(recv_start == recv_end);
-
-    auto deactivate_existing = std::bind(
-        [existing, new_worker, new_center, connect, reply, authorizer_reply](ConnectedSocket &cs) mutable {
-      // we need to delete time event in original thread
-      {
-        std::lock_guard<std::mutex> l(existing->lock);
-        existing->write_lock.lock();
-        existing->requeue_sent();
-        existing->outcoming_bl.clear();
-        existing->open_write = false;
-        existing->write_lock.unlock();
-        if (existing->state == STATE_NONE) {
-          existing->shutdown_socket();
-          existing->cs = std::move(cs);
-          existing->worker->references--;
-          new_worker->references++;
-          existing->logger = new_worker->get_perf_counter();
-          existing->worker = new_worker;
-          existing->center = new_center;
-          if (existing->delay_state)
-            existing->delay_state->set_center(new_center);
-        } else if (existing->state == STATE_CLOSED) {
-          auto back_to_close = std::bind(
-            [](ConnectedSocket &cs) mutable { cs.close(); }, std::move(cs));
-          new_center->submit_to(
-              new_center->get_id(), std::move(back_to_close), true);
-          return ;
-        } else {
-          ceph_abort();
-        }
-      }
-
-      // Before changing existing->center, it may already exists some events in existing->center's queue.
-      // Then if we mark down `existing`, it will execute in another thread and clean up connection.
-      // Previous event will result in segment fault
-      auto transfer_existing = [existing, connect, reply, authorizer_reply]() mutable {
-        std::lock_guard<std::mutex> l(existing->lock);
-        if (existing->state == STATE_CLOSED)
-          return ;
-        assert(existing->state == STATE_NONE);
-  
-        existing->state = STATE_ACCEPTING_WAIT_CONNECT_MSG;
-        existing->center->create_file_event(existing->cs.fd(), EVENT_READABLE, existing->read_handler);
-        reply.global_seq = existing->peer_global_seq;
-        if (existing->_reply_accept(CEPH_MSGR_TAG_RETRY_GLOBAL, connect, reply, authorizer_reply) < 0) {
-          // handle error
-          existing->fault();
-        }
-      };
-      if (existing->center->in_thread())
-        transfer_existing();
-      else
-        existing->center->submit_to(
-            existing->center->get_id(), std::move(transfer_existing), true);
-    }, std::move(temp_cs));
-
-    existing->center->submit_to(
-        existing->center->get_id(), std::move(deactivate_existing), true);
-    existing->write_lock.unlock();
-    existing->lock.unlock();
-    return 0;
-  }
-  existing->lock.unlock();
-
- open:
-  connect_seq = connect.connect_seq + 1;
-  peer_global_seq = connect.global_seq;
-  ldout(async_msgr->cct, 10) << __func__ << " accept success, connect_seq = "
-                             << connect_seq << " in_seq=" << in_seq << ", sending READY" << dendl;
-
-  int next_state;
-
-  // if it is a hard reset from peer, we don't need a round-trip to negotiate in/out sequence
-  if ((connect.features & CEPH_FEATURE_RECONNECT_SEQ) && !is_reset_from_peer) {
-    reply.tag = CEPH_MSGR_TAG_SEQ;
-    next_state = STATE_ACCEPTING_WAIT_SEQ;
-  } else {
-    reply.tag = CEPH_MSGR_TAG_READY;
-    next_state = STATE_ACCEPTING_READY;
-    discard_requeued_up_to(0);
-    is_reset_from_peer = false;
-    in_seq = 0;
-  }
-
-  // send READY reply
-  reply.features = policy.features_supported;
-  reply.global_seq = async_msgr->get_global_seq();
-  reply.connect_seq = connect_seq;
-  reply.flags = 0;
-  reply.authorizer_len = authorizer_reply.length();
-  if (policy.lossy)
-    reply.flags = reply.flags | CEPH_MSG_CONNECT_LOSSY;
-
-  set_features((uint64_t)reply.features & (uint64_t)connect.features);
-  ldout(async_msgr->cct, 10) << __func__ << " accept features " << get_features() << dendl;
 
-  session_security.reset(
-      get_auth_session_handler(async_msgr->cct, connect.authorizer_protocol,
-                               session_key, get_features()));
-
-  reply_bl.append((char*)&reply, sizeof(reply));
-
-  if (reply.authorizer_len)
-    reply_bl.append(authorizer_reply.c_str(), authorizer_reply.length());
-
-  if (reply.tag == CEPH_MSGR_TAG_SEQ) {
-    uint64_t s = in_seq;
-    reply_bl.append((char*)&s, sizeof(s));
-  }
-
-  lock.unlock();
-  // Because "replacing" will prevent other connections preempt this addr,
-  // it's safe that here we don't acquire Connection's lock
-  r = async_msgr->accept_conn(this);
-
-  inject_delay();
-  
-  lock.lock();
-  replacing = false;
-  if (r < 0) {
-    ldout(async_msgr->cct, 1) << __func__ << " existing race replacing process for addr=" << peer_addr
-                              << " just fail later one(this)" << dendl;
-    goto fail_registered;
-  }
-  if (state != STATE_ACCEPTING_WAIT_CONNECT_MSG_AUTH) {
-    ldout(async_msgr->cct, 1) << __func__ << " state changed while accept_conn, it must be mark_down" << dendl;
-    assert(state == STATE_CLOSED);
-    goto fail_registered;
+    case STATE_CONNECTION_ESTABLISHED: {
+      if (pendingReadLen) {
+        ssize_t r = read(*pendingReadLen, read_buffer, readCallback);
+        if (r <= 0) { // read all bytes, or an error occured
+          pendingReadLen.reset();
+          char *buf_tmp = read_buffer;
+          read_buffer = nullptr;
+          readCallback(buf_tmp, r);
+        }
+	logger->tinc(l_msgr_running_recv_time,
+	    ceph::mono_clock::now() - recv_start_time);
+        return;
+      }
+      break;
+    }
   }
 
-  r = try_send(reply_bl);
-  if (r < 0)
-    goto fail_registered;
-
-  // notify
-  dispatch_queue->queue_accept(this);
-  async_msgr->ms_deliver_handle_fast_accept(this);
-  once_ready = true;
+  protocol->read_event();
 
-  if (r == 0) {
-    state = next_state;
-    ldout(async_msgr->cct, 2) << __func__ << " accept write reply msg done" << dendl;
-  } else {
-    state = STATE_WAIT_SEND;
-    state_after_send = next_state;
-  }
+  logger->tinc(l_msgr_running_recv_time,
+               ceph::mono_clock::now() - recv_start_time);
+}
 
-  return 0;
+bool AsyncConnection::is_connected() {
+  return protocol->is_connected();
+}
 
- fail_registered:
-  ldout(async_msgr->cct, 10) << __func__ << " accept fault after register" << dendl;
-  inject_delay();
+void AsyncConnection::connect(const entity_addrvec_t &addrs, int type,
+                              entity_addr_t &target) {
 
- fail:
-  ldout(async_msgr->cct, 10) << __func__ << " failed to accept." << dendl;
-  return -1;
+  std::lock_guard<std::mutex> l(lock);
+  set_peer_type(type);
+  set_peer_addrs(addrs);
+  policy = msgr->get_policy(type);
+  target_addr = target;
+  _connect();
 }
 
 void AsyncConnection::_connect()
 {
-  ldout(async_msgr->cct, 10) << __func__ << " csq=" << connect_seq << dendl;
+  ldout(async_msgr->cct, 10) << __func__ << dendl;
 
   state = STATE_CONNECTING;
+  protocol->connect();
   // rescheduler connection in order to avoid lock dep
   // may called by external thread(send_message)
   center->dispatch_event_external(read_handler);
 }
 
-void AsyncConnection::accept(ConnectedSocket socket, entity_addr_t &addr)
+void AsyncConnection::accept(ConnectedSocket socket,
+			     const entity_addr_t &listen_addr,
+			     const entity_addr_t &peer_addr)
 {
-  ldout(async_msgr->cct, 10) << __func__ << " sd=" << socket.fd() << dendl;
-  assert(socket.fd() >= 0);
+  ldout(async_msgr->cct, 10) << __func__ << " sd=" << socket.fd()
+			     << " listen_addr " << listen_addr
+			     << " peer_addr " << peer_addr << dendl;
+  ceph_assert(socket.fd() >= 0);
 
   std::lock_guard<std::mutex> l(lock);
   cs = std::move(socket);
-  socket_addr = addr;
+  socket_addr = listen_addr;
+  target_addr = peer_addr; // until we know better
   state = STATE_ACCEPTING;
+  protocol->accept();
   // rescheduler connection in order to avoid lock dep
   center->dispatch_event_external(read_handler);
 }
 
 int AsyncConnection::send_message(Message *m)
 {
-  FUNCTRACE();
+  FUNCTRACE(async_msgr->cct);
   lgeneric_subdout(async_msgr->cct, ms,
-		   1) << "-- " << async_msgr->get_myaddr() << " --> "
-		      << get_peer_addr() << " -- "
+		   1) << "-- " << async_msgr->get_myaddrs() << " --> "
+		      << get_peer_addrs() << " -- "
 		      << *m << " -- " << m << " con "
-		      << m->get_connection().get()
+		      << this
 		      << dendl;
 
+  if (is_blackhole()) {
+    lgeneric_subdout(async_msgr->cct, ms, 0) << __func__ << ceph_entity_type_name(peer_type)
+      << " blackhole " << *m << dendl;
+    m->put();
+    return 0;
+  }
+
   // optimistic think it's ok to encode(actually may broken now)
   if (!m->get_priority())
     m->set_priority(async_msgr->get_default_send_priority());
@@ -1904,15 +539,17 @@ int AsyncConnection::send_message(Message *m)
   m->get_header().src = async_msgr->get_myname();
   m->set_connection(this);
 
+#if defined(WITH_EVENTTRACE)
   if (m->get_type() == CEPH_MSG_OSD_OP)
     OID_EVENT_TRACE_WITH_MSG(m, "SEND_MSG_OSD_OP_BEGIN", true);
   else if (m->get_type() == CEPH_MSG_OSD_OPREPLY)
     OID_EVENT_TRACE_WITH_MSG(m, "SEND_MSG_OSD_OPREPLY_BEGIN", true);
+#endif
 
-  if (async_msgr->get_myaddr() == get_peer_addr()) { //loopback connection
+  if (is_loopback) { //loopback connection
     ldout(async_msgr->cct, 20) << __func__ << " " << *m << " local" << dendl;
     std::lock_guard<std::mutex> l(write_lock);
-    if (can_write != WriteStatus::CLOSED) {
+    if (protocol->is_connected()) {
       dispatch_queue->local_delivery(m, m->get_priority());
     } else {
       ldout(async_msgr->cct, 10) << __func__ << " loopback connection closed."
@@ -1922,427 +559,79 @@ int AsyncConnection::send_message(Message *m)
     return 0;
   }
 
-  last_active = ceph::coarse_mono_clock::now();
   // we don't want to consider local message here, it's too lightweight which
   // may disturb users
   logger->inc(l_msgr_send_messages);
 
-  bufferlist bl;
-  uint64_t f = get_features();
-
-  // TODO: Currently not all messages supports reencode like MOSDMap, so here
-  // only let fast dispatch support messages prepare message
-  bool can_fast_prepare = async_msgr->ms_can_fast_dispatch(m);
-  if (can_fast_prepare)
-    prepare_send_message(f, m, bl);
-
-  std::lock_guard<std::mutex> l(write_lock);
-  // "features" changes will change the payload encoding
-  if (can_fast_prepare && (can_write == WriteStatus::NOWRITE || get_features() != f)) {
-    // ensure the correctness of message encoding
-    bl.clear();
-    m->get_payload().clear();
-    ldout(async_msgr->cct, 5) << __func__ << " clear encoded buffer previous "
-                              << f << " != " << get_features() << dendl;
-  }
-  if (can_write == WriteStatus::CLOSED) {
-    ldout(async_msgr->cct, 10) << __func__ << " connection closed."
-                               << " Drop message " << m << dendl;
-    m->put();
-  } else {
-    m->trace.event("async enqueueing message");
-    out_q[m->get_priority()].emplace_back(std::move(bl), m);
-    ldout(async_msgr->cct, 15) << __func__ << " inline write is denied, reschedule m=" << m << dendl;
-    if (can_write != WriteStatus::REPLACING)
-      center->dispatch_event_external(write_handler);
-  }
+  protocol->send_message(m);
   return 0;
 }
 
-void AsyncConnection::requeue_sent()
+entity_addr_t AsyncConnection::_infer_target_addr(const entity_addrvec_t& av)
 {
-  if (sent.empty())
-    return;
-
-  list<pair<bufferlist, Message*> >& rq = out_q[CEPH_MSG_PRIO_HIGHEST];
-  while (!sent.empty()) {
-    Message* m = sent.back();
-    sent.pop_back();
-    ldout(async_msgr->cct, 10) << __func__ << " " << *m << " for resend "
-                               << " (" << m->get_seq() << ")" << dendl;
-    rq.push_front(make_pair(bufferlist(), m));
-    out_seq--;
-  }
-}
-
-void AsyncConnection::discard_requeued_up_to(uint64_t seq)
-{
-  ldout(async_msgr->cct, 10) << __func__ << " " << seq << dendl;
-  std::lock_guard<std::mutex> l(write_lock);
-  if (out_q.count(CEPH_MSG_PRIO_HIGHEST) == 0)
-    return;
-  list<pair<bufferlist, Message*> >& rq = out_q[CEPH_MSG_PRIO_HIGHEST];
-  while (!rq.empty()) {
-    pair<bufferlist, Message*> p = rq.front();
-    if (p.second->get_seq() == 0 || p.second->get_seq() > seq)
-      break;
-    ldout(async_msgr->cct, 10) << __func__ << " " << *(p.second) << " for resend seq " << p.second->get_seq()
-                         << " <= " << seq << ", discarding" << dendl;
-    p.second->put();
-    rq.pop_front();
-    out_seq++;
-  }
-  if (rq.empty())
-    out_q.erase(CEPH_MSG_PRIO_HIGHEST);
-}
-
-/*
- * Tears down the AsyncConnection's message queues, and removes them from the DispatchQueue
- * Must hold write_lock prior to calling.
- */
-void AsyncConnection::discard_out_queue()
-{
-  ldout(async_msgr->cct, 10) << __func__ << " started" << dendl;
-
-  for (list<Message*>::iterator p = sent.begin(); p != sent.end(); ++p) {
-    ldout(async_msgr->cct, 20) << __func__ << " discard " << *p << dendl;
-    (*p)->put();
-  }
-  sent.clear();
-  for (map<int, list<pair<bufferlist, Message*> > >::iterator p = out_q.begin(); p != out_q.end(); ++p)
-    for (list<pair<bufferlist, Message*> >::iterator r = p->second.begin(); r != p->second.end(); ++r) {
-      ldout(async_msgr->cct, 20) << __func__ << " discard " << r->second << dendl;
-      r->second->put();
+  // pick the first addr of the same address family as socket_addr.  it could be
+  // an any: or v2: addr, we don't care.  it should not be a v1 addr.
+  for (auto& i : av.v) {
+    if (i.is_legacy()) {
+      continue;
+    }
+    if (i.get_family() == socket_addr.get_family()) {
+      ldout(async_msgr->cct,10) << __func__ << " " << av << " -> " << i << dendl;
+      return i;
     }
-  out_q.clear();
-}
-
-int AsyncConnection::randomize_out_seq()
-{
-  if (get_features() & CEPH_FEATURE_MSG_AUTH) {
-    // Set out_seq to a random value, so CRC won't be predictable.   Don't bother checking seq_error
-    // here.  We'll check it on the call.  PLR
-    uint64_t rand_seq;
-    int seq_error = get_random_bytes((char *)&rand_seq, sizeof(rand_seq));
-    rand_seq &= SEQ_MASK;
-    lsubdout(async_msgr->cct, ms, 10) << __func__ << " randomize_out_seq " << rand_seq << dendl;
-    out_seq = rand_seq;
-    return seq_error;
-  } else {
-    // previously, seq #'s always started at 0.
-    out_seq = 0;
-    return 0;
   }
+  ldout(async_msgr->cct,10) << __func__ << " " << av << " -> nothing to match "
+			    << socket_addr << dendl;
+  return {};
 }
 
 void AsyncConnection::fault()
 {
-  if (state == STATE_CLOSED || state == STATE_NONE) {
-    ldout(async_msgr->cct, 10) << __func__ << " connection is already closed" << dendl;
-    return ;
-  }
-
-  if (policy.lossy && !(state >= STATE_CONNECTING && state < STATE_CONNECTING_READY)) {
-    ldout(async_msgr->cct, 1) << __func__ << " on lossy channel, failing" << dendl;
-    _stop();
-    dispatch_queue->queue_reset(this);
-    return ;
-  }
-
-  write_lock.lock();
-  can_write = WriteStatus::NOWRITE;
   shutdown_socket();
   open_write = false;
 
   // queue delayed items immediately
   if (delay_state)
     delay_state->flush();
-  // requeue sent items
-  requeue_sent();
+
   recv_start = recv_end = 0;
   state_offset = 0;
-  replacing = false;
-  is_reset_from_peer = false;
-  outcoming_bl.clear();
-  if (!once_ready && !is_queued() &&
-      state >=STATE_ACCEPTING && state <= STATE_ACCEPTING_WAIT_CONNECT_MSG_AUTH) {
-    ldout(async_msgr->cct, 10) << __func__ << " with nothing to send and in the half "
-                              << " accept state just closed" << dendl;
-    write_lock.unlock();
-    _stop();
-    dispatch_queue->queue_reset(this);
-    return ;
-  }
-  reset_recv_state();
-  if (policy.standby && !is_queued() && state != STATE_WAIT) {
-    ldout(async_msgr->cct, 10) << __func__ << " with nothing to send, going to standby" << dendl;
-    state = STATE_STANDBY;
-    write_lock.unlock();
-    return;
-  }
-
-  write_lock.unlock();
-  if (!(state >= STATE_CONNECTING && state < STATE_CONNECTING_READY) &&
-      state != STATE_WAIT) { // STATE_WAIT is coming from STATE_CONNECTING_*
-    // policy maybe empty when state is in accept
-    if (policy.server) {
-      ldout(async_msgr->cct, 0) << __func__ << " server, going to standby" << dendl;
-      state = STATE_STANDBY;
-    } else {
-      ldout(async_msgr->cct, 0) << __func__ << " initiating reconnect" << dendl;
-      connect_seq++;
-      state = STATE_CONNECTING;
-    }
-    backoff = utime_t();
-    center->dispatch_event_external(read_handler);
-  } else {
-    if (state == STATE_WAIT) {
-      backoff.set_from_double(async_msgr->cct->_conf->ms_max_backoff);
-    } else if (backoff == utime_t()) {
-      backoff.set_from_double(async_msgr->cct->_conf->ms_initial_backoff);
-    } else {
-      backoff += backoff;
-      if (backoff > async_msgr->cct->_conf->ms_max_backoff)
-        backoff.set_from_double(async_msgr->cct->_conf->ms_max_backoff);
-    }
-
-    state = STATE_CONNECTING;
-    ldout(async_msgr->cct, 10) << __func__ << " waiting " << backoff << dendl;
-    // woke up again;
-    register_time_events.insert(center->create_time_event(
-            backoff.to_nsec()/1000, wakeup_handler));
-  }
-}
-
-void AsyncConnection::was_session_reset()
-{
-  ldout(async_msgr->cct,10) << __func__ << " started" << dendl;
-  std::lock_guard<std::mutex> l(write_lock);
-  if (delay_state)
-    delay_state->discard();
-  dispatch_queue->discard_queue(conn_id);
-  discard_out_queue();
-  // note: we need to clear outcoming_bl here, but was_session_reset may be
-  // called by other thread, so let caller clear this itself!
-  // outcoming_bl.clear();
-
-  dispatch_queue->queue_remote_reset(this);
-
-  if (randomize_out_seq()) {
-    ldout(async_msgr->cct, 15) << __func__ << " could not get random bytes to set seq number for session reset; set seq number to " << out_seq << dendl;
-  }
-
-  in_seq = 0;
-  connect_seq = 0;
-  // it's safe to directly set 0, double locked
-  ack_left = 0;
-  once_ready = false;
-  can_write = WriteStatus::NOWRITE;
+  outgoing_bl.clear();
 }
 
-void AsyncConnection::_stop()
-{
-  if (state == STATE_CLOSED)
-    return ;
-
-  if (delay_state)
-    delay_state->flush();
-
-  ldout(async_msgr->cct, 2) << __func__ << dendl;
-  std::lock_guard<std::mutex> l(write_lock);
-
-  reset_recv_state();
+void AsyncConnection::_stop() {
+  writeCallback.reset();
   dispatch_queue->discard_queue(conn_id);
-  discard_out_queue();
   async_msgr->unregister_conn(this);
   worker->release_worker();
 
   state = STATE_CLOSED;
   open_write = false;
-  can_write = WriteStatus::CLOSED;
+
   state_offset = 0;
   // Make sure in-queue events will been processed
   center->dispatch_event_external(EventCallbackRef(new C_clean_handler(this)));
 }
 
-void AsyncConnection::prepare_send_message(uint64_t features, Message *m, bufferlist &bl)
-{
-  ldout(async_msgr->cct, 20) << __func__ << " m" << " " << *m << dendl;
-
-  // associate message with Connection (for benefit of encode_payload)
-  if (m->empty_payload())
-    ldout(async_msgr->cct, 20) << __func__ << " encoding features "
-                               << features << " " << m << " " << *m << dendl;
-  else
-    ldout(async_msgr->cct, 20) << __func__ << " half-reencoding features "
-                               << features << " " << m << " " << *m << dendl;
-
-  // encode and copy out of *m
-  m->encode(features, msgr->crcflags);
-
-  bl.append(m->get_payload());
-  bl.append(m->get_middle());
-  bl.append(m->get_data());
-}
-
-ssize_t AsyncConnection::write_message(Message *m, bufferlist& bl, bool more)
-{
-  FUNCTRACE();
-  assert(center->in_thread());
-  m->set_seq(++out_seq);
-
-  if (msgr->crcflags & MSG_CRC_HEADER)
-    m->calc_header_crc();
-
-  ceph_msg_header& header = m->get_header();
-  ceph_msg_footer& footer = m->get_footer();
-
-  // TODO: let sign_message could be reentry?
-  // Now that we have all the crcs calculated, handle the
-  // digital signature for the message, if the AsyncConnection has session
-  // security set up.  Some session security options do not
-  // actually calculate and check the signature, but they should
-  // handle the calls to sign_message and check_signature.  PLR
-  if (session_security.get() == NULL) {
-    ldout(async_msgr->cct, 20) << __func__ << " no session security" << dendl;
-  } else {
-    if (session_security->sign_message(m)) {
-      ldout(async_msgr->cct, 20) << __func__ << " failed to sign m="
-                                 << m << "): sig = " << footer.sig << dendl;
-    } else {
-      ldout(async_msgr->cct, 20) << __func__ << " signed m=" << m
-                                 << "): sig = " << footer.sig << dendl;
-    }
-  }
-  
-  unsigned original_bl_len = outcoming_bl.length();
-
-  outcoming_bl.append(CEPH_MSGR_TAG_MSG);
-
-  if (has_feature(CEPH_FEATURE_NOSRCADDR)) {
-    outcoming_bl.append((char*)&header, sizeof(header));
-  } else {
-    ceph_msg_header_old oldheader;
-    memcpy(&oldheader, &header, sizeof(header));
-    oldheader.src.name = header.src;
-    oldheader.src.addr = get_peer_addr();
-    oldheader.orig_src = oldheader.src;
-    oldheader.reserved = header.reserved;
-    oldheader.crc = ceph_crc32c(0, (unsigned char*)&oldheader,
-                                sizeof(oldheader) - sizeof(oldheader.crc));
-    outcoming_bl.append((char*)&oldheader, sizeof(oldheader));
-  }
-
-  ldout(async_msgr->cct, 20) << __func__ << " sending message type=" << header.type
-                             << " src " << entity_name_t(header.src)
-                             << " front=" << header.front_len
-                             << " data=" << header.data_len
-                             << " off " << header.data_off << dendl;
-
-  if ((bl.length() <= ASYNC_COALESCE_THRESHOLD) && (bl.buffers().size() > 1)) {
-    for (const auto &pb : bl.buffers()) {
-      outcoming_bl.append((char*)pb.c_str(), pb.length());
-    }
-  } else {
-    outcoming_bl.claim_append(bl);  
-  }
-
-  // send footer; if receiver doesn't support signatures, use the old footer format
-  ceph_msg_footer_old old_footer;
-  if (has_feature(CEPH_FEATURE_MSG_AUTH)) {
-    outcoming_bl.append((char*)&footer, sizeof(footer));
-  } else {
-    if (msgr->crcflags & MSG_CRC_HEADER) {
-      old_footer.front_crc = footer.front_crc;
-      old_footer.middle_crc = footer.middle_crc;
-      old_footer.data_crc = footer.data_crc;
-    } else {
-       old_footer.front_crc = old_footer.middle_crc = 0;
-    }
-    old_footer.data_crc = msgr->crcflags & MSG_CRC_DATA ? footer.data_crc : 0;
-    old_footer.flags = footer.flags;
-    outcoming_bl.append((char*)&old_footer, sizeof(old_footer));
-  }
-
-  m->trace.event("async writing message");
-  ldout(async_msgr->cct, 20) << __func__ << " sending " << m->get_seq()
-                             << " " << m << dendl;
-  ssize_t total_send_size = outcoming_bl.length();
-  ssize_t rc = _try_send(more);
-  if (rc < 0) {
-    ldout(async_msgr->cct, 1) << __func__ << " error sending " << m << ", "
-                              << cpp_strerror(rc) << dendl;
-  } else if (rc == 0) {
-    logger->inc(l_msgr_send_bytes, total_send_size - original_bl_len);
-    ldout(async_msgr->cct, 10) << __func__ << " sending " << m << " done." << dendl;
-  } else {
-    logger->inc(l_msgr_send_bytes, total_send_size - outcoming_bl.length());
-    ldout(async_msgr->cct, 10) << __func__ << " sending " << m << " continuely." << dendl;
-  }
-  if (m->get_type() == CEPH_MSG_OSD_OP)
-    OID_EVENT_TRACE_WITH_MSG(m, "SEND_MSG_OSD_OP_END", false);
-  else if (m->get_type() == CEPH_MSG_OSD_OPREPLY)
-    OID_EVENT_TRACE_WITH_MSG(m, "SEND_MSG_OSD_OPREPLY_END", false);
-  m->put();
-
-  return rc;
+bool AsyncConnection::is_queued() const {
+  return outgoing_bl.length();
 }
 
-void AsyncConnection::reset_recv_state()
-{
-  // clean up state internal variables and states
-  if (state >= STATE_CONNECTING_SEND_CONNECT_MSG &&
-      state <= STATE_CONNECTING_READY) {
-    delete authorizer;
-    authorizer = NULL;
-    got_bad_auth = false;
-  }
-
-  if (state > STATE_OPEN_MESSAGE_THROTTLE_MESSAGE &&
-      state <= STATE_OPEN_MESSAGE_READ_FOOTER_AND_DISPATCH
-      && policy.throttler_messages) {
-    ldout(async_msgr->cct, 10) << __func__ << " releasing " << 1
-                               << " message to policy throttler "
-                               << policy.throttler_messages->get_current() << "/"
-                               << policy.throttler_messages->get_max() << dendl;
-    policy.throttler_messages->put();
+void AsyncConnection::shutdown_socket() {
+  for (auto &&t : register_time_events) center->delete_time_event(t);
+  register_time_events.clear();
+  if (last_tick_id) {
+    center->delete_time_event(last_tick_id);
+    last_tick_id = 0;
   }
-  if (state > STATE_OPEN_MESSAGE_THROTTLE_BYTES &&
-      state <= STATE_OPEN_MESSAGE_READ_FOOTER_AND_DISPATCH) {
-    if (policy.throttler_bytes) {
-      ldout(async_msgr->cct, 10) << __func__ << " releasing " << cur_msg_size
-                                 << " bytes to policy throttler "
-                                 << policy.throttler_bytes->get_current() << "/"
-                                 << policy.throttler_bytes->get_max() << dendl;
-      policy.throttler_bytes->put(cur_msg_size);
-    }
-  }
-  if (state > STATE_OPEN_MESSAGE_THROTTLE_DISPATCH_QUEUE &&
-      state <= STATE_OPEN_MESSAGE_READ_FOOTER_AND_DISPATCH) {
-    ldout(async_msgr->cct, 10) << __func__ << " releasing " << cur_msg_size
-                               << " bytes to dispatch_queue throttler "
-                               << dispatch_queue->dispatch_throttler.get_current() << "/"
-                               << dispatch_queue->dispatch_throttler.get_max() << dendl;
-    dispatch_queue->dispatch_throttle_release(cur_msg_size);
-  }
-}
-
-void AsyncConnection::handle_ack(uint64_t seq)
-{
-  ldout(async_msgr->cct, 15) << __func__ << " got ack seq " << seq << dendl;
-  // trim sent list
-  std::lock_guard<std::mutex> l(write_lock);
-  while (!sent.empty() && sent.front()->get_seq() <= seq) {
-    Message* m = sent.front();
-    sent.pop_front();
-    ldout(async_msgr->cct, 10) << __func__ << " got ack seq "
-                               << seq << " >= " << m->get_seq() << " on "
-                               << m << " " << *m << dendl;
-    m->put();
+  if (cs) {
+    center->delete_file_event(cs.fd(), EVENT_READABLE | EVENT_WRITABLE);
+    cs.shutdown();
+    cs.close();
   }
 }
 
-void AsyncConnection::DelayedDelivery::do_request(int id)
+void AsyncConnection::DelayedDelivery::do_request(uint64_t id)
 {
   Message *m = nullptr;
   {
@@ -2352,15 +641,7 @@ void AsyncConnection::DelayedDelivery::do_request(int id)
       return ;
     if (delay_queue.empty())
       return ;
-    utime_t release = delay_queue.front().first;
-    m = delay_queue.front().second;
-    string delay_msg_type = msgr->cct->_conf->ms_inject_delay_msg_type;
-    utime_t now = ceph_clock_now();
-    if ((release > now &&
-        (delay_msg_type.empty() || m->get_type_name() == delay_msg_type))) {
-      utime_t t = release - now;
-      t.sleep();
-    }
+    m = delay_queue.front();
     delay_queue.pop_front();
   }
   if (msgr->ms_can_fast_dispatch(m)) {
@@ -2370,13 +651,33 @@ void AsyncConnection::DelayedDelivery::do_request(int id)
   }
 }
 
+void AsyncConnection::DelayedDelivery::discard() {
+  stop_dispatch = true;
+  center->submit_to(center->get_id(),
+                    [this]() mutable {
+                      std::lock_guard<std::mutex> l(delay_lock);
+                      while (!delay_queue.empty()) {
+                        Message *m = delay_queue.front();
+                        dispatch_queue->dispatch_throttle_release(
+                            m->get_dispatch_throttle_size());
+                        m->put();
+                        delay_queue.pop_front();
+                      }
+                      for (auto i : register_time_events)
+                        center->delete_time_event(i);
+                      register_time_events.clear();
+                      stop_dispatch = false;
+                    },
+                    true);
+}
+
 void AsyncConnection::DelayedDelivery::flush() {
   stop_dispatch = true;
   center->submit_to(
       center->get_id(), [this] () mutable {
     std::lock_guard<std::mutex> l(delay_lock);
     while (!delay_queue.empty()) {
-      Message *m = delay_queue.front().second;
+      Message *m = delay_queue.front();
       if (msgr->ms_can_fast_dispatch(m)) {
         dispatch_queue->fast_dispatch(m);
       } else {
@@ -2393,130 +694,56 @@ void AsyncConnection::DelayedDelivery::flush() {
 
 void AsyncConnection::send_keepalive()
 {
-  ldout(async_msgr->cct, 10) << __func__ << dendl;
-  std::lock_guard<std::mutex> l(write_lock);
-  if (can_write != WriteStatus::CLOSED) {
-    keepalive = true;
-    center->dispatch_event_external(write_handler);
-  }
+  protocol->send_keepalive();
 }
 
 void AsyncConnection::mark_down()
 {
   ldout(async_msgr->cct, 1) << __func__ << dendl;
   std::lock_guard<std::mutex> l(lock);
-  _stop();
-}
-
-void AsyncConnection::_append_keepalive_or_ack(bool ack, utime_t *tp)
-{
-  ldout(async_msgr->cct, 10) << __func__ << dendl;
-  if (ack) {
-    assert(tp);
-    struct ceph_timespec ts;
-    tp->encode_timeval(&ts);
-    outcoming_bl.append(CEPH_MSGR_TAG_KEEPALIVE2_ACK);
-    outcoming_bl.append((char*)&ts, sizeof(ts));
-  } else if (has_feature(CEPH_FEATURE_MSGR_KEEPALIVE2)) {
-    struct ceph_timespec ts;
-    utime_t t = ceph_clock_now();
-    t.encode_timeval(&ts);
-    outcoming_bl.append(CEPH_MSGR_TAG_KEEPALIVE2);
-    outcoming_bl.append((char*)&ts, sizeof(ts));
-  } else {
-    outcoming_bl.append(CEPH_MSGR_TAG_KEEPALIVE);
-  }
+  protocol->stop();
 }
 
 void AsyncConnection::handle_write()
 {
   ldout(async_msgr->cct, 10) << __func__ << dendl;
-  ssize_t r = 0;
+  protocol->write_event();
+}
 
+void AsyncConnection::handle_write_callback() {
+  std::lock_guard<std::mutex> l(lock);
+  last_active = ceph::coarse_mono_clock::now();
+  recv_start_time = ceph::mono_clock::now();
   write_lock.lock();
-  if (can_write == WriteStatus::CANWRITE) {
-    if (keepalive) {
-      _append_keepalive_or_ack();
-      keepalive = false;
-    }
-
-    auto start = ceph::mono_clock::now();
-    bool more;
-    do {
-      bufferlist data;
-      Message *m = _get_next_outgoing(&data);
-      if (!m)
-        break;
-
-      if (!policy.lossy) {
-        // put on sent list
-        sent.push_back(m);
-        m->get();
-      }
-      more = _has_next_outgoing();
-      write_lock.unlock();
-
-      // send_message or requeue messages may not encode message
-      if (!data.length())
-        prepare_send_message(get_features(), m, data);
-
-      r = write_message(m, data, more);
-      if (r < 0) {
-        ldout(async_msgr->cct, 1) << __func__ << " send msg failed" << dendl;
-        goto fail;
-      }
-      write_lock.lock();
-      if (r > 0)
-        break;
-    } while (can_write == WriteStatus::CANWRITE);
+  if (writeCallback) {
+    auto callback = *writeCallback;
+    writeCallback.reset();
     write_lock.unlock();
-
-    uint64_t left = ack_left;
-    if (left) {
-      ceph_le64 s;
-      s = in_seq;
-      outcoming_bl.append(CEPH_MSGR_TAG_ACK);
-      outcoming_bl.append((char*)&s, sizeof(s));
-      ldout(async_msgr->cct, 10) << __func__ << " try send msg ack, acked " << left << " messages" << dendl;
-      ack_left -= left;
-      left = ack_left;
-      r = _try_send(left);
-    } else if (is_queued()) {
-      r = _try_send();
-    }
-
-    logger->tinc(l_msgr_running_send_time, ceph::mono_clock::now() - start);
-    if (r < 0) {
-      ldout(async_msgr->cct, 1) << __func__ << " send msg failed" << dendl;
-      goto fail;
-    }
-  } else {
-    write_lock.unlock();
-    lock.lock();
-    write_lock.lock();
-    if (state == STATE_STANDBY && !policy.server && is_queued()) {
-      ldout(async_msgr->cct, 10) << __func__ << " policy.server is false" << dendl;
-      _connect();
-    } else if (cs && state != STATE_NONE && state != STATE_CONNECTING && state != STATE_CONNECTING_RE && state != STATE_CLOSED) {
-      r = _try_send();
-      if (r < 0) {
-        ldout(async_msgr->cct, 1) << __func__ << " send outcoming bl failed" << dendl;
-        write_lock.unlock();
-        fault();
-        lock.unlock();
-        return ;
-      }
-    }
-    write_lock.unlock();
-    lock.unlock();
+    callback(0);
+    return;
   }
+  write_lock.unlock();
+}
 
-  return ;
-
- fail:
+void AsyncConnection::stop(bool queue_reset) {
   lock.lock();
-  fault();
+  bool need_queue_reset = (state != STATE_CLOSED) && queue_reset;
+  protocol->stop();
   lock.unlock();
+  if (need_queue_reset) dispatch_queue->queue_reset(this);
+}
+
+void AsyncConnection::cleanup() {
+  shutdown_socket();
+  delete read_handler;
+  delete write_handler;
+  delete write_callback_handler;
+  delete wakeup_handler;
+  delete tick_handler;
+  if (delay_state) {
+    delete delay_state;
+    delay_state = NULL;
+  }
 }
 
 void AsyncConnection::wakeup_from(uint64_t id)
@@ -2531,16 +758,32 @@ void AsyncConnection::tick(uint64_t id)
 {
   auto now = ceph::coarse_mono_clock::now();
   ldout(async_msgr->cct, 20) << __func__ << " last_id=" << last_tick_id
-                             << " last_active" << last_active << dendl;
+                             << " last_active=" << last_active << dendl;
   std::lock_guard<std::mutex> l(lock);
   last_tick_id = 0;
-  auto idle_period = std::chrono::duration_cast<std::chrono::microseconds>(now - last_active).count();
-  if (inactive_timeout_us < (uint64_t)idle_period) {
-    ldout(async_msgr->cct, 1) << __func__ << " idle(" << idle_period << ") more than "
-                              << inactive_timeout_us
-                              << " us, mark self fault." << dendl;
-    fault();
-  } else if (is_connected()) {
-    last_tick_id = center->create_time_event(inactive_timeout_us, tick_handler);
+  if (!is_connected()) {
+    if (connect_timeout_us <=
+        (uint64_t)std::chrono::duration_cast<std::chrono::microseconds>
+          (now - last_connect_started).count()) {
+      ldout(async_msgr->cct, 1) << __func__ << " see no progress in more than "
+                                << connect_timeout_us
+                                << " us during connecting, fault."
+                                << dendl;
+      protocol->fault();
+    } else {
+      last_tick_id = center->create_time_event(connect_timeout_us, tick_handler);
+    }
+  } else {
+    auto idle_period = std::chrono::duration_cast<std::chrono::microseconds>
+      (now - last_active).count();
+    if (inactive_timeout_us < (uint64_t)idle_period) {
+      ldout(async_msgr->cct, 1) << __func__ << " idle (" << idle_period
+                                << ") for more than " << inactive_timeout_us
+                                << " us, fault."
+                                << dendl;
+      protocol->fault();
+    } else {
+      last_tick_id = center->create_time_event(inactive_timeout_us, tick_handler);
+    }
   }
 }
diff --git a/src/msg/async/AsyncConnection.h b/src/msg/async/AsyncConnection.h
index 005b7c13ab2..20d095dfafb 100644
--- a/src/msg/async/AsyncConnection.h
+++ b/src/msg/async/AsyncConnection.h
@@ -1,4 +1,4 @@
-// -*- mode:C++; tab-width:8; c-basic-offset:2; indent-tabs-mode:t -*- 
+// -*- mode:C++; tab-width:8; c-basic-offset:2; indent-tabs-mode:t -*-
 // vim: ts=8 sw=2 smarttab
 /*
  * Ceph - scalable distributed file system
@@ -19,12 +19,12 @@
 
 #include <atomic>
 #include <pthread.h>
-#include <signal.h>
 #include <climits>
 #include <list>
 #include <mutex>
 #include <map>
-using namespace std;
+#include <functional>
+#include <optional>
 
 #include "auth/AuthSessionHandler.h"
 #include "common/ceph_time.h"
@@ -37,7 +37,9 @@ using namespace std;
 #include "Stack.h"
 
 class AsyncMessenger;
+class DispatchQueue;
 class Worker;
+class Protocol;
 
 static const int ASYNC_IOV_MAX = (IOV_MAX >= 1024 ? IOV_MAX / 4 : IOV_MAX);
 
@@ -49,89 +51,22 @@ static const int ASYNC_IOV_MAX = (IOV_MAX >= 1024 ? IOV_MAX / 4 : IOV_MAX);
  * sequence, try to reconnect peer endpoint.
  */
 class AsyncConnection : public Connection {
-
+  ssize_t read(unsigned len, char *buffer,
+               std::function<void(char *, ssize_t)> callback);
+  ssize_t read_until(unsigned needed, char *p);
   ssize_t read_bulk(char *buf, unsigned len);
-  ssize_t do_sendmsg(struct msghdr &msg, unsigned len, bool more);
-  ssize_t try_send(bufferlist &bl, bool more=false) {
-    std::lock_guard<std::mutex> l(write_lock);
-    outcoming_bl.claim_append(bl);
-    return _try_send(more);
-  }
+
+  ssize_t write(bufferlist &bl, std::function<void(ssize_t)> callback,
+                bool more=false);
   ssize_t _try_send(bool more=false);
-  ssize_t _send(Message *m);
-  void prepare_send_message(uint64_t features, Message *m, bufferlist &bl);
-  ssize_t read_until(unsigned needed, char *p);
-  ssize_t _process_connection();
+
   void _connect();
   void _stop();
-  int handle_connect_reply(ceph_msg_connect &connect, ceph_msg_connect_reply &r);
-  ssize_t handle_connect_msg(ceph_msg_connect &m, bufferlist &aubl, bufferlist &bl);
-  void was_session_reset();
   void fault();
-  void discard_out_queue();
-  void discard_requeued_up_to(uint64_t seq);
-  void requeue_sent();
-  int randomize_out_seq();
-  void handle_ack(uint64_t seq);
-  void _append_keepalive_or_ack(bool ack=false, utime_t *t=NULL);
-  ssize_t write_message(Message *m, bufferlist& bl, bool more);
   void inject_delay();
-  ssize_t _reply_accept(char tag, ceph_msg_connect &connect, ceph_msg_connect_reply &reply,
-                    bufferlist &authorizer_reply) {
-    bufferlist reply_bl;
-    reply.tag = tag;
-    reply.features = ((uint64_t)connect.features & policy.features_supported) | policy.features_required;
-    reply.authorizer_len = authorizer_reply.length();
-    reply_bl.append((char*)&reply, sizeof(reply));
-    if (reply.authorizer_len) {
-      reply_bl.append(authorizer_reply.c_str(), authorizer_reply.length());
-    }
-    ssize_t r = try_send(reply_bl);
-    if (r < 0) {
-      inject_delay();
-      return -1;
-    }
 
-    state = STATE_ACCEPTING_WAIT_CONNECT_MSG;
-    return 0;
-  }
-  bool is_queued() const {
-    return !out_q.empty() || outcoming_bl.length();
-  }
-  void shutdown_socket() {
-    for (auto &&t : register_time_events)
-      center->delete_time_event(t);
-    register_time_events.clear();
-    if (last_tick_id) {
-      center->delete_time_event(last_tick_id);
-      last_tick_id = 0;
-    }
-    if (cs) {
-      center->delete_file_event(cs.fd(), EVENT_READABLE|EVENT_WRITABLE);
-      cs.shutdown();
-      cs.close();
-    }
-  }
-  Message *_get_next_outgoing(bufferlist *bl) {
-    Message *m = 0;
-    while (!m && !out_q.empty()) {
-      map<int, list<pair<bufferlist, Message*> > >::reverse_iterator it = out_q.rbegin();
-      if (!it->second.empty()) {
-        list<pair<bufferlist, Message*> >::iterator p = it->second.begin();
-        m = p->second;
-        if (bl)
-          bl->swap(p->first);
-        it->second.erase(p);
-      }
-      if (it->second.empty())
-        out_q.erase(it->first);
-    }
-    return m;
-  }
-  bool _has_next_outgoing() const {
-    return !out_q.empty();
-  }
-  void reset_recv_state();
+  bool is_queued() const;
+  void shutdown_socket();
 
    /**
    * The DelayedDelivery is for injecting delays into Message delivery off
@@ -141,7 +76,7 @@ class AsyncConnection : public Connection {
    */
   class DelayedDelivery : public EventCallback {
     std::set<uint64_t> register_time_events; // need to delete it if stop
-    std::deque<std::pair<utime_t, Message*> > delay_queue;
+    std::deque<Message*> delay_queue;
     std::mutex delay_lock;
     AsyncMessenger *msgr;
     EventCenter *center;
@@ -155,56 +90,41 @@ class AsyncConnection : public Connection {
       : msgr(omsgr), center(c), dispatch_queue(q), conn_id(cid),
         stop_dispatch(false) { }
     ~DelayedDelivery() override {
-      assert(register_time_events.empty());
-      assert(delay_queue.empty());
+      ceph_assert(register_time_events.empty());
+      ceph_assert(delay_queue.empty());
     }
     void set_center(EventCenter *c) { center = c; }
-    void do_request(int id) override;
-    void queue(double delay_period, utime_t release, Message *m) {
+    void do_request(uint64_t id) override;
+    void queue(double delay_period, Message *m) {
       std::lock_guard<std::mutex> l(delay_lock);
-      delay_queue.push_back(std::make_pair(release, m));
+      delay_queue.push_back(m);
       register_time_events.insert(center->create_time_event(delay_period*1000000, this));
     }
-    void discard() {
-      stop_dispatch = true;
-      center->submit_to(center->get_id(), [this] () mutable {
-        std::lock_guard<std::mutex> l(delay_lock);
-        while (!delay_queue.empty()) {
-          Message *m = delay_queue.front().second;
-          dispatch_queue->dispatch_throttle_release(m->get_dispatch_throttle_size());
-          m->put();
-          delay_queue.pop_front();
-        }
-        for (auto i : register_time_events)
-          center->delete_time_event(i);
-        register_time_events.clear();
-        stop_dispatch = false;
-      }, true);
-    }
+    void discard();
     bool ready() const { return !stop_dispatch && delay_queue.empty() && register_time_events.empty(); }
     void flush();
   } *delay_state;
 
- public:
-  AsyncConnection(CephContext *cct, AsyncMessenger *m, DispatchQueue *q, Worker *w);
+private:
+  FRIEND_MAKE_REF(AsyncConnection);
+  AsyncConnection(CephContext *cct, AsyncMessenger *m, DispatchQueue *q,
+		  Worker *w, bool is_msgr2, bool local);
   ~AsyncConnection() override;
+  bool unregistered = false;
+public:
   void maybe_start_delay_thread();
 
   ostream& _conn_prefix(std::ostream *_dout);
 
-  bool is_connected() override {
-    return can_write.load() == WriteStatus::CANWRITE;
-  }
+  bool is_connected() override;
 
   // Only call when AsyncConnection first construct
-  void connect(const entity_addr_t& addr, int type) {
-    set_peer_type(type);
-    set_peer_addr(addr);
-    policy = msgr->get_policy(type);
-    _connect();
-  }
+  void connect(const entity_addrvec_t& addrs, int type, entity_addr_t& target);
+
   // Only call when AsyncConnection first construct
-  void accept(ConnectedSocket socket, entity_addr_t &addr);
+  void accept(ConnectedSocket socket,
+	      const entity_addr_t &listen_addr,
+	      const entity_addr_t &peer_addr);
   int send_message(Message *m) override;
 
   void send_keepalive() override;
@@ -213,197 +133,120 @@ class AsyncConnection : public Connection {
     std::lock_guard<std::mutex> l(lock);
     policy.lossy = true;
   }
-  
+
+  entity_addr_t get_peer_socket_addr() const override {
+    return target_addr;
+  }
+
+  int get_con_mode() const override;
+
+  bool is_unregistered() const {
+    return unregistered;
+  }
+
+  void unregister() {
+    unregistered = true;
+  }
+
  private:
   enum {
     STATE_NONE,
-    STATE_OPEN,
-    STATE_OPEN_KEEPALIVE2,
-    STATE_OPEN_KEEPALIVE2_ACK,
-    STATE_OPEN_TAG_ACK,
-    STATE_OPEN_MESSAGE_HEADER,
-    STATE_OPEN_MESSAGE_THROTTLE_MESSAGE,
-    STATE_OPEN_MESSAGE_THROTTLE_BYTES,
-    STATE_OPEN_MESSAGE_THROTTLE_DISPATCH_QUEUE,
-    STATE_OPEN_MESSAGE_READ_FRONT,
-    STATE_OPEN_MESSAGE_READ_MIDDLE,
-    STATE_OPEN_MESSAGE_READ_DATA_PREPARE,
-    STATE_OPEN_MESSAGE_READ_DATA,
-    STATE_OPEN_MESSAGE_READ_FOOTER_AND_DISPATCH,
-    STATE_OPEN_TAG_CLOSE,
-    STATE_WAIT_SEND,
     STATE_CONNECTING,
     STATE_CONNECTING_RE,
-    STATE_CONNECTING_WAIT_BANNER_AND_IDENTIFY,
-    STATE_CONNECTING_SEND_CONNECT_MSG,
-    STATE_CONNECTING_WAIT_CONNECT_REPLY,
-    STATE_CONNECTING_WAIT_CONNECT_REPLY_AUTH,
-    STATE_CONNECTING_WAIT_ACK_SEQ,
-    STATE_CONNECTING_READY,
     STATE_ACCEPTING,
-    STATE_ACCEPTING_WAIT_BANNER_ADDR,
-    STATE_ACCEPTING_WAIT_CONNECT_MSG,
-    STATE_ACCEPTING_WAIT_CONNECT_MSG_AUTH,
-    STATE_ACCEPTING_WAIT_SEQ,
-    STATE_ACCEPTING_READY,
-    STATE_STANDBY,
-    STATE_CLOSED,
-    STATE_WAIT,       // just wait for racing connection
+    STATE_CONNECTION_ESTABLISHED,
+    STATE_CLOSED
   };
 
-  static const int TCP_PREFETCH_MIN_SIZE;
+  static const uint32_t TCP_PREFETCH_MIN_SIZE;
   static const char *get_state_name(int state) {
       const char* const statenames[] = {"STATE_NONE",
-                                        "STATE_OPEN",
-                                        "STATE_OPEN_KEEPALIVE2",
-                                        "STATE_OPEN_KEEPALIVE2_ACK",
-                                        "STATE_OPEN_TAG_ACK",
-                                        "STATE_OPEN_MESSAGE_HEADER",
-                                        "STATE_OPEN_MESSAGE_THROTTLE_MESSAGE",
-                                        "STATE_OPEN_MESSAGE_THROTTLE_BYTES",
-                                        "STATE_OPEN_MESSAGE_THROTTLE_DISPATCH_QUEUE",
-                                        "STATE_OPEN_MESSAGE_READ_FRONT",
-                                        "STATE_OPEN_MESSAGE_READ_MIDDLE",
-                                        "STATE_OPEN_MESSAGE_READ_DATA_PREPARE",
-                                        "STATE_OPEN_MESSAGE_READ_DATA",
-                                        "STATE_OPEN_MESSAGE_READ_FOOTER_AND_DISPATCH",
-                                        "STATE_OPEN_TAG_CLOSE",
-                                        "STATE_WAIT_SEND",
                                         "STATE_CONNECTING",
                                         "STATE_CONNECTING_RE",
-                                        "STATE_CONNECTING_WAIT_BANNER_AND_IDENTIFY",
-                                        "STATE_CONNECTING_SEND_CONNECT_MSG",
-                                        "STATE_CONNECTING_WAIT_CONNECT_REPLY",
-                                        "STATE_CONNECTING_WAIT_CONNECT_REPLY_AUTH",
-                                        "STATE_CONNECTING_WAIT_ACK_SEQ",
-                                        "STATE_CONNECTING_READY",
                                         "STATE_ACCEPTING",
-                                        "STATE_ACCEPTING_WAIT_BANNER_ADDR",
-                                        "STATE_ACCEPTING_WAIT_CONNECT_MSG",
-                                        "STATE_ACCEPTING_WAIT_CONNECT_MSG_AUTH",
-                                        "STATE_ACCEPTING_WAIT_SEQ",
-                                        "STATE_ACCEPTING_READY",
-                                        "STATE_STANDBY",
-                                        "STATE_CLOSED",
-                                        "STATE_WAIT"};
+                                        "STATE_CONNECTION_ESTABLISHED",
+                                        "STATE_CLOSED"};
       return statenames[state];
   }
 
   AsyncMessenger *async_msgr;
   uint64_t conn_id;
   PerfCounters *logger;
-  int global_seq;
-  __u32 connect_seq, peer_global_seq;
-  std::atomic<uint64_t> out_seq{0};
-  std::atomic<uint64_t> ack_left{0}, in_seq{0};
   int state;
-  int state_after_send;
   ConnectedSocket cs;
   int port;
+public:
   Messenger::Policy policy;
+private:
 
   DispatchQueue *dispatch_queue;
 
   // lockfree, only used in own thread
-  bufferlist outcoming_bl;
+  bufferlist outgoing_bl;
   bool open_write = false;
 
   std::mutex write_lock;
-  enum class WriteStatus {
-    NOWRITE,
-    REPLACING,
-    CANWRITE,
-    CLOSED
-  };
-  std::atomic<WriteStatus> can_write;
-  list<Message*> sent; // the first bufferlist need to inject seq
-  map<int, list<pair<bufferlist, Message*> > > out_q;  // priority queue for outbound msgs
-  bool keepalive;
 
   std::mutex lock;
-  utime_t backoff;         // backoff time
   EventCallbackRef read_handler;
   EventCallbackRef write_handler;
+  EventCallbackRef write_callback_handler;
   EventCallbackRef wakeup_handler;
   EventCallbackRef tick_handler;
-  struct iovec msgvec[ASYNC_IOV_MAX];
   char *recv_buf;
   uint32_t recv_max_prefetch;
   uint32_t recv_start;
   uint32_t recv_end;
   set<uint64_t> register_time_events; // need to delete it if stop
+  ceph::coarse_mono_clock::time_point last_connect_started;
   ceph::coarse_mono_clock::time_point last_active;
+  ceph::mono_clock::time_point recv_start_time;
   uint64_t last_tick_id = 0;
+  const uint64_t connect_timeout_us;
   const uint64_t inactive_timeout_us;
 
   // Tis section are temp variables used by state transition
 
-  // Open state
-  utime_t recv_stamp;
-  utime_t throttle_stamp;
-  unsigned msg_left;
-  uint64_t cur_msg_size;
-  ceph_msg_header current_header;
-  bufferlist data_buf;
-  bufferlist::iterator data_blp;
-  bufferlist front, middle, data;
-  ceph_msg_connect connect_msg;
-  // Connecting state
-  bool got_bad_auth;
-  AuthAuthorizer *authorizer;
-  bufferlist authorizer_buf;
-  ceph_msg_connect_reply connect_reply;
   // Accepting state
-  entity_addr_t socket_addr;
-  CryptoKey session_key;
-  bool replacing;    // when replacing process happened, we will reply connect
-                     // side with RETRY tag and accept side will clear replaced
-                     // connection. So when connect side reissue connect_msg,
-                     // there won't exists conflicting connection so we use
-                     // "replacing" to skip RESETSESSION to avoid detect wrong
-                     // presentation
-  bool is_reset_from_peer;
-  bool once_ready;
-
-  // used only for local state, it will be overwrite when state transition
-  char *state_buffer;
+  bool msgr2 = false;
+  entity_addr_t socket_addr;  ///< local socket addr
+  entity_addr_t target_addr;  ///< which of the peer_addrs we're connecting to (as clienet) or should reconnect to (as peer)
+
+  entity_addr_t _infer_target_addr(const entity_addrvec_t& av);
+
   // used only by "read_until"
   uint64_t state_offset;
   Worker *worker;
   EventCenter *center;
-  ceph::shared_ptr<AuthSessionHandler> session_security;
+
+  std::unique_ptr<Protocol> protocol;
+
+  std::optional<std::function<void(ssize_t)>> writeCallback;
+  std::function<void(char *, ssize_t)> readCallback;
+  std::optional<unsigned> pendingReadLen;
+  char *read_buffer;
 
  public:
   // used by eventcallback
   void handle_write();
+  void handle_write_callback();
   void process();
   void wakeup_from(uint64_t id);
   void tick(uint64_t id);
   void local_deliver();
-  void stop(bool queue_reset) {
-    lock.lock();
-    bool need_queue_reset = (state != STATE_CLOSED) && queue_reset;
-    _stop();
-    lock.unlock();
-    if (need_queue_reset)
-      dispatch_queue->queue_reset(this);
-  }
-  void cleanup() {
-    shutdown_socket();
-    delete read_handler;
-    delete write_handler;
-    delete wakeup_handler;
-    delete tick_handler;
-    if (delay_state) {
-      delete delay_state;
-      delay_state = NULL;
-    }
-  }
+  void stop(bool queue_reset);
+  void cleanup();
   PerfCounters *get_perf_counter() {
     return logger;
   }
+
+  bool is_msgr2() const override;
+
+  friend class Protocol;
+  friend class ProtocolV1;
+  friend class ProtocolV2;
 }; /* AsyncConnection */
 
-typedef boost::intrusive_ptr<AsyncConnection> AsyncConnectionRef;
+using AsyncConnectionRef = ceph::ref_t<AsyncConnection>;
 
 #endif
diff --git a/src/msg/async/AsyncMessenger.cc b/src/msg/async/AsyncMessenger.cc
index 1913e8f4c6e..54e4a69f63a 100644
--- a/src/msg/async/AsyncMessenger.cc
+++ b/src/msg/async/AsyncMessenger.cc
@@ -1,4 +1,4 @@
-// -*- mode:C++; tab-width:8; c-basic-offset:2; indent-tabs-mode:t -*- 
+// -*- mode:C++; tab-width:8; c-basic-offset:2; indent-tabs-mode:t -*-
 // vim: ts=8 sw=2 smarttab
 /*
  * Ceph - scalable distributed file system
@@ -33,7 +33,7 @@
 #undef dout_prefix
 #define dout_prefix _prefix(_dout, this)
 static ostream& _prefix(std::ostream *_dout, AsyncMessenger *m) {
-  return *_dout << "-- " << m->get_myaddr() << " ";
+  return *_dout << "-- " << m->get_myaddrs() << " ";
 }
 
 static ostream& _prefix(std::ostream *_dout, Processor *p) {
@@ -50,7 +50,7 @@ class Processor::C_processor_accept : public EventCallback {
 
  public:
   explicit C_processor_accept(Processor *p): pro(p) {}
-  void do_request(int id) override {
+  void do_request(uint64_t id) override {
     pro->accept();
   }
 };
@@ -59,91 +59,93 @@ Processor::Processor(AsyncMessenger *r, Worker *w, CephContext *c)
   : msgr(r), net(c), worker(w),
     listen_handler(new C_processor_accept(this)) {}
 
-int Processor::bind(const entity_addr_t &bind_addr,
+int Processor::bind(const entity_addrvec_t &bind_addrs,
 		    const set<int>& avoid_ports,
-		    entity_addr_t* bound_addr)
+		    entity_addrvec_t* bound_addrs)
 {
-  const md_config_t *conf = msgr->cct->_conf;
-  // bind to a socket
-  ldout(msgr->cct, 10) << __func__ << dendl;
-
-  int family;
-  switch (bind_addr.get_family()) {
-    case AF_INET:
-    case AF_INET6:
-      family = bind_addr.get_family();
-      break;
-
-    default:
-      // bind_addr is empty
-      family = conf->ms_bind_ipv6 ? AF_INET6 : AF_INET;
-  }
+  const auto& conf = msgr->cct->_conf;
+  // bind to socket(s)
+  ldout(msgr->cct, 10) << __func__ << " " << bind_addrs << dendl;
 
   SocketOptions opts;
   opts.nodelay = msgr->cct->_conf->ms_tcp_nodelay;
   opts.rcbuf_size = msgr->cct->_conf->ms_tcp_rcvbuf;
 
-  // use whatever user specified (if anything)
-  entity_addr_t listen_addr = bind_addr;
-  if (listen_addr.get_type() == entity_addr_t::TYPE_NONE) {
-    listen_addr.set_type(entity_addr_t::TYPE_LEGACY);
-  }
-  listen_addr.set_family(family);
+  listen_sockets.resize(bind_addrs.v.size());
+  *bound_addrs = bind_addrs;
 
-  /* bind to port */
-  int r = -1;
+  for (unsigned k = 0; k < bind_addrs.v.size(); ++k) {
+    auto& listen_addr = bound_addrs->v[k];
 
-  for (int i = 0; i < conf->ms_bind_retry_count; i++) {
-    if (i > 0) {
-      lderr(msgr->cct) << __func__ << " was unable to bind. Trying again in "
-                       << conf->ms_bind_retry_delay << " seconds " << dendl;
-      sleep(conf->ms_bind_retry_delay);
-    }
+    /* bind to port */
+    int r = -1;
 
-    if (listen_addr.get_port()) {
-      worker->center.submit_to(worker->center.get_id(), [this, &listen_addr, &opts, &r]() {
-        r = worker->listen(listen_addr, opts, &listen_socket);
-      }, false);
-      if (r < 0) {
-        lderr(msgr->cct) << __func__ << " unable to bind to " << listen_addr
-                         << ": " << cpp_strerror(r) << dendl;
-        continue;
+    for (int i = 0; i < conf->ms_bind_retry_count; i++) {
+      if (i > 0) {
+	lderr(msgr->cct) << __func__ << " was unable to bind. Trying again in "
+			 << conf->ms_bind_retry_delay << " seconds " << dendl;
+	sleep(conf->ms_bind_retry_delay);
       }
-    } else {
-      // try a range of ports
-      for (int port = msgr->cct->_conf->ms_bind_port_min; port <= msgr->cct->_conf->ms_bind_port_max; port++) {
-        if (avoid_ports.count(port))
-          continue;
-
-        listen_addr.set_port(port);
-        worker->center.submit_to(worker->center.get_id(), [this, &listen_addr, &opts, &r]() {
-          r = worker->listen(listen_addr, opts, &listen_socket);
-        }, false);
-        if (r == 0)
-          break;
+
+      if (listen_addr.get_port()) {
+	worker->center.submit_to(
+	  worker->center.get_id(),
+	  [this, k, &listen_addr, &opts, &r]() {
+	    r = worker->listen(listen_addr, k, opts, &listen_sockets[k]);
+	  }, false);
+	if (r < 0) {
+	  lderr(msgr->cct) << __func__ << " unable to bind to " << listen_addr
+			   << ": " << cpp_strerror(r) << dendl;
+	  continue;
+	}
+      } else {
+	// try a range of ports
+	for (int port = msgr->cct->_conf->ms_bind_port_min;
+	     port <= msgr->cct->_conf->ms_bind_port_max;
+	     port++) {
+	  if (avoid_ports.count(port))
+	    continue;
+
+	  listen_addr.set_port(port);
+	  worker->center.submit_to(
+	    worker->center.get_id(),
+	    [this, k, &listen_addr, &opts, &r]() {
+	      r = worker->listen(listen_addr, k, opts, &listen_sockets[k]);
+	    }, false);
+	  if (r == 0)
+	    break;
+	}
+	if (r < 0) {
+	  lderr(msgr->cct) << __func__ << " unable to bind to " << listen_addr
+			   << " on any port in range "
+			   << msgr->cct->_conf->ms_bind_port_min
+			   << "-" << msgr->cct->_conf->ms_bind_port_max << ": "
+			   << cpp_strerror(r) << dendl;
+	  listen_addr.set_port(0); // Clear port before retry, otherwise we shall fail again.
+	  continue;
+	}
+	ldout(msgr->cct, 10) << __func__ << " bound on random port "
+			     << listen_addr << dendl;
       }
-      if (r < 0) {
-        lderr(msgr->cct) << __func__ << " unable to bind to " << listen_addr
-                         << " on any port in range " << msgr->cct->_conf->ms_bind_port_min
-                         << "-" << msgr->cct->_conf->ms_bind_port_max << ": "
-                         << cpp_strerror(r) << dendl;
-        listen_addr.set_port(0); // Clear port before retry, otherwise we shall fail again.
-        continue;
+      if (r == 0) {
+	break;
       }
-      ldout(msgr->cct, 10) << __func__ << " bound on random port " << listen_addr << dendl;
     }
-    if (r == 0)
-      break;
-  }
-  // It seems that binding completely failed, return with that exit status
-  if (r < 0) {
-    lderr(msgr->cct) << __func__ << " was unable to bind after " << conf->ms_bind_retry_count
-                     << " attempts: " << cpp_strerror(r) << dendl;
-    return r;
+
+    // It seems that binding completely failed, return with that exit status
+    if (r < 0) {
+      lderr(msgr->cct) << __func__ << " was unable to bind after "
+		       << conf->ms_bind_retry_count
+		       << " attempts: " << cpp_strerror(r) << dendl;
+      for (unsigned j = 0; j < k; ++j) {
+	// clean up previous bind
+	listen_sockets[j].abort_accept();
+      }
+      return r;
+    }
   }
 
-  ldout(msgr->cct, 10) << __func__ << " bound to " << listen_addr << dendl;
-  *bound_addr = listen_addr;
+  ldout(msgr->cct, 10) << __func__ << " bound to " << *bound_addrs << dendl;
   return 0;
 }
 
@@ -152,48 +154,79 @@ void Processor::start()
   ldout(msgr->cct, 1) << __func__ << dendl;
 
   // start thread
-  if (listen_socket) {
-    worker->center.submit_to(worker->center.get_id(), [this]() {
-      worker->center.create_file_event(listen_socket.fd(), EVENT_READABLE, listen_handler); }, false);
-  }
+  worker->center.submit_to(worker->center.get_id(), [this]() {
+      for (auto& listen_socket : listen_sockets) {
+	if (listen_socket) {
+          if (listen_socket.fd() == -1) {
+            ldout(msgr->cct, 1) << __func__ 
+                << " Error: processor restart after listen_socket.fd closed. " 
+                << this << dendl;
+            return;
+          }
+	  worker->center.create_file_event(listen_socket.fd(), EVENT_READABLE,
+					   listen_handler); }
+      }
+    }, false);
 }
 
 void Processor::accept()
 {
-  ldout(msgr->cct, 10) << __func__ << " listen_fd=" << listen_socket.fd() << dendl;
   SocketOptions opts;
   opts.nodelay = msgr->cct->_conf->ms_tcp_nodelay;
   opts.rcbuf_size = msgr->cct->_conf->ms_tcp_rcvbuf;
   opts.priority = msgr->get_socket_priority();
-  while (true) {
-    entity_addr_t addr;
-    ConnectedSocket cli_socket;
-    Worker *w = worker;
-    if (!msgr->get_stack()->support_local_listen_table())
-      w = msgr->get_stack()->get_worker();
-    int r = listen_socket.accept(&cli_socket, opts, &addr, w);
-    if (r == 0) {
-      ldout(msgr->cct, 10) << __func__ << " accepted incoming on sd " << cli_socket.fd() << dendl;
-
-      msgr->add_accept(w, std::move(cli_socket), addr);
-      continue;
-    } else {
-      if (r == -EINTR) {
-        continue;
-      } else if (r == -EAGAIN) {
-        break;
-      } else if (r == -EMFILE || r == -ENFILE) {
-        lderr(msgr->cct) << __func__ << " open file descriptions limit reached sd = " << listen_socket.fd()
-                         << " errno " << r << " " << cpp_strerror(r) << dendl;
-        break;
-      } else if (r == -ECONNABORTED) {
-        ldout(msgr->cct, 0) << __func__ << " it was closed because of rst arrived sd = " << listen_socket.fd()
-                            << " errno " << r << " " << cpp_strerror(r) << dendl;
-        continue;
+
+  for (auto& listen_socket : listen_sockets) {
+    ldout(msgr->cct, 10) << __func__ << " listen_fd=" << listen_socket.fd()
+			 << dendl;
+    unsigned accept_error_num = 0;
+
+    while (true) {
+      entity_addr_t addr;
+      ConnectedSocket cli_socket;
+      Worker *w = worker;
+      if (!msgr->get_stack()->support_local_listen_table())
+	w = msgr->get_stack()->get_worker();
+      else
+	++w->references;
+      int r = listen_socket.accept(&cli_socket, opts, &addr, w);
+      if (r == 0) {
+	ldout(msgr->cct, 10) << __func__ << " accepted incoming on sd "
+			     << cli_socket.fd() << dendl;
+
+	msgr->add_accept(
+	  w, std::move(cli_socket),
+	  msgr->get_myaddrs().v[listen_socket.get_addr_slot()],
+	  addr);
+	accept_error_num = 0;
+	continue;
       } else {
-        lderr(msgr->cct) << __func__ << " no incoming connection?"
-                         << " errno " << r << " " << cpp_strerror(r) << dendl;
-        break;
+	--w->references;
+	if (r == -EINTR) {
+	  continue;
+	} else if (r == -EAGAIN) {
+	  break;
+	} else if (r == -EMFILE || r == -ENFILE) {
+	  lderr(msgr->cct) << __func__ << " open file descriptions limit reached sd = " << listen_socket.fd()
+			   << " errno " << r << " " << cpp_strerror(r) << dendl;
+	  if (++accept_error_num > msgr->cct->_conf->ms_max_accept_failures) {
+	    lderr(msgr->cct) << "Proccessor accept has encountered enough error numbers, just do ceph_abort()." << dendl;
+	    ceph_abort();
+	  }
+	  continue;
+	} else if (r == -ECONNABORTED) {
+	  ldout(msgr->cct, 0) << __func__ << " it was closed because of rst arrived sd = " << listen_socket.fd()
+			      << " errno " << r << " " << cpp_strerror(r) << dendl;
+	  continue;
+	} else {
+	  lderr(msgr->cct) << __func__ << " no incoming connection?"
+			   << " errno " << r << " " << cpp_strerror(r) << dendl;
+	  if (++accept_error_num > msgr->cct->_conf->ms_max_accept_failures) {
+	    lderr(msgr->cct) << "Proccessor accept has encountered enough error numbers, just do ceph_abort()." << dendl;
+	    ceph_abort();
+	  }
+	  continue;
+	}
       }
     }
   }
@@ -203,12 +236,14 @@ void Processor::stop()
 {
   ldout(msgr->cct,10) << __func__ << dendl;
 
-  if (listen_socket) {
-    worker->center.submit_to(worker->center.get_id(), [this]() {
-      worker->center.delete_file_event(listen_socket.fd(), EVENT_READABLE);
-      listen_socket.abort_accept();
+  worker->center.submit_to(worker->center.get_id(), [this]() {
+      for (auto& listen_socket : listen_sockets) {
+	if (listen_socket) {
+	  worker->center.delete_file_event(listen_socket.fd(), EVENT_READABLE);
+	  listen_socket.abort_accept();
+	}
+      }
     }, false);
-  }
 }
 
 
@@ -216,7 +251,7 @@ struct StackSingleton {
   CephContext *cct;
   std::shared_ptr<NetworkStack> stack;
 
-  StackSingleton(CephContext *c): cct(c) {}
+  explicit StackSingleton(CephContext *c): cct(c) {}
   void ready(std::string &type) {
     if (!stack)
       stack = NetworkStack::create(cct, type);
@@ -232,7 +267,7 @@ class C_handle_reap : public EventCallback {
 
   public:
   explicit C_handle_reap(AsyncMessenger *m): msgr(m) {}
-  void do_request(int id) override {
+  void do_request(uint64_t id) override {
     // judge whether is a time event
     msgr->reap_dead();
   }
@@ -244,12 +279,9 @@ class C_handle_reap : public EventCallback {
 
 AsyncMessenger::AsyncMessenger(CephContext *cct, entity_name_t name,
                                const std::string &type, string mname, uint64_t _nonce)
-  : SimplePolicyMessenger(cct, name,mname, _nonce),
+  : SimplePolicyMessenger(cct, name),
     dispatch_queue(cct, this, mname),
-    lock("AsyncMessenger::lock"),
-    nonce(_nonce), need_addr(true), did_bind(false),
-    global_seq(0), deleted_lock("AsyncMessenger::deleted_lock"),
-    cluster_protocol(0), stopped(true)
+    nonce(_nonce)
 {
   std::string transport_type = "posix";
   if (type.find("rdma") != std::string::npos)
@@ -257,14 +289,14 @@ AsyncMessenger::AsyncMessenger(CephContext *cct, entity_name_t name,
   else if (type.find("dpdk") != std::string::npos)
     transport_type = "dpdk";
 
-  ceph_spin_init(&global_seq_lock);
-  StackSingleton *single;
-  cct->lookup_or_create_singleton_object<StackSingleton>(single, "AsyncMessenger::NetworkStack::"+transport_type);
+  auto single = &cct->lookup_or_create_singleton_object<StackSingleton>(
+    "AsyncMessenger::NetworkStack::" + transport_type, true, cct);
   single->ready(transport_type);
   stack = single->stack.get();
   stack->start();
   local_worker = stack->get_worker();
-  local_connection = new AsyncConnection(cct, this, &dispatch_queue, local_worker);
+  local_connection = ceph::make_ref<AsyncConnection>(cct, this, &dispatch_queue,
+					 local_worker, true, true);
   init_local_connection();
   reap_handler = new C_handle_reap(this);
   unsigned processor_num = 1;
@@ -281,26 +313,25 @@ AsyncMessenger::AsyncMessenger(CephContext *cct, entity_name_t name,
 AsyncMessenger::~AsyncMessenger()
 {
   delete reap_handler;
-  assert(!did_bind); // either we didn't bind or we shut down the Processor
-  local_connection->mark_down();
+  ceph_assert(!did_bind); // either we didn't bind or we shut down the Processor
   for (auto &&p : processors)
     delete p;
 }
 
 void AsyncMessenger::ready()
 {
-  ldout(cct,10) << __func__ << " " << get_myaddr() << dendl;
+  ldout(cct,10) << __func__ << " " << get_myaddrs() << dendl;
 
   stack->ready();
   if (pending_bind) {
-    int err = bind(pending_bind_addr);
+    int err = bindv(pending_bind_addrs);
     if (err) {
       lderr(cct) << __func__ << " postponed bind failed" << dendl;
       ceph_abort();
     }
   }
 
-  Mutex::Locker l(lock);
+  std::lock_guard l{lock};
   for (auto &&p : processors)
     p->start();
   dispatch_queue.start();
@@ -308,52 +339,69 @@ void AsyncMessenger::ready()
 
 int AsyncMessenger::shutdown()
 {
-  ldout(cct,10) << __func__ << " " << get_myaddr() << dendl;
+  ldout(cct,10) << __func__ << " " << get_myaddrs() << dendl;
 
   // done!  clean up.
   for (auto &&p : processors)
     p->stop();
   mark_down_all();
   // break ref cycles on the loopback connection
-  local_connection->set_priv(NULL);
+  local_connection->clear_priv();
+  local_connection->mark_down();
   did_bind = false;
-  lock.Lock();
-  stop_cond.Signal();
+  lock.lock();
+  stop_cond.notify_all();
   stopped = true;
-  lock.Unlock();
+  lock.unlock();
   stack->drain();
   return 0;
 }
 
-
 int AsyncMessenger::bind(const entity_addr_t &bind_addr)
 {
-  lock.Lock();
+  ldout(cct,10) << __func__ << " " << bind_addr << dendl;
+  // old bind() can take entity_addr_t(). new bindv() can take a
+  // 0.0.0.0-like address but needs type and family to be set.
+  auto a = bind_addr;
+  if (a == entity_addr_t()) {
+    a.set_type(entity_addr_t::TYPE_LEGACY);
+    if (cct->_conf->ms_bind_ipv6) {
+      a.set_family(AF_INET6);
+    } else {
+      a.set_family(AF_INET);
+    }
+  }
+  return bindv(entity_addrvec_t(a));
+}
+
+int AsyncMessenger::bindv(const entity_addrvec_t &bind_addrs)
+{
+  lock.lock();
 
   if (!pending_bind && started) {
     ldout(cct,10) << __func__ << " already started" << dendl;
-    lock.Unlock();
+    lock.unlock();
     return -1;
   }
 
-  ldout(cct,10) << __func__ << " bind " << bind_addr << dendl;
+  ldout(cct,10) << __func__ << " " << bind_addrs << dendl;
 
   if (!stack->is_ready()) {
     ldout(cct, 10) << __func__ << " Network Stack is not ready for bind yet - postponed" << dendl;
-    pending_bind_addr = bind_addr;
+    pending_bind_addrs = bind_addrs;
     pending_bind = true;
-    lock.Unlock();
+    lock.unlock();
     return 0;
   }
 
-  lock.Unlock();
+  lock.unlock();
 
   // bind to a socket
   set<int> avoid_ports;
-  entity_addr_t bound_addr;
+  entity_addrvec_t bound_addrs;
   unsigned i = 0;
   for (auto &&p : processors) {
-    int r = p->bind(bind_addr, avoid_ports, &bound_addr);
+    int r = p->bind(bind_addrs, avoid_ports, &bound_addrs);
     if (r) {
       // Note: this is related to local tcp listen table problem.
       // Posix(default kernel implementation) backend shares listen table
@@ -364,19 +412,19 @@ int AsyncMessenger::bind(const entity_addr_t &bind_addr)
       // it, like port is used case. But if the first worker successfully to bind
       // but the second worker failed, it's not expected and we need to assert
       // here
-      assert(i == 0);
+      ceph_assert(i == 0);
       return r;
     }
     ++i;
   }
-  _finish_bind(bind_addr, bound_addr);
+  _finish_bind(bind_addrs, bound_addrs);
   return 0;
 }
 
 int AsyncMessenger::rebind(const set<int>& avoid_ports)
 {
   ldout(cct,1) << __func__ << " rebind avoid " << avoid_ports << dendl;
-  assert(did_bind);
+  ceph_assert(did_bind);
 
   for (auto &&p : processors)
     p->stop();
@@ -385,25 +433,27 @@ int AsyncMessenger::rebind(const set<int>& avoid_ports)
   // adjust the nonce; we want our entity_addr_t to be truly unique.
   nonce += 1000000;
   ldout(cct, 10) << __func__ << " new nonce " << nonce
-		 << " and inst " << get_myinst() << dendl;
+		 << " and addr " << get_myaddrs() << dendl;
 
-  entity_addr_t bound_addr;
-  entity_addr_t bind_addr = get_myaddr();
-  bind_addr.set_port(0);
+  entity_addrvec_t bound_addrs;
+  entity_addrvec_t bind_addrs = get_myaddrs();
   set<int> new_avoid(avoid_ports);
-  new_avoid.insert(bind_addr.get_port());
-  ldout(cct, 10) << __func__ << " will try " << bind_addr
+  for (auto& a : bind_addrs.v) {
+    new_avoid.insert(a.get_port());
+    a.set_port(0);
+  }
+  ldout(cct, 10) << __func__ << " will try " << bind_addrs
 		 << " and avoid ports " << new_avoid << dendl;
   unsigned i = 0;
   for (auto &&p : processors) {
-    int r = p->bind(bind_addr, avoid_ports, &bound_addr);
+    int r = p->bind(bind_addrs, avoid_ports, &bound_addrs);
     if (r) {
-      assert(i == 0);
+      ceph_assert(i == 0);
       return r;
     }
     ++i;
   }
-  _finish_bind(bind_addr, bound_addr);
+  _finish_bind(bind_addrs, bound_addrs);
   for (auto &&p : processors) {
     p->start();
   }
@@ -414,9 +464,8 @@ int AsyncMessenger::client_bind(const entity_addr_t &bind_addr)
 {
   if (!cct->_conf->ms_bind_before_connect)
     return 0;
-  Mutex::Locker l(lock);
+  std::lock_guard l{lock};
   if (did_bind) {
-    assert(my_inst.addr == bind_addr);
     return 0;
   }
   if (started) {
@@ -425,63 +474,87 @@ int AsyncMessenger::client_bind(const entity_addr_t &bind_addr)
   }
   ldout(cct, 10) << __func__ << " " << bind_addr << dendl;
 
-  set_myaddr(bind_addr);
+  set_myaddrs(entity_addrvec_t(bind_addr));
   return 0;
 }
 
-void AsyncMessenger::_finish_bind(const entity_addr_t& bind_addr,
-				  const entity_addr_t& listen_addr)
+void AsyncMessenger::_finish_bind(const entity_addrvec_t& bind_addrs,
+				  const entity_addrvec_t& listen_addrs)
 {
-  set_myaddr(bind_addr);
-  if (bind_addr != entity_addr_t())
-    learned_addr(bind_addr);
+  set_myaddrs(bind_addrs);
+  for (auto& a : bind_addrs.v) {
+    if (!a.is_blank_ip()) {
+      learned_addr(a);
+    }
+  }
 
-  if (get_myaddr().get_port() == 0) {
-    set_myaddr(listen_addr);
+  if (get_myaddrs().front().get_port() == 0) {
+    set_myaddrs(listen_addrs);
+  }
+  entity_addrvec_t newaddrs = *my_addrs;
+  for (auto& a : newaddrs.v) {
+    a.set_nonce(nonce);
   }
-  entity_addr_t addr = get_myaddr();
-  addr.set_nonce(nonce);
-  set_myaddr(addr);
+  set_myaddrs(newaddrs);
 
   init_local_connection();
 
-  ldout(cct,1) << __func__ << " bind my_inst.addr is " << get_myaddr() << dendl;
+  ldout(cct,1) << __func__ << " bind my_addrs is " << get_myaddrs() << dendl;
   did_bind = true;
 }
 
+int AsyncMessenger::client_reset()
+{
+  mark_down_all();
+
+  std::scoped_lock l{lock};
+  // adjust the nonce; we want our entity_addr_t to be truly unique.
+  nonce += 1000000;
+  ldout(cct, 10) << __func__ << " new nonce " << nonce << dendl;
+
+  entity_addrvec_t newaddrs = *my_addrs;
+  for (auto& a : newaddrs.v) {
+    a.set_nonce(nonce);
+  }
+  set_myaddrs(newaddrs);
+  _init_local_connection();
+  return 0;
+}
+
 int AsyncMessenger::start()
 {
-  lock.Lock();
+  std::scoped_lock l{lock};
   ldout(cct,1) << __func__ << " start" << dendl;
 
   // register at least one entity, first!
-  assert(my_inst.name.type() >= 0);
+  ceph_assert(my_name.type() >= 0);
 
-  assert(!started);
+  ceph_assert(!started);
   started = true;
   stopped = false;
 
   if (!did_bind) {
-    my_inst.addr.nonce = nonce;
+    entity_addrvec_t newaddrs = *my_addrs;
+    for (auto& a : newaddrs.v) {
+      a.nonce = nonce;
+    }
+    set_myaddrs(newaddrs);
     _init_local_connection();
   }
 
-  lock.Unlock();
   return 0;
 }
 
 void AsyncMessenger::wait()
 {
-  lock.Lock();
-  if (!started) {
-    lock.Unlock();
-    return;
+  {
+    std::unique_lock locker{lock};
+    if (!started) {
+      return;
+    }
+    if (!stopped)
+      stop_cond.wait(locker);
   }
-  if (!stopped)
-    stop_cond.Wait(lock);
-
-  lock.Unlock();
-
   dispatch_queue.shutdown();
   if (dispatch_queue.is_started()) {
     ldout(cct, 10) << __func__ << ": waiting for dispatch queue" << dendl;
@@ -499,90 +572,118 @@ void AsyncMessenger::wait()
   started = false;
 }
 
-void AsyncMessenger::add_accept(Worker *w, ConnectedSocket cli_socket, entity_addr_t &addr)
+void AsyncMessenger::add_accept(Worker *w, ConnectedSocket cli_socket,
+				const entity_addr_t &listen_addr,
+				const entity_addr_t &peer_addr)
 {
-  lock.Lock();
-  AsyncConnectionRef conn = new AsyncConnection(cct, this, &dispatch_queue, w);
-  conn->accept(std::move(cli_socket), addr);
+  std::lock_guard l{lock};
+  auto conn = ceph::make_ref<AsyncConnection>(cct, this, &dispatch_queue, w,
+						listen_addr.is_msgr2(), false);
+  conn->accept(std::move(cli_socket), listen_addr, peer_addr);
   accepting_conns.insert(conn);
-  lock.Unlock();
 }
 
-AsyncConnectionRef AsyncMessenger::create_connect(const entity_addr_t& addr, int type)
+AsyncConnectionRef AsyncMessenger::create_connect(
+  const entity_addrvec_t& addrs, int type, bool anon)
 {
-  assert(lock.is_locked());
-  assert(addr != my_inst.addr);
+  ceph_assert(ceph_mutex_is_locked(lock));
 
-  ldout(cct, 10) << __func__ << " " << addr
+  ldout(cct, 10) << __func__ << " " << addrs
       << ", creating connection and registering" << dendl;
 
+  // here is where we decide which of the addrs to connect to.  always prefer
+  // the first one, if we support it.
+  entity_addr_t target;
+  for (auto& a : addrs.v) {
+    if (!a.is_msgr2() && !a.is_legacy()) {
+      continue;
+    }
+    // FIXME: for ipv4 vs ipv6, check whether local host can handle ipv6 before
+    // trying it?  for now, just pick whichever is listed first.
+    target = a;
+    break;
+  }
+
   // create connection
   Worker *w = stack->get_worker();
-  AsyncConnectionRef conn = new AsyncConnection(cct, this, &dispatch_queue, w);
-  conn->connect(addr, type);
-  assert(!conns.count(addr));
-  conns[addr] = conn;
+  auto conn = ceph::make_ref<AsyncConnection>(cct, this, &dispatch_queue, w,
+						target.is_msgr2(), false);
+  conn->anon = anon;
+  conn->connect(addrs, type, target);
+  if (anon) {
+    anon_conns.insert(conn);
+  } else {
+    ceph_assert(!conns.count(addrs));
+    ldout(cct, 10) << __func__ << " " << conn << " " << addrs << " "
+		   << *conn->peer_addrs << dendl;
+    conns[addrs] = conn;
+  }
   w->get_perf_counter()->inc(l_msgr_active_connections);
 
   return conn;
 }
 
-ConnectionRef AsyncMessenger::get_connection(const entity_inst_t& dest)
-{
-  Mutex::Locker l(lock);
-  if (my_inst.addr == dest.addr) {
-    // local
-    return local_connection;
-  }
 
-  AsyncConnectionRef conn = _lookup_conn(dest.addr);
-  if (conn) {
-    ldout(cct, 10) << __func__ << " " << dest << " existing " << conn << dendl;
-  } else {
-    conn = create_connect(dest.addr, dest.name.type());
-    ldout(cct, 10) << __func__ << " " << dest << " new " << conn << dendl;
-  }
+ConnectionRef AsyncMessenger::get_loopback_connection()
+{
+  return local_connection;
+}
 
-  return conn;
+bool AsyncMessenger::should_use_msgr2()
+{
+  // if we are bound to v1 only, and we are connecting to a v2 peer,
+  // we cannot use the peer's v2 address. otherwise the connection
+  // is assymetrical, because they would have to use v1 to connect
+  // to us, and we would use v2, and connection race detection etc
+  // would totally break down (among other things).  or, the other
+  // end will be confused that we advertise ourselve with a v1
+  // address only (that we bound to) but connected with protocol v2.
+  return !did_bind || get_myaddrs().has_msgr2();
 }
 
-ConnectionRef AsyncMessenger::get_loopback_connection()
+entity_addrvec_t AsyncMessenger::_filter_addrs(const entity_addrvec_t& addrs)
 {
-  return local_connection;
+  if (!should_use_msgr2()) {
+    ldout(cct, 10) << __func__ << " " << addrs << " limiting to v1 ()" << dendl;
+    entity_addrvec_t r;
+    for (auto& i : addrs.v) {
+      if (i.is_msgr2()) {
+	continue;
+      }
+      r.v.push_back(i);
+    }
+    return r;
+  } else {
+    return addrs;
+  }
 }
 
-int AsyncMessenger::_send_message(Message *m, const entity_inst_t& dest)
+int AsyncMessenger::send_to(Message *m, int type, const entity_addrvec_t& addrs)
 {
-  FUNCTRACE();
-  assert(m);
+  FUNCTRACE(cct);
+  ceph_assert(m);
 
+#if defined(WITH_EVENTTRACE)
   if (m->get_type() == CEPH_MSG_OSD_OP)
     OID_EVENT_TRACE(((MOSDOp *)m)->get_oid().name.c_str(), "SEND_MSG_OSD_OP");
   else if (m->get_type() == CEPH_MSG_OSD_OPREPLY)
     OID_EVENT_TRACE(((MOSDOpReply *)m)->get_oid().name.c_str(), "SEND_MSG_OSD_OP_REPLY");
+#endif
 
-  ldout(cct, 1) << __func__ << "--> " << dest.name << " "
-      << dest.addr << " -- " << *m << " -- ?+"
+  ldout(cct, 1) << __func__ << "--> " << ceph_entity_type_name(type) << " "
+      << addrs << " -- " << *m << " -- ?+"
       << m->get_data().length() << " " << m << dendl;
 
-  if (dest.addr == entity_addr_t()) {
+  if (addrs.empty()) {
     ldout(cct,0) << __func__ <<  " message " << *m
-        << " with empty dest " << dest.addr << dendl;
+        << " with empty dest " << addrs << dendl;
     m->put();
     return -EINVAL;
   }
 
-  AsyncConnectionRef conn = _lookup_conn(dest.addr);
-  submit_message(m, conn, dest.addr, dest.name.type());
-  return 0;
-}
-
-void AsyncMessenger::submit_message(Message *m, AsyncConnectionRef con,
-                                    const entity_addr_t& dest_addr, int dest_type)
-{
   if (cct->_conf->ms_dump_on_send) {
     m->encode(-1, MSG_CRC_ALL);
-    ldout(cct, 0) << __func__ << "submit_message " << *m << "\n";
+    ldout(cct, 0) << __func__ << " submit_message " << *m << "\n";
     m->get_payload().hexdump(*_dout);
     if (m->get_data().length() > 0) {
       *_dout << " data:\n";
@@ -592,106 +693,136 @@ void AsyncMessenger::submit_message(Message *m, AsyncConnectionRef con,
     m->clear_payload();
   }
 
-  // existing connection?
-  if (con) {
-    con->send_message(m);
-    return ;
+  connect_to(type, addrs, false)->send_message(m);
+  return 0;
+}
+
+ConnectionRef AsyncMessenger::connect_to(int type,
+					 const entity_addrvec_t& addrs,
+					 bool anon, bool not_local_dest)
+{
+  if (!not_local_dest) {
+    if (*my_addrs == addrs ||
+	(addrs.v.size() == 1 &&
+	 my_addrs->contains(addrs.front()))) {
+      // local
+      return local_connection;
+    }
   }
 
-  // local?
-  if (my_inst.addr == dest_addr) {
-    // local
-    local_connection->send_message(m);
-    return ;
+  auto av = _filter_addrs(addrs);
+  std::lock_guard l{lock};
+  if (anon) {
+    return create_connect(av, type, anon);
   }
 
-  // remote, no existing connection.
-  const Policy& policy = get_policy(dest_type);
-  if (policy.server) {
-    ldout(cct, 20) << __func__ << " " << *m << " remote, " << dest_addr
-        << ", lossy server for target type "
-        << ceph_entity_type_name(dest_type) << ", no session, dropping." << dendl;
-    m->put();
+  AsyncConnectionRef conn = _lookup_conn(av);
+  if (conn) {
+    ldout(cct, 10) << __func__ << " " << av << " existing " << conn << dendl;
   } else {
-    ldout(cct,20) << __func__ << " " << *m << " remote, " << dest_addr << ", new connection." << dendl;
-    con = create_connect(dest_addr, dest_type);
-    con->send_message(m);
+    conn = create_connect(av, type, false);
+    ldout(cct, 10) << __func__ << " " << av << " new " << conn << dendl;
   }
+
+  return conn;
 }
 
 /**
- * If my_inst.addr doesn't have an IP set, this function
+ * If my_addr doesn't have an IP set, this function
  * will fill it in from the passed addr. Otherwise it does nothing and returns.
  */
-void AsyncMessenger::set_addr_unknowns(const entity_addr_t &addr)
+bool AsyncMessenger::set_addr_unknowns(const entity_addrvec_t &addrs)
 {
-  Mutex::Locker l(lock);
-  if (my_inst.addr.is_blank_ip()) {
-    int port = my_inst.addr.get_port();
-    my_inst.addr.u = addr.u;
-    my_inst.addr.set_port(port);
+  ldout(cct,1) << __func__ << " " << addrs << dendl;
+  bool ret = false;
+  std::lock_guard l{lock};
+
+  entity_addrvec_t newaddrs = *my_addrs;
+  for (auto& a : newaddrs.v) {
+    if (a.is_blank_ip()) {
+      int type = a.get_type();
+      int port = a.get_port();
+      uint32_t nonce = a.get_nonce();
+      for (auto& b : addrs.v) {
+	if (a.get_family() == b.get_family()) {
+	  ldout(cct,1) << __func__ << " assuming my addr " << a
+		       << " matches provided addr " << b << dendl;
+	  a = b;
+	  a.set_nonce(nonce);
+	  a.set_type(type);
+	  a.set_port(port);
+	  ret = true;
+	  break;
+	}
+      }
+    }
+  }
+  set_myaddrs(newaddrs);
+  if (ret) {
     _init_local_connection();
   }
+  ldout(cct,1) << __func__ << " now " << *my_addrs << dendl;
+  return ret;
 }
 
-void AsyncMessenger::set_addr(const entity_addr_t &addr)
+void AsyncMessenger::set_addrs(const entity_addrvec_t &addrs)
 {
-  Mutex::Locker l(lock);
-  entity_addr_t t = addr;
-  t.set_nonce(nonce);
-  set_myaddr(t);
+  std::lock_guard l{lock};
+  auto t = addrs;
+  for (auto& a : t.v) {
+    a.set_nonce(nonce);
+  }
+  set_myaddrs(t);
   _init_local_connection();
 }
 
 void AsyncMessenger::shutdown_connections(bool queue_reset)
 {
   ldout(cct,1) << __func__ << " " << dendl;
-  lock.Lock();
-  for (set<AsyncConnectionRef>::iterator q = accepting_conns.begin();
-       q != accepting_conns.end(); ++q) {
-    AsyncConnectionRef p = *q;
-    ldout(cct, 5) << __func__ << " accepting_conn " << p.get() << dendl;
-    p->stop(queue_reset);
+  std::lock_guard l{lock};
+  for (const auto& c : accepting_conns) {
+    ldout(cct, 5) << __func__ << " accepting_conn " << c << dendl;
+    c->stop(queue_reset);
   }
   accepting_conns.clear();
 
-  while (!conns.empty()) {
-    ceph::unordered_map<entity_addr_t, AsyncConnectionRef>::iterator it = conns.begin();
-    AsyncConnectionRef p = it->second;
-    ldout(cct, 5) << __func__ << " mark down " << it->first << " " << p << dendl;
-    conns.erase(it);
-    p->get_perf_counter()->dec(l_msgr_active_connections);
-    p->stop(queue_reset);
+  for (const auto& [e, c] : conns) {
+    ldout(cct, 5) << __func__ << " mark down " << e << " " << c << dendl;
+    c->stop(queue_reset);
   }
+  conns.clear();
+
+  for (const auto& c : anon_conns) {
+    ldout(cct, 5) << __func__ << " mark down " << c << dendl;
+    c->stop(queue_reset);
+  }
+  anon_conns.clear();
 
   {
-    Mutex::Locker l(deleted_lock);
-    while (!deleted_conns.empty()) {
-      set<AsyncConnectionRef>::iterator it = deleted_conns.begin();
-      AsyncConnectionRef p = *it;
-      ldout(cct, 5) << __func__ << " delete " << p << dendl;
-      deleted_conns.erase(it);
+    std::lock_guard l{deleted_lock};
+    for (const auto& c : deleted_conns) {
+      ldout(cct, 5) << __func__ << " delete " << c << dendl;
+      c->get_perf_counter()->dec(l_msgr_active_connections);
     }
+    deleted_conns.clear();
   }
-  lock.Unlock();
 }
 
-void AsyncMessenger::mark_down(const entity_addr_t& addr)
+void AsyncMessenger::mark_down_addrs(const entity_addrvec_t& addrs)
 {
-  lock.Lock();
-  AsyncConnectionRef p = _lookup_conn(addr);
-  if (p) {
-    ldout(cct, 1) << __func__ << " " << addr << " -- " << p << dendl;
-    p->stop(true);
+  std::lock_guard l{lock};
+  const AsyncConnectionRef& conn = _lookup_conn(addrs);
+  if (conn) {
+    ldout(cct, 1) << __func__ << " " << addrs << " -- " << conn << dendl;
+    conn->stop(true);
   } else {
-    ldout(cct, 1) << __func__ << " " << addr << " -- connection dne" << dendl;
+    ldout(cct, 1) << __func__ << " " << addrs << " -- connection dne" << dendl;
   }
-  lock.Unlock();
 }
 
 int AsyncMessenger::get_proto_version(int peer_type, bool connect) const
 {
-  int my_type = my_inst.name.type();
+  int my_type = my_name.type();
 
   // set reply protocol version
   if (peer_type == my_type) {
@@ -708,48 +839,106 @@ int AsyncMessenger::get_proto_version(int peer_type, bool connect) const
   return 0;
 }
 
-void AsyncMessenger::learned_addr(const entity_addr_t &peer_addr_for_me)
+int AsyncMessenger::accept_conn(const AsyncConnectionRef& conn)
+{
+  std::lock_guard l{lock};
+  if (conn->policy.server &&
+      conn->policy.lossy &&
+      !conn->policy.register_lossy_clients) {
+    anon_conns.insert(conn);
+    conn->get_perf_counter()->inc(l_msgr_active_connections);
+    return 0;
+  }
+  auto it = conns.find(*conn->peer_addrs);
+  if (it != conns.end()) {
+    auto& existing = it->second;
+
+    // lazy delete, see "deleted_conns"
+    // If conn already in, we will return 0
+    std::lock_guard l{deleted_lock};
+    if (deleted_conns.erase(existing)) {
+      it->second->get_perf_counter()->dec(l_msgr_active_connections);
+      conns.erase(it);
+    } else if (conn != existing) {
+      return -1;
+    }
+  }
+  ldout(cct, 10) << __func__ << " " << conn << " " << *conn->peer_addrs << dendl;
+  conns[*conn->peer_addrs] = conn;
+  conn->get_perf_counter()->inc(l_msgr_active_connections);
+  accepting_conns.erase(conn);
+  return 0;
+}
+
+
+bool AsyncMessenger::learned_addr(const entity_addr_t &peer_addr_for_me)
 {
   // be careful here: multiple threads may block here, and readers of
-  // my_inst.addr do NOT hold any lock.
+  // my_addr do NOT hold any lock.
 
   // this always goes from true -> false under the protection of the
   // mutex.  if it is already false, we need not retake the mutex at
   // all.
   if (!need_addr)
-    return ;
-  lock.Lock();
+    return false;
+  std::lock_guard l(lock);
   if (need_addr) {
-    need_addr = false;
-    entity_addr_t t = peer_addr_for_me;
-    t.set_port(my_inst.addr.get_port());
-    t.set_nonce(my_inst.addr.get_nonce());
-    my_inst.addr = t;
-    ldout(cct, 1) << __func__ << " learned my addr " << my_inst.addr << dendl;
+    if (my_addrs->empty()) {
+      auto a = peer_addr_for_me;
+      a.set_type(entity_addr_t::TYPE_ANY);
+      a.set_nonce(nonce);
+      if (!did_bind) {
+	a.set_port(0);
+      }
+      set_myaddrs(entity_addrvec_t(a));
+      ldout(cct,10) << __func__ << " had no addrs" << dendl;
+    } else {
+      // fix all addrs of the same family, regardless of type (msgr2 vs legacy)
+      entity_addrvec_t newaddrs = *my_addrs;
+      for (auto& a : newaddrs.v) {
+	if (a.is_blank_ip() &&
+	    a.get_family() == peer_addr_for_me.get_family()) {
+	  entity_addr_t t = peer_addr_for_me;
+	  if (!did_bind) {
+	    t.set_type(entity_addr_t::TYPE_ANY);
+	    t.set_port(0);
+	  } else {	  
+	    t.set_type(a.get_type());
+	    t.set_port(a.get_port());
+	  }
+	  t.set_nonce(a.get_nonce());
+	  ldout(cct,10) << __func__ << " " << a << " -> " << t << dendl;
+	  a = t;
+	}
+      }
+      set_myaddrs(newaddrs);
+    }
+    ldout(cct, 1) << __func__ << " learned my addr " << *my_addrs
+		  << " (peer_addr_for_me " << peer_addr_for_me << ")" << dendl;
     _init_local_connection();
+    need_addr = false;
+    return true;
   }
-  lock.Unlock();
+  return false;
 }
 
-int AsyncMessenger::reap_dead()
+void AsyncMessenger::reap_dead()
 {
   ldout(cct, 1) << __func__ << " start" << dendl;
-  int num = 0;
 
-  Mutex::Locker l1(lock);
-  Mutex::Locker l2(deleted_lock);
+  std::lock_guard l1{lock};
 
-  while (!deleted_conns.empty()) {
-    auto it = deleted_conns.begin();
-    AsyncConnectionRef p = *it;
-    ldout(cct, 5) << __func__ << " delete " << p << dendl;
-    auto conns_it = conns.find(p->peer_addr);
-    if (conns_it != conns.end() && conns_it->second == p)
-      conns.erase(conns_it);
-    accepting_conns.erase(p);
-    deleted_conns.erase(it);
-    ++num;
+  {
+    std::lock_guard l2{deleted_lock};
+    for (auto& c : deleted_conns) {
+      ldout(cct, 5) << __func__ << " delete " << c << dendl;
+      auto conns_it = conns.find(*c->peer_addrs);
+      if (conns_it != conns.end() && conns_it->second == c)
+        conns.erase(conns_it);
+      accepting_conns.erase(c);
+      anon_conns.erase(c);
+      c->get_perf_counter()->dec(l_msgr_active_connections);
+    }
+    deleted_conns.clear();
   }
-
-  return num;
 }
diff --git a/src/msg/async/AsyncMessenger.h b/src/msg/async/AsyncMessenger.h
index 7ebc7777c93..9c420b18d53 100644
--- a/src/msg/async/AsyncMessenger.h
+++ b/src/msg/async/AsyncMessenger.h
@@ -1,4 +1,4 @@
-// -*- mode:C++; tab-width:8; c-basic-offset:2; indent-tabs-mode:t -*- 
+// -*- mode:C++; tab-width:8; c-basic-offset:2; indent-tabs-mode:t -*-
 // vim: ts=8 sw=2 smarttab
 /*
  * Ceph - scalable distributed file system
@@ -17,26 +17,24 @@
 #ifndef CEPH_ASYNCMESSENGER_H
 #define CEPH_ASYNCMESSENGER_H
 
+#include <map>
+
 #include "include/types.h"
 #include "include/xlist.h"
-
-#include <map>
-using namespace std;
+#include "include/spinlock.h"
 #include "include/unordered_map.h"
 #include "include/unordered_set.h"
 
-#include "common/Mutex.h"
+#include "common/ceph_mutex.h"
 #include "common/Cond.h"
 #include "common/Thread.h"
 
-#include "include/Spinlock.h"
-
 #include "msg/SimplePolicyMessenger.h"
 #include "msg/DispatchQueue.h"
-#include "include/assert.h"
 #include "AsyncConnection.h"
 #include "Event.h"
 
+#include "include/ceph_assert.h"
 
 class AsyncMessenger;
 
@@ -48,7 +46,7 @@ class Processor {
   AsyncMessenger *msgr;
   NetHandler net;
   Worker *worker;
-  ServerSocket listen_socket;
+  vector<ServerSocket> listen_sockets;
   EventCallbackRef listen_handler;
 
   class C_processor_accept;
@@ -58,9 +56,9 @@ class Processor {
   ~Processor() { delete listen_handler; };
 
   void stop();
-  int bind(const entity_addr_t &bind_addr,
+  int bind(const entity_addrvec_t &bind_addrs,
 	   const set<int>& avoid_ports,
-	   entity_addr_t* bound_addr);
+	   entity_addrvec_t* bound_addrs);
   void start();
   void accept();
 };
@@ -95,8 +93,8 @@ public:
   /** @defgroup Accessors
    * @{
    */
-  void set_addr_unknowns(const entity_addr_t &addr) override;
-  void set_addr(const entity_addr_t &addr) override;
+  bool set_addr_unknowns(const entity_addrvec_t &addr) override;
+  void set_addrs(const entity_addrvec_t &addrs) override;
 
   int get_dispatch_queue_len() override {
     return dispatch_queue.get_queue_len();
@@ -112,14 +110,20 @@ public:
    * @{
    */
   void set_cluster_protocol(int p) override {
-    assert(!started && !did_bind);
+    ceph_assert(!started && !did_bind);
     cluster_protocol = p;
   }
 
   int bind(const entity_addr_t& bind_addr) override;
   int rebind(const set<int>& avoid_ports) override;
+  int bindv(const entity_addrvec_t& bind_addrs) override;
+
   int client_bind(const entity_addr_t& bind_addr) override;
 
+  int client_reset() override;
+
+  bool should_use_msgr2() override;
+
   /** @} Configuration functions */
 
   /**
@@ -136,11 +140,7 @@ public:
    * @defgroup Messaging
    * @{
    */
-  int send_message(Message *m, const entity_inst_t& dest) override {
-    Mutex::Locker l(lock);
-
-    return _send_message(m, dest);
-  }
+  int send_to(Message *m, int type, const entity_addrvec_t& addrs) override;
 
   /** @} // Messaging */
 
@@ -148,9 +148,14 @@ public:
    * @defgroup Connection Management
    * @{
    */
-  ConnectionRef get_connection(const entity_inst_t& dest) override;
+  ConnectionRef connect_to(int type,
+			   const entity_addrvec_t& addrs,
+			   bool anon, bool not_local_dest=false) override;
   ConnectionRef get_loopback_connection() override;
-  void mark_down(const entity_addr_t& addr) override;
+  void mark_down(const entity_addr_t& addr) override {
+    mark_down_addrs(entity_addrvec_t(addr));
+  }
+  void mark_down_addrs(const entity_addrvec_t& addrs) override;
   void mark_down_all() override {
     shutdown_connections(true);
   }
@@ -188,36 +193,22 @@ private:
    * Initiate the connection. (This function returning does not guarantee
    * connection success.)
    *
-   * @param addr The address of the entity to connect to.
+   * @param addrs The address(es) of the entity to connect to.
    * @param type The peer type of the entity at the address.
    *
    * @return a pointer to the newly-created connection. Caller does not own a
    * reference; take one if you need it.
    */
-  AsyncConnectionRef create_connect(const entity_addr_t& addr, int type);
+  AsyncConnectionRef create_connect(const entity_addrvec_t& addrs, int type,
+				    bool anon);
 
-  /**
-   * Queue up a Message for delivery to the entity specified
-   * by addr and dest_type.
-   * submit_message() is responsible for creating
-   * new AsyncConnection (and closing old ones) as necessary.
-   *
-   * @param m The Message to queue up. This function eats a reference.
-   * @param con The existing Connection to use, or NULL if you don't know of one.
-   * @param dest_addr The address to send the Message to.
-   * @param dest_type The peer type of the address we're sending to
-   * just drop silently under failure.
-   */
-  void submit_message(Message *m, AsyncConnectionRef con,
-                      const entity_addr_t& dest_addr, int dest_type);
 
-  int _send_message(Message *m, const entity_inst_t& dest);
-  void _finish_bind(const entity_addr_t& bind_addr,
-		    const entity_addr_t& listen_addr);
+  void _finish_bind(const entity_addrvec_t& bind_addrs,
+		    const entity_addrvec_t& listen_addrs);
 
- private:
-  static const uint64_t ReapDeadConnectionThreshold = 5;
+  entity_addrvec_t _filter_addrs(const entity_addrvec_t& addrs);
 
+ private:
   NetworkStack *stack;
   std::vector<Processor*> processors;
   friend class Processor;
@@ -229,20 +220,20 @@ private:
   std::string ms_type;
 
   /// overall lock used for AsyncMessenger data structures
-  Mutex lock;
+  ceph::mutex lock = ceph::make_mutex("AsyncMessenger::lock");
   // AsyncMessenger stuff
   /// approximately unique ID set by the Constructor for use in entity_addr_t
   uint64_t nonce;
 
   /// true, specifying we haven't learned our addr; set false when we find it.
   // maybe this should be protected by the lock?
-  bool need_addr;
+  bool need_addr = true;
 
   /**
-   * set to bind address if bind was called before NetworkStack was ready to
+   * set to bind addresses if bind was called before NetworkStack was ready to
    * bind
    */
-  entity_addr_t pending_bind_addr;
+  entity_addrvec_t pending_bind_addrs;
 
   /**
    * false; set to true if a pending bind exists
@@ -258,11 +249,11 @@ private:
    *  false; set to true if the AsyncMessenger bound to a specific address;
    *  and set false again by Accepter::stop().
    */
-  bool did_bind;
+  bool did_bind = false;
   /// counter for the global seq our connection protocol uses
-  __u32 global_seq;
+  __u32 global_seq = 0;
   /// lock to protect the global_seq
-  ceph_spinlock_t global_seq_lock;
+  ceph::spinlock global_seq_lock;
 
   /**
    * hash map of addresses to Asyncconnection
@@ -270,15 +261,18 @@ private:
    * NOTE: a Asyncconnection* with state CLOSED may still be in the map but is considered
    * invalid and can be replaced by anyone holding the msgr lock
    */
-  ceph::unordered_map<entity_addr_t, AsyncConnectionRef> conns;
+  ceph::unordered_map<entity_addrvec_t, AsyncConnectionRef> conns;
 
   /**
-   * list of connection are in teh process of accepting
+   * list of connection are in the process of accepting
    *
    * These are not yet in the conns map.
    */
   set<AsyncConnectionRef> accepting_conns;
 
+  /// anonymous outgoing connections
+  set<AsyncConnectionRef> anon_conns;
+
   /**
    * list of connection are closed which need to be clean up
    *
@@ -290,38 +284,44 @@ private:
    * deleted for AsyncConnection. "_lookup_conn" must ensure not return a
    * AsyncConnection in this set.
    */
-  Mutex deleted_lock;
+  ceph::mutex deleted_lock = ceph::make_mutex("AsyncMessenger::deleted_lock");
   set<AsyncConnectionRef> deleted_conns;
 
   EventCallbackRef reap_handler;
 
   /// internal cluster protocol version, if any, for talking to entities of the same type.
-  int cluster_protocol;
-
-  Cond  stop_cond;
-  bool stopped;
-
-  AsyncConnectionRef _lookup_conn(const entity_addr_t& k) {
-    assert(lock.is_locked());
-    ceph::unordered_map<entity_addr_t, AsyncConnectionRef>::iterator p = conns.find(k);
-    if (p == conns.end())
-      return NULL;
+  int cluster_protocol = 0;
+
+  ceph::condition_variable  stop_cond;
+  bool stopped = true;
+
+  /* You must hold this->lock for the duration of use! */
+  const auto& _lookup_conn(const entity_addrvec_t& k) {
+    static const AsyncConnectionRef nullref;
+    ceph_assert(ceph_mutex_is_locked(lock));
+    auto p = conns.find(k);
+    if (p == conns.end()) {
+      return nullref;
+    }
 
     // lazy delete, see "deleted_conns"
-    Mutex::Locker l(deleted_lock);
-    if (deleted_conns.erase(p->second)) {
-      p->second->get_perf_counter()->dec(l_msgr_active_connections);
-      conns.erase(p);
-      return NULL;
+    // don't worry omit, Connection::send_message can handle this case.
+    if (p->second->is_unregistered()) {
+      std::lock_guard l{deleted_lock};
+      if (deleted_conns.erase(p->second)) {
+	p->second->get_perf_counter()->dec(l_msgr_active_connections);
+	conns.erase(p);
+	return nullref;
+      }
     }
 
     return p->second;
   }
 
   void _init_local_connection() {
-    assert(lock.is_locked());
-    local_connection->peer_addr = my_inst.addr;
-    local_connection->peer_type = my_inst.name.type();
+    ceph_assert(ceph_mutex_is_locked(lock));
+    local_connection->peer_addrs = *my_addrs;
+    local_connection->peer_type = my_name.type();
     local_connection->set_features(CEPH_FEATURES_ALL);
     ms_deliver_handle_fast_connect(local_connection.get());
   }
@@ -331,7 +331,7 @@ private:
 public:
 
   /// con used for sending messages to ourselves
-  ConnectionRef local_connection;
+  AsyncConnectionRef local_connection;
 
   /**
    * @defgroup AsyncMessenger internals
@@ -340,54 +340,24 @@ public:
   /**
    * This wraps _lookup_conn.
    */
-  AsyncConnectionRef lookup_conn(const entity_addr_t& k) {
-    Mutex::Locker l(lock);
-    return _lookup_conn(k);
-  }
-
-  int accept_conn(AsyncConnectionRef conn) {
-    Mutex::Locker l(lock);
-    auto it = conns.find(conn->peer_addr);
-    if (it != conns.end()) {
-      AsyncConnectionRef existing = it->second;
-
-      // lazy delete, see "deleted_conns"
-      // If conn already in, we will return 0
-      Mutex::Locker l(deleted_lock);
-      if (deleted_conns.erase(existing)) {
-        existing->get_perf_counter()->dec(l_msgr_active_connections);
-        conns.erase(it);
-      } else if (conn != existing) {
-        return -1;
-      }
-    }
-    conns[conn->peer_addr] = conn;
-    conn->get_perf_counter()->inc(l_msgr_active_connections);
-    accepting_conns.erase(conn);
-    return 0;
+  AsyncConnectionRef lookup_conn(const entity_addrvec_t& k) {
+    std::lock_guard l{lock};
+    return _lookup_conn(k); /* make new ref! */
   }
 
-  void learned_addr(const entity_addr_t &peer_addr_for_me);
-  void add_accept(Worker *w, ConnectedSocket cli_socket, entity_addr_t &addr);
+  int accept_conn(const AsyncConnectionRef& conn);
+  bool learned_addr(const entity_addr_t &peer_addr_for_me);
+  void add_accept(Worker *w, ConnectedSocket cli_socket,
+		  const entity_addr_t &listen_addr,
+		  const entity_addr_t &peer_addr);
   NetworkStack *get_stack() {
     return stack;
   }
 
-  /**
-   * This wraps ms_deliver_get_authorizer. We use it for AsyncConnection.
-   */
-  AuthAuthorizer *get_authorizer(int peer_type, bool force_new) {
-    return ms_deliver_get_authorizer(peer_type, force_new);
+  uint64_t get_nonce() const {
+    return nonce;
   }
 
-  /**
-   * This wraps ms_deliver_verify_authorizer; we use it for AsyncConnection.
-   */
-  bool verify_authorizer(Connection *con, int peer_type, int protocol, bufferlist& auth, bufferlist& auth_reply,
-                         bool& isvalid, CryptoKey& session_key) {
-    return ms_deliver_verify_authorizer(con, peer_type, protocol, auth,
-                                        auth_reply, isvalid, session_key);
-  }
   /**
    * Increment the global sequence for this AsyncMessenger and return it.
    * This is for the connect protocol, although it doesn't hurt if somebody
@@ -396,11 +366,12 @@ public:
    * @return a global sequence ID that nobody else has seen.
    */
   __u32 get_global_seq(__u32 old=0) {
-    ceph_spin_lock(&global_seq_lock);
+    std::lock_guard<ceph::spinlock> lg(global_seq_lock);
+
     if (old > global_seq)
       global_seq = old;
     __u32 ret = ++global_seq;
-    ceph_spin_unlock(&global_seq_lock);
+
     return ret;
   }
   /**
@@ -415,7 +386,8 @@ public:
    * is used for delivering messages back to ourself.
    */
   void init_local_connection() {
-    Mutex::Locker l(lock);
+    std::lock_guard l{lock};
+    local_connection->is_loopback = true;
     _init_local_connection();
   }
 
@@ -424,11 +396,12 @@ public:
    *
    * See "deleted_conns"
    */
-  void unregister_conn(AsyncConnectionRef conn) {
-    Mutex::Locker l(deleted_lock);
-    deleted_conns.insert(conn);
+  void unregister_conn(const AsyncConnectionRef& conn) {
+    std::lock_guard l{deleted_lock};
+    deleted_conns.emplace(std::move(conn));
+    conn->unregister();
 
-    if (deleted_conns.size() >= ReapDeadConnectionThreshold) {
+    if (deleted_conns.size() >= cct->_conf->ms_async_reap_threshold) {
       local_worker->center.dispatch_event_external(reap_handler);
     }
   }
@@ -440,7 +413,7 @@ public:
    *
    * See "deleted_conns"
    */
-  int reap_dead();
+  void reap_dead();
 
   /**
    * @} // AsyncMessenger Internals
diff --git a/src/msg/async/Event.cc b/src/msg/async/Event.cc
index 690a245317f..4d2fb339416 100644
--- a/src/msg/async/Event.cc
+++ b/src/msg/async/Event.cc
@@ -14,6 +14,7 @@
  *
  */
 
+#include "include/compat.h"
 #include "common/errno.h"
 #include "Event.h"
 
@@ -41,7 +42,7 @@ class C_handle_notify : public EventCallback {
 
  public:
   C_handle_notify(EventCenter *c, CephContext *cc): center(c), cct(cc) {}
-  void do_request(int fd_or_id) override {
+  void do_request(uint64_t fd_or_id) override {
     char c[256];
     int r = 0;
     do {
@@ -99,15 +100,15 @@ ostream& EventCenter::_event_prefix(std::ostream *_dout)
                 << " time_id=" << time_event_next_id << ").";
 }
 
-int EventCenter::init(int n, unsigned i, const std::string &t)
+int EventCenter::init(int nevent, unsigned center_id, const std::string &type)
 {
   // can't init multi times
-  assert(nevent == 0);
+  ceph_assert(this->nevent == 0);
 
-  type = t;
-  idx = i;
+  this->type = type;
+  this->center_id = center_id;
 
-  if (t == "dpdk") {
+  if (type == "dpdk") {
 #ifdef HAVE_DPDK
     driver = new DPDKDriver(cct);
 #endif
@@ -128,22 +129,23 @@ int EventCenter::init(int n, unsigned i, const std::string &t)
     return -1;
   }
 
-  int r = driver->init(this, n);
+  int r = driver->init(this, nevent);
   if (r < 0) {
     lderr(cct) << __func__ << " failed to init event driver." << dendl;
     return r;
   }
 
-  file_events.resize(n);
-  nevent = n;
+  file_events.resize(nevent);
+  this->nevent = nevent;
 
   if (!driver->need_wakeup())
     return 0;
 
   int fds[2];
-  if (pipe(fds) < 0) {
-    lderr(cct) << __func__ << " can't create notify pipe" << dendl;
-    return -errno;
+  if (pipe_cloexec(fds, 0) < 0) {
+    int e = errno;
+    lderr(cct) << __func__ << " can't create notify pipe: " << cpp_strerror(e) << dendl;
+    return -e;
   }
 
   notify_receive_fd = fds[0];
@@ -171,7 +173,8 @@ EventCenter::~EventCenter()
       external_events.pop_front();
     }
   }
-  assert(time_events.empty());
+  time_events.clear();
+  //assert(time_events.empty());
 
   if (notify_receive_fd >= 0)
     ::close(notify_receive_fd);
@@ -187,27 +190,28 @@ EventCenter::~EventCenter()
 void EventCenter::set_owner()
 {
   owner = pthread_self();
-  ldout(cct, 2) << __func__ << " idx=" << idx << " owner=" << owner << dendl;
+  ldout(cct, 2) << __func__ << " center_id=" << center_id << " owner=" << owner << dendl;
   if (!global_centers) {
-    cct->lookup_or_create_singleton_object<EventCenter::AssociatedCenters>(
-        global_centers, "AsyncMessenger::EventCenter::global_center::"+type);
-    assert(global_centers);
-    global_centers->centers[idx] = this;
+    global_centers = &cct->lookup_or_create_singleton_object<
+      EventCenter::AssociatedCenters>(
+	"AsyncMessenger::EventCenter::global_center::" + type, true);
+    ceph_assert(global_centers);
+    global_centers->centers[center_id] = this;
     if (driver->need_wakeup()) {
       notify_handler = new C_handle_notify(this, cct);
       int r = create_file_event(notify_receive_fd, EVENT_READABLE, notify_handler);
-      assert(r == 0);
+      ceph_assert(r == 0);
     }
   }
 }
 
 int EventCenter::create_file_event(int fd, int mask, EventCallbackRef ctxt)
 {
-  assert(in_thread());
+  ceph_assert(in_thread());
   int r = 0;
   if (fd >= nevent) {
     int new_size = nevent << 2;
-    while (fd > new_size)
+    while (fd >= new_size)
       new_size <<= 2;
     ldout(cct, 20) << __func__ << " event count exceed " << nevent << ", expand to " << new_size << dendl;
     r = driver->resize_events(new_size);
@@ -230,7 +234,9 @@ int EventCenter::create_file_event(int fd, int mask, EventCallbackRef ctxt)
     // Actually we don't allow any failed error code, caller doesn't prepare to
     // handle error status. So now we need to assert failure here. In practice,
     // add_event shouldn't report error, otherwise it must be a innermost bug!
-    assert(0 == "BUG!");
+    lderr(cct) << __func__ << " add event failed, ret=" << r << " fd=" << fd
+               << " mask=" << mask << " original mask is " << event->mask << dendl;
+    ceph_abort_msg("BUG!");
     return r;
   }
 
@@ -248,7 +254,7 @@ int EventCenter::create_file_event(int fd, int mask, EventCallbackRef ctxt)
 
 void EventCenter::delete_file_event(int fd, int mask)
 {
-  assert(in_thread() && fd >= 0);
+  ceph_assert(in_thread() && fd >= 0);
   if (fd >= nevent) {
     ldout(cct, 1) << __func__ << " delete event fd=" << fd << " is equal or greater than nevent=" << nevent
                   << "mask=" << mask << dendl;
@@ -263,7 +269,7 @@ void EventCenter::delete_file_event(int fd, int mask)
   int r = driver->del_event(fd, event->mask, mask);
   if (r < 0) {
     // see create_file_event
-    assert(0 == "BUG!");
+    ceph_abort_msg("BUG!");
   }
 
   if (mask & EVENT_READABLE && event->read_cb) {
@@ -280,7 +286,7 @@ void EventCenter::delete_file_event(int fd, int mask)
 
 uint64_t EventCenter::create_time_event(uint64_t microseconds, EventCallbackRef ctxt)
 {
-  assert(in_thread());
+  ceph_assert(in_thread());
   uint64_t id = time_event_next_id++;
 
   ldout(cct, 30) << __func__ << " id=" << id << " trigger after " << microseconds << "us"<< dendl;
@@ -297,7 +303,7 @@ uint64_t EventCenter::create_time_event(uint64_t microseconds, EventCallbackRef
 
 void EventCenter::delete_time_event(uint64_t id)
 {
-  assert(in_thread());
+  ceph_assert(in_thread());
   ldout(cct, 30) << __func__ << " id=" << id << dendl;
   if (id >= time_event_next_id || id == 0)
     return ;
@@ -355,68 +361,60 @@ int EventCenter::process_time_events()
   return processed;
 }
 
-int EventCenter::process_events(int timeout_microseconds,  ceph::timespan *working_dur)
+int EventCenter::process_events(unsigned timeout_microseconds,  ceph::timespan *working_dur)
 {
   struct timeval tv;
   int numevents;
   bool trigger_time = false;
   auto now = clock_type::now();
+  clock_type::time_point end_time = now + std::chrono::microseconds(timeout_microseconds);
 
   auto it = time_events.begin();
-  bool blocking = pollers.empty() && !external_num_events.load();
-  // If exists external events or poller, don't block
-  if (!blocking) {
-    if (it != time_events.end() && now >= it->first)
-      trigger_time = true;
-    tv.tv_sec = 0;
-    tv.tv_usec = 0;
-  } else {
-    clock_type::time_point shortest;
-    shortest = now + std::chrono::microseconds(timeout_microseconds); 
-
-    if (it != time_events.end() && shortest >= it->first) {
-      ldout(cct, 30) << __func__ << " shortest is " << shortest << " it->first is " << it->first << dendl;
-      shortest = it->first;
-      trigger_time = true;
-      if (shortest > now) {
-        timeout_microseconds = std::chrono::duration_cast<std::chrono::microseconds>(
-            shortest - now).count();
-      } else {
-        shortest = now;
-        timeout_microseconds = 0;
-      }
+  if (it != time_events.end() && end_time >= it->first) {
+    trigger_time = true;
+    end_time = it->first;
+
+    if (end_time > now) {
+      timeout_microseconds = std::chrono::duration_cast<std::chrono::microseconds>(end_time - now).count();
+    } else {
+      timeout_microseconds = 0;
     }
-    tv.tv_sec = timeout_microseconds / 1000000;
-    tv.tv_usec = timeout_microseconds % 1000000;
   }
 
+  bool blocking = pollers.empty() && !external_num_events.load();
+  if (!blocking)
+    timeout_microseconds = 0;
+  tv.tv_sec = timeout_microseconds / 1000000;
+  tv.tv_usec = timeout_microseconds % 1000000;
+
   ldout(cct, 30) << __func__ << " wait second " << tv.tv_sec << " usec " << tv.tv_usec << dendl;
   vector<FiredFileEvent> fired_events;
   numevents = driver->event_wait(fired_events, &tv);
   auto working_start = ceph::mono_clock::now();
-  for (int j = 0; j < numevents; j++) {
+  for (int event_id = 0; event_id < numevents; event_id++) {
     int rfired = 0;
     FileEvent *event;
     EventCallbackRef cb;
-    event = _get_file_event(fired_events[j].fd);
+    event = _get_file_event(fired_events[event_id].fd);
 
     /* note the event->mask & mask & ... code: maybe an already processed
     * event removed an element that fired and we still didn't
     * processed, so we check if the event is still valid. */
-    if (event->mask & fired_events[j].mask & EVENT_READABLE) {
+    if (event->mask & fired_events[event_id].mask & EVENT_READABLE) {
       rfired = 1;
       cb = event->read_cb;
-      cb->do_request(fired_events[j].fd);
+      cb->do_request(fired_events[event_id].fd);
     }
 
-    if (event->mask & fired_events[j].mask & EVENT_WRITABLE) {
+    if (event->mask & fired_events[event_id].mask & EVENT_WRITABLE) {
       if (!rfired || event->read_cb != event->write_cb) {
         cb = event->write_cb;
-        cb->do_request(fired_events[j].fd);
+        cb->do_request(fired_events[event_id].fd);
       }
     }
 
-    ldout(cct, 30) << __func__ << " event_wq process is " << fired_events[j].fd << " mask is " << fired_events[j].mask << dendl;
+    ldout(cct, 30) << __func__ << " event_wq process is " << fired_events[event_id].fd
+                   << " mask is " << fired_events[event_id].mask << dendl;
   }
 
   if (trigger_time)
@@ -428,12 +426,12 @@ int EventCenter::process_events(int timeout_microseconds,  ceph::timespan *worki
     cur_process.swap(external_events);
     external_num_events.store(0);
     external_lock.unlock();
+    numevents += cur_process.size();
     while (!cur_process.empty()) {
       EventCallbackRef e = cur_process.front();
       ldout(cct, 30) << __func__ << " do " << e << dendl;
       e->do_request(0);
       cur_process.pop_front();
-      numevents++;
     }
   }
 
@@ -449,12 +447,16 @@ int EventCenter::process_events(int timeout_microseconds,  ceph::timespan *worki
 
 void EventCenter::dispatch_event_external(EventCallbackRef e)
 {
-  external_lock.lock();
-  external_events.push_back(e);
-  bool wake = !external_num_events.load();
-  uint64_t num = ++external_num_events;
-  external_lock.unlock();
-  if (!in_thread() && wake)
+  uint64_t num = 0;
+  {
+    std::lock_guard lock{external_lock};
+    if (external_num_events > 0 && *external_events.rbegin() == e) {
+      return;
+    }
+    external_events.push_back(e);
+    num = ++external_num_events;
+  }
+  if (num == 1 && !in_thread())
     wakeup();
 
   ldout(cct, 30) << __func__ << " " << e << " pending " << num << dendl;
diff --git a/src/msg/async/Event.h b/src/msg/async/Event.h
index b4743c1b278..5d6d04c9e8e 100644
--- a/src/msg/async/Event.h
+++ b/src/msg/async/Event.h
@@ -54,7 +54,7 @@ class EventCenter;
 class EventCallback {
 
  public:
-  virtual void do_request(int fd_or_id) = 0;
+  virtual void do_request(uint64_t fd_or_id) = 0;
   virtual ~EventCallback() {}       // we want a virtual destructor!!!
 };
 
@@ -94,7 +94,8 @@ class EventCenter {
 
   struct AssociatedCenters {
     EventCenter *centers[MAX_EVENTCENTER];
-    AssociatedCenters(CephContext *c) {
+    AssociatedCenters() {
+      // FIPS zeroization audit 20191115: this memset is not security related.
       memset(centers, 0, MAX_EVENTCENTER * sizeof(EventCenter*));
     }
   };
@@ -155,7 +156,7 @@ class EventCenter {
   std::string type;
   int nevent;
   // Used only to external event
-  pthread_t owner;
+  pthread_t owner = 0;
   std::mutex external_lock;
   std::atomic_ulong external_num_events;
   deque<EventCallbackRef> external_events;
@@ -172,12 +173,12 @@ class EventCenter {
   int notify_send_fd;
   NetHandler net;
   EventCallbackRef notify_handler;
-  unsigned idx;
+  unsigned center_id;
   AssociatedCenters *global_centers = nullptr;
 
   int process_time_events();
   FileEvent *_get_file_event(int fd) {
-    assert(fd < nevent);
+    ceph_assert(fd < nevent);
     return &file_events[fd];
   }
 
@@ -187,14 +188,14 @@ class EventCenter {
     external_num_events(0),
     driver(NULL), time_event_next_id(1),
     notify_receive_fd(-1), notify_send_fd(-1), net(c),
-    notify_handler(NULL), idx(0) { }
+    notify_handler(NULL), center_id(0) { }
   ~EventCenter();
   ostream& _event_prefix(std::ostream *_dout);
 
-  int init(int nevent, unsigned idx, const std::string &t);
+  int init(int nevent, unsigned center_id, const std::string &type);
   void set_owner();
   pthread_t get_owner() const { return owner; }
-  unsigned get_id() const { return idx; }
+  unsigned get_id() const { return center_id; }
 
   EventDriver *get_driver() { return driver; }
 
@@ -203,7 +204,7 @@ class EventCenter {
   uint64_t create_time_event(uint64_t milliseconds, EventCallbackRef ctxt);
   void delete_file_event(int fd, int mask);
   void delete_time_event(uint64_t id);
-  int process_events(int timeout_microseconds, ceph::timespan *working_dur = nullptr);
+  int process_events(unsigned timeout_microseconds, ceph::timespan *working_dur = nullptr);
   void wakeup();
 
   // Used by external thread
@@ -221,9 +222,9 @@ class EventCenter {
     func f;
     bool nonwait;
    public:
-    C_submit_event(func &&_f, bool nw)
-      : f(std::move(_f)), nonwait(nw) {}
-    void do_request(int id) override {
+    C_submit_event(func &&_f, bool nowait)
+      : f(std::move(_f)), nonwait(nowait) {}
+    void do_request(uint64_t id) override {
       f();
       lock.lock();
       cond.notify_all();
@@ -234,7 +235,7 @@ class EventCenter {
         delete this;
     }
     void wait() {
-      assert(!nonwait);
+      ceph_assert(!nonwait);
       std::unique_lock<std::mutex> l(lock);
       while (!done)
         cond.wait(l);
@@ -243,17 +244,16 @@ class EventCenter {
 
  public:
   template <typename func>
-  void submit_to(int i, func &&f, bool nowait = false) {
-    assert(i < MAX_EVENTCENTER && global_centers);
+  void submit_to(int i, func &&f, bool always_async = false) {
+    ceph_assert(i < MAX_EVENTCENTER && global_centers);
     EventCenter *c = global_centers->centers[i];
-    assert(c);
-    if (!nowait && c->in_thread()) {
-      f();
-      return ;
-    }
-    if (nowait) {
+    ceph_assert(c);
+    if (always_async) {
       C_submit_event<func> *event = new C_submit_event<func>(std::move(f), true);
       c->dispatch_event_external(event);
+    } else if (c->in_thread()) {
+      f();
+      return;
     } else {
       C_submit_event<func> event(std::move(f), false);
       c->dispatch_event_external(&event);
diff --git a/src/msg/async/EventEpoll.cc b/src/msg/async/EventEpoll.cc
index 17f3a294659..000aaf4fcbc 100644
--- a/src/msg/async/EventEpoll.cc
+++ b/src/msg/async/EventEpoll.cc
@@ -15,6 +15,7 @@
  */
 
 #include "common/errno.h"
+#include <fcntl.h>
 #include "EventEpoll.h"
 
 #define dout_subsys ceph_subsys_ms
@@ -24,12 +25,11 @@
 
 int EpollDriver::init(EventCenter *c, int nevent)
 {
-  events = (struct epoll_event*)malloc(sizeof(struct epoll_event)*nevent);
+  events = (struct epoll_event*)calloc(nevent, sizeof(struct epoll_event));
   if (!events) {
     lderr(cct) << __func__ << " unable to malloc memory. " << dendl;
     return -ENOMEM;
   }
-  memset(events, 0, sizeof(struct epoll_event)*nevent);
 
   epfd = epoll_create(1024); /* 1024 is just an hint for the kernel */
   if (epfd == -1) {
@@ -37,8 +37,16 @@ int EpollDriver::init(EventCenter *c, int nevent)
                        << cpp_strerror(errno) << dendl;
     return -errno;
   }
+  if (::fcntl(epfd, F_SETFD, FD_CLOEXEC) == -1) {
+    int e = errno;
+    ::close(epfd);
+    lderr(cct) << __func__ << " unable to set cloexec: "
+                       << cpp_strerror(e) << dendl;
 
-  size = nevent;
+    return -e;
+  }
+
+  this->nevent = nevent;
 
   return 0;
 }
@@ -74,16 +82,18 @@ int EpollDriver::del_event(int fd, int cur_mask, int delmask)
 {
   ldout(cct, 20) << __func__ << " del event fd=" << fd << " cur_mask=" << cur_mask
                  << " delmask=" << delmask << " to " << epfd << dendl;
-  struct epoll_event ee;
+  struct epoll_event ee = {0};
   int mask = cur_mask & (~delmask);
   int r = 0;
 
-  ee.events = 0;
-  if (mask & EVENT_READABLE) ee.events |= EPOLLIN;
-  if (mask & EVENT_WRITABLE) ee.events |= EPOLLOUT;
-  ee.data.u64 = 0; /* avoid valgrind warning */
-  ee.data.fd = fd;
   if (mask != EVENT_NONE) {
+    ee.events = EPOLLET;
+    ee.data.fd = fd;
+    if (mask & EVENT_READABLE)
+      ee.events |= EPOLLIN;
+    if (mask & EVENT_WRITABLE)
+      ee.events |= EPOLLOUT;
+
     if ((r = epoll_ctl(epfd, EPOLL_CTL_MOD, fd, &ee)) < 0) {
       lderr(cct) << __func__ << " epoll_ctl: modify fd=" << fd << " mask=" << mask
                  << " failed." << cpp_strerror(errno) << dendl;
@@ -110,23 +120,22 @@ int EpollDriver::event_wait(vector<FiredFileEvent> &fired_events, struct timeval
 {
   int retval, numevents = 0;
 
-  retval = epoll_wait(epfd, events, size,
+  retval = epoll_wait(epfd, events, nevent,
                       tvp ? (tvp->tv_sec*1000 + tvp->tv_usec/1000) : -1);
   if (retval > 0) {
-    int j;
-
     numevents = retval;
     fired_events.resize(numevents);
-    for (j = 0; j < numevents; j++) {
+
+    for (int event_id = 0; event_id < numevents; event_id++) {
       int mask = 0;
-      struct epoll_event *e = events + j;
+      struct epoll_event *e = &events[event_id];
 
       if (e->events & EPOLLIN) mask |= EVENT_READABLE;
       if (e->events & EPOLLOUT) mask |= EVENT_WRITABLE;
       if (e->events & EPOLLERR) mask |= EVENT_READABLE|EVENT_WRITABLE;
       if (e->events & EPOLLHUP) mask |= EVENT_READABLE|EVENT_WRITABLE;
-      fired_events[j].fd = e->data.fd;
-      fired_events[j].mask = mask;
+      fired_events[event_id].fd = e->data.fd;
+      fired_events[event_id].mask = mask;
     }
   }
   return numevents;
diff --git a/src/msg/async/EventEpoll.h b/src/msg/async/EventEpoll.h
index abc4b8bbbfb..0221f90d34c 100644
--- a/src/msg/async/EventEpoll.h
+++ b/src/msg/async/EventEpoll.h
@@ -26,10 +26,10 @@ class EpollDriver : public EventDriver {
   int epfd;
   struct epoll_event *events;
   CephContext *cct;
-  int size;
+  int nevent;
 
  public:
-  explicit EpollDriver(CephContext *c): epfd(-1), events(NULL), cct(c), size(0) {}
+  explicit EpollDriver(CephContext *c): epfd(-1), events(NULL), cct(c), nevent(0) {}
   ~EpollDriver() override {
     if (epfd != -1)
       close(epfd);
diff --git a/src/msg/async/EventKqueue.cc b/src/msg/async/EventKqueue.cc
index fd787188482..d6ba4a3db36 100644
--- a/src/msg/async/EventKqueue.cc
+++ b/src/msg/async/EventKqueue.cc
@@ -73,7 +73,7 @@ int KqueueDriver::test_thread_change(const char* funcname) {
   } else if ((kqfd != -1) && (test_kqfd() < 0)) {
     // should this ever happen?
     // It would be strange to change kqfd with thread change.
-    // Might nee to change this into an assert() in the future.
+    // Might nee to change this into an ceph_assert() in the future.
     ldout(cct,0) << funcname << " Warning: Recreating old kqfd. "
                  << "This should not happen!!!"  << dendl;
     kqfd = -1;
@@ -198,12 +198,12 @@ int KqueueDriver::resize_events(int newsize)
 {
   ldout(cct,30) << __func__ << " kqfd = " << kqfd << "newsize = " << newsize 
                 << dendl;
-  if(newsize > sav_max) {
-    sav_events = (struct SaveEvent*)realloc( sav_events, 
-                    sizeof(struct SaveEvent)*newsize);
+  if (newsize > sav_max) {
+    sav_events = (struct SaveEvent*)realloc(sav_events, sizeof(struct SaveEvent)*newsize);
     if (!sav_events) {
       lderr(cct) << __func__ << " unable to realloc memory: "
                              << cpp_strerror(errno) << dendl;
+      ceph_assert(sav_events);
       return -ENOMEM;
     }
     memset(&sav_events[size], 0, sizeof(struct SaveEvent)*(newsize-sav_max));
diff --git a/src/msg/async/PosixStack.cc b/src/msg/async/PosixStack.cc
index d0e6b5af083..0fc344c2ff4 100644
--- a/src/msg/async/PosixStack.cc
+++ b/src/msg/async/PosixStack.cc
@@ -26,11 +26,12 @@
 
 #include "include/buffer.h"
 #include "include/str_list.h"
-#include "include/sock_compat.h"
 #include "common/errno.h"
 #include "common/strtol.h"
 #include "common/dout.h"
-#include "common/simple_spin.h"
+#include "msg/Messenger.h"
+#include "include/compat.h"
+#include "include/sock_compat.h"
 
 #define dout_subsys ceph_subsys_ms
 #undef dout_prefix
@@ -41,11 +42,6 @@ class PosixConnectedSocketImpl final : public ConnectedSocketImpl {
   int _fd;
   entity_addr_t sa;
   bool connected;
-#if !defined(MSG_NOSIGNAL) && !defined(SO_NOSIGPIPE)
-  sigset_t sigpipe_mask;
-  bool sigpipe_pending;
-  bool sigpipe_unblock;
-#endif
 
  public:
   explicit PosixConnectedSocketImpl(NetHandler &h, const entity_addr_t &sa, int f, bool connected)
@@ -66,10 +62,6 @@ class PosixConnectedSocketImpl final : public ConnectedSocketImpl {
     }
   }
 
-  ssize_t zero_copy_read(bufferptr&) override {
-    return -EOPNOTSUPP;
-  }
-
   ssize_t read(char *buf, size_t len) override {
     ssize_t r = ::read(_fd, buf, len);
     if (r < 0)
@@ -77,83 +69,15 @@ class PosixConnectedSocketImpl final : public ConnectedSocketImpl {
     return r;
   }
 
-  /*
-   SIGPIPE suppression - for platforms without SO_NOSIGPIPE or MSG_NOSIGNAL
-    http://krokisplace.blogspot.in/2010/02/suppressing-sigpipe-in-library.html 
-    http://www.microhowto.info/howto/ignore_sigpipe_without_affecting_other_threads_in_a_process.html 
-  */
-  static void suppress_sigpipe()
-  {
-  #if !defined(MSG_NOSIGNAL) && !defined(SO_NOSIGPIPE)
-    /*
-      We want to ignore possible SIGPIPE that we can generate on write.
-      SIGPIPE is delivered *synchronously* and *only* to the thread
-      doing the write.  So if it is reported as already pending (which
-      means the thread blocks it), then we do nothing: if we generate
-      SIGPIPE, it will be merged with the pending one (there's no
-      queuing), and that suits us well.  If it is not pending, we block
-      it in this thread (and we avoid changing signal action, because it
-      is per-process).
-    */
-    sigset_t pending;
-    sigemptyset(&pending);
-    sigpending(&pending);
-    sigpipe_pending = sigismember(&pending, SIGPIPE);
-    if (!sigpipe_pending) {
-      sigset_t blocked;
-      sigemptyset(&blocked);
-      pthread_sigmask(SIG_BLOCK, &sigpipe_mask, &blocked);
-
-      /* Maybe is was blocked already?  */
-      sigpipe_unblock = ! sigismember(&blocked, SIGPIPE);
-    }
-  #endif /* !defined(MSG_NOSIGNAL) && !defined(SO_NOSIGPIPE) */
-  }
-
-  static void restore_sigpipe()
-  {
-  #if !defined(MSG_NOSIGNAL) && !defined(SO_NOSIGPIPE)
-    /*
-      If SIGPIPE was pending already we do nothing.  Otherwise, if it
-      become pending (i.e., we generated it), then we sigwait() it (thus
-      clearing pending status).  Then we unblock SIGPIPE, but only if it
-      were us who blocked it.
-    */
-    if (!sigpipe_pending) {
-      sigset_t pending;
-      sigemptyset(&pending);
-      sigpending(&pending);
-      if (sigismember(&pending, SIGPIPE)) {
-        /*
-          Protect ourselves from a situation when SIGPIPE was sent
-          by the user to the whole process, and was delivered to
-          other thread before we had a chance to wait for it.
-        */
-        static const struct timespec nowait = { 0, 0 };
-        TEMP_FAILURE_RETRY(sigtimedwait(&sigpipe_mask, NULL, &nowait));
-      }
-
-      if (sigpipe_unblock)
-        pthread_sigmask(SIG_UNBLOCK, &sigpipe_mask, NULL);
-    }
-  #endif  /* !defined(MSG_NOSIGNAL) && !defined(SO_NOSIGPIPE) */
-  }
-
   // return the sent length
-  // < 0 means error occured
+  // < 0 means error occurred
   static ssize_t do_sendmsg(int fd, struct msghdr &msg, unsigned len, bool more)
   {
-    suppress_sigpipe();
-
     size_t sent = 0;
     while (1) {
+      MSGR_SIGPIPE_STOPPER;
       ssize_t r;
-  #if defined(MSG_NOSIGNAL)
       r = ::sendmsg(fd, &msg, MSG_NOSIGNAL | (more ? MSG_MORE : 0));
-  #else
-      r = ::sendmsg(fd, &msg, (more ? MSG_MORE : 0));
-  #endif /* defined(MSG_NOSIGNAL) */
-
       if (r < 0) {
         if (errno == EINTR) {
           continue;
@@ -179,32 +103,29 @@ class PosixConnectedSocketImpl final : public ConnectedSocketImpl {
         }
       }
     }
-    restore_sigpipe();
     return (ssize_t)sent;
   }
 
   ssize_t send(bufferlist &bl, bool more) override {
     size_t sent_bytes = 0;
-    std::list<bufferptr>::const_iterator pb = bl.buffers().begin();
-    uint64_t left_pbrs = bl.buffers().size();
+    auto pb = std::cbegin(bl.buffers());
+    uint64_t left_pbrs = bl.get_num_buffers();
     while (left_pbrs) {
       struct msghdr msg;
       struct iovec msgvec[IOV_MAX];
-      uint64_t size = MIN(left_pbrs, IOV_MAX);
+      uint64_t size = std::min<uint64_t>(left_pbrs, IOV_MAX);
       left_pbrs -= size;
+      // FIPS zeroization audit 20191115: this memset is not security related.
       memset(&msg, 0, sizeof(msg));
-      msg.msg_iovlen = 0;
+      msg.msg_iovlen = size;
       msg.msg_iov = msgvec;
       unsigned msglen = 0;
-      while (size > 0) {
-        msgvec[msg.msg_iovlen].iov_base = (void*)(pb->c_str());
-        msgvec[msg.msg_iovlen].iov_len = pb->length();
-        msg.msg_iovlen++;
-        msglen += pb->length();
-        ++pb;
-        size--;
+      for (auto iov = msgvec; iov != msgvec + size; iov++) {
+	iov->iov_base = (void*)(pb->c_str());
+	iov->iov_len = pb->length();
+	msglen += pb->length();
+	++pb;
       }
-
       ssize_t r = do_sendmsg(_fd, msg, msglen, left_pbrs || more);
       if (r < 0)
         return r;
@@ -246,10 +167,14 @@ class PosixServerSocketImpl : public ServerSocketImpl {
   int _fd;
 
  public:
-  explicit PosixServerSocketImpl(NetHandler &h, int f): handler(h), _fd(f) {}
+  explicit PosixServerSocketImpl(NetHandler &h, int f,
+				 const entity_addr_t& listen_addr, unsigned slot)
+    : ServerSocketImpl(listen_addr.get_type(), slot),
+      handler(h), _fd(f) {}
   int accept(ConnectedSocket *sock, const SocketOptions &opts, entity_addr_t *out, Worker *w) override;
   void abort_accept() override {
     ::close(_fd);
+    _fd = -1;
   }
   int fd() const override {
     return _fd;
@@ -257,15 +182,14 @@ class PosixServerSocketImpl : public ServerSocketImpl {
 };
 
 int PosixServerSocketImpl::accept(ConnectedSocket *sock, const SocketOptions &opt, entity_addr_t *out, Worker *w) {
-  assert(sock);
+  ceph_assert(sock);
   sockaddr_storage ss;
   socklen_t slen = sizeof(ss);
-  int sd = ::accept(_fd, (sockaddr*)&ss, &slen);
+  int sd = accept_cloexec(_fd, (sockaddr*)&ss, &slen);
   if (sd < 0) {
     return -errno;
   }
 
-  handler.set_close_on_exec(sd);
   int r = handler.set_nonblock(sd);
   if (r < 0) {
     ::close(sd);
@@ -278,8 +202,9 @@ int PosixServerSocketImpl::accept(ConnectedSocket *sock, const SocketOptions &op
     return -errno;
   }
 
-  assert(NULL != out); //out should not be NULL in accept connection
+  ceph_assert(NULL != out); //out should not be NULL in accept connection
 
+  out->set_type(addr_type);
   out->set_sockaddr((sockaddr*)&ss);
   handler.set_priority(sd, opt.priority, out->get_family());
 
@@ -292,7 +217,9 @@ void PosixWorker::initialize()
 {
 }
 
-int PosixWorker::listen(entity_addr_t &sa, const SocketOptions &opt,
+int PosixWorker::listen(entity_addr_t &sa,
+			unsigned addr_slot,
+			const SocketOptions &opt,
                         ServerSocket *sock)
 {
   int listen_sd = net.create_socket(sa.get_family(), true);
@@ -306,7 +233,6 @@ int PosixWorker::listen(entity_addr_t &sa, const SocketOptions &opt,
     return -errno;
   }
 
-  net.set_close_on_exec(listen_sd);
   r = net.set_socket_options(listen_sd, opt.nodelay, opt.rcbuf_size);
   if (r < 0) {
     ::close(listen_sd);
@@ -332,7 +258,7 @@ int PosixWorker::listen(entity_addr_t &sa, const SocketOptions &opt,
 
   *sock = ServerSocket(
           std::unique_ptr<PosixServerSocketImpl>(
-              new PosixServerSocketImpl(net, listen_sd)));
+	    new PosixServerSocketImpl(net, listen_sd, sa, addr_slot)));
   return 0;
 }
 
@@ -358,14 +284,4 @@ int PosixWorker::connect(const entity_addr_t &addr, const SocketOptions &opts, C
 PosixNetworkStack::PosixNetworkStack(CephContext *c, const string &t)
     : NetworkStack(c, t)
 {
-  vector<string> corestrs;
-  get_str_vec(cct->_conf->ms_async_affinity_cores, corestrs);
-  for (auto & corestr : corestrs) {
-    string err;
-    int coreid = strict_strtol(corestr.c_str(), 10, &err);
-    if (err == "")
-      coreids.push_back(coreid);
-    else
-      lderr(cct) << __func__ << " failed to parse " << corestr << " in " << cct->_conf->ms_async_affinity_cores << dendl;
-  }
 }
diff --git a/src/msg/async/PosixStack.h b/src/msg/async/PosixStack.h
index 0fb00a8537b..f1aaccd4b82 100644
--- a/src/msg/async/PosixStack.h
+++ b/src/msg/async/PosixStack.h
@@ -30,29 +30,25 @@ class PosixWorker : public Worker {
  public:
   PosixWorker(CephContext *c, unsigned i)
       : Worker(c, i), net(c) {}
-  int listen(entity_addr_t &sa, const SocketOptions &opt,
-                     ServerSocket *socks) override;
+  int listen(entity_addr_t &sa,
+	     unsigned addr_slot,
+	     const SocketOptions &opt,
+	     ServerSocket *socks) override;
   int connect(const entity_addr_t &addr, const SocketOptions &opts, ConnectedSocket *socket) override;
 };
 
 class PosixNetworkStack : public NetworkStack {
-  vector<int> coreids;
   vector<std::thread> threads;
 
  public:
   explicit PosixNetworkStack(CephContext *c, const string &t);
 
-  int get_cpuid(int id) const {
-    if (coreids.empty())
-      return -1;
-    return coreids[id % coreids.size()];
-  }
   void spawn_worker(unsigned i, std::function<void ()> &&func) override {
     threads.resize(i+1);
     threads[i] = std::thread(func);
   }
   void join_worker(unsigned i) override {
-    assert(threads.size() > i && threads[i].joinable());
+    ceph_assert(threads.size() > i && threads[i].joinable());
     threads[i].join();
   }
 };
diff --git a/src/msg/async/Protocol.cc b/src/msg/async/Protocol.cc
new file mode 100644
index 00000000000..4bdc065ebb9
--- /dev/null
+++ b/src/msg/async/Protocol.cc
@@ -0,0 +1,14 @@
+#include "Protocol.h"
+
+#include "AsyncConnection.h"
+#include "AsyncMessenger.h"
+
+Protocol::Protocol(int type, AsyncConnection *connection)
+  : proto_type(type),
+    connection(connection),
+    messenger(connection->async_msgr),
+    cct(connection->async_msgr->cct) {
+  auth_meta.reset(new AuthConnectionMeta());
+}
+
+Protocol::~Protocol() {}
diff --git a/src/msg/async/Protocol.h b/src/msg/async/Protocol.h
new file mode 100644
index 00000000000..cccba183567
--- /dev/null
+++ b/src/msg/async/Protocol.h
@@ -0,0 +1,140 @@
+// -*- mode:C++; tab-width:8; c-basic-offset:2; indent-tabs-mode:t -*-
+// vim: ts=8 sw=2 smarttab
+
+#ifndef _MSG_ASYNC_PROTOCOL_
+#define _MSG_ASYNC_PROTOCOL_
+
+#include <list>
+#include <map>
+
+#include "AsyncConnection.h"
+#include "include/buffer.h"
+#include "include/msgr.h"
+
+/*
+ * Continuation Helper Classes
+ */
+
+#include <memory>
+#include <tuple>
+
+template <class C>
+class Ct {
+public:
+  virtual ~Ct() {}
+  virtual Ct<C> *call(C *foo) const = 0;
+};
+
+template <class C, typename... Args>
+class CtFun : public Ct<C> {
+private:
+  using fn_t = Ct<C> *(C::*)(Args...);
+  fn_t _f;
+  std::tuple<Args...> _params;
+
+  template <std::size_t... Is>
+  inline Ct<C> *_call(C *foo, std::index_sequence<Is...>) const {
+    return (foo->*_f)(std::get<Is>(_params)...);
+  }
+
+public:
+  CtFun(fn_t f) : _f(f) {}
+
+  inline void setParams(Args... args) { _params = std::make_tuple(args...); }
+  inline Ct<C> *call(C *foo) const override {
+    return _call(foo, std::index_sequence_for<Args...>());
+  }
+};
+
+using rx_buffer_t =
+    std::unique_ptr<buffer::ptr_node, buffer::ptr_node::disposer>;
+
+template <class C>
+class CtRxNode : public Ct<C> {
+  using fn_t = Ct<C> *(C::*)(rx_buffer_t&&, int r);
+  fn_t _f;
+
+public:
+  mutable rx_buffer_t node;
+  int r;
+
+  CtRxNode(fn_t f) : _f(f) {}
+  void setParams(rx_buffer_t &&node, int r) {
+    this->node = std::move(node);
+    this->r = r;
+  }
+  inline Ct<C> *call(C *foo) const override {
+    return (foo->*_f)(std::move(node), r);
+  }
+};
+
+template <class C> using CONTINUATION_TYPE = CtFun<C>;
+template <class C> using CONTINUATION_TX_TYPE = CtFun<C, int>;
+template <class C> using CONTINUATION_RX_TYPE = CtFun<C, char*, int>;
+template <class C> using CONTINUATION_RXBPTR_TYPE = CtRxNode<C>;
+
+#define CONTINUATION_DECL(C, F, ...)                    \
+  CtFun<C, ##__VA_ARGS__> F##_cont { (&C::F) };
+
+#define CONTINUATION(F) F##_cont
+#define CONTINUE(F, ...) (F##_cont.setParams(__VA_ARGS__), &F##_cont)
+
+#define CONTINUATION_RUN(CT)                                      \
+  {                                                               \
+    Ct<std::remove_reference<decltype(*this)>::type> *_cont = &CT;\
+    do {                                                          \
+      _cont = _cont->call(this);                                  \
+    } while (_cont);                                              \
+  }
+
+#define READ_HANDLER_CONTINUATION_DECL(C, F) \
+  CONTINUATION_DECL(C, F, char *, int)
+
+#define READ_BPTR_HANDLER_CONTINUATION_DECL(C, F) \
+  CtRxNode<C> F##_cont { (&C::F) };
+
+#define WRITE_HANDLER_CONTINUATION_DECL(C, F) CONTINUATION_DECL(C, F, int)
+
+//////////////////////////////////////////////////////////////////////
+
+class AsyncMessenger;
+
+class Protocol {
+public:
+  const int proto_type;
+protected:
+  AsyncConnection *connection;
+  AsyncMessenger *messenger;
+  CephContext *cct;
+public:
+  std::shared_ptr<AuthConnectionMeta> auth_meta;
+
+public:
+  Protocol(int type, AsyncConnection *connection);
+  virtual ~Protocol();
+
+  // prepare protocol for connecting to peer
+  virtual void connect() = 0;
+  // prepare protocol for accepting peer connections
+  virtual void accept() = 0;
+  // true -> protocol is ready for sending messages
+  virtual bool is_connected() = 0;
+  // stop connection
+  virtual void stop() = 0;
+  // signal and handle connection failure
+  virtual void fault() = 0;
+  // send message
+  virtual void send_message(Message *m) = 0;
+  // send keepalive
+  virtual void send_keepalive() = 0;
+
+  virtual void read_event() = 0;
+  virtual void write_event() = 0;
+  virtual bool is_queued() = 0;
+
+  int get_con_mode() const {
+    return auth_meta->con_mode;
+  }
+};
+
+#endif /* _MSG_ASYNC_PROTOCOL_ */
diff --git a/src/msg/async/ProtocolV1.cc b/src/msg/async/ProtocolV1.cc
new file mode 100644
index 00000000000..46b3f269871
--- /dev/null
+++ b/src/msg/async/ProtocolV1.cc
@@ -0,0 +1,2617 @@
+// -*- mode:C++; tab-width:8; c-basic-offset:2; indent-tabs-mode:t -*-
+// vim: ts=8 sw=2 smarttab
+
+#include "ProtocolV1.h"
+
+#include "common/errno.h"
+
+#include "AsyncConnection.h"
+#include "AsyncMessenger.h"
+#include "common/EventTrace.h"
+#include "include/random.h"
+#include "auth/AuthClient.h"
+#include "auth/AuthServer.h"
+
+#define dout_subsys ceph_subsys_ms
+#undef dout_prefix
+#define dout_prefix _conn_prefix(_dout)
+ostream &ProtocolV1::_conn_prefix(std::ostream *_dout) {
+  return *_dout << "--1- " << messenger->get_myaddrs() << " >> "
+                << *connection->peer_addrs
+		<< " conn("
+                << connection << " " << this
+                << " :" << connection->port << " s=" << get_state_name(state)
+                << " pgs=" << peer_global_seq << " cs=" << connect_seq
+                << " l=" << connection->policy.lossy << ").";
+}
+
+#define WRITE(B, C) write(CONTINUATION(C), B)
+
+#define READ(L, C) read(CONTINUATION(C), L)
+
+#define READB(L, B, C) read(CONTINUATION(C), L, B)
+
+// Constant to limit starting sequence number to 2^31.  Nothing special about
+// it, just a big number.  PLR
+#define SEQ_MASK 0x7fffffff
+
+const int ASYNC_COALESCE_THRESHOLD = 256;
+
+using namespace std;
+
+static void alloc_aligned_buffer(bufferlist &data, unsigned len, unsigned off) {
+  // create a buffer to read into that matches the data alignment
+  unsigned alloc_len = 0;
+  unsigned left = len;
+  unsigned head = 0;
+  if (off & ~CEPH_PAGE_MASK) {
+    // head
+    alloc_len += CEPH_PAGE_SIZE;
+    head = std::min<uint64_t>(CEPH_PAGE_SIZE - (off & ~CEPH_PAGE_MASK), left);
+    left -= head;
+  }
+  alloc_len += left;
+  bufferptr ptr(buffer::create_small_page_aligned(alloc_len));
+  if (head) ptr.set_offset(CEPH_PAGE_SIZE - head);
+  data.push_back(std::move(ptr));
+}
+
+/**
+ * Protocol V1
+ **/
+
+ProtocolV1::ProtocolV1(AsyncConnection *connection)
+    : Protocol(1, connection),
+      temp_buffer(nullptr),
+      can_write(WriteStatus::NOWRITE),
+      keepalive(false),
+      connect_seq(0),
+      peer_global_seq(0),
+      msg_left(0),
+      cur_msg_size(0),
+      replacing(false),
+      is_reset_from_peer(false),
+      once_ready(false),
+      state(NONE),
+      global_seq(0),
+      wait_for_seq(false) {
+  temp_buffer = new char[4096];
+}
+
+ProtocolV1::~ProtocolV1() {
+  ceph_assert(out_q.empty());
+  ceph_assert(sent.empty());
+
+  delete[] temp_buffer;
+}
+
+void ProtocolV1::connect() {
+  this->state = START_CONNECT;
+
+  // reset connect state variables
+  authorizer_buf.clear();
+  // FIPS zeroization audit 20191115: these memsets are not security related.
+  memset(&connect_msg, 0, sizeof(connect_msg));
+  memset(&connect_reply, 0, sizeof(connect_reply));
+
+  global_seq = messenger->get_global_seq();
+}
+
+void ProtocolV1::accept() { this->state = START_ACCEPT; }
+
+bool ProtocolV1::is_connected() {
+  return can_write.load() == WriteStatus::CANWRITE;
+}
+
+void ProtocolV1::stop() {
+  ldout(cct, 20) << __func__ << dendl;
+  if (state == CLOSED) {
+    return;
+  }
+
+  if (connection->delay_state) connection->delay_state->flush();
+
+  ldout(cct, 2) << __func__ << dendl;
+  std::lock_guard<std::mutex> l(connection->write_lock);
+
+  reset_recv_state();
+  discard_out_queue();
+
+  connection->_stop();
+
+  can_write = WriteStatus::CLOSED;
+  state = CLOSED;
+}
+
+void ProtocolV1::fault() {
+  ldout(cct, 20) << __func__ << dendl;
+
+  if (state == CLOSED || state == NONE) {
+    ldout(cct, 10) << __func__ << " connection is already closed" << dendl;
+    return;
+  }
+
+  if (connection->policy.lossy && state != START_CONNECT &&
+      state != CONNECTING) {
+    ldout(cct, 1) << __func__ << " on lossy channel, failing" << dendl;
+    stop();
+    connection->dispatch_queue->queue_reset(connection);
+    return;
+  }
+
+  connection->write_lock.lock();
+  can_write = WriteStatus::NOWRITE;
+  is_reset_from_peer = false;
+
+  // requeue sent items
+  requeue_sent();
+
+  if (!once_ready && out_q.empty() && state >= START_ACCEPT &&
+      state <= ACCEPTING_WAIT_CONNECT_MSG_AUTH && !replacing) {
+    ldout(cct, 10) << __func__ << " with nothing to send and in the half "
+                   << " accept state just closed" << dendl;
+    connection->write_lock.unlock();
+    stop();
+    connection->dispatch_queue->queue_reset(connection);
+    return;
+  }
+  replacing = false;
+
+  connection->fault();
+
+  reset_recv_state();
+
+  if (connection->policy.standby && out_q.empty() && !keepalive &&
+      state != WAIT) {
+    ldout(cct, 10) << __func__ << " with nothing to send, going to standby"
+                   << dendl;
+    state = STANDBY;
+    connection->write_lock.unlock();
+    return;
+  }
+
+  connection->write_lock.unlock();
+
+  if ((state >= START_CONNECT && state <= CONNECTING_SEND_CONNECT_MSG) ||
+      state == WAIT) {
+    // backoff!
+    if (state == WAIT) {
+      backoff.set_from_double(cct->_conf->ms_max_backoff);
+    } else if (backoff == utime_t()) {
+      backoff.set_from_double(cct->_conf->ms_initial_backoff);
+    } else {
+      backoff += backoff;
+      if (backoff > cct->_conf->ms_max_backoff)
+        backoff.set_from_double(cct->_conf->ms_max_backoff);
+    }
+
+    global_seq = messenger->get_global_seq();
+    state = START_CONNECT;
+    connection->state = AsyncConnection::STATE_CONNECTING;
+    ldout(cct, 10) << __func__ << " waiting " << backoff << dendl;
+    // woke up again;
+    connection->register_time_events.insert(
+        connection->center->create_time_event(backoff.to_nsec() / 1000,
+                                              connection->wakeup_handler));
+  } else {
+    // policy maybe empty when state is in accept
+    if (connection->policy.server) {
+      ldout(cct, 0) << __func__ << " server, going to standby" << dendl;
+      state = STANDBY;
+    } else {
+      ldout(cct, 0) << __func__ << " initiating reconnect" << dendl;
+      connect_seq++;
+      global_seq = messenger->get_global_seq();
+      state = START_CONNECT;
+      connection->state = AsyncConnection::STATE_CONNECTING;
+    }
+    backoff = utime_t();
+    connection->center->dispatch_event_external(connection->read_handler);
+  }
+}
+
+void ProtocolV1::send_message(Message *m) {
+  bufferlist bl;
+  uint64_t f = connection->get_features();
+
+  // TODO: Currently not all messages supports reencode like MOSDMap, so here
+  // only let fast dispatch support messages prepare message
+  bool can_fast_prepare = messenger->ms_can_fast_dispatch(m);
+  if (can_fast_prepare) {
+    prepare_send_message(f, m, bl);
+  }
+
+  std::lock_guard<std::mutex> l(connection->write_lock);
+  // "features" changes will change the payload encoding
+  if (can_fast_prepare &&
+      (can_write == WriteStatus::NOWRITE || connection->get_features() != f)) {
+    // ensure the correctness of message encoding
+    bl.clear();
+    m->clear_payload();
+    ldout(cct, 5) << __func__ << " clear encoded buffer previous " << f
+                  << " != " << connection->get_features() << dendl;
+  }
+  if (can_write == WriteStatus::CLOSED) {
+    ldout(cct, 10) << __func__ << " connection closed."
+                   << " Drop message " << m << dendl;
+    m->put();
+  } else {
+    m->queue_start = ceph::mono_clock::now();
+    m->trace.event("async enqueueing message");
+    out_q[m->get_priority()].emplace_back(std::move(bl), m);
+    ldout(cct, 15) << __func__ << " inline write is denied, reschedule m=" << m
+                   << dendl;
+    if (can_write != WriteStatus::REPLACING && !write_in_progress) {
+      write_in_progress = true;
+      connection->center->dispatch_event_external(connection->write_handler);
+    }
+  }
+}
+
+void ProtocolV1::prepare_send_message(uint64_t features, Message *m,
+                                      bufferlist &bl) {
+  ldout(cct, 20) << __func__ << " m " << *m << dendl;
+
+  // associate message with Connection (for benefit of encode_payload)
+  ldout(cct, 20) << __func__ << (m->empty_payload() ? " encoding features " : " half-reencoding features ")
+		 << features << " " << m  << " " << *m << dendl;
+
+  // encode and copy out of *m
+  // in write_message we update header.seq and need recalc crc
+  // so skip calc header in encode function.
+  m->encode(features, messenger->crcflags, true);
+
+  bl.append(m->get_payload());
+  bl.append(m->get_middle());
+  bl.append(m->get_data());
+}
+
+void ProtocolV1::send_keepalive() {
+  ldout(cct, 10) << __func__ << dendl;
+  std::lock_guard<std::mutex> l(connection->write_lock);
+  if (can_write != WriteStatus::CLOSED) {
+    keepalive = true;
+    connection->center->dispatch_event_external(connection->write_handler);
+  }
+}
+
+void ProtocolV1::read_event() {
+  ldout(cct, 20) << __func__ << dendl;
+  switch (state) {
+    case START_CONNECT:
+      CONTINUATION_RUN(CONTINUATION(send_client_banner));
+      break;
+    case START_ACCEPT:
+      CONTINUATION_RUN(CONTINUATION(send_server_banner));
+      break;
+    case OPENED:
+      CONTINUATION_RUN(CONTINUATION(wait_message));
+      break;
+    case THROTTLE_MESSAGE:
+      CONTINUATION_RUN(CONTINUATION(throttle_message));
+      break;
+    case THROTTLE_BYTES:
+      CONTINUATION_RUN(CONTINUATION(throttle_bytes));
+      break;
+    case THROTTLE_DISPATCH_QUEUE:
+      CONTINUATION_RUN(CONTINUATION(throttle_dispatch_queue));
+      break;
+    default:
+      break;
+  }
+}
+
+void ProtocolV1::write_event() {
+  ldout(cct, 10) << __func__ << dendl;
+  ssize_t r = 0;
+
+  connection->write_lock.lock();
+  if (can_write == WriteStatus::CANWRITE) {
+    if (keepalive) {
+      append_keepalive_or_ack();
+      keepalive = false;
+    }
+
+    auto start = ceph::mono_clock::now();
+    bool more;
+    do {
+      bufferlist data;
+      Message *m = _get_next_outgoing(&data);
+      if (!m) {
+        break;
+      }
+
+      if (!connection->policy.lossy) {
+        // put on sent list
+        sent.push_back(m);
+        m->get();
+      }
+      more = !out_q.empty();
+      connection->write_lock.unlock();
+
+      // send_message or requeue messages may not encode message
+      if (!data.length()) {
+        prepare_send_message(connection->get_features(), m, data);
+      }
+
+      if (m->queue_start != ceph::mono_time()) {
+        connection->logger->tinc(l_msgr_send_messages_queue_lat,
+				 ceph::mono_clock::now() - m->queue_start);
+      }
+
+      r = write_message(m, data, more);
+
+      connection->write_lock.lock();
+      if (r == 0) {
+        ;
+      } else if (r < 0) {
+        ldout(cct, 1) << __func__ << " send msg failed" << dendl;
+        break;
+      } else if (r > 0) {
+	// Outbound message in-progress, thread will be re-awoken
+	// when the outbound socket is writeable again
+	break;
+      }
+    } while (can_write == WriteStatus::CANWRITE);
+    write_in_progress = false;
+    connection->write_lock.unlock();
+
+    // if r > 0 mean data still lefted, so no need _try_send.
+    if (r == 0) {
+      uint64_t left = ack_left;
+      if (left) {
+        ceph_le64 s;
+        s = in_seq;
+        connection->outgoing_bl.append(CEPH_MSGR_TAG_ACK);
+        connection->outgoing_bl.append((char *)&s, sizeof(s));
+        ldout(cct, 10) << __func__ << " try send msg ack, acked " << left
+                       << " messages" << dendl;
+        ack_left -= left;
+        left = ack_left;
+        r = connection->_try_send(left);
+      } else if (is_queued()) {
+        r = connection->_try_send();
+      }
+    }
+
+    connection->logger->tinc(l_msgr_running_send_time,
+                             ceph::mono_clock::now() - start);
+    if (r < 0) {
+      ldout(cct, 1) << __func__ << " send msg failed" << dendl;
+      connection->lock.lock();
+      fault();
+      connection->lock.unlock();
+      return;
+    }
+  } else {
+    write_in_progress = false;
+    connection->write_lock.unlock();
+    connection->lock.lock();
+    connection->write_lock.lock();
+    if (state == STANDBY && !connection->policy.server && is_queued()) {
+      ldout(cct, 10) << __func__ << " policy.server is false" << dendl;
+      connection->_connect();
+    } else if (connection->cs && state != NONE && state != CLOSED &&
+               state != START_CONNECT) {
+      r = connection->_try_send();
+      if (r < 0) {
+        ldout(cct, 1) << __func__ << " send outcoming bl failed" << dendl;
+        connection->write_lock.unlock();
+        fault();
+        connection->lock.unlock();
+        return;
+      }
+    }
+    connection->write_lock.unlock();
+    connection->lock.unlock();
+  }
+}
+
+bool ProtocolV1::is_queued() {
+  return !out_q.empty() || connection->is_queued();
+}
+
+void ProtocolV1::run_continuation(CtPtr pcontinuation) {
+  if (pcontinuation) {
+    CONTINUATION_RUN(*pcontinuation);
+  }
+}
+
+CtPtr ProtocolV1::read(CONTINUATION_RX_TYPE<ProtocolV1> &next,
+                       int len, char *buffer) {
+  if (!buffer) {
+    buffer = temp_buffer;
+  }
+  ssize_t r = connection->read(len, buffer,
+                               [&next, this](char *buffer, int r) {
+                                 next.setParams(buffer, r);
+                                 CONTINUATION_RUN(next);
+                               });
+  if (r <= 0) {
+    next.setParams(buffer, r);
+    return &next;
+  }
+
+  return nullptr;
+}
+
+CtPtr ProtocolV1::write(CONTINUATION_TX_TYPE<ProtocolV1> &next,
+                        bufferlist &buffer) {
+  ssize_t r = connection->write(buffer, [&next, this](int r) {
+    next.setParams(r);
+    CONTINUATION_RUN(next);
+  });
+  if (r <= 0) {
+    next.setParams(r);
+    return &next;
+  }
+
+  return nullptr;
+}
+
+CtPtr ProtocolV1::ready() {
+  ldout(cct, 25) << __func__ << dendl;
+
+  // make sure no pending tick timer
+  if (connection->last_tick_id) {
+    connection->center->delete_time_event(connection->last_tick_id);
+  }
+  connection->last_tick_id = connection->center->create_time_event(
+      connection->inactive_timeout_us, connection->tick_handler);
+
+  connection->write_lock.lock();
+  can_write = WriteStatus::CANWRITE;
+  if (is_queued()) {
+    connection->center->dispatch_event_external(connection->write_handler);
+  }
+  connection->write_lock.unlock();
+  connection->maybe_start_delay_thread();
+
+  state = OPENED;
+  return wait_message();
+}
+
+CtPtr ProtocolV1::wait_message() {
+  if (state != OPENED) {  // must have changed due to a replace
+    return nullptr;
+  }
+
+  ldout(cct, 20) << __func__ << dendl;
+
+  return READ(sizeof(char), handle_message);
+}
+
+CtPtr ProtocolV1::handle_message(char *buffer, int r) {
+  ldout(cct, 20) << __func__ << " r=" << r << dendl;
+
+  if (r < 0) {
+    ldout(cct, 1) << __func__ << " read tag failed" << dendl;
+    return _fault();
+  }
+
+  char tag = buffer[0];
+  ldout(cct, 20) << __func__ << " process tag " << (int)tag << dendl;
+
+  if (tag == CEPH_MSGR_TAG_KEEPALIVE) {
+    ldout(cct, 20) << __func__ << " got KEEPALIVE" << dendl;
+    connection->set_last_keepalive(ceph_clock_now());
+  } else if (tag == CEPH_MSGR_TAG_KEEPALIVE2) {
+    return READ(sizeof(ceph_timespec), handle_keepalive2);
+  } else if (tag == CEPH_MSGR_TAG_KEEPALIVE2_ACK) {
+    return READ(sizeof(ceph_timespec), handle_keepalive2_ack);
+  } else if (tag == CEPH_MSGR_TAG_ACK) {
+    return READ(sizeof(ceph_le64), handle_tag_ack);
+  } else if (tag == CEPH_MSGR_TAG_MSG) {
+    recv_stamp = ceph_clock_now();
+    ldout(cct, 20) << __func__ << " begin MSG" << dendl;
+    return READ(sizeof(ceph_msg_header), handle_message_header);
+  } else if (tag == CEPH_MSGR_TAG_CLOSE) {
+    ldout(cct, 20) << __func__ << " got CLOSE" << dendl;
+    stop();
+  } else {
+    ldout(cct, 0) << __func__ << " bad tag " << (int)tag << dendl;
+    return _fault();
+  }
+  return nullptr;
+}
+
+CtPtr ProtocolV1::handle_keepalive2(char *buffer, int r) {
+  ldout(cct, 20) << __func__ << " r=" << r << dendl;
+
+  if (r < 0) {
+    ldout(cct, 1) << __func__ << " read keeplive timespec failed" << dendl;
+    return _fault();
+  }
+
+  ldout(cct, 30) << __func__ << " got KEEPALIVE2 tag ..." << dendl;
+
+  ceph_timespec *t;
+  t = (ceph_timespec *)buffer;
+  utime_t kp_t = utime_t(*t);
+  connection->write_lock.lock();
+  append_keepalive_or_ack(true, &kp_t);
+  connection->write_lock.unlock();
+
+  ldout(cct, 20) << __func__ << " got KEEPALIVE2 " << kp_t << dendl;
+  connection->set_last_keepalive(ceph_clock_now());
+
+  if (is_connected()) {
+    connection->center->dispatch_event_external(connection->write_handler);
+  }
+
+  return CONTINUE(wait_message);
+}
+
+void ProtocolV1::append_keepalive_or_ack(bool ack, utime_t *tp) {
+  ldout(cct, 10) << __func__ << dendl;
+  if (ack) {
+    ceph_assert(tp);
+    struct ceph_timespec ts;
+    tp->encode_timeval(&ts);
+    connection->outgoing_bl.append(CEPH_MSGR_TAG_KEEPALIVE2_ACK);
+    connection->outgoing_bl.append((char *)&ts, sizeof(ts));
+  } else if (connection->has_feature(CEPH_FEATURE_MSGR_KEEPALIVE2)) {
+    struct ceph_timespec ts;
+    utime_t t = ceph_clock_now();
+    t.encode_timeval(&ts);
+    connection->outgoing_bl.append(CEPH_MSGR_TAG_KEEPALIVE2);
+    connection->outgoing_bl.append((char *)&ts, sizeof(ts));
+  } else {
+    connection->outgoing_bl.append(CEPH_MSGR_TAG_KEEPALIVE);
+  }
+}
+
+CtPtr ProtocolV1::handle_keepalive2_ack(char *buffer, int r) {
+  ldout(cct, 20) << __func__ << " r=" << r << dendl;
+
+  if (r < 0) {
+    ldout(cct, 1) << __func__ << " read keeplive timespec failed" << dendl;
+    return _fault();
+  }
+
+  ceph_timespec *t;
+  t = (ceph_timespec *)buffer;
+  connection->set_last_keepalive_ack(utime_t(*t));
+  ldout(cct, 20) << __func__ << " got KEEPALIVE_ACK" << dendl;
+
+  return CONTINUE(wait_message);
+}
+
+CtPtr ProtocolV1::handle_tag_ack(char *buffer, int r) {
+  ldout(cct, 20) << __func__ << " r=" << r << dendl;
+
+  if (r < 0) {
+    ldout(cct, 1) << __func__ << " read ack seq failed" << dendl;
+    return _fault();
+  }
+
+  ceph_le64 seq;
+  seq = *(ceph_le64 *)buffer;
+  ldout(cct, 20) << __func__ << " got ACK" << dendl;
+
+  ldout(cct, 15) << __func__ << " got ack seq " << seq << dendl;
+  // trim sent list
+  static const int max_pending = 128;
+  int i = 0;
+  auto now = ceph::mono_clock::now();
+  Message *pending[max_pending];
+  connection->write_lock.lock();
+  while (!sent.empty() && sent.front()->get_seq() <= seq && i < max_pending) {
+    Message *m = sent.front();
+    sent.pop_front();
+    pending[i++] = m;
+    ldout(cct, 10) << __func__ << " got ack seq " << seq
+                   << " >= " << m->get_seq() << " on " << m << " " << *m
+                   << dendl;
+  }
+  connection->write_lock.unlock();
+  connection->logger->tinc(l_msgr_handle_ack_lat, ceph::mono_clock::now() - now);
+  for (int k = 0; k < i; k++) {
+    pending[k]->put();
+  }
+
+  return CONTINUE(wait_message);
+}
+
+CtPtr ProtocolV1::handle_message_header(char *buffer, int r) {
+  ldout(cct, 20) << __func__ << " r=" << r << dendl;
+
+  if (r < 0) {
+    ldout(cct, 1) << __func__ << " read message header failed" << dendl;
+    return _fault();
+  }
+
+  ldout(cct, 20) << __func__ << " got MSG header" << dendl;
+
+  current_header = *((ceph_msg_header *)buffer);
+
+  ldout(cct, 20) << __func__ << " got envelope type=" << current_header.type << " src "
+                 << entity_name_t(current_header.src) << " front=" << current_header.front_len
+                 << " data=" << current_header.data_len << " off " << current_header.data_off
+                 << dendl;
+
+  if (messenger->crcflags & MSG_CRC_HEADER) {
+    __u32 header_crc = 0;
+    header_crc = ceph_crc32c(0, (unsigned char *)&current_header,
+                             sizeof(current_header) - sizeof(current_header.crc));
+    // verify header crc
+    if (header_crc != current_header.crc) {
+      ldout(cct, 0) << __func__ << " got bad header crc " << header_crc
+                    << " != " << current_header.crc << dendl;
+      return _fault();
+    }
+  }
+
+  // Reset state
+  data_buf.clear();
+  front.clear();
+  middle.clear();
+  data.clear();
+
+  state = THROTTLE_MESSAGE;
+  return CONTINUE(throttle_message);
+}
+
+CtPtr ProtocolV1::throttle_message() {
+  ldout(cct, 20) << __func__ << dendl;
+
+  if (connection->policy.throttler_messages) {
+    ldout(cct, 10) << __func__ << " wants " << 1
+                   << " message from policy throttler "
+                   << connection->policy.throttler_messages->get_current()
+                   << "/" << connection->policy.throttler_messages->get_max()
+                   << dendl;
+    if (!connection->policy.throttler_messages->get_or_fail()) {
+      ldout(cct, 10) << __func__ << " wants 1 message from policy throttle "
+                     << connection->policy.throttler_messages->get_current()
+                     << "/" << connection->policy.throttler_messages->get_max()
+                     << " failed, just wait." << dendl;
+      // following thread pool deal with th full message queue isn't a
+      // short time, so we can wait a ms.
+      if (connection->register_time_events.empty()) {
+        connection->register_time_events.insert(
+            connection->center->create_time_event(1000,
+                                                  connection->wakeup_handler));
+      }
+      return nullptr;
+    }
+  }
+
+  state = THROTTLE_BYTES;
+  return CONTINUE(throttle_bytes);
+}
+
+CtPtr ProtocolV1::throttle_bytes() {
+  ldout(cct, 20) << __func__ << dendl;
+
+  cur_msg_size = current_header.front_len + current_header.middle_len +
+                 current_header.data_len;
+  if (cur_msg_size) {
+    if (connection->policy.throttler_bytes) {
+      ldout(cct, 10) << __func__ << " wants " << cur_msg_size
+                     << " bytes from policy throttler "
+                     << connection->policy.throttler_bytes->get_current() << "/"
+                     << connection->policy.throttler_bytes->get_max() << dendl;
+      if (!connection->policy.throttler_bytes->get_or_fail(cur_msg_size)) {
+        ldout(cct, 10) << __func__ << " wants " << cur_msg_size
+                       << " bytes from policy throttler "
+                       << connection->policy.throttler_bytes->get_current()
+                       << "/" << connection->policy.throttler_bytes->get_max()
+                       << " failed, just wait." << dendl;
+        // following thread pool deal with th full message queue isn't a
+        // short time, so we can wait a ms.
+        if (connection->register_time_events.empty()) {
+          connection->register_time_events.insert(
+              connection->center->create_time_event(
+                  1000, connection->wakeup_handler));
+        }
+        return nullptr;
+      }
+    }
+  }
+
+  state = THROTTLE_DISPATCH_QUEUE;
+  return CONTINUE(throttle_dispatch_queue);
+}
+
+CtPtr ProtocolV1::throttle_dispatch_queue() {
+  ldout(cct, 20) << __func__ << dendl;
+
+  if (cur_msg_size) {
+    if (!connection->dispatch_queue->dispatch_throttler.get_or_fail(
+            cur_msg_size)) {
+      ldout(cct, 10)
+          << __func__ << " wants " << cur_msg_size
+          << " bytes from dispatch throttle "
+          << connection->dispatch_queue->dispatch_throttler.get_current() << "/"
+          << connection->dispatch_queue->dispatch_throttler.get_max()
+          << " failed, just wait." << dendl;
+      // following thread pool deal with th full message queue isn't a
+      // short time, so we can wait a ms.
+      if (connection->register_time_events.empty()) {
+        connection->register_time_events.insert(
+            connection->center->create_time_event(1000,
+                                                  connection->wakeup_handler));
+      }
+      return nullptr;
+    }
+  }
+
+  throttle_stamp = ceph_clock_now();
+
+  state = READ_MESSAGE_FRONT;
+  return read_message_front();
+}
+
+CtPtr ProtocolV1::read_message_front() {
+  ldout(cct, 20) << __func__ << dendl;
+
+  unsigned front_len = current_header.front_len;
+  if (front_len) {
+    if (!front.length()) {
+      front.push_back(buffer::create(front_len));
+    }
+    return READB(front_len, front.c_str(), handle_message_front);
+  }
+  return read_message_middle();
+}
+
+CtPtr ProtocolV1::handle_message_front(char *buffer, int r) {
+  ldout(cct, 20) << __func__ << " r=" << r << dendl;
+
+  if (r < 0) {
+    ldout(cct, 1) << __func__ << " read message front failed" << dendl;
+    return _fault();
+  }
+
+  ldout(cct, 20) << __func__ << " got front " << front.length() << dendl;
+
+  return read_message_middle();
+}
+
+CtPtr ProtocolV1::read_message_middle() {
+  ldout(cct, 20) << __func__ << dendl;
+
+  if (current_header.middle_len) {
+    if (!middle.length()) {
+      middle.push_back(buffer::create(current_header.middle_len));
+    }
+    return READB(current_header.middle_len, middle.c_str(),
+                 handle_message_middle);
+  }
+
+  return read_message_data_prepare();
+}
+
+CtPtr ProtocolV1::handle_message_middle(char *buffer, int r) {
+  ldout(cct, 20) << __func__ << " r" << r << dendl;
+
+  if (r < 0) {
+    ldout(cct, 1) << __func__ << " read message middle failed" << dendl;
+    return _fault();
+  }
+
+  ldout(cct, 20) << __func__ << " got middle " << middle.length() << dendl;
+
+  return read_message_data_prepare();
+}
+
+CtPtr ProtocolV1::read_message_data_prepare() {
+  ldout(cct, 20) << __func__ << dendl;
+
+  unsigned data_len = current_header.data_len;
+  unsigned data_off = current_header.data_off;
+
+  if (data_len) {
+    // get a buffer
+#if 0
+    // rx_buffers is broken by design... see
+    //  http://tracker.ceph.com/issues/22480
+    map<ceph_tid_t, pair<bufferlist, int> >::iterator p =
+        connection->rx_buffers.find(current_header.tid);
+    if (p != connection->rx_buffers.end()) {
+      ldout(cct, 10) << __func__ << " seleting rx buffer v " << p->second.second
+                     << " at offset " << data_off << " len "
+                     << p->second.first.length() << dendl;
+      data_buf = p->second.first;
+      // make sure it's big enough
+      if (data_buf.length() < data_len)
+        data_buf.push_back(buffer::create(data_len - data_buf.length()));
+      data_blp = data_buf.begin();
+    } else {
+      ldout(cct, 20) << __func__ << " allocating new rx buffer at offset "
+                     << data_off << dendl;
+      alloc_aligned_buffer(data_buf, data_len, data_off);
+      data_blp = data_buf.begin();
+    }
+#else
+    ldout(cct, 20) << __func__ << " allocating new rx buffer at offset "
+		   << data_off << dendl;
+    alloc_aligned_buffer(data_buf, data_len, data_off);
+    data_blp = data_buf.begin();
+#endif
+  }
+
+  msg_left = data_len;
+
+  return CONTINUE(read_message_data);
+}
+
+CtPtr ProtocolV1::read_message_data() {
+  ldout(cct, 20) << __func__ << " msg_left=" << msg_left << dendl;
+
+  if (msg_left > 0) {
+    bufferptr bp = data_blp.get_current_ptr();
+    unsigned read_len = std::min(bp.length(), msg_left);
+
+    return READB(read_len, bp.c_str(), handle_message_data);
+  }
+
+  return read_message_footer();
+}
+
+CtPtr ProtocolV1::handle_message_data(char *buffer, int r) {
+  ldout(cct, 20) << __func__ << " r=" << r << dendl;
+
+  if (r < 0) {
+    ldout(cct, 1) << __func__ << " read data error " << dendl;
+    return _fault();
+  }
+
+  bufferptr bp = data_blp.get_current_ptr();
+  unsigned read_len = std::min(bp.length(), msg_left);
+  ceph_assert(read_len <
+	      static_cast<unsigned>(std::numeric_limits<int>::max()));
+  data_blp += read_len;
+  data.append(bp, 0, read_len);
+  msg_left -= read_len;
+
+  return CONTINUE(read_message_data);
+}
+
+CtPtr ProtocolV1::read_message_footer() {
+  ldout(cct, 20) << __func__ << dendl;
+
+  state = READ_FOOTER_AND_DISPATCH;
+
+  unsigned len;
+  if (connection->has_feature(CEPH_FEATURE_MSG_AUTH)) {
+    len = sizeof(ceph_msg_footer);
+  } else {
+    len = sizeof(ceph_msg_footer_old);
+  }
+
+  return READ(len, handle_message_footer);
+}
+
+CtPtr ProtocolV1::handle_message_footer(char *buffer, int r) {
+  ldout(cct, 20) << __func__ << " r=" << r << dendl;
+
+  if (r < 0) {
+    ldout(cct, 1) << __func__ << " read footer data error " << dendl;
+    return _fault();
+  }
+
+  ceph_msg_footer footer;
+  ceph_msg_footer_old old_footer;
+
+  if (connection->has_feature(CEPH_FEATURE_MSG_AUTH)) {
+    footer = *((ceph_msg_footer *)buffer);
+  } else {
+    old_footer = *((ceph_msg_footer_old *)buffer);
+    footer.front_crc = old_footer.front_crc;
+    footer.middle_crc = old_footer.middle_crc;
+    footer.data_crc = old_footer.data_crc;
+    footer.sig = 0;
+    footer.flags = old_footer.flags;
+  }
+
+  int aborted = (footer.flags & CEPH_MSG_FOOTER_COMPLETE) == 0;
+  ldout(cct, 10) << __func__ << " aborted = " << aborted << dendl;
+  if (aborted) {
+    ldout(cct, 0) << __func__ << " got " << front.length() << " + "
+                  << middle.length() << " + " << data.length()
+                  << " byte message.. ABORTED" << dendl;
+    return _fault();
+  }
+
+  ldout(cct, 20) << __func__ << " got " << front.length() << " + "
+                 << middle.length() << " + " << data.length() << " byte message"
+                 << dendl;
+  Message *message = decode_message(cct, messenger->crcflags, current_header,
+                                    footer, front, middle, data, connection);
+  if (!message) {
+    ldout(cct, 1) << __func__ << " decode message failed " << dendl;
+    return _fault();
+  }
+
+  //
+  //  Check the signature if one should be present.  A zero return indicates
+  //  success. PLR
+  //
+
+  if (session_security.get() == NULL) {
+    ldout(cct, 10) << __func__ << " no session security set" << dendl;
+  } else {
+    if (session_security->check_message_signature(message)) {
+      ldout(cct, 0) << __func__ << " Signature check failed" << dendl;
+      message->put();
+      return _fault();
+    }
+  }
+  message->set_byte_throttler(connection->policy.throttler_bytes);
+  message->set_message_throttler(connection->policy.throttler_messages);
+
+  // store reservation size in message, so we don't get confused
+  // by messages entering the dispatch queue through other paths.
+  message->set_dispatch_throttle_size(cur_msg_size);
+
+  message->set_recv_stamp(recv_stamp);
+  message->set_throttle_stamp(throttle_stamp);
+  message->set_recv_complete_stamp(ceph_clock_now());
+
+  // check received seq#.  if it is old, drop the message.
+  // note that incoming messages may skip ahead.  this is convenient for the
+  // client side queueing because messages can't be renumbered, but the (kernel)
+  // client will occasionally pull a message out of the sent queue to send
+  // elsewhere.  in that case it doesn't matter if we "got" it or not.
+  uint64_t cur_seq = in_seq;
+  if (message->get_seq() <= cur_seq) {
+    ldout(cct, 0) << __func__ << " got old message " << message->get_seq()
+                  << " <= " << cur_seq << " " << message << " " << *message
+                  << ", discarding" << dendl;
+    message->put();
+    if (connection->has_feature(CEPH_FEATURE_RECONNECT_SEQ) &&
+        cct->_conf->ms_die_on_old_message) {
+      ceph_assert(0 == "old msgs despite reconnect_seq feature");
+    }
+    return nullptr;
+  }
+  if (message->get_seq() > cur_seq + 1) {
+    ldout(cct, 0) << __func__ << " missed message?  skipped from seq "
+                  << cur_seq << " to " << message->get_seq() << dendl;
+    if (cct->_conf->ms_die_on_skipped_message) {
+      ceph_assert(0 == "skipped incoming seq");
+    }
+  }
+
+#if defined(WITH_EVENTTRACE)
+  if (message->get_type() == CEPH_MSG_OSD_OP ||
+      message->get_type() == CEPH_MSG_OSD_OPREPLY) {
+    utime_t ltt_processed_stamp = ceph_clock_now();
+    double usecs_elapsed =
+      ((double)(ltt_processed_stamp.to_nsec() - recv_stamp.to_nsec())) / 1000;
+    ostringstream buf;
+    if (message->get_type() == CEPH_MSG_OSD_OP)
+      OID_ELAPSED_WITH_MSG(message, usecs_elapsed, "TIME_TO_DECODE_OSD_OP",
+                           false);
+    else
+      OID_ELAPSED_WITH_MSG(message, usecs_elapsed, "TIME_TO_DECODE_OSD_OPREPLY",
+                           false);
+  }
+#endif
+
+  // note last received message.
+  in_seq = message->get_seq();
+  ldout(cct, 5) << " rx " << message->get_source() << " seq "
+                << message->get_seq() << " " << message << " " << *message
+                << dendl;
+
+  bool need_dispatch_writer = false;
+  if (!connection->policy.lossy) {
+    ack_left++;
+    need_dispatch_writer = true;
+  }
+
+  state = OPENED;
+
+  ceph::mono_time fast_dispatch_time;
+
+  if (connection->is_blackhole()) {
+    ldout(cct, 10) << __func__ << " blackhole " << *message << dendl;
+    message->put();
+    goto out;
+  }
+
+  connection->logger->inc(l_msgr_recv_messages);
+  connection->logger->inc(
+      l_msgr_recv_bytes,
+      cur_msg_size + sizeof(ceph_msg_header) + sizeof(ceph_msg_footer));
+
+  messenger->ms_fast_preprocess(message);
+  fast_dispatch_time = ceph::mono_clock::now();
+  connection->logger->tinc(l_msgr_running_recv_time,
+			   fast_dispatch_time - connection->recv_start_time);
+  if (connection->delay_state) {
+    double delay_period = 0;
+    if (rand() % 10000 < cct->_conf->ms_inject_delay_probability * 10000.0) {
+      delay_period =
+          cct->_conf->ms_inject_delay_max * (double)(rand() % 10000) / 10000.0;
+      ldout(cct, 1) << "queue_received will delay after "
+                    << (ceph_clock_now() + delay_period) << " on " << message
+                    << " " << *message << dendl;
+    }
+    connection->delay_state->queue(delay_period, message);
+  } else if (messenger->ms_can_fast_dispatch(message)) {
+    connection->lock.unlock();
+    connection->dispatch_queue->fast_dispatch(message);
+    connection->recv_start_time = ceph::mono_clock::now();
+    connection->logger->tinc(l_msgr_running_fast_dispatch_time,
+                             connection->recv_start_time - fast_dispatch_time);
+    connection->lock.lock();
+  } else {
+    connection->dispatch_queue->enqueue(message, message->get_priority(),
+                                        connection->conn_id);
+  }
+
+ out:
+  // clean up local buffer references
+  data_buf.clear();
+  front.clear();
+  middle.clear();
+  data.clear();
+
+  if (need_dispatch_writer && connection->is_connected()) {
+    connection->center->dispatch_event_external(connection->write_handler);
+  }
+
+  return CONTINUE(wait_message);
+}
+
+void ProtocolV1::session_reset() {
+  ldout(cct, 10) << __func__ << " started" << dendl;
+
+  std::lock_guard<std::mutex> l(connection->write_lock);
+  if (connection->delay_state) {
+    connection->delay_state->discard();
+  }
+
+  connection->dispatch_queue->discard_queue(connection->conn_id);
+  discard_out_queue();
+  // note: we need to clear outgoing_bl here, but session_reset may be
+  // called by other thread, so let caller clear this itself!
+  // outgoing_bl.clear();
+
+  connection->dispatch_queue->queue_remote_reset(connection);
+
+  randomize_out_seq();
+
+  in_seq = 0;
+  connect_seq = 0;
+  // it's safe to directly set 0, double locked
+  ack_left = 0;
+  once_ready = false;
+  can_write = WriteStatus::NOWRITE;
+}
+
+void ProtocolV1::randomize_out_seq() {
+  if (connection->get_features() & CEPH_FEATURE_MSG_AUTH) {
+    // Set out_seq to a random value, so CRC won't be predictable.
+    auto rand_seq = ceph::util::generate_random_number<uint64_t>(0, SEQ_MASK);
+    ldout(cct, 10) << __func__ << " randomize_out_seq " << rand_seq << dendl;
+    out_seq = rand_seq;
+  } else {
+    // previously, seq #'s always started at 0.
+    out_seq = 0;
+  }
+}
+
+ssize_t ProtocolV1::write_message(Message *m, bufferlist &bl, bool more) {
+  FUNCTRACE(cct);
+  ceph_assert(connection->center->in_thread());
+  m->set_seq(++out_seq);
+
+  if (messenger->crcflags & MSG_CRC_HEADER) {
+    m->calc_header_crc();
+  }
+
+  ceph_msg_header &header = m->get_header();
+  ceph_msg_footer &footer = m->get_footer();
+
+  // TODO: let sign_message could be reentry?
+  // Now that we have all the crcs calculated, handle the
+  // digital signature for the message, if the AsyncConnection has session
+  // security set up.  Some session security options do not
+  // actually calculate and check the signature, but they should
+  // handle the calls to sign_message and check_signature.  PLR
+  if (session_security.get() == NULL) {
+    ldout(cct, 20) << __func__ << " no session security" << dendl;
+  } else {
+    if (session_security->sign_message(m)) {
+      ldout(cct, 20) << __func__ << " failed to sign m=" << m
+                     << "): sig = " << footer.sig << dendl;
+    } else {
+      ldout(cct, 20) << __func__ << " signed m=" << m
+                     << "): sig = " << footer.sig << dendl;
+    }
+  }
+
+  connection->outgoing_bl.append(CEPH_MSGR_TAG_MSG);
+  connection->outgoing_bl.append((char *)&header, sizeof(header));
+
+  ldout(cct, 20) << __func__ << " sending message type=" << header.type
+                 << " src " << entity_name_t(header.src)
+                 << " front=" << header.front_len << " data=" << header.data_len
+                 << " off " << header.data_off << dendl;
+
+  if ((bl.length() <= ASYNC_COALESCE_THRESHOLD) && (bl.get_num_buffers() > 1)) {
+    for (const auto &pb : bl.buffers()) {
+      connection->outgoing_bl.append((char *)pb.c_str(), pb.length());
+    }
+  } else {
+    connection->outgoing_bl.claim_append(bl);
+  }
+
+  // send footer; if receiver doesn't support signatures, use the old footer
+  // format
+  ceph_msg_footer_old old_footer;
+  if (connection->has_feature(CEPH_FEATURE_MSG_AUTH)) {
+    connection->outgoing_bl.append((char *)&footer, sizeof(footer));
+  } else {
+    if (messenger->crcflags & MSG_CRC_HEADER) {
+      old_footer.front_crc = footer.front_crc;
+      old_footer.middle_crc = footer.middle_crc;
+    } else {
+      old_footer.front_crc = old_footer.middle_crc = 0;
+    }
+    old_footer.data_crc =
+        messenger->crcflags & MSG_CRC_DATA ? footer.data_crc : 0;
+    old_footer.flags = footer.flags;
+    connection->outgoing_bl.append((char *)&old_footer, sizeof(old_footer));
+  }
+
+  m->trace.event("async writing message");
+  ldout(cct, 20) << __func__ << " sending " << m->get_seq() << " " << m
+                 << dendl;
+  ssize_t total_send_size = connection->outgoing_bl.length();
+  ssize_t rc = connection->_try_send(more);
+  if (rc < 0) {
+    ldout(cct, 1) << __func__ << " error sending " << m << ", "
+                  << cpp_strerror(rc) << dendl;
+  } else {
+    connection->logger->inc(
+        l_msgr_send_bytes, total_send_size - connection->outgoing_bl.length());
+    ldout(cct, 10) << __func__ << " sending " << m
+                   << (rc ? " continuely." : " done.") << dendl;
+  }
+
+#if defined(WITH_EVENTTRACE)
+  if (m->get_type() == CEPH_MSG_OSD_OP)
+    OID_EVENT_TRACE_WITH_MSG(m, "SEND_MSG_OSD_OP_END", false);
+  else if (m->get_type() == CEPH_MSG_OSD_OPREPLY)
+    OID_EVENT_TRACE_WITH_MSG(m, "SEND_MSG_OSD_OPREPLY_END", false);
+#endif
+  m->put();
+
+  return rc;
+}
+
+void ProtocolV1::requeue_sent() {
+  write_in_progress = false;
+  if (sent.empty()) {
+    return;
+  }
+
+  list<pair<bufferlist, Message *> > &rq = out_q[CEPH_MSG_PRIO_HIGHEST];
+  out_seq -= sent.size();
+  while (!sent.empty()) {
+    Message *m = sent.back();
+    sent.pop_back();
+    ldout(cct, 10) << __func__ << " " << *m << " for resend "
+                   << " (" << m->get_seq() << ")" << dendl;
+    m->clear_payload();
+    rq.push_front(make_pair(bufferlist(), m));
+  }
+}
+
+uint64_t ProtocolV1::discard_requeued_up_to(uint64_t out_seq, uint64_t seq) {
+  ldout(cct, 10) << __func__ << " " << seq << dendl;
+  std::lock_guard<std::mutex> l(connection->write_lock);
+  if (out_q.count(CEPH_MSG_PRIO_HIGHEST) == 0) {
+    return seq;
+  }
+  list<pair<bufferlist, Message *> > &rq = out_q[CEPH_MSG_PRIO_HIGHEST];
+  uint64_t count = out_seq;
+  while (!rq.empty()) {
+    pair<bufferlist, Message *> p = rq.front();
+    if (p.second->get_seq() == 0 || p.second->get_seq() > seq) break;
+    ldout(cct, 10) << __func__ << " " << *(p.second) << " for resend seq "
+                   << p.second->get_seq() << " <= " << seq << ", discarding"
+                   << dendl;
+    p.second->put();
+    rq.pop_front();
+    count++;
+  }
+  if (rq.empty()) out_q.erase(CEPH_MSG_PRIO_HIGHEST);
+  return count;
+}
+
+/*
+ * Tears down the message queues, and removes them from the
+ * DispatchQueue Must hold write_lock prior to calling.
+ */
+void ProtocolV1::discard_out_queue() {
+  ldout(cct, 10) << __func__ << " started" << dendl;
+
+  for (list<Message *>::iterator p = sent.begin(); p != sent.end(); ++p) {
+    ldout(cct, 20) << __func__ << " discard " << *p << dendl;
+    (*p)->put();
+  }
+  sent.clear();
+  for (map<int, list<pair<bufferlist, Message *> > >::iterator p =
+           out_q.begin();
+       p != out_q.end(); ++p) {
+    for (list<pair<bufferlist, Message *> >::iterator r = p->second.begin();
+         r != p->second.end(); ++r) {
+      ldout(cct, 20) << __func__ << " discard " << r->second << dendl;
+      r->second->put();
+    }
+  }
+  out_q.clear();
+  write_in_progress = false;
+}
+
+void ProtocolV1::reset_security()
+{
+  ldout(cct, 5) << __func__ << dendl;
+
+  auth_meta.reset(new AuthConnectionMeta);
+  authorizer_more.clear();
+  session_security.reset();
+}
+
+void ProtocolV1::reset_recv_state()
+{
+  ldout(cct, 5) << __func__ << dendl;
+
+  // execute in the same thread that uses the `session_security`.
+  // We need to do the warp because holding `write_lock` is not
+  // enough as `write_event()` releases it just before calling
+  // `write_message()`. `submit_to()` here is NOT blocking.
+  if (!connection->center->in_thread()) {
+    connection->center->submit_to(connection->center->get_id(), [this] {
+      ldout(cct, 5) << "reset_recv_state (warped) reseting security handlers"
+                    << dendl;
+      // Possibly unnecessary. See the comment in `deactivate_existing`.
+      std::lock_guard<std::mutex> l(connection->lock);
+      std::lock_guard<std::mutex> wl(connection->write_lock);
+      reset_security();
+    }, /* always_async = */true);
+  } else {
+    reset_security();
+  }
+
+  // clean read and write callbacks
+  connection->pendingReadLen.reset();
+  connection->writeCallback.reset();
+
+  if (state > THROTTLE_MESSAGE && state <= READ_FOOTER_AND_DISPATCH &&
+      connection->policy.throttler_messages) {
+    ldout(cct, 10) << __func__ << " releasing " << 1
+                   << " message to policy throttler "
+                   << connection->policy.throttler_messages->get_current()
+                   << "/" << connection->policy.throttler_messages->get_max()
+                   << dendl;
+    connection->policy.throttler_messages->put();
+  }
+  if (state > THROTTLE_BYTES && state <= READ_FOOTER_AND_DISPATCH) {
+    if (connection->policy.throttler_bytes) {
+      ldout(cct, 10) << __func__ << " releasing " << cur_msg_size
+                     << " bytes to policy throttler "
+                     << connection->policy.throttler_bytes->get_current() << "/"
+                     << connection->policy.throttler_bytes->get_max() << dendl;
+      connection->policy.throttler_bytes->put(cur_msg_size);
+    }
+  }
+  if (state > THROTTLE_DISPATCH_QUEUE && state <= READ_FOOTER_AND_DISPATCH) {
+    ldout(cct, 10)
+        << __func__ << " releasing " << cur_msg_size
+        << " bytes to dispatch_queue throttler "
+        << connection->dispatch_queue->dispatch_throttler.get_current() << "/"
+        << connection->dispatch_queue->dispatch_throttler.get_max() << dendl;
+    connection->dispatch_queue->dispatch_throttle_release(cur_msg_size);
+  }
+}
+
+Message *ProtocolV1::_get_next_outgoing(bufferlist *bl) {
+  Message *m = 0;
+  if (!out_q.empty()) {
+    map<int, list<pair<bufferlist, Message *> > >::reverse_iterator it =
+        out_q.rbegin();
+    ceph_assert(!it->second.empty());
+    list<pair<bufferlist, Message *> >::iterator p = it->second.begin();
+    m = p->second;
+    if (p->first.length() && bl) {
+      assert(bl->length() == 0);
+      bl->swap(p->first);
+    }
+    it->second.erase(p);
+    if (it->second.empty()) out_q.erase(it->first);
+  }
+  return m;
+}
+
+/**
+ * Client Protocol V1
+ **/
+
+CtPtr ProtocolV1::send_client_banner() {
+  ldout(cct, 20) << __func__ << dendl;
+  state = CONNECTING;
+
+  bufferlist bl;
+  bl.append(CEPH_BANNER, strlen(CEPH_BANNER));
+  return WRITE(bl, handle_client_banner_write);
+}
+
+CtPtr ProtocolV1::handle_client_banner_write(int r) {
+  ldout(cct, 20) << __func__ << " r=" << r << dendl;
+
+  if (r < 0) {
+    ldout(cct, 1) << __func__ << " write client banner failed" << dendl;
+    return _fault();
+  }
+  ldout(cct, 10) << __func__ << " connect write banner done: "
+                 << connection->get_peer_addr() << dendl;
+
+  return wait_server_banner();
+}
+
+CtPtr ProtocolV1::wait_server_banner() {
+  state = CONNECTING_WAIT_BANNER_AND_IDENTIFY;
+
+  ldout(cct, 20) << __func__ << dendl;
+
+  bufferlist myaddrbl;
+  unsigned banner_len = strlen(CEPH_BANNER);
+  unsigned need_len = banner_len + sizeof(ceph_entity_addr) * 2;
+  return READ(need_len, handle_server_banner_and_identify);
+}
+
+CtPtr ProtocolV1::handle_server_banner_and_identify(char *buffer, int r) {
+  ldout(cct, 20) << __func__ << " r=" << r << dendl;
+
+  if (r < 0) {
+    ldout(cct, 1) << __func__ << " read banner and identify addresses failed"
+                  << dendl;
+    return _fault();
+  }
+
+  unsigned banner_len = strlen(CEPH_BANNER);
+  if (memcmp(buffer, CEPH_BANNER, banner_len)) {
+    ldout(cct, 0) << __func__ << " connect protocol error (bad banner) on peer "
+                  << connection->get_peer_addr() << dendl;
+    return _fault();
+  }
+
+  bufferlist bl;
+  entity_addr_t paddr, peer_addr_for_me;
+
+  bl.append(buffer + banner_len, sizeof(ceph_entity_addr) * 2);
+  auto p = bl.cbegin();
+  try {
+    decode(paddr, p);
+    decode(peer_addr_for_me, p);
+  } catch (const buffer::error &e) {
+    lderr(cct) << __func__ << " decode peer addr failed " << dendl;
+    return _fault();
+  }
+  ldout(cct, 20) << __func__ << " connect read peer addr " << paddr
+                 << " on socket " << connection->cs.fd() << dendl;
+
+  entity_addr_t peer_addr = connection->peer_addrs->legacy_addr();
+  if (peer_addr != paddr) {
+    if (paddr.is_blank_ip() && peer_addr.get_port() == paddr.get_port() &&
+        peer_addr.get_nonce() == paddr.get_nonce()) {
+      ldout(cct, 0) << __func__ << " connect claims to be " << paddr << " not "
+                    << peer_addr << " - presumably this is the same node!"
+                    << dendl;
+    } else {
+      ldout(cct, 10) << __func__ << " connect claims to be " << paddr << " not "
+                     << peer_addr << dendl;
+      return _fault();
+    }
+  }
+
+  ldout(cct, 20) << __func__ << " connect peer addr for me is "
+                 << peer_addr_for_me << dendl;
+  if (messenger->get_myaddrs().empty() ||
+      messenger->get_myaddrs().front().is_blank_ip()) {
+    sockaddr_storage ss;
+    socklen_t len = sizeof(ss);
+    getsockname(connection->cs.fd(), (sockaddr *)&ss, &len);
+    entity_addr_t a;
+    if (cct->_conf->ms_learn_addr_from_peer) {
+      ldout(cct, 1) << __func__ << " peer " << connection->target_addr
+		    << " says I am " << peer_addr_for_me << " (socket says "
+		    << (sockaddr*)&ss << ")" << dendl;
+      a = peer_addr_for_me;
+    } else {
+      ldout(cct, 1) << __func__ << " socket to  " << connection->target_addr
+		    << " says I am " << (sockaddr*)&ss
+		    << " (peer says " << peer_addr_for_me << ")" << dendl;
+      a.set_sockaddr((sockaddr *)&ss);
+    }
+    a.set_type(entity_addr_t::TYPE_LEGACY); // anything but NONE; learned_addr ignores this
+    a.set_port(0);
+    connection->lock.unlock();
+    messenger->learned_addr(a);
+    if (cct->_conf->ms_inject_internal_delays &&
+	cct->_conf->ms_inject_socket_failures) {
+      if (rand() % cct->_conf->ms_inject_socket_failures == 0) {
+	ldout(cct, 10) << __func__ << " sleep for "
+		       << cct->_conf->ms_inject_internal_delays << dendl;
+	utime_t t;
+	t.set_from_double(cct->_conf->ms_inject_internal_delays);
+	t.sleep();
+      }
+    }
+    connection->lock.lock();
+    if (state != CONNECTING_WAIT_BANNER_AND_IDENTIFY) {
+      ldout(cct, 1) << __func__
+                  << " state changed while learned_addr, mark_down or "
+		    << " replacing must be happened just now" << dendl;
+      return nullptr;
+    }
+  }
+
+  bufferlist myaddrbl;
+  encode(messenger->get_myaddr_legacy(), myaddrbl, 0);  // legacy
+  return WRITE(myaddrbl, handle_my_addr_write);
+}
+
+CtPtr ProtocolV1::handle_my_addr_write(int r) {
+  ldout(cct, 20) << __func__ << " r=" << r << dendl;
+
+  if (r < 0) {
+    ldout(cct, 2) << __func__ << " connect couldn't write my addr, "
+                  << cpp_strerror(r) << dendl;
+    return _fault();
+  }
+  ldout(cct, 10) << __func__ << " connect sent my addr "
+                 << messenger->get_myaddr_legacy() << dendl;
+
+  return CONTINUE(send_connect_message);
+}
+
+CtPtr ProtocolV1::send_connect_message()
+{
+  state = CONNECTING_SEND_CONNECT_MSG;
+
+  ldout(cct, 20) << __func__ << dendl;
+  ceph_assert(messenger->auth_client);
+
+  bufferlist auth_bl;
+  vector<uint32_t> preferred_modes;
+
+  if (connection->peer_type != CEPH_ENTITY_TYPE_MON ||
+      messenger->get_myname().type() == CEPH_ENTITY_TYPE_MON) {
+    if (authorizer_more.length()) {
+      ldout(cct,10) << __func__ << " using augmented (challenge) auth payload"
+		    << dendl;
+      auth_bl = authorizer_more;
+    } else {
+      auto am = auth_meta;
+      authorizer_more.clear();
+      connection->lock.unlock();
+      int r = messenger->auth_client->get_auth_request(
+	connection, am.get(),
+	&am->auth_method, &preferred_modes, &auth_bl);
+      connection->lock.lock();
+      if (r < 0) {
+	return _fault();
+      }
+      if (state != CONNECTING_SEND_CONNECT_MSG) {
+	ldout(cct, 1) << __func__ << " state changed!" << dendl;
+	return _fault();
+      }
+    }
+  }
+
+  ceph_msg_connect connect;
+  connect.features = connection->policy.features_supported;
+  connect.host_type = messenger->get_myname().type();
+  connect.global_seq = global_seq;
+  connect.connect_seq = connect_seq;
+  connect.protocol_version =
+      messenger->get_proto_version(connection->peer_type, true);
+  if (auth_bl.length()) {
+    ldout(cct, 10) << __func__
+                   << " connect_msg.authorizer_len=" << auth_bl.length()
+                   << " protocol=" << auth_meta->auth_method << dendl;
+    connect.authorizer_protocol = auth_meta->auth_method;
+    connect.authorizer_len = auth_bl.length();
+  } else {
+    connect.authorizer_protocol = 0;
+    connect.authorizer_len = 0;
+  }
+
+  connect.flags = 0;
+  if (connection->policy.lossy) {
+    connect.flags |=
+        CEPH_MSG_CONNECT_LOSSY;  // this is fyi, actually, server decides!
+  }
+
+  bufferlist bl;
+  bl.append((char *)&connect, sizeof(connect));
+  if (auth_bl.length()) {
+    bl.append(auth_bl.c_str(), auth_bl.length());
+  }
+
+  ldout(cct, 10) << __func__ << " connect sending gseq=" << global_seq
+                 << " cseq=" << connect_seq
+                 << " proto=" << connect.protocol_version << dendl;
+
+  return WRITE(bl, handle_connect_message_write);
+}
+
+CtPtr ProtocolV1::handle_connect_message_write(int r) {
+  ldout(cct, 20) << __func__ << " r=" << r << dendl;
+
+  if (r < 0) {
+    ldout(cct, 2) << __func__ << " connect couldn't send reply "
+                  << cpp_strerror(r) << dendl;
+    return _fault();
+  }
+
+  ldout(cct, 20) << __func__
+                 << " connect wrote (self +) cseq, waiting for reply" << dendl;
+
+  return wait_connect_reply();
+}
+
+CtPtr ProtocolV1::wait_connect_reply() {
+  ldout(cct, 20) << __func__ << dendl;
+
+  // FIPS zeroization audit 20191115: this memset is not security related.
+  memset(&connect_reply, 0, sizeof(connect_reply));
+  return READ(sizeof(connect_reply), handle_connect_reply_1);
+}
+
+CtPtr ProtocolV1::handle_connect_reply_1(char *buffer, int r) {
+  ldout(cct, 20) << __func__ << " r=" << r << dendl;
+
+  if (r < 0) {
+    ldout(cct, 1) << __func__ << " read connect reply failed" << dendl;
+    return _fault();
+  }
+
+  connect_reply = *((ceph_msg_connect_reply *)buffer);
+
+  ldout(cct, 20) << __func__ << " connect got reply tag "
+                 << (int)connect_reply.tag << " connect_seq "
+                 << connect_reply.connect_seq << " global_seq "
+                 << connect_reply.global_seq << " proto "
+                 << connect_reply.protocol_version << " flags "
+                 << (int)connect_reply.flags << " features "
+                 << connect_reply.features << dendl;
+
+  if (connect_reply.authorizer_len) {
+    return wait_connect_reply_auth();
+  }
+
+  return handle_connect_reply_2();
+}
+
+CtPtr ProtocolV1::wait_connect_reply_auth() {
+  ldout(cct, 20) << __func__ << dendl;
+
+  ldout(cct, 10) << __func__
+                 << " reply.authorizer_len=" << connect_reply.authorizer_len
+                 << dendl;
+
+  ceph_assert(connect_reply.authorizer_len < 4096);
+
+  return READ(connect_reply.authorizer_len, handle_connect_reply_auth);
+}
+
+CtPtr ProtocolV1::handle_connect_reply_auth(char *buffer, int r) {
+  ldout(cct, 20) << __func__ << " r=" << r << dendl;
+
+  if (r < 0) {
+    ldout(cct, 1) << __func__ << " read connect reply authorizer failed"
+                  << dendl;
+    return _fault();
+  }
+
+  bufferlist authorizer_reply;
+  authorizer_reply.append(buffer, connect_reply.authorizer_len);
+
+  if (connection->peer_type != CEPH_ENTITY_TYPE_MON ||
+      messenger->get_myname().type() == CEPH_ENTITY_TYPE_MON) {
+    auto am = auth_meta;
+    bool more = (connect_reply.tag == CEPH_MSGR_TAG_CHALLENGE_AUTHORIZER);
+    bufferlist auth_retry_bl;
+    int r;
+    connection->lock.unlock();
+    if (more) {
+      r = messenger->auth_client->handle_auth_reply_more(
+	connection, am.get(), authorizer_reply, &auth_retry_bl);
+    } else {
+      // these aren't used for v1
+      CryptoKey skey;
+      string con_secret;
+      r = messenger->auth_client->handle_auth_done(
+	connection, am.get(),
+	0 /* global id */, 0 /* con mode */,
+	authorizer_reply,
+	&skey, &con_secret);
+    }
+    connection->lock.lock();
+    if (state != CONNECTING_SEND_CONNECT_MSG) {
+      ldout(cct, 1) << __func__ << " state changed" << dendl;
+      return _fault();
+    }
+    if (r < 0) {
+      return _fault();
+    }
+    if (more && r == 0) {
+      authorizer_more = auth_retry_bl;
+      return CONTINUE(send_connect_message);
+    }
+  }
+
+  return handle_connect_reply_2();
+}
+
+CtPtr ProtocolV1::handle_connect_reply_2() {
+  ldout(cct, 20) << __func__ << dendl;
+
+  if (connect_reply.tag == CEPH_MSGR_TAG_FEATURES) {
+    ldout(cct, 0) << __func__ << " connect protocol feature mismatch, my "
+                  << std::hex << connection->policy.features_supported
+                  << " < peer " << connect_reply.features << " missing "
+                  << (connect_reply.features &
+                      ~connection->policy.features_supported)
+                  << std::dec << dendl;
+    return _fault();
+  }
+
+  if (connect_reply.tag == CEPH_MSGR_TAG_BADPROTOVER) {
+    ldout(cct, 0) << __func__ << " connect protocol version mismatch, my "
+                  << messenger->get_proto_version(connection->peer_type, true)
+                  << " != " << connect_reply.protocol_version << dendl;
+    return _fault();
+  }
+
+  if (connect_reply.tag == CEPH_MSGR_TAG_BADAUTHORIZER) {
+    ldout(cct, 0) << __func__ << " connect got BADAUTHORIZER" << dendl;
+    authorizer_more.clear();
+    return _fault();
+  }
+
+  if (connect_reply.tag == CEPH_MSGR_TAG_RESETSESSION) {
+    ldout(cct, 0) << __func__ << " connect got RESETSESSION" << dendl;
+    session_reset();
+    connect_seq = 0;
+
+    // see session_reset
+    connection->outgoing_bl.clear();
+
+    return CONTINUE(send_connect_message);
+  }
+
+  if (connect_reply.tag == CEPH_MSGR_TAG_RETRY_GLOBAL) {
+    global_seq = messenger->get_global_seq(connect_reply.global_seq);
+    ldout(cct, 5) << __func__ << " connect got RETRY_GLOBAL "
+                  << connect_reply.global_seq << " chose new " << global_seq
+                  << dendl;
+    return CONTINUE(send_connect_message);
+  }
+
+  if (connect_reply.tag == CEPH_MSGR_TAG_RETRY_SESSION) {
+    ceph_assert(connect_reply.connect_seq > connect_seq);
+    ldout(cct, 5) << __func__ << " connect got RETRY_SESSION " << connect_seq
+                  << " -> " << connect_reply.connect_seq << dendl;
+    connect_seq = connect_reply.connect_seq;
+    return CONTINUE(send_connect_message);
+  }
+
+  if (connect_reply.tag == CEPH_MSGR_TAG_WAIT) {
+    ldout(cct, 1) << __func__ << " connect got WAIT (connection race)" << dendl;
+    state = WAIT;
+    return _fault();
+  }
+
+  uint64_t feat_missing;
+  feat_missing =
+      connection->policy.features_required & ~(uint64_t)connect_reply.features;
+  if (feat_missing) {
+    ldout(cct, 1) << __func__ << " missing required features " << std::hex
+                  << feat_missing << std::dec << dendl;
+    return _fault();
+  }
+
+  if (connect_reply.tag == CEPH_MSGR_TAG_SEQ) {
+    ldout(cct, 10)
+        << __func__
+        << " got CEPH_MSGR_TAG_SEQ, reading acked_seq and writing in_seq"
+        << dendl;
+
+    return wait_ack_seq();
+  }
+
+  if (connect_reply.tag == CEPH_MSGR_TAG_READY) {
+    ldout(cct, 10) << __func__ << " got CEPH_MSGR_TAG_READY " << dendl;
+  }
+
+  return client_ready();
+}
+
+CtPtr ProtocolV1::wait_ack_seq() {
+  ldout(cct, 20) << __func__ << dendl;
+
+  return READ(sizeof(uint64_t), handle_ack_seq);
+}
+
+CtPtr ProtocolV1::handle_ack_seq(char *buffer, int r) {
+  ldout(cct, 20) << __func__ << " r=" << r << dendl;
+
+  if (r < 0) {
+    ldout(cct, 1) << __func__ << " read connect ack seq failed" << dendl;
+    return _fault();
+  }
+
+  uint64_t newly_acked_seq = 0;
+
+  newly_acked_seq = *((uint64_t *)buffer);
+  ldout(cct, 2) << __func__ << " got newly_acked_seq " << newly_acked_seq
+                << " vs out_seq " << out_seq << dendl;
+  out_seq = discard_requeued_up_to(out_seq, newly_acked_seq);
+
+  bufferlist bl;
+  uint64_t s = in_seq;
+  bl.append((char *)&s, sizeof(s));
+
+  return WRITE(bl, handle_in_seq_write);
+}
+
+CtPtr ProtocolV1::handle_in_seq_write(int r) {
+  ldout(cct, 20) << __func__ << " r=" << r << dendl;
+
+  if (r < 0) {
+    ldout(cct, 10) << __func__ << " failed to send in_seq " << dendl;
+    return _fault();
+  }
+
+  ldout(cct, 10) << __func__ << " send in_seq done " << dendl;
+
+  return client_ready();
+}
+
+CtPtr ProtocolV1::client_ready() {
+  ldout(cct, 20) << __func__ << dendl;
+
+  // hooray!
+  peer_global_seq = connect_reply.global_seq;
+  connection->policy.lossy = connect_reply.flags & CEPH_MSG_CONNECT_LOSSY;
+
+  once_ready = true;
+  connect_seq += 1;
+  ceph_assert(connect_seq == connect_reply.connect_seq);
+  backoff = utime_t();
+  connection->set_features((uint64_t)connect_reply.features &
+                           (uint64_t)connection->policy.features_supported);
+  ldout(cct, 10) << __func__ << " connect success " << connect_seq
+                 << ", lossy = " << connection->policy.lossy << ", features "
+                 << connection->get_features() << dendl;
+
+  // If we have an authorizer, get a new AuthSessionHandler to deal with
+  // ongoing security of the connection.  PLR
+  if (auth_meta->authorizer) {
+    ldout(cct, 10) << __func__ << " setting up session_security with auth "
+		   << auth_meta->authorizer.get() << dendl;
+    session_security.reset(get_auth_session_handler(
+        cct, auth_meta->authorizer->protocol,
+	auth_meta->session_key,
+        connection->get_features()));
+  } else {
+    // We have no authorizer, so we shouldn't be applying security to messages
+    // in this AsyncConnection.  PLR
+    ldout(cct, 10) << __func__ << " no authorizer, clearing session_security"
+		   << dendl;
+    session_security.reset();
+  }
+
+  if (connection->delay_state) {
+    ceph_assert(connection->delay_state->ready());
+  }
+  connection->dispatch_queue->queue_connect(connection);
+  messenger->ms_deliver_handle_fast_connect(connection);
+
+  return ready();
+}
+
+/**
+ * Server Protocol V1
+ **/
+
+CtPtr ProtocolV1::send_server_banner() {
+  ldout(cct, 20) << __func__ << dendl;
+  state = ACCEPTING;
+
+  bufferlist bl;
+
+  bl.append(CEPH_BANNER, strlen(CEPH_BANNER));
+
+  // as a server, we should have a legacy addr if we accepted this connection.
+  auto legacy = messenger->get_myaddrs().legacy_addr();
+  encode(legacy, bl, 0);  // legacy
+  connection->port = legacy.get_port();
+  encode(connection->target_addr, bl, 0);  // legacy
+
+  ldout(cct, 1) << __func__ << " sd=" << connection->cs.fd()
+		<< " legacy " << legacy
+		<< " socket_addr " << connection->socket_addr
+		<< " target_addr " << connection->target_addr
+		<< dendl;
+
+  return WRITE(bl, handle_server_banner_write);
+}
+
+CtPtr ProtocolV1::handle_server_banner_write(int r) {
+  ldout(cct, 20) << __func__ << " r=" << r << dendl;
+
+  if (r < 0) {
+    ldout(cct, 1) << " write server banner failed" << dendl;
+    return _fault();
+  }
+  ldout(cct, 10) << __func__ << " write banner and addr done: "
+                 << connection->get_peer_addr() << dendl;
+
+  return wait_client_banner();
+}
+
+CtPtr ProtocolV1::wait_client_banner() {
+  ldout(cct, 20) << __func__ << dendl;
+
+  return READ(strlen(CEPH_BANNER) + sizeof(ceph_entity_addr),
+              handle_client_banner);
+}
+
+CtPtr ProtocolV1::handle_client_banner(char *buffer, int r) {
+  ldout(cct, 20) << __func__ << " r=" << r << dendl;
+
+  if (r < 0) {
+    ldout(cct, 1) << __func__ << " read peer banner and addr failed" << dendl;
+    return _fault();
+  }
+
+  if (memcmp(buffer, CEPH_BANNER, strlen(CEPH_BANNER))) {
+    ldout(cct, 1) << __func__ << " accept peer sent bad banner '" << buffer
+                  << "' (should be '" << CEPH_BANNER << "')" << dendl;
+    return _fault();
+  }
+
+  bufferlist addr_bl;
+  entity_addr_t peer_addr;
+
+  addr_bl.append(buffer + strlen(CEPH_BANNER), sizeof(ceph_entity_addr));
+  try {
+    auto ti = addr_bl.cbegin();
+    decode(peer_addr, ti);
+  } catch (const buffer::error &e) {
+    lderr(cct) << __func__ << " decode peer_addr failed " << dendl;
+    return _fault();
+  }
+
+  ldout(cct, 10) << __func__ << " accept peer addr is " << peer_addr << dendl;
+  if (peer_addr.is_blank_ip()) {
+    // peer apparently doesn't know what ip they have; figure it out for them.
+    int port = peer_addr.get_port();
+    peer_addr.set_sockaddr(connection->target_addr.get_sockaddr());
+    peer_addr.set_port(port);
+
+    ldout(cct, 0) << __func__ << " accept peer addr is really " << peer_addr
+                  << " (socket is " << connection->target_addr << ")" << dendl;
+  }
+  connection->set_peer_addr(peer_addr);  // so that connection_state gets set up
+  connection->target_addr = peer_addr;
+
+  return CONTINUE(wait_connect_message);
+}
+
+CtPtr ProtocolV1::wait_connect_message() {
+  ldout(cct, 20) << __func__ << dendl;
+
+  // FIPS zeroization audit 20191115: this memset is not security related.
+  memset(&connect_msg, 0, sizeof(connect_msg));
+  return READ(sizeof(connect_msg), handle_connect_message_1);
+}
+
+CtPtr ProtocolV1::handle_connect_message_1(char *buffer, int r) {
+  ldout(cct, 20) << __func__ << " r=" << r << dendl;
+
+  if (r < 0) {
+    ldout(cct, 1) << __func__ << " read connect msg failed" << dendl;
+    return _fault();
+  }
+
+  connect_msg = *((ceph_msg_connect *)buffer);
+
+  state = ACCEPTING_WAIT_CONNECT_MSG_AUTH;
+
+  if (connect_msg.authorizer_len) {
+    return wait_connect_message_auth();
+  }
+
+  return handle_connect_message_2();
+}
+
+CtPtr ProtocolV1::wait_connect_message_auth() {
+  ldout(cct, 20) << __func__ << dendl;
+  authorizer_buf.clear();
+  authorizer_buf.push_back(buffer::create(connect_msg.authorizer_len));
+  return READB(connect_msg.authorizer_len, authorizer_buf.c_str(),
+               handle_connect_message_auth);
+}
+
+CtPtr ProtocolV1::handle_connect_message_auth(char *buffer, int r) {
+  ldout(cct, 20) << __func__ << " r=" << r << dendl;
+
+  if (r < 0) {
+    ldout(cct, 1) << __func__ << " read connect authorizer failed" << dendl;
+    return _fault();
+  }
+
+  return handle_connect_message_2();
+}
+
+CtPtr ProtocolV1::handle_connect_message_2() {
+  ldout(cct, 20) << __func__ << dendl;
+
+  ldout(cct, 20) << __func__ << " accept got peer connect_seq "
+                 << connect_msg.connect_seq << " global_seq "
+                 << connect_msg.global_seq << dendl;
+
+  connection->set_peer_type(connect_msg.host_type);
+  connection->policy = messenger->get_policy(connect_msg.host_type);
+
+  ldout(cct, 10) << __func__ << " accept of host_type " << connect_msg.host_type
+                 << ", policy.lossy=" << connection->policy.lossy
+                 << " policy.server=" << connection->policy.server
+                 << " policy.standby=" << connection->policy.standby
+                 << " policy.resetcheck=" << connection->policy.resetcheck
+		 << " features 0x" << std::hex << (uint64_t)connect_msg.features
+		 << std::dec
+                 << dendl;
+
+  ceph_msg_connect_reply reply;
+  bufferlist authorizer_reply;
+
+  // FIPS zeroization audit 20191115: this memset is not security related.
+  memset(&reply, 0, sizeof(reply));
+  reply.protocol_version =
+      messenger->get_proto_version(connection->peer_type, false);
+
+  // mismatch?
+  ldout(cct, 10) << __func__ << " accept my proto " << reply.protocol_version
+                 << ", their proto " << connect_msg.protocol_version << dendl;
+
+  if (connect_msg.protocol_version != reply.protocol_version) {
+    return send_connect_message_reply(CEPH_MSGR_TAG_BADPROTOVER, reply,
+                                      authorizer_reply);
+  }
+
+  // require signatures for cephx?
+  if (connect_msg.authorizer_protocol == CEPH_AUTH_CEPHX) {
+    if (connection->peer_type == CEPH_ENTITY_TYPE_OSD ||
+        connection->peer_type == CEPH_ENTITY_TYPE_MDS ||
+        connection->peer_type == CEPH_ENTITY_TYPE_MGR) {
+      if (cct->_conf->cephx_require_signatures ||
+          cct->_conf->cephx_cluster_require_signatures) {
+        ldout(cct, 10)
+            << __func__
+            << " using cephx, requiring MSG_AUTH feature bit for cluster"
+            << dendl;
+        connection->policy.features_required |= CEPH_FEATURE_MSG_AUTH;
+      }
+      if (cct->_conf->cephx_require_version >= 2 ||
+          cct->_conf->cephx_cluster_require_version >= 2) {
+        ldout(cct, 10)
+            << __func__
+            << " using cephx, requiring cephx v2 feature bit for cluster"
+            << dendl;
+        connection->policy.features_required |= CEPH_FEATUREMASK_CEPHX_V2;
+      }
+    } else {
+      if (cct->_conf->cephx_require_signatures ||
+          cct->_conf->cephx_service_require_signatures) {
+        ldout(cct, 10)
+            << __func__
+            << " using cephx, requiring MSG_AUTH feature bit for service"
+            << dendl;
+        connection->policy.features_required |= CEPH_FEATURE_MSG_AUTH;
+      }
+      if (cct->_conf->cephx_require_version >= 2 ||
+          cct->_conf->cephx_service_require_version >= 2) {
+        ldout(cct, 10)
+            << __func__
+            << " using cephx, requiring cephx v2 feature bit for service"
+            << dendl;
+        connection->policy.features_required |= CEPH_FEATUREMASK_CEPHX_V2;
+      }
+    }
+  }
+
+  uint64_t feat_missing =
+      connection->policy.features_required & ~(uint64_t)connect_msg.features;
+  if (feat_missing) {
+    ldout(cct, 1) << __func__ << " peer missing required features " << std::hex
+                  << feat_missing << std::dec << dendl;
+    return send_connect_message_reply(CEPH_MSGR_TAG_FEATURES, reply,
+                                      authorizer_reply);
+  }
+
+  bufferlist auth_bl_copy = authorizer_buf;
+  auto am = auth_meta;
+  am->auth_method = connect_msg.authorizer_protocol;
+  if (!HAVE_FEATURE((uint64_t)connect_msg.features, CEPHX_V2)) {
+    // peer doesn't support it and we won't get here if we require it
+    am->skip_authorizer_challenge = true;
+  }
+  connection->lock.unlock();
+  ldout(cct,10) << __func__ << " authorizor_protocol "
+		<< connect_msg.authorizer_protocol
+		<< " len " << auth_bl_copy.length()
+		<< dendl;
+  bool more = (bool)auth_meta->authorizer_challenge;
+  int r = messenger->auth_server->handle_auth_request(
+    connection,
+    am.get(),
+    more,
+    am->auth_method,
+    auth_bl_copy,
+    &authorizer_reply);
+  if (r < 0) {
+    connection->lock.lock();
+    if (state != ACCEPTING_WAIT_CONNECT_MSG_AUTH) {
+      ldout(cct, 1) << __func__ << " state changed" << dendl;
+      return _fault();
+    }
+    ldout(cct, 0) << __func__ << ": got bad authorizer, auth_reply_len="
+		  << authorizer_reply.length() << dendl;
+    session_security.reset();
+    return send_connect_message_reply(CEPH_MSGR_TAG_BADAUTHORIZER, reply,
+				      authorizer_reply);
+  }
+  if (r == 0) {
+    connection->lock.lock();
+    if (state != ACCEPTING_WAIT_CONNECT_MSG_AUTH) {
+      ldout(cct, 1) << __func__ << " state changed" << dendl;
+      return _fault();
+    }
+    ldout(cct, 10) << __func__ << ": challenging authorizer" << dendl;
+    ceph_assert(authorizer_reply.length());
+    return send_connect_message_reply(CEPH_MSGR_TAG_CHALLENGE_AUTHORIZER,
+				      reply, authorizer_reply);
+  }
+
+  // We've verified the authorizer for this AsyncConnection, so set up the
+  // session security structure.  PLR
+  ldout(cct, 10) << __func__ << " accept setting up session_security." << dendl;
+
+  if (connection->policy.server &&
+      connection->policy.lossy &&
+      !connection->policy.register_lossy_clients) {
+    // incoming lossy client, no need to register this connection
+    // new session
+    ldout(cct, 10) << __func__ << " accept new session" << dendl;
+    connection->lock.lock();
+    return open(reply, authorizer_reply);
+  }
+
+  AsyncConnectionRef existing = messenger->lookup_conn(*connection->peer_addrs);
+
+  connection->inject_delay();
+
+  connection->lock.lock();
+  if (state != ACCEPTING_WAIT_CONNECT_MSG_AUTH) {
+    ldout(cct, 1) << __func__ << " state changed" << dendl;
+    return _fault();
+  }
+
+  if (existing == connection) {
+    existing = nullptr;
+  }
+  if (existing && existing->protocol->proto_type != 1) {
+    ldout(cct,1) << __func__ << " existing " << existing << " proto "
+		 << existing->protocol.get() << " version is "
+		 << existing->protocol->proto_type << ", marking down" << dendl;
+    existing->mark_down();
+    existing = nullptr;
+  }
+
+  if (existing) {
+    // There is no possible that existing connection will acquire this
+    // connection's lock
+    existing->lock.lock();  // skip lockdep check (we are locking a second
+                            // AsyncConnection here)
+
+    ldout(cct,10) << __func__ << " existing=" << existing << " exproto="
+		  << existing->protocol.get() << dendl;
+    ProtocolV1 *exproto = dynamic_cast<ProtocolV1 *>(existing->protocol.get());
+    ceph_assert(exproto);
+    ceph_assert(exproto->proto_type == 1);
+
+    if (exproto->state == CLOSED) {
+      ldout(cct, 1) << __func__ << " existing " << existing
+		    << " already closed." << dendl;
+      existing->lock.unlock();
+      existing = nullptr;
+
+      return open(reply, authorizer_reply);
+    }
+
+    if (exproto->replacing) {
+      ldout(cct, 1) << __func__
+                    << " existing racing replace happened while replacing."
+                    << " existing_state="
+                    << connection->get_state_name(existing->state) << dendl;
+      reply.global_seq = exproto->peer_global_seq;
+      existing->lock.unlock();
+      return send_connect_message_reply(CEPH_MSGR_TAG_RETRY_GLOBAL, reply,
+                                        authorizer_reply);
+    }
+
+    if (connect_msg.global_seq < exproto->peer_global_seq) {
+      ldout(cct, 10) << __func__ << " accept existing " << existing << ".gseq "
+                     << exproto->peer_global_seq << " > "
+                     << connect_msg.global_seq << ", RETRY_GLOBAL" << dendl;
+      reply.global_seq = exproto->peer_global_seq;  // so we can send it below..
+      existing->lock.unlock();
+      return send_connect_message_reply(CEPH_MSGR_TAG_RETRY_GLOBAL, reply,
+                                        authorizer_reply);
+    } else {
+      ldout(cct, 10) << __func__ << " accept existing " << existing << ".gseq "
+                     << exproto->peer_global_seq
+                     << " <= " << connect_msg.global_seq << ", looks ok"
+                     << dendl;
+    }
+
+    if (existing->policy.lossy) {
+      ldout(cct, 0)
+          << __func__
+          << " accept replacing existing (lossy) channel (new one lossy="
+          << connection->policy.lossy << ")" << dendl;
+      exproto->session_reset();
+      return replace(existing, reply, authorizer_reply);
+    }
+
+    ldout(cct, 1) << __func__ << " accept connect_seq "
+                  << connect_msg.connect_seq
+                  << " vs existing csq=" << exproto->connect_seq
+                  << " existing_state="
+                  << connection->get_state_name(existing->state) << dendl;
+
+    if (connect_msg.connect_seq == 0 && exproto->connect_seq > 0) {
+      ldout(cct, 0)
+          << __func__
+          << " accept peer reset, then tried to connect to us, replacing"
+          << dendl;
+      // this is a hard reset from peer
+      is_reset_from_peer = true;
+      if (connection->policy.resetcheck) {
+        exproto->session_reset();  // this resets out_queue, msg_ and
+                                   // connect_seq #'s
+      }
+      return replace(existing, reply, authorizer_reply);
+    }
+
+    if (connect_msg.connect_seq < exproto->connect_seq) {
+      // old attempt, or we sent READY but they didn't get it.
+      ldout(cct, 10) << __func__ << " accept existing " << existing << ".cseq "
+                     << exproto->connect_seq << " > " << connect_msg.connect_seq
+                     << ", RETRY_SESSION" << dendl;
+      reply.connect_seq = exproto->connect_seq + 1;
+      existing->lock.unlock();
+      return send_connect_message_reply(CEPH_MSGR_TAG_RETRY_SESSION, reply,
+                                        authorizer_reply);
+    }
+
+    if (connect_msg.connect_seq == exproto->connect_seq) {
+      // if the existing connection successfully opened, and/or
+      // subsequently went to standby, then the peer should bump
+      // their connect_seq and retry: this is not a connection race
+      // we need to resolve here.
+      if (exproto->state == OPENED || exproto->state == STANDBY) {
+        ldout(cct, 10) << __func__ << " accept connection race, existing "
+                       << existing << ".cseq " << exproto->connect_seq
+                       << " == " << connect_msg.connect_seq
+                       << ", OPEN|STANDBY, RETRY_SESSION " << dendl;
+        // if connect_seq both zero, dont stuck into dead lock. it's ok to
+        // replace
+        if (connection->policy.resetcheck && exproto->connect_seq == 0) {
+          return replace(existing, reply, authorizer_reply);
+        }
+
+        reply.connect_seq = exproto->connect_seq + 1;
+        existing->lock.unlock();
+        return send_connect_message_reply(CEPH_MSGR_TAG_RETRY_SESSION, reply,
+                                          authorizer_reply);
+      }
+
+      // connection race?
+      if (connection->peer_addrs->legacy_addr() < messenger->get_myaddr_legacy() ||
+          existing->policy.server) {
+        // incoming wins
+        ldout(cct, 10) << __func__ << " accept connection race, existing "
+                       << existing << ".cseq " << exproto->connect_seq
+                       << " == " << connect_msg.connect_seq
+                       << ", or we are server, replacing my attempt" << dendl;
+        return replace(existing, reply, authorizer_reply);
+      } else {
+        // our existing outgoing wins
+        ldout(messenger->cct, 10)
+            << __func__ << " accept connection race, existing " << existing
+            << ".cseq " << exproto->connect_seq
+            << " == " << connect_msg.connect_seq << ", sending WAIT" << dendl;
+        ceph_assert(connection->peer_addrs->legacy_addr() >
+                    messenger->get_myaddr_legacy());
+        existing->lock.unlock();
+	// make sure we follow through with opening the existing
+	// connection (if it isn't yet open) since we know the peer
+	// has something to send to us.
+	existing->send_keepalive();
+        return send_connect_message_reply(CEPH_MSGR_TAG_WAIT, reply,
+                                          authorizer_reply);
+      }
+    }
+
+    ceph_assert(connect_msg.connect_seq > exproto->connect_seq);
+    ceph_assert(connect_msg.global_seq >= exproto->peer_global_seq);
+    if (connection->policy.resetcheck &&  // RESETSESSION only used by servers;
+                                          // peers do not reset each other
+        exproto->connect_seq == 0) {
+      ldout(cct, 0) << __func__ << " accept we reset (peer sent cseq "
+                    << connect_msg.connect_seq << ", " << existing
+                    << ".cseq = " << exproto->connect_seq
+                    << "), sending RESETSESSION " << dendl;
+      existing->lock.unlock();
+      return send_connect_message_reply(CEPH_MSGR_TAG_RESETSESSION, reply,
+                                        authorizer_reply);
+    }
+
+    // reconnect
+    ldout(cct, 10) << __func__ << " accept peer sent cseq "
+                   << connect_msg.connect_seq << " > " << exproto->connect_seq
+                   << dendl;
+    return replace(existing, reply, authorizer_reply);
+  }  // existing
+  else if (!replacing && connect_msg.connect_seq > 0) {
+    // we reset, and they are opening a new session
+    ldout(cct, 0) << __func__ << " accept we reset (peer sent cseq "
+                  << connect_msg.connect_seq << "), sending RESETSESSION"
+                  << dendl;
+    return send_connect_message_reply(CEPH_MSGR_TAG_RESETSESSION, reply,
+                                      authorizer_reply);
+  } else {
+    // new session
+    ldout(cct, 10) << __func__ << " accept new session" << dendl;
+    existing = nullptr;
+    return open(reply, authorizer_reply);
+  }
+}
+
+CtPtr ProtocolV1::send_connect_message_reply(char tag,
+                                             ceph_msg_connect_reply &reply,
+                                             bufferlist &authorizer_reply) {
+  ldout(cct, 20) << __func__ << dendl;
+  bufferlist reply_bl;
+  reply.tag = tag;
+  reply.features =
+      ((uint64_t)connect_msg.features & connection->policy.features_supported) |
+      connection->policy.features_required;
+  reply.authorizer_len = authorizer_reply.length();
+  reply_bl.append((char *)&reply, sizeof(reply));
+
+  ldout(cct, 10) << __func__ << " reply features 0x" << std::hex
+		 << reply.features << " = (policy sup 0x"
+		 << connection->policy.features_supported
+		 << " & connect 0x" << (uint64_t)connect_msg.features
+		 << ") | policy req 0x"
+		 << connection->policy.features_required
+		 << dendl;
+
+  if (reply.authorizer_len) {
+    reply_bl.append(authorizer_reply.c_str(), authorizer_reply.length());
+    authorizer_reply.clear();
+  }
+
+  return WRITE(reply_bl, handle_connect_message_reply_write);
+}
+
+CtPtr ProtocolV1::handle_connect_message_reply_write(int r) {
+  ldout(cct, 20) << __func__ << " r=" << r << dendl;
+
+  if (r < 0) {
+    ldout(cct, 1) << " write connect message reply failed" << dendl;
+    connection->inject_delay();
+    return _fault();
+  }
+
+  return CONTINUE(wait_connect_message);
+}
+
+CtPtr ProtocolV1::replace(const AsyncConnectionRef& existing,
+                          ceph_msg_connect_reply &reply,
+                          bufferlist &authorizer_reply) {
+  ldout(cct, 10) << __func__ << " accept replacing " << existing << dendl;
+
+  connection->inject_delay();
+  if (existing->policy.lossy) {
+    // disconnect from the Connection
+    ldout(cct, 1) << __func__ << " replacing on lossy channel, failing existing"
+                  << dendl;
+    existing->protocol->stop();
+    existing->dispatch_queue->queue_reset(existing.get());
+  } else {
+    ceph_assert(can_write == WriteStatus::NOWRITE);
+    existing->write_lock.lock();
+
+    ProtocolV1 *exproto = dynamic_cast<ProtocolV1 *>(existing->protocol.get());
+
+    // reset the in_seq if this is a hard reset from peer,
+    // otherwise we respect our original connection's value
+    if (is_reset_from_peer) {
+      exproto->is_reset_from_peer = true;
+    }
+
+    connection->center->delete_file_event(connection->cs.fd(),
+                                          EVENT_READABLE | EVENT_WRITABLE);
+
+    if (existing->delay_state) {
+      existing->delay_state->flush();
+      ceph_assert(!connection->delay_state);
+    }
+    exproto->reset_recv_state();
+
+    exproto->connect_msg.features = connect_msg.features;
+
+    auto temp_cs = std::move(connection->cs);
+    EventCenter *new_center = connection->center;
+    Worker *new_worker = connection->worker;
+    // avoid _stop shutdown replacing socket
+    // queue a reset on the new connection, which we're dumping for the old
+    stop();
+
+    connection->dispatch_queue->queue_reset(connection);
+    ldout(messenger->cct, 1)
+        << __func__ << " stop myself to swap existing" << dendl;
+    exproto->can_write = WriteStatus::REPLACING;
+    exproto->replacing = true;
+    exproto->write_in_progress = false;
+    existing->state_offset = 0;
+    // avoid previous thread modify event
+    exproto->state = NONE;
+    existing->state = AsyncConnection::STATE_NONE;
+    // Discard existing prefetch buffer in `recv_buf`
+    existing->recv_start = existing->recv_end = 0;
+    // there shouldn't exist any buffer
+    ceph_assert(connection->recv_start == connection->recv_end);
+
+    auto deactivate_existing = std::bind(
+        [existing, new_worker, new_center, exproto, reply,
+         authorizer_reply](ConnectedSocket &cs) mutable {
+          // we need to delete time event in original thread
+          {
+            std::lock_guard<std::mutex> l(existing->lock);
+            existing->write_lock.lock();
+            exproto->requeue_sent();
+            existing->outgoing_bl.clear();
+            existing->open_write = false;
+            existing->write_lock.unlock();
+            if (exproto->state == NONE) {
+              existing->shutdown_socket();
+              existing->cs = std::move(cs);
+              existing->worker->references--;
+              new_worker->references++;
+              existing->logger = new_worker->get_perf_counter();
+              existing->worker = new_worker;
+              existing->center = new_center;
+              if (existing->delay_state)
+                existing->delay_state->set_center(new_center);
+            } else if (exproto->state == CLOSED) {
+              auto back_to_close =
+                  std::bind([](ConnectedSocket &cs) mutable { cs.close(); },
+                            std::move(cs));
+              new_center->submit_to(new_center->get_id(),
+                                    std::move(back_to_close), true);
+              return;
+            } else {
+              ceph_abort();
+            }
+          }
+
+          // Before changing existing->center, it may already exists some
+          // events in existing->center's queue. Then if we mark down
+          // `existing`, it will execute in another thread and clean up
+          // connection. Previous event will result in segment fault
+          auto transfer_existing = [existing, exproto, reply,
+                                    authorizer_reply]() mutable {
+            std::lock_guard<std::mutex> l(existing->lock);
+            if (exproto->state == CLOSED) return;
+            ceph_assert(exproto->state == NONE);
+
+            // we have called shutdown_socket above
+            ceph_assert(existing->last_tick_id == 0);
+            // restart timer since we are going to re-build connection
+            existing->last_connect_started = ceph::coarse_mono_clock::now();
+            existing->last_tick_id = existing->center->create_time_event(
+              existing->connect_timeout_us, existing->tick_handler);
+            existing->state = AsyncConnection::STATE_CONNECTION_ESTABLISHED;
+            exproto->state = ACCEPTING;
+
+            existing->center->create_file_event(
+                existing->cs.fd(), EVENT_READABLE, existing->read_handler);
+            reply.global_seq = exproto->peer_global_seq;
+            exproto->run_continuation(exproto->send_connect_message_reply(
+                CEPH_MSGR_TAG_RETRY_GLOBAL, reply, authorizer_reply));
+          };
+          if (existing->center->in_thread())
+            transfer_existing();
+          else
+            existing->center->submit_to(existing->center->get_id(),
+                                        std::move(transfer_existing), true);
+        },
+        std::move(temp_cs));
+
+    existing->center->submit_to(existing->center->get_id(),
+                                std::move(deactivate_existing), true);
+    existing->write_lock.unlock();
+    existing->lock.unlock();
+    return nullptr;
+  }
+  existing->lock.unlock();
+
+  return open(reply, authorizer_reply);
+}
+
+CtPtr ProtocolV1::open(ceph_msg_connect_reply &reply,
+                       bufferlist &authorizer_reply) {
+  ldout(cct, 20) << __func__ << dendl;
+
+  connect_seq = connect_msg.connect_seq + 1;
+  peer_global_seq = connect_msg.global_seq;
+  ldout(cct, 10) << __func__ << " accept success, connect_seq = " << connect_seq
+                 << " in_seq=" << in_seq << ", sending READY" << dendl;
+
+  // if it is a hard reset from peer, we don't need a round-trip to negotiate
+  // in/out sequence
+  if ((connect_msg.features & CEPH_FEATURE_RECONNECT_SEQ) &&
+      !is_reset_from_peer) {
+    reply.tag = CEPH_MSGR_TAG_SEQ;
+    wait_for_seq = true;
+  } else {
+    reply.tag = CEPH_MSGR_TAG_READY;
+    wait_for_seq = false;
+    out_seq = discard_requeued_up_to(out_seq, 0);
+    is_reset_from_peer = false;
+    in_seq = 0;
+  }
+
+  // send READY reply
+  reply.features = connection->policy.features_supported;
+  reply.global_seq = messenger->get_global_seq();
+  reply.connect_seq = connect_seq;
+  reply.flags = 0;
+  reply.authorizer_len = authorizer_reply.length();
+  if (connection->policy.lossy) {
+    reply.flags = reply.flags | CEPH_MSG_CONNECT_LOSSY;
+  }
+
+  connection->set_features((uint64_t)reply.features &
+                           (uint64_t)connect_msg.features);
+  ldout(cct, 10) << __func__ << " accept features "
+                 << connection->get_features()
+		 << " authorizer_protocol "
+		 << connect_msg.authorizer_protocol << dendl;
+
+  session_security.reset(
+    get_auth_session_handler(cct, auth_meta->auth_method,
+			     auth_meta->session_key,
+			     connection->get_features()));
+
+  bufferlist reply_bl;
+  reply_bl.append((char *)&reply, sizeof(reply));
+
+  if (reply.authorizer_len) {
+    reply_bl.append(authorizer_reply.c_str(), authorizer_reply.length());
+  }
+
+  if (reply.tag == CEPH_MSGR_TAG_SEQ) {
+    uint64_t s = in_seq;
+    reply_bl.append((char *)&s, sizeof(s));
+  }
+
+  connection->lock.unlock();
+  // Because "replacing" will prevent other connections preempt this addr,
+  // it's safe that here we don't acquire Connection's lock
+  ssize_t r = messenger->accept_conn(connection);
+
+  connection->inject_delay();
+
+  connection->lock.lock();
+  replacing = false;
+  if (r < 0) {
+    ldout(cct, 1) << __func__ << " existing race replacing process for addr = "
+                  << connection->peer_addrs->legacy_addr()
+                  << " just fail later one(this)" << dendl;
+    ldout(cct, 10) << "accept fault after register" << dendl;
+    connection->inject_delay();
+    return _fault();
+  }
+  if (state != ACCEPTING_WAIT_CONNECT_MSG_AUTH) {
+    ldout(cct, 1) << __func__
+                  << " state changed while accept_conn, it must be mark_down"
+                  << dendl;
+    ceph_assert(state == CLOSED || state == NONE);
+    ldout(cct, 10) << "accept fault after register" << dendl;
+    messenger->unregister_conn(connection);
+    connection->inject_delay();
+    return _fault();
+  }
+
+  return WRITE(reply_bl, handle_ready_connect_message_reply_write);
+}
+
+CtPtr ProtocolV1::handle_ready_connect_message_reply_write(int r) {
+  ldout(cct, 20) << __func__ << " r=" << r << dendl;
+
+  if (r < 0) {
+    ldout(cct, 1) << __func__ << " write ready connect message reply failed"
+                  << dendl;
+    return _fault();
+  }
+
+  // notify
+  connection->dispatch_queue->queue_accept(connection);
+  messenger->ms_deliver_handle_fast_accept(connection);
+  once_ready = true;
+
+  state = ACCEPTING_HANDLED_CONNECT_MSG;
+
+  if (wait_for_seq) {
+    return wait_seq();
+  }
+
+  return server_ready();
+}
+
+CtPtr ProtocolV1::wait_seq() {
+  ldout(cct, 20) << __func__ << dendl;
+
+  return READ(sizeof(uint64_t), handle_seq);
+}
+
+CtPtr ProtocolV1::handle_seq(char *buffer, int r) {
+  ldout(cct, 20) << __func__ << " r=" << r << dendl;
+
+  if (r < 0) {
+    ldout(cct, 1) << __func__ << " read ack seq failed" << dendl;
+    return _fault();
+  }
+
+  uint64_t newly_acked_seq = *(uint64_t *)buffer;
+  ldout(cct, 2) << __func__ << " accept get newly_acked_seq " << newly_acked_seq
+                << dendl;
+  out_seq = discard_requeued_up_to(out_seq, newly_acked_seq);
+
+  return server_ready();
+}
+
+CtPtr ProtocolV1::server_ready() {
+  ldout(cct, 20) << __func__ << " session_security is "
+		 << session_security
+		 << dendl;
+
+  ldout(cct, 20) << __func__ << " accept done" << dendl;
+  // FIPS zeroization audit 20191115: this memset is not security related.
+  memset(&connect_msg, 0, sizeof(connect_msg));
+
+  if (connection->delay_state) {
+    ceph_assert(connection->delay_state->ready());
+  }
+
+  return ready();
+}
diff --git a/src/msg/async/ProtocolV1.h b/src/msg/async/ProtocolV1.h
new file mode 100644
index 00000000000..3f63a68b2e5
--- /dev/null
+++ b/src/msg/async/ProtocolV1.h
@@ -0,0 +1,303 @@
+// -*- mode:C++; tab-width:8; c-basic-offset:2; indent-tabs-mode:t -*-
+// vim: ts=8 sw=2 smarttab
+
+#ifndef _MSG_ASYNC_PROTOCOL_V1_
+#define _MSG_ASYNC_PROTOCOL_V1_
+
+#include "Protocol.h"
+
+class ProtocolV1;
+using CtPtr = Ct<ProtocolV1>*;
+
+class ProtocolV1 : public Protocol {
+/*
+ *  ProtocolV1 State Machine
+ *
+
+    send_server_banner                             send_client_banner
+            |                                              |
+            v                                              v
+    wait_client_banner                              wait_server_banner
+            |                                              |
+            |                                              v
+            v                                 handle_server_banner_and_identify
+    wait_connect_message <---------\                       |
+      |     |                      |                       v
+      |  wait_connect_message_auth |           send_connect_message <----------\
+      |     |                      |                       |                   |
+      v     v                      |                       |                   |
+handle_connect_message_2           |                       v                   |
+        |           |              |            wait_connect_reply             |
+        v           v              |              |        |                   |
+     replace -> send_connect_message_reply        |        V                   |
+        |                                         |   wait_connect_reply_auth  |
+        |                                         |        |                   |
+        v                                         v        v                   |
+      open ---\                                 handle_connect_reply_2 --------/
+        |     |                                            |
+        |     v                                            v
+        |   wait_seq                                  wait_ack_seq
+        |     |                                            |
+        v     v                                            v
+    server_ready                                      client_ready
+            |                                              |
+            \------------------> wait_message <------------/
+                                 |  ^   |  ^
+        /------------------------/  |   |  |
+        |                           |   |  \----------------- ------------\
+        v                /----------/   v                                 |
+handle_keepalive2        |        handle_message_header      read_message_footer
+handle_keepalive2_ack    |              |                                 ^
+handle_tag_ack           |              v                                 |
+        |                |        throttle_message             read_message_data
+        \----------------/              |                                 ^
+                                        v                                 |
+                             read_message_front --> read_message_middle --/
+*/
+
+protected:
+
+  enum State {
+    NONE = 0,
+    START_CONNECT,
+    CONNECTING,
+    CONNECTING_WAIT_BANNER_AND_IDENTIFY,
+    CONNECTING_SEND_CONNECT_MSG,
+    START_ACCEPT,
+    ACCEPTING,
+    ACCEPTING_WAIT_CONNECT_MSG_AUTH,
+    ACCEPTING_HANDLED_CONNECT_MSG,
+    OPENED,
+    THROTTLE_MESSAGE,
+    THROTTLE_BYTES,
+    THROTTLE_DISPATCH_QUEUE,
+    READ_MESSAGE_FRONT,
+    READ_FOOTER_AND_DISPATCH,
+    CLOSED,
+    WAIT,
+    STANDBY
+  };
+
+  static const char *get_state_name(int state) {
+    const char *const statenames[] = {"NONE",
+                                      "START_CONNECT",
+                                      "CONNECTING",
+                                      "CONNECTING_WAIT_BANNER_AND_IDENTIFY",
+                                      "CONNECTING_SEND_CONNECT_MSG",
+                                      "START_ACCEPT",
+                                      "ACCEPTING",
+                                      "ACCEPTING_WAIT_CONNECT_MSG_AUTH",
+                                      "ACCEPTING_HANDLED_CONNECT_MSG",
+                                      "OPENED",
+                                      "THROTTLE_MESSAGE",
+                                      "THROTTLE_BYTES",
+                                      "THROTTLE_DISPATCH_QUEUE",
+                                      "READ_MESSAGE_FRONT",
+                                      "READ_FOOTER_AND_DISPATCH",
+                                      "CLOSED",
+                                      "WAIT",
+                                      "STANDBY"};
+    return statenames[state];
+  }
+
+  char *temp_buffer;
+
+  enum class WriteStatus { NOWRITE, REPLACING, CANWRITE, CLOSED };
+  std::atomic<WriteStatus> can_write;
+  std::list<Message *> sent;  // the first bufferlist need to inject seq
+  // priority queue for outbound msgs
+  std::map<int, std::list<std::pair<bufferlist, Message *>>> out_q;
+  bool keepalive;
+  bool write_in_progress = false;
+
+  __u32 connect_seq, peer_global_seq;
+  std::atomic<uint64_t> in_seq{0};
+  std::atomic<uint64_t> out_seq{0};
+  std::atomic<uint64_t> ack_left{0};
+
+  std::shared_ptr<AuthSessionHandler> session_security;
+
+  // Open state
+  ceph_msg_connect connect_msg;
+  ceph_msg_connect_reply connect_reply;
+  bufferlist authorizer_buf;  // auth(orizer) payload read off the wire
+  bufferlist authorizer_more;  // connect-side auth retry (we added challenge)
+
+  utime_t backoff;  // backoff time
+  utime_t recv_stamp;
+  utime_t throttle_stamp;
+  unsigned msg_left;
+  uint64_t cur_msg_size;
+  ceph_msg_header current_header;
+  bufferlist data_buf;
+  bufferlist::iterator data_blp;
+  bufferlist front, middle, data;
+
+  bool replacing;  // when replacing process happened, we will reply connect
+                   // side with RETRY tag and accept side will clear replaced
+                   // connection. So when connect side reissue connect_msg,
+                   // there won't exists conflicting connection so we use
+                   // "replacing" to skip RESETSESSION to avoid detect wrong
+                   // presentation
+  bool is_reset_from_peer;
+  bool once_ready;
+
+  State state;
+
+  void run_continuation(CtPtr pcontinuation);
+  CtPtr read(CONTINUATION_RX_TYPE<ProtocolV1> &next, int len,
+             char *buffer = nullptr);
+  CtPtr write(CONTINUATION_TX_TYPE<ProtocolV1> &next,bufferlist &bl);
+  inline CtPtr _fault() {  // helper fault method that stops continuation
+    fault();
+    return nullptr;
+  }
+
+  CONTINUATION_DECL(ProtocolV1, wait_message);
+  READ_HANDLER_CONTINUATION_DECL(ProtocolV1, handle_message);
+  READ_HANDLER_CONTINUATION_DECL(ProtocolV1, handle_keepalive2);
+  READ_HANDLER_CONTINUATION_DECL(ProtocolV1, handle_keepalive2_ack);
+  READ_HANDLER_CONTINUATION_DECL(ProtocolV1, handle_tag_ack);
+  READ_HANDLER_CONTINUATION_DECL(ProtocolV1, handle_message_header);
+  CONTINUATION_DECL(ProtocolV1, throttle_message);
+  CONTINUATION_DECL(ProtocolV1, throttle_bytes);
+  CONTINUATION_DECL(ProtocolV1, throttle_dispatch_queue);
+  READ_HANDLER_CONTINUATION_DECL(ProtocolV1, handle_message_front);
+  READ_HANDLER_CONTINUATION_DECL(ProtocolV1, handle_message_middle);
+  CONTINUATION_DECL(ProtocolV1, read_message_data);
+  READ_HANDLER_CONTINUATION_DECL(ProtocolV1, handle_message_data);
+  READ_HANDLER_CONTINUATION_DECL(ProtocolV1, handle_message_footer);
+
+  CtPtr ready();
+  CtPtr wait_message();
+  CtPtr handle_message(char *buffer, int r);
+
+  CtPtr handle_keepalive2(char *buffer, int r);
+  void append_keepalive_or_ack(bool ack = false, utime_t *t = nullptr);
+  CtPtr handle_keepalive2_ack(char *buffer, int r);
+  CtPtr handle_tag_ack(char *buffer, int r);
+
+  CtPtr handle_message_header(char *buffer, int r);
+  CtPtr throttle_message();
+  CtPtr throttle_bytes();
+  CtPtr throttle_dispatch_queue();
+  CtPtr read_message_front();
+  CtPtr handle_message_front(char *buffer, int r);
+  CtPtr read_message_middle();
+  CtPtr handle_message_middle(char *buffer, int r);
+  CtPtr read_message_data_prepare();
+  CtPtr read_message_data();
+  CtPtr handle_message_data(char *buffer, int r);
+  CtPtr read_message_footer();
+  CtPtr handle_message_footer(char *buffer, int r);
+
+  void session_reset();
+  void randomize_out_seq();
+
+  Message *_get_next_outgoing(bufferlist *bl);
+
+  void prepare_send_message(uint64_t features, Message *m, bufferlist &bl);
+  ssize_t write_message(Message *m, bufferlist &bl, bool more);
+
+  void requeue_sent();
+  uint64_t discard_requeued_up_to(uint64_t out_seq, uint64_t seq);
+  void discard_out_queue();
+
+  void reset_recv_state();
+  void reset_security();
+
+  ostream &_conn_prefix(std::ostream *_dout);
+
+public:
+  ProtocolV1(AsyncConnection *connection);
+  virtual ~ProtocolV1();
+
+  virtual void connect() override;
+  virtual void accept() override;
+  virtual bool is_connected() override;
+  virtual void stop() override;
+  virtual void fault() override;
+  virtual void send_message(Message *m) override;
+  virtual void send_keepalive() override;
+
+  virtual void read_event() override;
+  virtual void write_event() override;
+  virtual bool is_queued() override;
+
+  // Client Protocol
+private:
+  int global_seq;
+
+  CONTINUATION_DECL(ProtocolV1, send_client_banner);
+  WRITE_HANDLER_CONTINUATION_DECL(ProtocolV1, handle_client_banner_write);
+  READ_HANDLER_CONTINUATION_DECL(ProtocolV1, handle_server_banner_and_identify);
+  WRITE_HANDLER_CONTINUATION_DECL(ProtocolV1, handle_my_addr_write);
+  CONTINUATION_DECL(ProtocolV1, send_connect_message);
+  WRITE_HANDLER_CONTINUATION_DECL(ProtocolV1, handle_connect_message_write);
+  READ_HANDLER_CONTINUATION_DECL(ProtocolV1, handle_connect_reply_1);
+  READ_HANDLER_CONTINUATION_DECL(ProtocolV1, handle_connect_reply_auth);
+  READ_HANDLER_CONTINUATION_DECL(ProtocolV1, handle_ack_seq);
+  WRITE_HANDLER_CONTINUATION_DECL(ProtocolV1, handle_in_seq_write);
+
+  CtPtr send_client_banner();
+  CtPtr handle_client_banner_write(int r);
+  CtPtr wait_server_banner();
+  CtPtr handle_server_banner_and_identify(char *buffer, int r);
+  CtPtr handle_my_addr_write(int r);
+  CtPtr send_connect_message();
+  CtPtr handle_connect_message_write(int r);
+  CtPtr wait_connect_reply();
+  CtPtr handle_connect_reply_1(char *buffer, int r);
+  CtPtr wait_connect_reply_auth();
+  CtPtr handle_connect_reply_auth(char *buffer, int r);
+  CtPtr handle_connect_reply_2();
+  CtPtr wait_ack_seq();
+  CtPtr handle_ack_seq(char *buffer, int r);
+  CtPtr handle_in_seq_write(int r);
+  CtPtr client_ready();
+
+  // Server Protocol
+protected:
+  bool wait_for_seq;
+
+  CONTINUATION_DECL(ProtocolV1, send_server_banner);
+  WRITE_HANDLER_CONTINUATION_DECL(ProtocolV1, handle_server_banner_write);
+  READ_HANDLER_CONTINUATION_DECL(ProtocolV1, handle_client_banner);
+  CONTINUATION_DECL(ProtocolV1, wait_connect_message);
+  READ_HANDLER_CONTINUATION_DECL(ProtocolV1, handle_connect_message_1);
+  READ_HANDLER_CONTINUATION_DECL(ProtocolV1, handle_connect_message_auth);
+  WRITE_HANDLER_CONTINUATION_DECL(ProtocolV1,
+                                  handle_connect_message_reply_write);
+  WRITE_HANDLER_CONTINUATION_DECL(ProtocolV1,
+                                  handle_ready_connect_message_reply_write);
+  READ_HANDLER_CONTINUATION_DECL(ProtocolV1, handle_seq);
+
+  CtPtr send_server_banner();
+  CtPtr handle_server_banner_write(int r);
+  CtPtr wait_client_banner();
+  CtPtr handle_client_banner(char *buffer, int r);
+  CtPtr wait_connect_message();
+  CtPtr handle_connect_message_1(char *buffer, int r);
+  CtPtr wait_connect_message_auth();
+  CtPtr handle_connect_message_auth(char *buffer, int r);
+  CtPtr handle_connect_message_2();
+  CtPtr send_connect_message_reply(char tag, ceph_msg_connect_reply &reply,
+                                   bufferlist &authorizer_reply);
+  CtPtr handle_connect_message_reply_write(int r);
+  CtPtr replace(const AsyncConnectionRef& existing, ceph_msg_connect_reply &reply,
+                bufferlist &authorizer_reply);
+  CtPtr open(ceph_msg_connect_reply &reply, bufferlist &authorizer_reply);
+  CtPtr handle_ready_connect_message_reply_write(int r);
+  CtPtr wait_seq();
+  CtPtr handle_seq(char *buffer, int r);
+  CtPtr server_ready();
+};
+
+class LoopbackProtocolV1 : public ProtocolV1 {
+public:
+  LoopbackProtocolV1(AsyncConnection *connection) : ProtocolV1(connection) {
+    this->can_write = WriteStatus::CANWRITE;
+  }
+};
+
+#endif /* _MSG_ASYNC_PROTOCOL_V1_ */
diff --git a/src/msg/async/ProtocolV2.cc b/src/msg/async/ProtocolV2.cc
new file mode 100644
index 00000000000..4a237e744e6
--- /dev/null
+++ b/src/msg/async/ProtocolV2.cc
@@ -0,0 +1,2894 @@
+// -*- mode:C++; tab-width:8; c-basic-offset:2; indent-tabs-mode:t -*-
+// vim: ts=8 sw=2 smarttab
+
+#include <type_traits>
+
+#include "ProtocolV2.h"
+#include "AsyncMessenger.h"
+
+#include "common/EventTrace.h"
+#include "common/ceph_crypto.h"
+#include "common/errno.h"
+#include "include/random.h"
+#include "auth/AuthClient.h"
+#include "auth/AuthServer.h"
+
+#define dout_subsys ceph_subsys_ms
+#undef dout_prefix
+#define dout_prefix _conn_prefix(_dout)
+ostream &ProtocolV2::_conn_prefix(std::ostream *_dout) {
+  return *_dout << "--2- " << messenger->get_myaddrs() << " >> "
+                << *connection->peer_addrs << " conn(" << connection << " "
+                << this
+		<< " " << ceph_con_mode_name(auth_meta->con_mode)
+		<< " :" << connection->port
+                << " s=" << get_state_name(state) << " pgs=" << peer_global_seq
+                << " cs=" << connect_seq << " l=" << connection->policy.lossy
+                << " rev1=" << HAVE_MSGR2_FEATURE(peer_supported_features,
+                                                  REVISION_1)
+                << " rx=" << session_stream_handlers.rx.get()
+                << " tx=" << session_stream_handlers.tx.get()
+                << ").";
+}
+
+using namespace ceph::msgr::v2;
+
+using CtPtr = Ct<ProtocolV2> *;
+using CtRef = Ct<ProtocolV2> &;
+
+void ProtocolV2::run_continuation(CtPtr pcontinuation) {
+  if (pcontinuation) {
+    run_continuation(*pcontinuation);
+  }
+}
+
+void ProtocolV2::run_continuation(CtRef continuation) {
+  try {
+    CONTINUATION_RUN(continuation)
+  } catch (const buffer::error &e) {
+    lderr(cct) << __func__ << " failed decoding of frame header: " << e
+               << dendl;
+    _fault();
+  } catch (const ceph::crypto::onwire::MsgAuthError &e) {
+    lderr(cct) << __func__ << " " << e.what() << dendl;
+    _fault();
+  } catch (const DecryptionError &) {
+    lderr(cct) << __func__ << " failed to decrypt frame payload" << dendl;
+  }
+}
+
+#define WRITE(B, D, C) write(D, CONTINUATION(C), B)
+
+#define READ(L, C) read(CONTINUATION(C), buffer::ptr_node::create(buffer::create(L)))
+
+#define READ_RXBUF(B, C) read(CONTINUATION(C), B)
+
+#ifdef UNIT_TESTS_BUILT
+
+#define INTERCEPT(S) { \
+if(connection->interceptor) { \
+  auto a = connection->interceptor->intercept(connection, (S)); \
+  if (a == Interceptor::ACTION::FAIL) { \
+    return _fault(); \
+  } else if (a == Interceptor::ACTION::STOP) { \
+    stop(); \
+    connection->dispatch_queue->queue_reset(connection); \
+    return nullptr; \
+  }}}
+  
+#else
+#define INTERCEPT(S)
+#endif
+
+ProtocolV2::ProtocolV2(AsyncConnection *connection)
+    : Protocol(2, connection),
+      state(NONE),
+      peer_supported_features(0),
+      client_cookie(0),
+      server_cookie(0),
+      global_seq(0),
+      connect_seq(0),
+      peer_global_seq(0),
+      message_seq(0),
+      reconnecting(false),
+      replacing(false),
+      can_write(false),
+      bannerExchangeCallback(nullptr),
+      tx_frame_asm(&session_stream_handlers, false),
+      rx_frame_asm(&session_stream_handlers, false),
+      next_tag(static_cast<Tag>(0)),
+      keepalive(false) {
+}
+
+ProtocolV2::~ProtocolV2() {
+}
+
+void ProtocolV2::connect() {
+  ldout(cct, 1) << __func__ << dendl;
+  state = START_CONNECT;
+  pre_auth.enabled = true;
+}
+
+void ProtocolV2::accept() {
+  ldout(cct, 1) << __func__ << dendl;
+  state = START_ACCEPT;
+}
+
+bool ProtocolV2::is_connected() { return can_write; }
+
+/*
+ * Tears down the message queues, and removes them from the
+ * DispatchQueue Must hold write_lock prior to calling.
+ */
+void ProtocolV2::discard_out_queue() {
+  ldout(cct, 10) << __func__ << " started" << dendl;
+
+  for (list<Message *>::iterator p = sent.begin(); p != sent.end(); ++p) {
+    ldout(cct, 20) << __func__ << " discard " << *p << dendl;
+    (*p)->put();
+  }
+  sent.clear();
+  for (auto& [ prio, entries ] : out_queue) {
+    static_cast<void>(prio);
+    for (auto& entry : entries) {
+      ldout(cct, 20) << __func__ << " discard " << *entry.m << dendl;
+      entry.m->put();
+    }
+  }
+  out_queue.clear();
+  write_in_progress = false;
+}
+
+void ProtocolV2::reset_session() {
+  ldout(cct, 1) << __func__ << dendl;
+
+  std::lock_guard<std::mutex> l(connection->write_lock);
+  if (connection->delay_state) {
+    connection->delay_state->discard();
+  }
+
+  connection->dispatch_queue->discard_queue(connection->conn_id);
+  discard_out_queue();
+  connection->outgoing_bl.clear();
+
+  connection->dispatch_queue->queue_remote_reset(connection);
+
+  out_seq = 0;
+  in_seq = 0;
+  client_cookie = 0;
+  server_cookie = 0;
+  connect_seq = 0;
+  peer_global_seq = 0;
+  message_seq = 0;
+  ack_left = 0;
+  can_write = false;
+}
+
+void ProtocolV2::stop() {
+  ldout(cct, 1) << __func__ << dendl;
+  if (state == CLOSED) {
+    return;
+  }
+
+  if (connection->delay_state) connection->delay_state->flush();
+
+  std::lock_guard<std::mutex> l(connection->write_lock);
+
+  reset_recv_state();
+  discard_out_queue();
+
+  connection->_stop();
+
+  can_write = false;
+  state = CLOSED;
+}
+
+void ProtocolV2::fault() { _fault(); }
+
+void ProtocolV2::requeue_sent() {
+  write_in_progress = false;
+  if (sent.empty()) {
+    return;
+  }
+
+  auto& rq = out_queue[CEPH_MSG_PRIO_HIGHEST];
+  out_seq -= sent.size();
+  while (!sent.empty()) {
+    Message *m = sent.back();
+    sent.pop_back();
+    ldout(cct, 5) << __func__ << " requeueing message m=" << m
+                  << " seq=" << m->get_seq() << " type=" << m->get_type() << " "
+                  << *m << dendl;
+    m->clear_payload();
+    rq.emplace_front(out_queue_entry_t{false, m});
+  }
+}
+
+uint64_t ProtocolV2::discard_requeued_up_to(uint64_t out_seq, uint64_t seq) {
+  ldout(cct, 10) << __func__ << " " << seq << dendl;
+  std::lock_guard<std::mutex> l(connection->write_lock);
+  if (out_queue.count(CEPH_MSG_PRIO_HIGHEST) == 0) {
+    return seq;
+  }
+  auto& rq = out_queue[CEPH_MSG_PRIO_HIGHEST];
+  uint64_t count = out_seq;
+  while (!rq.empty()) {
+    Message* const m = rq.front().m;
+    if (m->get_seq() == 0 || m->get_seq() > seq) break;
+    ldout(cct, 5) << __func__ << " discarding message m=" << m
+                  << " seq=" << m->get_seq() << " ack_seq=" << seq << " "
+                  << *m << dendl;
+    m->put();
+    rq.pop_front();
+    count++;
+  }
+  if (rq.empty()) out_queue.erase(CEPH_MSG_PRIO_HIGHEST);
+  return count;
+}
+
+void ProtocolV2::reset_security() {
+  ldout(cct, 5) << __func__ << dendl;
+
+  auth_meta.reset(new AuthConnectionMeta);
+  session_stream_handlers.rx.reset(nullptr);
+  session_stream_handlers.tx.reset(nullptr);
+  pre_auth.rxbuf.clear();
+  pre_auth.txbuf.clear();
+}
+
+// it's expected the `write_lock` is held while calling this method.
+void ProtocolV2::reset_recv_state() {
+  ldout(cct, 5) << __func__ << dendl;
+
+  if (!connection->center->in_thread()) {
+    // execute in the same thread that uses the rx/tx handlers. We need
+    // to do the warp because holding `write_lock` is not enough as
+    // `write_event()` unlocks it just before calling `write_message()`.
+    // `submit_to()` here is NOT blocking.
+    connection->center->submit_to(connection->center->get_id(), [this] {
+      ldout(cct, 5) << "reset_recv_state (warped) reseting crypto handlers"
+                    << dendl;
+      // Possibly unnecessary. See the comment in `deactivate_existing`.
+      std::lock_guard<std::mutex> l(connection->lock);
+      std::lock_guard<std::mutex> wl(connection->write_lock);
+      reset_security();
+    }, /* always_async = */true);
+  } else {
+    reset_security();
+  }
+
+  // clean read and write callbacks
+  connection->pendingReadLen.reset();
+  connection->writeCallback.reset();
+
+  next_tag = static_cast<Tag>(0);
+
+  reset_throttle();
+}
+
+size_t ProtocolV2::get_current_msg_size() const {
+  ceph_assert(rx_frame_asm.get_num_segments() > 0);
+  size_t sum = 0;
+  // we don't include SegmentIndex::Msg::HEADER.
+  for (size_t i = 1; i < rx_frame_asm.get_num_segments(); i++) {
+    sum += rx_frame_asm.get_segment_logical_len(i);
+  }
+  return sum;
+}
+
+void ProtocolV2::reset_throttle() {
+  if (state > THROTTLE_MESSAGE && state <= THROTTLE_DONE &&
+      connection->policy.throttler_messages) {
+    ldout(cct, 10) << __func__ << " releasing " << 1
+                   << " message to policy throttler "
+                   << connection->policy.throttler_messages->get_current()
+                   << "/" << connection->policy.throttler_messages->get_max()
+                   << dendl;
+    connection->policy.throttler_messages->put();
+  }
+  if (state > THROTTLE_BYTES && state <= THROTTLE_DONE) {
+    if (connection->policy.throttler_bytes) {
+      const size_t cur_msg_size = get_current_msg_size();
+      ldout(cct, 10) << __func__ << " releasing " << cur_msg_size
+                     << " bytes to policy throttler "
+                     << connection->policy.throttler_bytes->get_current() << "/"
+                     << connection->policy.throttler_bytes->get_max() << dendl;
+      connection->policy.throttler_bytes->put(cur_msg_size);
+    }
+  }
+  if (state > THROTTLE_DISPATCH_QUEUE && state <= THROTTLE_DONE) {
+    const size_t cur_msg_size = get_current_msg_size();
+    ldout(cct, 10)
+        << __func__ << " releasing " << cur_msg_size
+        << " bytes to dispatch_queue throttler "
+        << connection->dispatch_queue->dispatch_throttler.get_current() << "/"
+        << connection->dispatch_queue->dispatch_throttler.get_max() << dendl;
+    connection->dispatch_queue->dispatch_throttle_release(cur_msg_size);
+  }
+}
+
+CtPtr ProtocolV2::_fault() {
+  ldout(cct, 10) << __func__ << dendl;
+
+  if (state == CLOSED || state == NONE) {
+    ldout(cct, 10) << __func__ << " connection is already closed" << dendl;
+    return nullptr;
+  }
+
+  if (connection->policy.lossy &&
+      !(state >= START_CONNECT && state <= SESSION_RECONNECTING)) {
+    ldout(cct, 2) << __func__ << " on lossy channel, failing" << dendl;
+    stop();
+    connection->dispatch_queue->queue_reset(connection);
+    return nullptr;
+  }
+
+  connection->write_lock.lock();
+
+  can_write = false;
+  // requeue sent items
+  requeue_sent();
+
+  if (out_queue.empty() && state >= START_ACCEPT &&
+      state <= SESSION_ACCEPTING && !replacing) {
+    ldout(cct, 2) << __func__ << " with nothing to send and in the half "
+                   << " accept state just closed" << dendl;
+    connection->write_lock.unlock();
+    stop();
+    connection->dispatch_queue->queue_reset(connection);
+    return nullptr;
+  }
+
+  replacing = false;
+  connection->fault();
+  reset_recv_state();
+
+  reconnecting = false;
+
+  if (connection->policy.standby && out_queue.empty() && !keepalive &&
+      state != WAIT) {
+    ldout(cct, 1) << __func__ << " with nothing to send, going to standby"
+                  << dendl;
+    state = STANDBY;
+    connection->write_lock.unlock();
+    return nullptr;
+  }
+  if (connection->policy.server) {
+    ldout(cct, 1) << __func__ << " server, going to standby, even though i have stuff queued" << dendl;
+    state = STANDBY;
+    connection->write_lock.unlock();
+    return nullptr;
+  }
+
+  connection->write_lock.unlock();
+
+  if (!(state >= START_CONNECT && state <= SESSION_RECONNECTING) &&
+      state != WAIT &&
+      state != SESSION_ACCEPTING /* due to connection race */) {
+    // policy maybe empty when state is in accept
+    if (connection->policy.server) {
+      ldout(cct, 1) << __func__ << " server, going to standby" << dendl;
+      state = STANDBY;
+    } else {
+      ldout(cct, 1) << __func__ << " initiating reconnect" << dendl;
+      connect_seq++;
+      global_seq = messenger->get_global_seq();
+      state = START_CONNECT;
+      pre_auth.enabled = true;
+      connection->state = AsyncConnection::STATE_CONNECTING;
+    }
+    backoff = utime_t();
+    connection->center->dispatch_event_external(connection->read_handler);
+  } else {
+    if (state == WAIT) {
+      backoff.set_from_double(cct->_conf->ms_max_backoff);
+    } else if (backoff == utime_t()) {
+      backoff.set_from_double(cct->_conf->ms_initial_backoff);
+    } else {
+      backoff += backoff;
+      if (backoff > cct->_conf->ms_max_backoff)
+        backoff.set_from_double(cct->_conf->ms_max_backoff);
+    }
+
+    if (server_cookie) {
+      connect_seq++;
+    }
+
+    global_seq = messenger->get_global_seq();
+    state = START_CONNECT;
+    pre_auth.enabled = true;
+    connection->state = AsyncConnection::STATE_CONNECTING;
+    ldout(cct, 1) << __func__ << " waiting " << backoff << dendl;
+    // woke up again;
+    connection->register_time_events.insert(
+        connection->center->create_time_event(backoff.to_nsec() / 1000,
+                                              connection->wakeup_handler));
+  }
+  return nullptr;
+}
+
+void ProtocolV2::prepare_send_message(uint64_t features,
+				      Message *m) {
+  ldout(cct, 20) << __func__ << " m=" << *m << dendl;
+
+  // associate message with Connection (for benefit of encode_payload)
+  ldout(cct, 20) << __func__ << (m->empty_payload() ? " encoding features " : " half-reencoding features ")
+		 << features << " " << m  << " " << *m << dendl;
+
+  // encode and copy out of *m
+  m->encode(features, 0);
+}
+
+void ProtocolV2::send_message(Message *m) {
+  uint64_t f = connection->get_features();
+
+  // TODO: Currently not all messages supports reencode like MOSDMap, so here
+  // only let fast dispatch support messages prepare message
+  const bool can_fast_prepare = messenger->ms_can_fast_dispatch(m);
+  if (can_fast_prepare) {
+    prepare_send_message(f, m);
+  }
+
+  std::lock_guard<std::mutex> l(connection->write_lock);
+  bool is_prepared = can_fast_prepare;
+  // "features" changes will change the payload encoding
+  if (can_fast_prepare && (!can_write || connection->get_features() != f)) {
+    // ensure the correctness of message encoding
+    m->clear_payload();
+    is_prepared = false;
+    ldout(cct, 10) << __func__ << " clear encoded buffer previous " << f
+                   << " != " << connection->get_features() << dendl;
+  }
+  if (state == CLOSED) {
+    ldout(cct, 10) << __func__ << " connection closed."
+                   << " Drop message " << m << dendl;
+    m->put();
+  } else {
+    ldout(cct, 5) << __func__ << " enqueueing message m=" << m
+                  << " type=" << m->get_type() << " " << *m << dendl;
+    m->queue_start = ceph::mono_clock::now();
+    m->trace.event("async enqueueing message");
+    out_queue[m->get_priority()].emplace_back(
+      out_queue_entry_t{is_prepared, m});
+    ldout(cct, 15) << __func__ << " inline write is denied, reschedule m=" << m
+                   << dendl;
+    if (((!replacing && can_write) || state == STANDBY) && !write_in_progress) {
+      write_in_progress = true;
+      connection->center->dispatch_event_external(connection->write_handler);
+    }
+  }
+}
+
+void ProtocolV2::send_keepalive() {
+  ldout(cct, 10) << __func__ << dendl;
+  std::lock_guard<std::mutex> l(connection->write_lock);
+  if (state != CLOSED) {
+    keepalive = true;
+    connection->center->dispatch_event_external(connection->write_handler);
+  }
+}
+
+void ProtocolV2::read_event() {
+  ldout(cct, 20) << __func__ << dendl;
+
+  switch (state) {
+    case START_CONNECT:
+      run_continuation(CONTINUATION(start_client_banner_exchange));
+      break;
+    case START_ACCEPT:
+      run_continuation(CONTINUATION(start_server_banner_exchange));
+      break;
+    case READY:
+      run_continuation(CONTINUATION(read_frame));
+      break;
+    case THROTTLE_MESSAGE:
+      run_continuation(CONTINUATION(throttle_message));
+      break;
+    case THROTTLE_BYTES:
+      run_continuation(CONTINUATION(throttle_bytes));
+      break;
+    case THROTTLE_DISPATCH_QUEUE:
+      run_continuation(CONTINUATION(throttle_dispatch_queue));
+      break;
+    default:
+      break;
+  }
+}
+
+ProtocolV2::out_queue_entry_t ProtocolV2::_get_next_outgoing() {
+  out_queue_entry_t out_entry;
+
+  if (!out_queue.empty()) {
+    auto it = out_queue.rbegin();
+    auto& entries = it->second;
+    ceph_assert(!entries.empty());
+    out_entry = entries.front();
+    entries.pop_front();
+    if (entries.empty()) {
+      out_queue.erase(it->first);
+    }
+  }
+  return out_entry;
+}
+
+ssize_t ProtocolV2::write_message(Message *m, bool more) {
+  FUNCTRACE(cct);
+  ceph_assert(connection->center->in_thread());
+  m->set_seq(++out_seq);
+
+  connection->lock.lock();
+  uint64_t ack_seq = in_seq;
+  ack_left = 0;
+  connection->lock.unlock();
+
+  ceph_msg_header &header = m->get_header();
+  ceph_msg_footer &footer = m->get_footer();
+
+  ceph_msg_header2 header2{header.seq,        header.tid,
+                           header.type,       header.priority,
+                           header.version,
+                           init_le32(0),      header.data_off,
+                           init_le64(ack_seq),
+                           footer.flags,      header.compat_version,
+                           header.reserved};
+
+  auto message = MessageFrame::Encode(
+			     header2,
+			     m->get_payload(),
+			     m->get_middle(),
+			     m->get_data());
+  if (!append_frame(message)) {
+    m->put();
+    return -EILSEQ;
+  }
+
+  ldout(cct, 5) << __func__ << " sending message m=" << m
+                << " seq=" << m->get_seq() << " " << *m << dendl;
+
+  m->trace.event("async writing message");
+  ldout(cct, 20) << __func__ << " sending m=" << m << " seq=" << m->get_seq()
+                 << " src=" << entity_name_t(messenger->get_myname())
+                 << " off=" << header2.data_off
+                 << dendl;
+  ssize_t total_send_size = connection->outgoing_bl.length();
+  ssize_t rc = connection->_try_send(more);
+  if (rc < 0) {
+    ldout(cct, 1) << __func__ << " error sending " << m << ", "
+                  << cpp_strerror(rc) << dendl;
+  } else {
+    connection->logger->inc(
+        l_msgr_send_bytes, total_send_size - connection->outgoing_bl.length());
+    ldout(cct, 10) << __func__ << " sending " << m
+                   << (rc ? " continuely." : " done.") << dendl;
+  }
+
+#if defined(WITH_EVENTTRACE)
+  if (m->get_type() == CEPH_MSG_OSD_OP)
+    OID_EVENT_TRACE_WITH_MSG(m, "SEND_MSG_OSD_OP_END", false);
+  else if (m->get_type() == CEPH_MSG_OSD_OPREPLY)
+    OID_EVENT_TRACE_WITH_MSG(m, "SEND_MSG_OSD_OPREPLY_END", false);
+#endif
+  m->put();
+
+  return rc;
+}
+
+template <class F>
+bool ProtocolV2::append_frame(F& frame) {
+  ceph::bufferlist bl;
+  try {
+    bl = frame.get_buffer(tx_frame_asm);
+  } catch (ceph::crypto::onwire::TxHandlerError &e) {
+    ldout(cct, 1) << __func__ << " " << e.what() << dendl;
+    return false;
+  }
+
+  ldout(cct, 25) << __func__ << " assembled frame " << bl.length()
+                 << " bytes " << tx_frame_asm << dendl;
+  connection->outgoing_bl.append(bl);
+  return true;
+}
+
+void ProtocolV2::handle_message_ack(uint64_t seq) {
+  if (connection->policy.lossy) {  // lossy connections don't keep sent messages
+    return;
+  }
+
+  ldout(cct, 15) << __func__ << " seq=" << seq << dendl;
+
+  // trim sent list
+  static const int max_pending = 128;
+  int i = 0;
+  Message *pending[max_pending];
+  auto now = ceph::mono_clock::now();
+  connection->write_lock.lock();
+  while (!sent.empty() && sent.front()->get_seq() <= seq && i < max_pending) {
+    Message *m = sent.front();
+    sent.pop_front();
+    pending[i++] = m;
+    ldout(cct, 10) << __func__ << " got ack seq " << seq
+                   << " >= " << m->get_seq() << " on " << m << " " << *m
+                   << dendl;
+  }
+  connection->write_lock.unlock();
+  connection->logger->tinc(l_msgr_handle_ack_lat, ceph::mono_clock::now() - now);
+  for (int k = 0; k < i; k++) {
+    pending[k]->put();
+  }
+}
+
+void ProtocolV2::write_event() {
+  ldout(cct, 10) << __func__ << dendl;
+  ssize_t r = 0;
+
+  connection->write_lock.lock();
+  if (can_write) {
+    if (keepalive) {
+      ldout(cct, 10) << __func__ << " appending keepalive" << dendl;
+      auto keepalive_frame = KeepAliveFrame::Encode();
+      if (!append_frame(keepalive_frame)) {
+        connection->write_lock.unlock();
+        connection->lock.lock();
+        fault();
+        connection->lock.unlock();
+        return;
+      }
+      keepalive = false;
+    }
+
+    auto start = ceph::mono_clock::now();
+    bool more;
+    do {
+      const auto out_entry = _get_next_outgoing();
+      if (!out_entry.m) {
+        break;
+      }
+
+      if (!connection->policy.lossy) {
+        // put on sent list
+        sent.push_back(out_entry.m);
+        out_entry.m->get();
+      }
+      more = !out_queue.empty();
+      connection->write_lock.unlock();
+
+      // send_message or requeue messages may not encode message
+      if (!out_entry.is_prepared) {
+        prepare_send_message(connection->get_features(), out_entry.m);
+      }
+
+      if (out_entry.m->queue_start != ceph::mono_time()) {
+        connection->logger->tinc(l_msgr_send_messages_queue_lat,
+				 ceph::mono_clock::now() -
+				 out_entry.m->queue_start);
+      }
+
+      r = write_message(out_entry.m, more);
+
+      connection->write_lock.lock();
+      if (r == 0) {
+        ;
+      } else if (r < 0) {
+        ldout(cct, 1) << __func__ << " send msg failed" << dendl;
+        break;
+      } else if (r > 0) {
+	// Outbound message in-progress, thread will be re-awoken
+	// when the outbound socket is writeable again
+        break;
+      }
+    } while (can_write);
+    write_in_progress = false;
+
+    // if r > 0 mean data still lefted, so no need _try_send.
+    if (r == 0) {
+      uint64_t left = ack_left;
+      if (left) {
+        ldout(cct, 10) << __func__ << " try send msg ack, acked " << left
+                       << " messages" << dendl;
+        auto ack_frame = AckFrame::Encode(in_seq);
+        if (append_frame(ack_frame)) {
+          ack_left -= left;
+          left = ack_left;
+          r = connection->_try_send(left);
+        } else {
+          r = -EILSEQ;
+        }
+      } else if (is_queued()) {
+        r = connection->_try_send();
+      }
+    }
+    connection->write_lock.unlock();
+
+    connection->logger->tinc(l_msgr_running_send_time,
+                             ceph::mono_clock::now() - start);
+    if (r < 0) {
+      ldout(cct, 1) << __func__ << " send msg failed" << dendl;
+      connection->lock.lock();
+      fault();
+      connection->lock.unlock();
+      return;
+    }
+  } else {
+    write_in_progress = false;
+    connection->write_lock.unlock();
+    connection->lock.lock();
+    connection->write_lock.lock();
+    if (state == STANDBY && !connection->policy.server && is_queued()) {
+      ldout(cct, 10) << __func__ << " policy.server is false" << dendl;
+      if (server_cookie) {  // only increment connect_seq if there is a session
+        connect_seq++;
+      }
+      connection->_connect();
+    } else if (connection->cs && state != NONE && state != CLOSED &&
+               state != START_CONNECT) {
+      r = connection->_try_send();
+      if (r < 0) {
+        ldout(cct, 1) << __func__ << " send outcoming bl failed" << dendl;
+        connection->write_lock.unlock();
+        fault();
+        connection->lock.unlock();
+        return;
+      }
+    }
+    connection->write_lock.unlock();
+    connection->lock.unlock();
+  }
+}
+
+bool ProtocolV2::is_queued() {
+  return !out_queue.empty() || connection->is_queued();
+}
+
+CtPtr ProtocolV2::read(CONTINUATION_RXBPTR_TYPE<ProtocolV2> &next,
+                       rx_buffer_t &&buffer) {
+  const auto len = buffer->length();
+  const auto buf = buffer->c_str();
+  next.node = std::move(buffer);
+  ssize_t r = connection->read(len, buf,
+    [&next, this](char *buffer, int r) {
+      if (unlikely(pre_auth.enabled) && r >= 0) {
+        pre_auth.rxbuf.append(*next.node);
+	ceph_assert(!cct->_conf->ms_die_on_bug ||
+		    pre_auth.rxbuf.length() < 20000000);
+      }
+      next.r = r;
+      run_continuation(next);
+    });
+  if (r <= 0) {
+    // error or done synchronously
+    if (unlikely(pre_auth.enabled) && r >= 0) {
+      pre_auth.rxbuf.append(*next.node);
+      ceph_assert(!cct->_conf->ms_die_on_bug ||
+		  pre_auth.rxbuf.length() < 20000000);
+    }
+    next.r = r;
+    return &next;
+  }
+
+  return nullptr;
+}
+
+template <class F>
+CtPtr ProtocolV2::write(const std::string &desc,
+                        CONTINUATION_TYPE<ProtocolV2> &next,
+                        F &frame) {
+  ceph::bufferlist bl;
+  try {
+    bl = frame.get_buffer(tx_frame_asm);
+  } catch (ceph::crypto::onwire::TxHandlerError &e) {
+    ldout(cct, 1) << __func__ << " " << e.what() << dendl;
+    return _fault();
+  }
+
+  ldout(cct, 25) << __func__ << " assembled frame " << bl.length()
+                 << " bytes " << tx_frame_asm << dendl;
+  return write(desc, next, bl);
+}
+
+CtPtr ProtocolV2::write(const std::string &desc,
+                        CONTINUATION_TYPE<ProtocolV2> &next,
+                        bufferlist &buffer) {
+  if (unlikely(pre_auth.enabled)) {
+    pre_auth.txbuf.append(buffer);
+    ceph_assert(!cct->_conf->ms_die_on_bug ||
+		pre_auth.txbuf.length() < 20000000);
+  }
+
+  ssize_t r =
+      connection->write(buffer, [&next, desc, this](int r) {
+        if (r < 0) {
+          ldout(cct, 1) << __func__ << " " << desc << " write failed r=" << r
+                        << " (" << cpp_strerror(r) << ")" << dendl;
+          connection->inject_delay();
+          _fault();
+        }
+        run_continuation(next);
+      });
+
+  if (r < 0) {
+    ldout(cct, 1) << __func__ << " " << desc << " write failed r=" << r
+                  << " (" << cpp_strerror(r) << ")" << dendl;
+    return _fault();
+  } else if (r == 0) {
+    next.setParams();
+    return &next;
+  }
+
+  return nullptr;
+}
+
+CtPtr ProtocolV2::_banner_exchange(CtRef callback) {
+  ldout(cct, 20) << __func__ << dendl;
+  bannerExchangeCallback = &callback;
+
+  bufferlist banner_payload;
+  encode((uint64_t)CEPH_MSGR2_SUPPORTED_FEATURES, banner_payload, 0);
+  encode((uint64_t)CEPH_MSGR2_REQUIRED_FEATURES, banner_payload, 0);
+
+  bufferlist bl;
+  bl.append(CEPH_BANNER_V2_PREFIX, strlen(CEPH_BANNER_V2_PREFIX));
+  encode((uint16_t)banner_payload.length(), bl, 0);
+  bl.claim_append(banner_payload);
+
+  INTERCEPT(state == BANNER_CONNECTING ? 3 : 4);
+
+  return WRITE(bl, "banner", _wait_for_peer_banner);
+}
+
+CtPtr ProtocolV2::_wait_for_peer_banner() {
+  unsigned banner_len = strlen(CEPH_BANNER_V2_PREFIX) + sizeof(ceph_le16);
+  return READ(banner_len, _handle_peer_banner);
+}
+
+CtPtr ProtocolV2::_handle_peer_banner(rx_buffer_t &&buffer, int r) {
+  ldout(cct, 20) << __func__ << " r=" << r << dendl;
+
+  if (r < 0) {
+    ldout(cct, 1) << __func__ << " read peer banner failed r=" << r << " ("
+                  << cpp_strerror(r) << ")" << dendl;
+    return _fault();
+  }
+
+  unsigned banner_prefix_len = strlen(CEPH_BANNER_V2_PREFIX);
+
+  if (memcmp(buffer->c_str(), CEPH_BANNER_V2_PREFIX, banner_prefix_len)) {
+    if (memcmp(buffer->c_str(), CEPH_BANNER, strlen(CEPH_BANNER)) == 0) {
+      lderr(cct) << __func__ << " peer " << *connection->peer_addrs
+                 << " is using msgr V1 protocol" << dendl;
+      return _fault();
+    }
+    ldout(cct, 1) << __func__ << " accept peer sent bad banner" << dendl;
+    return _fault();
+  }
+
+  uint16_t payload_len;
+  bufferlist bl;
+  buffer->set_offset(banner_prefix_len);
+  buffer->set_length(sizeof(ceph_le16));
+  bl.push_back(std::move(buffer));
+  auto ti = bl.cbegin();
+  try {
+    decode(payload_len, ti);
+  } catch (const buffer::error &e) {
+    lderr(cct) << __func__ << " decode banner payload len failed " << dendl;
+    return _fault();
+  }
+
+  INTERCEPT(state == BANNER_CONNECTING ? 5 : 6);
+
+  return READ(payload_len, _handle_peer_banner_payload);
+}
+
+CtPtr ProtocolV2::_handle_peer_banner_payload(rx_buffer_t &&buffer, int r) {
+  ldout(cct, 20) << __func__ << " r=" << r << dendl;
+
+  if (r < 0) {
+    ldout(cct, 1) << __func__ << " read peer banner payload failed r=" << r
+                  << " (" << cpp_strerror(r) << ")" << dendl;
+    return _fault();
+  }
+
+  uint64_t peer_supported_features;
+  uint64_t peer_required_features;
+
+  bufferlist bl;
+  bl.push_back(std::move(buffer));
+  auto ti = bl.cbegin();
+  try {
+    decode(peer_supported_features, ti);
+    decode(peer_required_features, ti);
+  } catch (const buffer::error &e) {
+    lderr(cct) << __func__ << " decode banner payload failed " << dendl;
+    return _fault();
+  }
+
+  ldout(cct, 1) << __func__ << " supported=" << std::hex
+                << peer_supported_features << " required=" << std::hex
+                << peer_required_features << std::dec << dendl;
+
+  // Check feature bit compatibility
+
+  uint64_t supported_features = CEPH_MSGR2_SUPPORTED_FEATURES;
+  uint64_t required_features = CEPH_MSGR2_REQUIRED_FEATURES;
+
+  if ((required_features & peer_supported_features) != required_features) {
+    ldout(cct, 1) << __func__ << " peer does not support all required features"
+                  << " required=" << std::hex << required_features
+                  << " supported=" << std::hex << peer_supported_features
+                  << std::dec << dendl;
+    stop();
+    connection->dispatch_queue->queue_reset(connection);
+    return nullptr;
+  }
+  if ((supported_features & peer_required_features) != peer_required_features) {
+    ldout(cct, 1) << __func__ << " we do not support all peer required features"
+                  << " required=" << std::hex << peer_required_features
+                  << " supported=" << supported_features << std::dec << dendl;
+    stop();
+    connection->dispatch_queue->queue_reset(connection);
+    return nullptr;
+  }
+
+  this->peer_supported_features = peer_supported_features;
+  if (peer_required_features == 0) {
+    this->connection_features = msgr2_required;
+  }
+
+  // if the peer supports msgr2.1, switch to it
+  bool is_rev1 = HAVE_MSGR2_FEATURE(peer_supported_features, REVISION_1);
+  tx_frame_asm.set_is_rev1(is_rev1);
+  rx_frame_asm.set_is_rev1(is_rev1);
+
+  if (state == BANNER_CONNECTING) {
+    state = HELLO_CONNECTING;
+  }
+  else {
+    ceph_assert(state == BANNER_ACCEPTING);
+    state = HELLO_ACCEPTING;
+  }
+
+  auto hello = HelloFrame::Encode(messenger->get_mytype(),
+                                  connection->target_addr);
+
+  INTERCEPT(state == HELLO_CONNECTING ? 7 : 8);
+
+  return WRITE(hello, "hello frame", read_frame);
+}
+
+CtPtr ProtocolV2::handle_hello(ceph::bufferlist &payload)
+{
+  ldout(cct, 20) << __func__
+		 << " payload.length()=" << payload.length() << dendl;
+
+  if (state != HELLO_CONNECTING && state != HELLO_ACCEPTING) {
+    lderr(cct) << __func__ << " not in hello exchange state!" << dendl;
+    return _fault();
+  }
+
+  auto hello = HelloFrame::Decode(payload);
+
+  ldout(cct, 5) << __func__ << " received hello:"
+                << " peer_type=" << (int)hello.entity_type()
+                << " peer_addr_for_me=" << hello.peer_addr() << dendl;
+
+  sockaddr_storage ss;
+  socklen_t len = sizeof(ss);
+  getsockname(connection->cs.fd(), (sockaddr *)&ss, &len);
+  ldout(cct, 5) << __func__ << " getsockname says I am " << (sockaddr *)&ss
+		<< " when talking to " << connection->target_addr << dendl;
+
+  if (connection->get_peer_type() == -1) {
+    connection->set_peer_type(hello.entity_type());
+
+    ceph_assert(state == HELLO_ACCEPTING);
+    connection->policy = messenger->get_policy(hello.entity_type());
+    ldout(cct, 10) << __func__ << " accept of host_type "
+                   << (int)hello.entity_type()
+                   << ", policy.lossy=" << connection->policy.lossy
+                   << " policy.server=" << connection->policy.server
+                   << " policy.standby=" << connection->policy.standby
+                   << " policy.resetcheck=" << connection->policy.resetcheck
+                   << dendl;
+  } else {
+    ceph_assert(state == HELLO_CONNECTING);
+    if (connection->get_peer_type() != hello.entity_type()) {
+      ldout(cct, 1) << __func__ << " connection peer type does not match what"
+                    << " peer advertises " << connection->get_peer_type()
+                    << " != " << (int)hello.entity_type() << dendl;
+      stop();
+      connection->dispatch_queue->queue_reset(connection);
+      return nullptr;
+    }
+  }
+
+  if (messenger->get_myaddrs().empty() ||
+      messenger->get_myaddrs().front().is_blank_ip()) {
+    entity_addr_t a;
+    if (cct->_conf->ms_learn_addr_from_peer) {
+      ldout(cct, 1) << __func__ << " peer " << connection->target_addr
+		    << " says I am " << hello.peer_addr() << " (socket says "
+		    << (sockaddr*)&ss << ")" << dendl;
+      a = hello.peer_addr();
+    } else {
+      ldout(cct, 1) << __func__ << " socket to  " << connection->target_addr
+		    << " says I am " << (sockaddr*)&ss
+		    << " (peer says " << hello.peer_addr() << ")" << dendl;
+      a.set_sockaddr((sockaddr *)&ss);
+    }
+    a.set_type(entity_addr_t::TYPE_MSGR2); // anything but NONE; learned_addr ignores this
+    a.set_port(0);
+    connection->lock.unlock();
+    messenger->learned_addr(a);
+    if (cct->_conf->ms_inject_internal_delays &&
+        cct->_conf->ms_inject_socket_failures) {
+      if (rand() % cct->_conf->ms_inject_socket_failures == 0) {
+        ldout(cct, 10) << __func__ << " sleep for "
+                       << cct->_conf->ms_inject_internal_delays << dendl;
+        utime_t t;
+        t.set_from_double(cct->_conf->ms_inject_internal_delays);
+        t.sleep();
+      }
+    }
+    connection->lock.lock();
+    if (state != HELLO_CONNECTING) {
+      ldout(cct, 1) << __func__
+                    << " state changed while learned_addr, mark_down or "
+                    << " replacing must be happened just now" << dendl;
+      return nullptr;
+    }
+  }
+
+
+
+  CtPtr callback;
+  callback = bannerExchangeCallback;
+  bannerExchangeCallback = nullptr;
+  ceph_assert(callback);
+  return callback;
+}
+
+CtPtr ProtocolV2::read_frame() {
+  if (state == CLOSED) {
+    return nullptr;
+  }
+
+  ldout(cct, 20) << __func__ << dendl;
+  rx_preamble.clear();
+  rx_epilogue.clear();
+  rx_segments_data.clear();
+
+  return READ(rx_frame_asm.get_preamble_onwire_len(),
+              handle_read_frame_preamble_main);
+}
+
+CtPtr ProtocolV2::handle_read_frame_preamble_main(rx_buffer_t &&buffer, int r) {
+  ldout(cct, 20) << __func__ << " r=" << r << dendl;
+
+  if (r < 0) {
+    ldout(cct, 1) << __func__ << " read frame preamble failed r=" << r
+                  << " (" << cpp_strerror(r) << ")" << dendl;
+    return _fault();
+  }
+
+  rx_preamble.push_back(std::move(buffer));
+
+  ldout(cct, 30) << __func__ << " preamble\n";
+  rx_preamble.hexdump(*_dout);
+  *_dout << dendl;
+
+  try {
+    next_tag = rx_frame_asm.disassemble_preamble(rx_preamble);
+  } catch (FrameError& e) {
+    ldout(cct, 1) << __func__ << " " << e.what() << dendl;
+    return _fault();
+  } catch (ceph::crypto::onwire::MsgAuthError&) {
+    ldout(cct, 1) << __func__ << "bad auth tag" << dendl;
+    return _fault();
+  }
+
+  ldout(cct, 25) << __func__ << " disassembled preamble " << rx_frame_asm
+                 << dendl;
+
+  if (session_stream_handlers.rx) {
+    ldout(cct, 30) << __func__ << " preamble after decrypt\n";
+    rx_preamble.hexdump(*_dout);
+    *_dout << dendl;
+  }
+
+  // does it need throttle?
+  if (next_tag == Tag::MESSAGE) {
+    if (state != READY) {
+      lderr(cct) << __func__ << " not in ready state!" << dendl;
+      return _fault();
+    }
+    state = THROTTLE_MESSAGE;
+    return CONTINUE(throttle_message);
+  } else {
+    return read_frame_segment();
+  }
+}
+
+CtPtr ProtocolV2::handle_read_frame_dispatch() {
+  ldout(cct, 10) << __func__
+                 << " tag=" << static_cast<uint32_t>(next_tag) << dendl;
+
+  switch (next_tag) {
+    case Tag::HELLO:
+    case Tag::AUTH_REQUEST:
+    case Tag::AUTH_BAD_METHOD:
+    case Tag::AUTH_REPLY_MORE:
+    case Tag::AUTH_REQUEST_MORE:
+    case Tag::AUTH_DONE:
+    case Tag::AUTH_SIGNATURE:
+    case Tag::CLIENT_IDENT:
+    case Tag::SERVER_IDENT:
+    case Tag::IDENT_MISSING_FEATURES:
+    case Tag::SESSION_RECONNECT:
+    case Tag::SESSION_RESET:
+    case Tag::SESSION_RETRY:
+    case Tag::SESSION_RETRY_GLOBAL:
+    case Tag::SESSION_RECONNECT_OK:
+    case Tag::KEEPALIVE2:
+    case Tag::KEEPALIVE2_ACK:
+    case Tag::ACK:
+    case Tag::WAIT:
+      return handle_frame_payload();
+    case Tag::MESSAGE:
+      return handle_message();
+    default: {
+      lderr(cct) << __func__
+                 << " received unknown tag=" << static_cast<uint32_t>(next_tag)
+                 << dendl;
+      return _fault();
+    }
+  }
+
+  return nullptr;
+}
+
+CtPtr ProtocolV2::read_frame_segment() {
+  size_t seg_idx = rx_segments_data.size();
+  ldout(cct, 20) << __func__ << " seg_idx=" << seg_idx << dendl;
+  rx_segments_data.emplace_back();
+
+  uint32_t onwire_len = rx_frame_asm.get_segment_onwire_len(seg_idx);
+  if (onwire_len == 0) {
+    return _handle_read_frame_segment();
+  }
+
+  rx_buffer_t rx_buffer;
+  uint16_t align = rx_frame_asm.get_segment_align(seg_idx);
+  try {
+    rx_buffer = buffer::ptr_node::create(buffer::create_aligned(
+        onwire_len, align));
+  } catch (const ceph::buffer::bad_alloc&) {
+    // Catching because of potential issues with satisfying alignment.
+    ldout(cct, 1) << __func__ << " can't allocate aligned rx_buffer"
+                  << " len=" << onwire_len
+                  << " align=" << align
+                  << dendl;
+    return _fault();
+  }
+
+  return READ_RXBUF(std::move(rx_buffer), handle_read_frame_segment);
+}
+
+CtPtr ProtocolV2::handle_read_frame_segment(rx_buffer_t &&rx_buffer, int r) {
+  ldout(cct, 20) << __func__ << " r=" << r << dendl;
+
+  if (r < 0) {
+    ldout(cct, 1) << __func__ << " read frame segment failed r=" << r << " ("
+                  << cpp_strerror(r) << ")" << dendl;
+    return _fault();
+  }
+
+  rx_segments_data.back().push_back(std::move(rx_buffer));
+  return _handle_read_frame_segment();
+}
+
+CtPtr ProtocolV2::_handle_read_frame_segment() {
+  if (rx_segments_data.size() == rx_frame_asm.get_num_segments()) {
+    // OK, all segments planned to read are read. Can go with epilogue.
+    uint32_t epilogue_onwire_len = rx_frame_asm.get_epilogue_onwire_len();
+    if (epilogue_onwire_len == 0) {
+      return _handle_read_frame_epilogue_main();
+    }
+    return READ(epilogue_onwire_len, handle_read_frame_epilogue_main);
+  }
+  // TODO: for makeshift only. This will be more generic and throttled
+  return read_frame_segment();
+}
+
+CtPtr ProtocolV2::handle_frame_payload() {
+  ceph_assert(!rx_segments_data.empty());
+  auto& payload = rx_segments_data.back();
+
+  ldout(cct, 30) << __func__ << "\n";
+  payload.hexdump(*_dout);
+  *_dout << dendl;
+
+  switch (next_tag) {
+    case Tag::HELLO:
+      return handle_hello(payload);
+    case Tag::AUTH_REQUEST:
+      return handle_auth_request(payload);
+    case Tag::AUTH_BAD_METHOD:
+      return handle_auth_bad_method(payload);
+    case Tag::AUTH_REPLY_MORE:
+      return handle_auth_reply_more(payload);
+    case Tag::AUTH_REQUEST_MORE:
+      return handle_auth_request_more(payload);
+    case Tag::AUTH_DONE:
+      return handle_auth_done(payload);
+    case Tag::AUTH_SIGNATURE:
+      return handle_auth_signature(payload);
+    case Tag::CLIENT_IDENT:
+      return handle_client_ident(payload);
+    case Tag::SERVER_IDENT:
+      return handle_server_ident(payload);
+    case Tag::IDENT_MISSING_FEATURES:
+      return handle_ident_missing_features(payload);
+    case Tag::SESSION_RECONNECT:
+      return handle_reconnect(payload);
+    case Tag::SESSION_RESET:
+      return handle_session_reset(payload);
+    case Tag::SESSION_RETRY:
+      return handle_session_retry(payload);
+    case Tag::SESSION_RETRY_GLOBAL:
+      return handle_session_retry_global(payload);
+    case Tag::SESSION_RECONNECT_OK:
+      return handle_reconnect_ok(payload);
+    case Tag::KEEPALIVE2:
+      return handle_keepalive2(payload);
+    case Tag::KEEPALIVE2_ACK:
+      return handle_keepalive2_ack(payload);
+    case Tag::ACK:
+      return handle_message_ack(payload);
+    case Tag::WAIT:
+      return handle_wait(payload);
+    default:
+      ceph_abort();
+  }
+  return nullptr;
+}
+
+CtPtr ProtocolV2::ready() {
+  ldout(cct, 25) << __func__ << dendl;
+
+  reconnecting = false;
+  replacing = false;
+
+  // make sure no pending tick timer
+  if (connection->last_tick_id) {
+    connection->center->delete_time_event(connection->last_tick_id);
+  }
+  connection->last_tick_id = connection->center->create_time_event(
+      connection->inactive_timeout_us, connection->tick_handler);
+
+  {
+    std::lock_guard<std::mutex> l(connection->write_lock);
+    can_write = true;
+    if (!out_queue.empty()) {
+      connection->center->dispatch_event_external(connection->write_handler);
+    }
+  }
+
+  connection->maybe_start_delay_thread();
+
+  state = READY;
+  ldout(cct, 1) << __func__ << " entity=" << peer_name << " client_cookie="
+                << std::hex << client_cookie << " server_cookie="
+                << server_cookie << std::dec << " in_seq=" << in_seq
+                << " out_seq=" << out_seq << dendl;
+
+  INTERCEPT(15);
+
+  return CONTINUE(read_frame);
+}
+
+CtPtr ProtocolV2::handle_read_frame_epilogue_main(rx_buffer_t &&buffer, int r)
+{
+  ldout(cct, 20) << __func__ << " r=" << r << dendl;
+
+  if (r < 0) {
+    ldout(cct, 1) << __func__ << " read frame epilogue failed r=" << r
+                  << " (" << cpp_strerror(r) << ")" << dendl;
+    return _fault();
+  }
+
+  rx_epilogue.push_back(std::move(buffer));
+  return _handle_read_frame_epilogue_main();
+}
+
+CtPtr ProtocolV2::_handle_read_frame_epilogue_main() {
+  bool aborted;
+  try {
+    rx_frame_asm.disassemble_first_segment(rx_preamble, rx_segments_data[0]);
+    aborted = !rx_frame_asm.disassemble_remaining_segments(
+        rx_segments_data.data(), rx_epilogue);
+  } catch (FrameError& e) {
+    ldout(cct, 1) << __func__ << " " << e.what() << dendl;
+    return _fault();
+  } catch (ceph::crypto::onwire::MsgAuthError&) {
+    ldout(cct, 1) << __func__ << "bad auth tag" << dendl;
+    return _fault();
+  }
+
+  // we do have a mechanism that allows transmitter to start sending message
+  // and abort after putting entire data field on wire. This will be used by
+  // the kernel client to avoid unnecessary buffering.
+  if (aborted) {
+    reset_throttle();
+    state = READY;
+    return CONTINUE(read_frame);
+  }
+  return handle_read_frame_dispatch();
+}
+
+CtPtr ProtocolV2::handle_message() {
+  ldout(cct, 20) << __func__ << dendl;
+  ceph_assert(state == THROTTLE_DONE);
+
+#if defined(WITH_EVENTTRACE)
+  utime_t ltt_recv_stamp = ceph_clock_now();
+#endif
+  recv_stamp = ceph_clock_now();
+
+  const size_t cur_msg_size = get_current_msg_size();
+  auto msg_frame = MessageFrame::Decode(rx_segments_data);
+
+  // XXX: paranoid copy just to avoid oops
+  ceph_msg_header2 current_header = msg_frame.header();
+
+  ldout(cct, 5) << __func__
+		<< " got " << msg_frame.front_len()
+		<< " + " << msg_frame.middle_len()
+		<< " + " << msg_frame.data_len()
+		<< " byte message."
+		<< " envelope type=" << current_header.type
+		<< " src " << peer_name
+		<< " off " << current_header.data_off
+                << dendl;
+
+  INTERCEPT(16);
+  ceph_msg_header header{current_header.seq,
+                         current_header.tid,
+                         current_header.type,
+                         current_header.priority,
+                         current_header.version,
+                         init_le32(msg_frame.front_len()),
+                         init_le32(msg_frame.middle_len()),
+                         init_le32(msg_frame.data_len()),
+                         current_header.data_off,
+                         peer_name,
+                         current_header.compat_version,
+                         current_header.reserved,
+                         init_le32(0)};
+  ceph_msg_footer footer{init_le32(0), init_le32(0),
+	                 init_le32(0), init_le64(0), current_header.flags};
+
+  Message *message = decode_message(cct, 0, header, footer,
+      msg_frame.front(),
+      msg_frame.middle(),
+      msg_frame.data(),
+      connection);
+  if (!message) {
+    ldout(cct, 1) << __func__ << " decode message failed " << dendl;
+    return _fault();
+  } else {
+    state = READ_MESSAGE_COMPLETE;
+  }
+
+  INTERCEPT(17);
+
+  message->set_byte_throttler(connection->policy.throttler_bytes);
+  message->set_message_throttler(connection->policy.throttler_messages);
+
+  // store reservation size in message, so we don't get confused
+  // by messages entering the dispatch queue through other paths.
+  message->set_dispatch_throttle_size(cur_msg_size);
+
+  message->set_recv_stamp(recv_stamp);
+  message->set_throttle_stamp(throttle_stamp);
+  message->set_recv_complete_stamp(ceph_clock_now());
+
+  // check received seq#.  if it is old, drop the message.
+  // note that incoming messages may skip ahead.  this is convenient for the
+  // client side queueing because messages can't be renumbered, but the (kernel)
+  // client will occasionally pull a message out of the sent queue to send
+  // elsewhere.  in that case it doesn't matter if we "got" it or not.
+  uint64_t cur_seq = in_seq;
+  if (message->get_seq() <= cur_seq) {
+    ldout(cct, 0) << __func__ << " got old message " << message->get_seq()
+                  << " <= " << cur_seq << " " << message << " " << *message
+                  << ", discarding" << dendl;
+    message->put();
+    if (connection->has_feature(CEPH_FEATURE_RECONNECT_SEQ) &&
+        cct->_conf->ms_die_on_old_message) {
+      ceph_assert(0 == "old msgs despite reconnect_seq feature");
+    }
+    return nullptr;
+  }
+  if (message->get_seq() > cur_seq + 1) {
+    ldout(cct, 0) << __func__ << " missed message?  skipped from seq "
+                  << cur_seq << " to " << message->get_seq() << dendl;
+    if (cct->_conf->ms_die_on_skipped_message) {
+      ceph_assert(0 == "skipped incoming seq");
+    }
+  }
+
+#if defined(WITH_EVENTTRACE)
+  if (message->get_type() == CEPH_MSG_OSD_OP ||
+      message->get_type() == CEPH_MSG_OSD_OPREPLY) {
+    utime_t ltt_processed_stamp = ceph_clock_now();
+    double usecs_elapsed =
+        (ltt_processed_stamp.to_nsec() - ltt_recv_stamp.to_nsec()) / 1000;
+    ostringstream buf;
+    if (message->get_type() == CEPH_MSG_OSD_OP)
+      OID_ELAPSED_WITH_MSG(message, usecs_elapsed, "TIME_TO_DECODE_OSD_OP",
+                           false);
+    else
+      OID_ELAPSED_WITH_MSG(message, usecs_elapsed, "TIME_TO_DECODE_OSD_OPREPLY",
+                           false);
+  }
+#endif
+
+  // note last received message.
+  in_seq = message->get_seq();
+  ldout(cct, 5) << __func__ << " received message m=" << message
+                << " seq=" << message->get_seq()
+                << " from=" << message->get_source() << " type=" << header.type
+                << " " << *message << dendl;
+
+  bool need_dispatch_writer = false;
+  if (!connection->policy.lossy) {
+    ack_left++;
+    need_dispatch_writer = true;
+  }
+
+  state = READY;
+
+  ceph::mono_time fast_dispatch_time;
+
+  if (connection->is_blackhole()) {
+    ldout(cct, 10) << __func__ << " blackhole " << *message << dendl;
+    message->put();
+    goto out;
+  }
+
+  connection->logger->inc(l_msgr_recv_messages);
+  connection->logger->inc(l_msgr_recv_bytes,
+                          rx_frame_asm.get_frame_onwire_len());
+
+  messenger->ms_fast_preprocess(message);
+  fast_dispatch_time = ceph::mono_clock::now();
+  connection->logger->tinc(l_msgr_running_recv_time,
+			   fast_dispatch_time - connection->recv_start_time);
+  if (connection->delay_state) {
+    double delay_period = 0;
+    if (rand() % 10000 < cct->_conf->ms_inject_delay_probability * 10000.0) {
+      delay_period =
+          cct->_conf->ms_inject_delay_max * (double)(rand() % 10000) / 10000.0;
+      ldout(cct, 1) << "queue_received will delay after "
+                    << (ceph_clock_now() + delay_period) << " on " << message
+                    << " " << *message << dendl;
+    }
+    connection->delay_state->queue(delay_period, message);
+  } else if (messenger->ms_can_fast_dispatch(message)) {
+    connection->lock.unlock();
+    connection->dispatch_queue->fast_dispatch(message);
+    connection->recv_start_time = ceph::mono_clock::now();
+    connection->logger->tinc(l_msgr_running_fast_dispatch_time,
+                             connection->recv_start_time - fast_dispatch_time);
+    connection->lock.lock();
+    // we might have been reused by another connection
+    // let's check if that is the case
+    if (state != READY) {
+      // yes, that was the case, let's do nothing
+      return nullptr;
+    }
+  } else {
+    connection->dispatch_queue->enqueue(message, message->get_priority(),
+                                        connection->conn_id);
+  }
+
+  handle_message_ack(current_header.ack_seq);
+
+ out:
+  if (need_dispatch_writer && connection->is_connected()) {
+    connection->center->dispatch_event_external(connection->write_handler);
+  }
+
+  return CONTINUE(read_frame);
+}
+
+
+CtPtr ProtocolV2::throttle_message() {
+  ldout(cct, 20) << __func__ << dendl;
+
+  if (connection->policy.throttler_messages) {
+    ldout(cct, 10) << __func__ << " wants " << 1
+                   << " message from policy throttler "
+                   << connection->policy.throttler_messages->get_current()
+                   << "/" << connection->policy.throttler_messages->get_max()
+                   << dendl;
+    if (!connection->policy.throttler_messages->get_or_fail()) {
+      ldout(cct, 10) << __func__ << " wants 1 message from policy throttle "
+                     << connection->policy.throttler_messages->get_current()
+                     << "/" << connection->policy.throttler_messages->get_max()
+                     << " failed, just wait." << dendl;
+      // following thread pool deal with th full message queue isn't a
+      // short time, so we can wait a ms.
+      if (connection->register_time_events.empty()) {
+        connection->register_time_events.insert(
+            connection->center->create_time_event(1000,
+                                                  connection->wakeup_handler));
+      }
+      return nullptr;
+    }
+  }
+
+  state = THROTTLE_BYTES;
+  return CONTINUE(throttle_bytes);
+}
+
+CtPtr ProtocolV2::throttle_bytes() {
+  ldout(cct, 20) << __func__ << dendl;
+
+  const size_t cur_msg_size = get_current_msg_size();
+  if (cur_msg_size) {
+    if (connection->policy.throttler_bytes) {
+      ldout(cct, 10) << __func__ << " wants " << cur_msg_size
+                     << " bytes from policy throttler "
+                     << connection->policy.throttler_bytes->get_current() << "/"
+                     << connection->policy.throttler_bytes->get_max() << dendl;
+      if (!connection->policy.throttler_bytes->get_or_fail(cur_msg_size)) {
+        ldout(cct, 10) << __func__ << " wants " << cur_msg_size
+                       << " bytes from policy throttler "
+                       << connection->policy.throttler_bytes->get_current()
+                       << "/" << connection->policy.throttler_bytes->get_max()
+                       << " failed, just wait." << dendl;
+        // following thread pool deal with th full message queue isn't a
+        // short time, so we can wait a ms.
+        if (connection->register_time_events.empty()) {
+          connection->register_time_events.insert(
+              connection->center->create_time_event(
+                  1000, connection->wakeup_handler));
+        }
+        return nullptr;
+      }
+    }
+  }
+
+  state = THROTTLE_DISPATCH_QUEUE;
+  return CONTINUE(throttle_dispatch_queue);
+}
+
+CtPtr ProtocolV2::throttle_dispatch_queue() {
+  ldout(cct, 20) << __func__ << dendl;
+
+  const size_t cur_msg_size = get_current_msg_size();
+  if (cur_msg_size) {
+    if (!connection->dispatch_queue->dispatch_throttler.get_or_fail(
+            cur_msg_size)) {
+      ldout(cct, 10)
+          << __func__ << " wants " << cur_msg_size
+          << " bytes from dispatch throttle "
+          << connection->dispatch_queue->dispatch_throttler.get_current() << "/"
+          << connection->dispatch_queue->dispatch_throttler.get_max()
+          << " failed, just wait." << dendl;
+      // following thread pool deal with th full message queue isn't a
+      // short time, so we can wait a ms.
+      if (connection->register_time_events.empty()) {
+        connection->register_time_events.insert(
+            connection->center->create_time_event(1000,
+                                                  connection->wakeup_handler));
+      }
+      return nullptr;
+    }
+  }
+
+  throttle_stamp = ceph_clock_now();
+  state = THROTTLE_DONE;
+
+  return read_frame_segment();
+}
+
+CtPtr ProtocolV2::handle_keepalive2(ceph::bufferlist &payload)
+{
+  ldout(cct, 20) << __func__
+		 << " payload.length()=" << payload.length() << dendl;
+
+  if (state != READY) {
+    lderr(cct) << __func__ << " not in ready state!" << dendl;
+    return _fault();
+  }
+
+  auto keepalive_frame = KeepAliveFrame::Decode(payload);
+
+  ldout(cct, 30) << __func__ << " got KEEPALIVE2 tag ..." << dendl;
+
+  connection->write_lock.lock();
+  auto keepalive_ack_frame = KeepAliveFrameAck::Encode(keepalive_frame.timestamp());
+  if (!append_frame(keepalive_ack_frame)) {
+    connection->write_lock.unlock();
+    return _fault();
+  }
+  connection->write_lock.unlock();
+
+  ldout(cct, 20) << __func__ << " got KEEPALIVE2 "
+                 << keepalive_frame.timestamp() << dendl;
+  connection->set_last_keepalive(ceph_clock_now());
+
+  if (is_connected()) {
+    connection->center->dispatch_event_external(connection->write_handler);
+  }
+
+  return CONTINUE(read_frame);
+}
+
+CtPtr ProtocolV2::handle_keepalive2_ack(ceph::bufferlist &payload)
+{
+  ldout(cct, 20) << __func__
+		 << " payload.length()=" << payload.length() << dendl;
+
+  if (state != READY) {
+    lderr(cct) << __func__ << " not in ready state!" << dendl;
+    return _fault();
+  }
+
+  auto keepalive_ack_frame = KeepAliveFrameAck::Decode(payload);
+  connection->set_last_keepalive_ack(keepalive_ack_frame.timestamp());
+  ldout(cct, 20) << __func__ << " got KEEPALIVE_ACK" << dendl;
+
+  return CONTINUE(read_frame);
+}
+
+CtPtr ProtocolV2::handle_message_ack(ceph::bufferlist &payload)
+{
+  ldout(cct, 20) << __func__
+		 << " payload.length()=" << payload.length() << dendl;
+
+  if (state != READY) {
+    lderr(cct) << __func__ << " not in ready state!" << dendl;
+    return _fault();
+  }
+
+  auto ack = AckFrame::Decode(payload);
+  handle_message_ack(ack.seq());
+  return CONTINUE(read_frame);
+}
+
+/* Client Protocol Methods */
+
+CtPtr ProtocolV2::start_client_banner_exchange() {
+  ldout(cct, 20) << __func__ << dendl;
+
+  INTERCEPT(1);
+
+  state = BANNER_CONNECTING;
+
+  global_seq = messenger->get_global_seq();
+
+  return _banner_exchange(CONTINUATION(post_client_banner_exchange));
+}
+
+CtPtr ProtocolV2::post_client_banner_exchange() {
+  ldout(cct, 20) << __func__ << dendl;
+
+  state = AUTH_CONNECTING;
+
+  return send_auth_request();
+}
+
+CtPtr ProtocolV2::send_auth_request(std::vector<uint32_t> &allowed_methods) {
+  ceph_assert(messenger->auth_client);
+  ldout(cct, 20) << __func__ << " peer_type " << (int)connection->peer_type
+		 << " auth_client " << messenger->auth_client << dendl;
+
+  bufferlist bl;
+  vector<uint32_t> preferred_modes;
+  auto am = auth_meta;
+  connection->lock.unlock();
+  int r = messenger->auth_client->get_auth_request(
+    connection, am.get(),
+    &am->auth_method, &preferred_modes, &bl);
+  connection->lock.lock();
+  if (state != AUTH_CONNECTING) {
+    ldout(cct, 1) << __func__ << " state changed!" << dendl;
+    return _fault();
+  }
+  if (r < 0) {
+    ldout(cct, 0) << __func__ << " get_initial_auth_request returned " << r
+		  << dendl;
+    stop();
+    connection->dispatch_queue->queue_reset(connection);
+    return nullptr;
+  }
+
+  INTERCEPT(9);
+
+  auto frame = AuthRequestFrame::Encode(auth_meta->auth_method, preferred_modes,
+                                        bl);
+  return WRITE(frame, "auth request", read_frame);
+}
+
+CtPtr ProtocolV2::handle_auth_bad_method(ceph::bufferlist &payload) {
+  ldout(cct, 20) << __func__
+		 << " payload.length()=" << payload.length() << dendl;
+
+  if (state != AUTH_CONNECTING) {
+    lderr(cct) << __func__ << " not in auth connect state!" << dendl;
+    return _fault();
+  }
+
+  auto bad_method = AuthBadMethodFrame::Decode(payload);
+  ldout(cct, 1) << __func__ << " method=" << bad_method.method()
+		<< " result " << cpp_strerror(bad_method.result())
+                << ", allowed methods=" << bad_method.allowed_methods()
+		<< ", allowed modes=" << bad_method.allowed_modes()
+                << dendl;
+  ceph_assert(messenger->auth_client);
+  auto am = auth_meta;
+  connection->lock.unlock();
+  int r = messenger->auth_client->handle_auth_bad_method(
+    connection,
+    am.get(),
+    bad_method.method(), bad_method.result(),
+    bad_method.allowed_methods(),
+    bad_method.allowed_modes());
+  connection->lock.lock();
+  if (state != AUTH_CONNECTING || r < 0) {
+    return _fault();
+  }
+  return send_auth_request(bad_method.allowed_methods());
+}
+
+CtPtr ProtocolV2::handle_auth_reply_more(ceph::bufferlist &payload)
+{
+  ldout(cct, 20) << __func__
+		 << " payload.length()=" << payload.length() << dendl;
+
+  if (state != AUTH_CONNECTING) {
+    lderr(cct) << __func__ << " not in auth connect state!" << dendl;
+    return _fault();
+  }
+
+  auto auth_more = AuthReplyMoreFrame::Decode(payload);
+  ldout(cct, 5) << __func__
+                << " auth reply more len=" << auth_more.auth_payload().length()
+                << dendl;
+  ceph_assert(messenger->auth_client);
+  ceph::bufferlist reply;
+  auto am = auth_meta;
+  connection->lock.unlock();
+  int r = messenger->auth_client->handle_auth_reply_more(
+    connection, am.get(), auth_more.auth_payload(), &reply);
+  connection->lock.lock();
+  if (state != AUTH_CONNECTING) {
+    ldout(cct, 1) << __func__ << " state changed!" << dendl;
+    return _fault();
+  }
+  if (r < 0) {
+    lderr(cct) << __func__ << " auth_client handle_auth_reply_more returned "
+	       << r << dendl;
+    return _fault();
+  }
+  auto more_reply = AuthRequestMoreFrame::Encode(reply);
+  return WRITE(more_reply, "auth request more", read_frame);
+}
+
+CtPtr ProtocolV2::handle_auth_done(ceph::bufferlist &payload)
+{
+  ldout(cct, 20) << __func__
+		 << " payload.length()=" << payload.length() << dendl;
+
+  if (state != AUTH_CONNECTING) {
+    lderr(cct) << __func__ << " not in auth connect state!" << dendl;
+    return _fault();
+  }
+
+  auto auth_done = AuthDoneFrame::Decode(payload);
+
+  ceph_assert(messenger->auth_client);
+  auto am = auth_meta;
+  connection->lock.unlock();
+  int r = messenger->auth_client->handle_auth_done(
+    connection,
+    am.get(),
+    auth_done.global_id(),
+    auth_done.con_mode(),
+    auth_done.auth_payload(),
+    &am->session_key,
+    &am->connection_secret);
+  connection->lock.lock();
+  if (state != AUTH_CONNECTING) {
+    ldout(cct, 1) << __func__ << " state changed!" << dendl;
+    return _fault();
+  }
+  if (r < 0) {
+    return _fault();
+  }
+  auth_meta->con_mode = auth_done.con_mode();
+  bool is_rev1 = HAVE_MSGR2_FEATURE(peer_supported_features, REVISION_1);
+  session_stream_handlers = ceph::crypto::onwire::rxtx_t::create_handler_pair(
+      cct, *auth_meta, /*new_nonce_format=*/is_rev1, /*crossed=*/false);
+
+  state = AUTH_CONNECTING_SIGN;
+
+  const auto sig = auth_meta->session_key.empty() ? sha256_digest_t() :
+    auth_meta->session_key.hmac_sha256(cct, pre_auth.rxbuf);
+  auto sig_frame = AuthSignatureFrame::Encode(sig);
+  pre_auth.enabled = false;
+  pre_auth.rxbuf.clear();
+  return WRITE(sig_frame, "auth signature", read_frame);
+}
+
+CtPtr ProtocolV2::finish_client_auth() {
+  if (!server_cookie) {
+    ceph_assert(connect_seq == 0);
+    state = SESSION_CONNECTING;
+    return send_client_ident();
+  } else {  // reconnecting to previous session
+    state = SESSION_RECONNECTING;
+    ceph_assert(connect_seq > 0);
+    return send_reconnect();
+  }
+}
+
+CtPtr ProtocolV2::send_client_ident() {
+  ldout(cct, 20) << __func__ << dendl;
+
+  if (!connection->policy.lossy && !client_cookie) {
+    client_cookie = ceph::util::generate_random_number<uint64_t>(1, -1ll);
+  }
+
+  uint64_t flags = 0;
+  if (connection->policy.lossy) {
+    flags |= CEPH_MSG_CONNECT_LOSSY;
+  }
+
+  auto client_ident = ClientIdentFrame::Encode(
+      messenger->get_myaddrs(),
+      connection->target_addr,
+      messenger->get_myname().num(),
+      global_seq,
+      connection->policy.features_supported,
+      connection->policy.features_required | msgr2_required,
+      flags,
+      client_cookie);
+
+  ldout(cct, 5) << __func__ << " sending identification: "
+                << "addrs=" << messenger->get_myaddrs()
+                << " target=" << connection->target_addr
+                << " gid=" << messenger->get_myname().num()
+                << " global_seq=" << global_seq
+                << " features_supported=" << std::hex
+                << connection->policy.features_supported
+                << " features_required="
+		            << (connection->policy.features_required | msgr2_required)
+                << " flags=" << flags
+                << " cookie=" << client_cookie << std::dec << dendl;
+
+  INTERCEPT(11);
+
+  return WRITE(client_ident, "client ident", read_frame);
+}
+
+CtPtr ProtocolV2::send_reconnect() {
+  ldout(cct, 20) << __func__ << dendl;
+
+  auto reconnect = ReconnectFrame::Encode(messenger->get_myaddrs(),
+                                          client_cookie,
+                                          server_cookie,
+                                          global_seq,
+                                          connect_seq,
+                                          in_seq);
+
+  ldout(cct, 5) << __func__ << " reconnect to session: client_cookie="
+                << std::hex << client_cookie << " server_cookie="
+                << server_cookie << std::dec
+                << " gs=" << global_seq << " cs=" << connect_seq
+                << " ms=" << in_seq << dendl;
+
+  INTERCEPT(13);
+
+  return WRITE(reconnect, "reconnect", read_frame);
+}
+
+CtPtr ProtocolV2::handle_ident_missing_features(ceph::bufferlist &payload)
+{
+  ldout(cct, 20) << __func__
+		 << " payload.length()=" << payload.length() << dendl;
+
+  if (state != SESSION_CONNECTING) {
+    lderr(cct) << __func__ << " not in session connect state!" << dendl;
+    return _fault();
+  }
+
+  auto ident_missing =
+      IdentMissingFeaturesFrame::Decode(payload);
+  lderr(cct) << __func__
+             << " client does not support all server features: " << std::hex
+             << ident_missing.features() << std::dec << dendl;
+
+  return _fault();
+}
+
+CtPtr ProtocolV2::handle_session_reset(ceph::bufferlist &payload)
+{
+  ldout(cct, 20) << __func__
+		 << " payload.length()=" << payload.length() << dendl;
+
+  if (state != SESSION_RECONNECTING) {
+    lderr(cct) << __func__ << " not in session reconnect state!" << dendl;
+    return _fault();
+  }
+
+  auto reset = ResetFrame::Decode(payload);
+
+  ldout(cct, 1) << __func__ << " received session reset full=" << reset.full()
+                << dendl;
+  if (reset.full()) {
+    reset_session();
+  } else {
+    server_cookie = 0;
+    connect_seq = 0;
+    in_seq = 0;
+  }
+
+  state = SESSION_CONNECTING;
+  return send_client_ident();
+}
+
+CtPtr ProtocolV2::handle_session_retry(ceph::bufferlist &payload)
+{
+  ldout(cct, 20) << __func__
+		 << " payload.length()=" << payload.length() << dendl;
+
+  if (state != SESSION_RECONNECTING) {
+    lderr(cct) << __func__ << " not in session reconnect state!" << dendl;
+    return _fault();
+  }
+
+  auto retry = RetryFrame::Decode(payload);
+  connect_seq = retry.connect_seq() + 1;
+
+  ldout(cct, 1) << __func__
+                << " received session retry connect_seq=" << retry.connect_seq()
+                << ", inc to cs=" << connect_seq << dendl;
+
+  return send_reconnect();
+}
+
+CtPtr ProtocolV2::handle_session_retry_global(ceph::bufferlist &payload)
+{
+  ldout(cct, 20) << __func__
+		 << " payload.length()=" << payload.length() << dendl;
+
+  if (state != SESSION_RECONNECTING) {
+    lderr(cct) << __func__ << " not in session reconnect state!" << dendl;
+    return _fault();
+  }
+
+  auto retry = RetryGlobalFrame::Decode(payload);
+  global_seq = messenger->get_global_seq(retry.global_seq());
+
+  ldout(cct, 1) << __func__ << " received session retry global global_seq="
+                << retry.global_seq() << ", choose new gs=" << global_seq
+                << dendl;
+
+  return send_reconnect();
+}
+
+CtPtr ProtocolV2::handle_wait(ceph::bufferlist &payload) {
+  ldout(cct, 20) << __func__
+		 << " received WAIT (connection race)"
+		 << " payload.length()=" << payload.length()
+		 << dendl;
+
+  if (state != SESSION_CONNECTING && state != SESSION_RECONNECTING) {
+    lderr(cct) << __func__ << " not in session (re)connect state!" << dendl;
+    return _fault();
+  }
+
+  state = WAIT;
+  WaitFrame::Decode(payload);
+  return _fault();
+}
+
+CtPtr ProtocolV2::handle_reconnect_ok(ceph::bufferlist &payload)
+{
+  ldout(cct, 20) << __func__
+		 << " payload.length()=" << payload.length() << dendl;
+
+  if (state != SESSION_RECONNECTING) {
+    lderr(cct) << __func__ << " not in session reconnect state!" << dendl;
+    return _fault();
+  }
+
+  auto reconnect_ok = ReconnectOkFrame::Decode(payload);
+  ldout(cct, 5) << __func__
+                << " reconnect accepted: sms=" << reconnect_ok.msg_seq()
+                << dendl;
+
+  out_seq = discard_requeued_up_to(out_seq, reconnect_ok.msg_seq());
+
+  backoff = utime_t();
+  ldout(cct, 10) << __func__ << " reconnect success " << connect_seq
+                 << ", lossy = " << connection->policy.lossy << ", features "
+                 << connection->get_features() << dendl;
+
+  if (connection->delay_state) {
+    ceph_assert(connection->delay_state->ready());
+  }
+
+  connection->dispatch_queue->queue_connect(connection);
+  messenger->ms_deliver_handle_fast_connect(connection);
+
+  return ready();
+}
+
+CtPtr ProtocolV2::handle_server_ident(ceph::bufferlist &payload)
+{
+  ldout(cct, 20) << __func__
+		 << " payload.length()=" << payload.length() << dendl;
+
+  if (state != SESSION_CONNECTING) {
+    lderr(cct) << __func__ << " not in session connect state!" << dendl;
+    return _fault();
+  }
+
+  auto server_ident = ServerIdentFrame::Decode(payload);
+  ldout(cct, 5) << __func__ << " received server identification:"
+                << " addrs=" << server_ident.addrs()
+                << " gid=" << server_ident.gid()
+                << " global_seq=" << server_ident.global_seq()
+                << " features_supported=" << std::hex
+                << server_ident.supported_features()
+                << " features_required=" << server_ident.required_features()
+                << " flags=" << server_ident.flags()
+                << " cookie=" << server_ident.cookie() << std::dec << dendl;
+
+  // is this who we intended to talk to?
+  // be a bit forgiving here, since we may be connecting based on addresses parsed out
+  // of mon_host or something.
+  if (!server_ident.addrs().contains(connection->target_addr)) {
+    ldout(cct,1) << __func__ << " peer identifies as " << server_ident.addrs()
+		 << ", does not include " << connection->target_addr << dendl;
+    return _fault();
+  }
+
+  server_cookie = server_ident.cookie();
+
+  connection->set_peer_addrs(server_ident.addrs());
+  peer_name = entity_name_t(connection->get_peer_type(), server_ident.gid());
+  connection->set_features(server_ident.supported_features() &
+                           connection->policy.features_supported);
+  peer_global_seq = server_ident.global_seq();
+
+  connection->policy.lossy = server_ident.flags() & CEPH_MSG_CONNECT_LOSSY;
+
+  backoff = utime_t();
+  ldout(cct, 10) << __func__ << " connect success " << connect_seq
+                 << ", lossy = " << connection->policy.lossy << ", features "
+                 << connection->get_features() << dendl;
+
+  if (connection->delay_state) {
+    ceph_assert(connection->delay_state->ready());
+  }
+
+  connection->dispatch_queue->queue_connect(connection);
+  messenger->ms_deliver_handle_fast_connect(connection);
+
+  return ready();
+}
+
+/* Server Protocol Methods */
+
+CtPtr ProtocolV2::start_server_banner_exchange() {
+  ldout(cct, 20) << __func__ << dendl;
+
+  INTERCEPT(2);
+
+  state = BANNER_ACCEPTING;
+
+  return _banner_exchange(CONTINUATION(post_server_banner_exchange));
+}
+
+CtPtr ProtocolV2::post_server_banner_exchange() {
+  ldout(cct, 20) << __func__ << dendl;
+
+  state = AUTH_ACCEPTING;
+
+  return CONTINUE(read_frame);
+}
+
+CtPtr ProtocolV2::handle_auth_request(ceph::bufferlist &payload) {
+  ldout(cct, 20) << __func__ << " payload.length()=" << payload.length()
+                 << dendl;
+
+  if (state != AUTH_ACCEPTING) {
+    lderr(cct) << __func__ << " not in auth accept state!" << dendl;
+    return _fault();
+  }
+
+  auto request = AuthRequestFrame::Decode(payload);
+  ldout(cct, 10) << __func__ << " AuthRequest(method=" << request.method()
+		 << ", preferred_modes=" << request.preferred_modes()
+                 << ", payload_len=" << request.auth_payload().length() << ")"
+                 << dendl;
+  auth_meta->auth_method = request.method();
+  auth_meta->con_mode = messenger->auth_server->pick_con_mode(
+    connection->get_peer_type(), auth_meta->auth_method,
+    request.preferred_modes());
+  if (auth_meta->con_mode == CEPH_CON_MODE_UNKNOWN) {
+    return _auth_bad_method(-EOPNOTSUPP);
+  }
+  return _handle_auth_request(request.auth_payload(), false);
+}
+
+CtPtr ProtocolV2::_auth_bad_method(int r)
+{
+  ceph_assert(r < 0);
+  std::vector<uint32_t> allowed_methods;
+  std::vector<uint32_t> allowed_modes;
+  messenger->auth_server->get_supported_auth_methods(
+    connection->get_peer_type(), &allowed_methods, &allowed_modes);
+  ldout(cct, 1) << __func__ << " auth_method " << auth_meta->auth_method
+		<< " r " << cpp_strerror(r)
+		<< ", allowed_methods " << allowed_methods
+		<< ", allowed_modes " << allowed_modes
+		<< dendl;
+  auto bad_method = AuthBadMethodFrame::Encode(auth_meta->auth_method, r,
+                                               allowed_methods, allowed_modes);
+  return WRITE(bad_method, "bad auth method", read_frame);
+}
+
+CtPtr ProtocolV2::_handle_auth_request(bufferlist& auth_payload, bool more)
+{
+  if (!messenger->auth_server) {
+    return _fault();
+  }
+  bufferlist reply;
+  auto am = auth_meta;
+  connection->lock.unlock();
+  int r = messenger->auth_server->handle_auth_request(
+    connection, am.get(),
+    more, am->auth_method, auth_payload,
+    &reply);
+  connection->lock.lock();
+  if (state != AUTH_ACCEPTING && state != AUTH_ACCEPTING_MORE) {
+    ldout(cct, 1) << __func__
+                  << " state changed while accept, it must be mark_down"
+                  << dendl;
+    ceph_assert(state == CLOSED);
+    return _fault();
+  }
+  if (r == 1) {
+    INTERCEPT(10);
+    state = AUTH_ACCEPTING_SIGN;
+
+    auto auth_done = AuthDoneFrame::Encode(connection->peer_global_id,
+                                           auth_meta->con_mode,
+                                           reply);
+    return WRITE(auth_done, "auth done", finish_auth);
+  } else if (r == 0) {
+    state = AUTH_ACCEPTING_MORE;
+
+    auto more = AuthReplyMoreFrame::Encode(reply);
+    return WRITE(more, "auth reply more", read_frame);
+  } else if (r == -EBUSY) {
+    // kick the client and maybe they'll come back later
+    return _fault();
+  } else {
+    return _auth_bad_method(r);
+  }
+}
+
+CtPtr ProtocolV2::finish_auth()
+{
+  ceph_assert(auth_meta);
+  // TODO: having a possibility to check whether we're server or client could
+  // allow reusing finish_auth().
+  bool is_rev1 = HAVE_MSGR2_FEATURE(peer_supported_features, REVISION_1);
+  session_stream_handlers = ceph::crypto::onwire::rxtx_t::create_handler_pair(
+      cct, *auth_meta, /*new_nonce_format=*/is_rev1, /*crossed=*/true);
+
+  const auto sig = auth_meta->session_key.empty() ? sha256_digest_t() :
+    auth_meta->session_key.hmac_sha256(cct, pre_auth.rxbuf);
+  auto sig_frame = AuthSignatureFrame::Encode(sig);
+  pre_auth.enabled = false;
+  pre_auth.rxbuf.clear();
+  return WRITE(sig_frame, "auth signature", read_frame);
+}
+
+CtPtr ProtocolV2::handle_auth_request_more(ceph::bufferlist &payload)
+{
+  ldout(cct, 20) << __func__
+		 << " payload.length()=" << payload.length() << dendl;
+
+  if (state != AUTH_ACCEPTING_MORE) {
+    lderr(cct) << __func__ << " not in auth accept more state!" << dendl;
+    return _fault();
+  }
+
+  auto auth_more = AuthRequestMoreFrame::Decode(payload);
+  return _handle_auth_request(auth_more.auth_payload(), true);
+}
+
+CtPtr ProtocolV2::handle_auth_signature(ceph::bufferlist &payload)
+{
+  ldout(cct, 20) << __func__
+		 << " payload.length()=" << payload.length() << dendl;
+
+  if (state != AUTH_ACCEPTING_SIGN && state != AUTH_CONNECTING_SIGN) {
+    lderr(cct) << __func__
+               << " pre-auth verification signature seen in wrong state!"
+               << dendl;
+    return _fault();
+  }
+
+  auto sig_frame = AuthSignatureFrame::Decode(payload);
+
+  const auto actual_tx_sig = auth_meta->session_key.empty() ?
+    sha256_digest_t() : auth_meta->session_key.hmac_sha256(cct, pre_auth.txbuf);
+  if (sig_frame.signature() != actual_tx_sig) {
+    ldout(cct, 2) << __func__ << " pre-auth signature mismatch"
+                  << " actual_tx_sig=" << actual_tx_sig
+                  << " sig_frame.signature()=" << sig_frame.signature()
+                  << dendl;
+    return _fault();
+  } else {
+    ldout(cct, 20) << __func__ << " pre-auth signature success"
+                   << " sig_frame.signature()=" << sig_frame.signature()
+                   << dendl;
+    pre_auth.txbuf.clear();
+  }
+
+  if (state == AUTH_ACCEPTING_SIGN) {
+    // server had sent AuthDone and client responded with correct pre-auth
+    // signature. we can start accepting new sessions/reconnects.
+    state = SESSION_ACCEPTING;
+    return CONTINUE(read_frame);
+  } else if (state == AUTH_CONNECTING_SIGN) {
+    // this happened at client side
+    return finish_client_auth();
+  } else {
+    ceph_abort("state corruption");
+  }
+}
+
+CtPtr ProtocolV2::handle_client_ident(ceph::bufferlist &payload)
+{
+  ldout(cct, 20) << __func__
+		 << " payload.length()=" << payload.length() << dendl;
+
+  if (state != SESSION_ACCEPTING) {
+    lderr(cct) << __func__ << " not in session accept state!" << dendl;
+    return _fault();
+  }
+
+  auto client_ident = ClientIdentFrame::Decode(payload);
+
+  ldout(cct, 5) << __func__ << " received client identification:"
+                << " addrs=" << client_ident.addrs()
+		            << " target=" << client_ident.target_addr()
+                << " gid=" << client_ident.gid()
+                << " global_seq=" << client_ident.global_seq()
+                << " features_supported=" << std::hex
+                << client_ident.supported_features()
+                << " features_required=" << client_ident.required_features()
+                << " flags=" << client_ident.flags()
+                << " cookie=" << client_ident.cookie() << std::dec << dendl;
+
+  if (client_ident.addrs().empty() ||
+      client_ident.addrs().front() == entity_addr_t()) {
+    ldout(cct,5) << __func__ << " oops, client_ident.addrs() is empty" << dendl;
+    return _fault();  // a v2 peer should never do this
+  }
+  if (!messenger->get_myaddrs().contains(client_ident.target_addr())) {
+    ldout(cct,5) << __func__ << " peer is trying to reach "
+		 << client_ident.target_addr()
+		 << " which is not us (" << messenger->get_myaddrs() << ")"
+		 << dendl;
+    return _fault();
+  }
+
+  connection->set_peer_addrs(client_ident.addrs());
+  connection->target_addr = connection->_infer_target_addr(client_ident.addrs());
+
+  peer_name = entity_name_t(connection->get_peer_type(), client_ident.gid());
+  connection->set_peer_id(client_ident.gid());
+
+  client_cookie = client_ident.cookie();
+
+  uint64_t feat_missing =
+    (connection->policy.features_required | msgr2_required) &
+    ~(uint64_t)client_ident.supported_features();
+  if (feat_missing) {
+    ldout(cct, 1) << __func__ << " peer missing required features " << std::hex
+                  << feat_missing << std::dec << dendl;
+    auto ident_missing_features =
+        IdentMissingFeaturesFrame::Encode(feat_missing);
+
+    return WRITE(ident_missing_features, "ident missing features", read_frame);
+  }
+
+  connection_features =
+      client_ident.supported_features() & connection->policy.features_supported;
+
+  peer_global_seq = client_ident.global_seq();
+
+  if (connection->policy.server &&
+      connection->policy.lossy &&
+      !connection->policy.register_lossy_clients) {
+    // incoming lossy client, no need to register this connection
+  } else {
+    // Looks good so far, let's check if there is already an existing connection
+    // to this peer.
+    connection->lock.unlock();
+    AsyncConnectionRef existing = messenger->lookup_conn(
+      *connection->peer_addrs);
+
+    if (existing &&
+	existing->protocol->proto_type != 2) {
+      ldout(cct,1) << __func__ << " existing " << existing << " proto "
+		   << existing->protocol.get() << " version is "
+		   << existing->protocol->proto_type << ", marking down"
+		   << dendl;
+      existing->mark_down();
+      existing = nullptr;
+    }
+
+    connection->inject_delay();
+
+    connection->lock.lock();
+    if (state != SESSION_ACCEPTING) {
+      ldout(cct, 1) << __func__
+		    << " state changed while accept, it must be mark_down"
+		    << dendl;
+      ceph_assert(state == CLOSED);
+      return _fault();
+    }
+
+    if (existing) {
+      return handle_existing_connection(existing);
+    }
+  }
+
+  // if everything is OK reply with server identification
+  return send_server_ident();
+}
+
+CtPtr ProtocolV2::handle_reconnect(ceph::bufferlist &payload)
+{
+  ldout(cct, 20) << __func__
+		 << " payload.length()=" << payload.length() << dendl;
+
+  if (state != SESSION_ACCEPTING) {
+    lderr(cct) << __func__ << " not in session accept state!" << dendl;
+    return _fault();
+  }
+
+  auto reconnect = ReconnectFrame::Decode(payload);
+
+  ldout(cct, 5) << __func__
+                << " received reconnect:" 
+                << " client_cookie=" << std::hex << reconnect.client_cookie()
+                << " server_cookie=" << reconnect.server_cookie() << std::dec
+                << " gs=" << reconnect.global_seq()
+                << " cs=" << reconnect.connect_seq()
+                << " ms=" << reconnect.msg_seq()
+		            << dendl;
+
+  // Should we check if one of the ident.addrs match connection->target_addr
+  // as we do in ProtocolV1?
+  connection->set_peer_addrs(reconnect.addrs());
+  connection->target_addr = connection->_infer_target_addr(reconnect.addrs());
+  peer_global_seq = reconnect.global_seq();
+
+  connection->lock.unlock();
+  AsyncConnectionRef existing = messenger->lookup_conn(*connection->peer_addrs);
+
+  if (existing &&
+      existing->protocol->proto_type != 2) {
+    ldout(cct,1) << __func__ << " existing " << existing << " proto "
+		 << existing->protocol.get() << " version is "
+		 << existing->protocol->proto_type << ", marking down" << dendl;
+    existing->mark_down();
+    existing = nullptr;
+  }
+
+  connection->inject_delay();
+
+  connection->lock.lock();
+  if (state != SESSION_ACCEPTING) {
+    ldout(cct, 1) << __func__
+                  << " state changed while accept, it must be mark_down"
+                  << dendl;
+    ceph_assert(state == CLOSED);
+    return _fault();
+  }
+
+  if (!existing) {
+    // there is no existing connection therefore cannot reconnect to previous
+    // session
+    ldout(cct, 0) << __func__
+                  << " no existing connection exists, reseting client" << dendl;
+    auto reset = ResetFrame::Encode(true);
+    return WRITE(reset, "session reset", read_frame);
+  }
+
+  std::lock_guard<std::mutex> l(existing->lock);
+
+  ProtocolV2 *exproto = dynamic_cast<ProtocolV2 *>(existing->protocol.get());
+  if (!exproto) {
+    ldout(cct, 1) << __func__ << " existing=" << existing << dendl;
+    ceph_assert(false);
+  }
+
+  if (exproto->state == CLOSED) {
+    ldout(cct, 5) << __func__ << " existing " << existing
+                  << " already closed. Reseting client" << dendl;
+    auto reset = ResetFrame::Encode(true);
+    return WRITE(reset, "session reset", read_frame);
+  }
+
+  if (exproto->replacing) {
+    ldout(cct, 1) << __func__
+                  << " existing racing replace happened while replacing."
+                  << " existing=" << existing << dendl;
+    auto retry = RetryGlobalFrame::Encode(exproto->peer_global_seq);
+    return WRITE(retry, "session retry", read_frame);
+  }
+
+  if (exproto->client_cookie != reconnect.client_cookie()) {
+    ldout(cct, 1) << __func__ << " existing=" << existing
+                  << " client cookie mismatch, I must have reseted:"
+                  << " cc=" << std::hex << exproto->client_cookie
+                  << " rcc=" << reconnect.client_cookie()
+                  << ", reseting client." << std::dec
+                  << dendl;
+    auto reset = ResetFrame::Encode(connection->policy.resetcheck);
+    return WRITE(reset, "session reset", read_frame);
+  } else if (exproto->server_cookie == 0) {
+    // this happens when:
+    //   - a connects to b
+    //   - a sends client_ident
+    //   - b gets client_ident, sends server_ident and sets cookie X
+    //   - connection fault
+    //   - b reconnects to a with cookie X, connect_seq=1
+    //   - a has cookie==0
+    ldout(cct, 1) << __func__ << " I was a client and didn't received the"
+                  << " server_ident. Asking peer to resume session"
+                  << " establishment" << dendl;
+    auto reset = ResetFrame::Encode(false);
+    return WRITE(reset, "session reset", read_frame);
+  }
+
+  if (exproto->peer_global_seq > reconnect.global_seq()) {
+    ldout(cct, 5) << __func__
+                  << " stale global_seq: sgs=" << exproto->peer_global_seq
+                  << " cgs=" << reconnect.global_seq()
+                  << ", ask client to retry global" << dendl;
+    auto retry = RetryGlobalFrame::Encode(exproto->peer_global_seq);
+
+    INTERCEPT(18);
+
+    return WRITE(retry, "session retry", read_frame);
+  }
+
+  if (exproto->connect_seq > reconnect.connect_seq()) {
+    ldout(cct, 5) << __func__
+                  << " stale connect_seq scs=" << exproto->connect_seq
+                  << " ccs=" << reconnect.connect_seq()
+                  << " , ask client to retry" << dendl;
+    auto retry = RetryFrame::Encode(exproto->connect_seq);
+    return WRITE(retry, "session retry", read_frame);
+  }
+
+  if (exproto->connect_seq == reconnect.connect_seq()) {
+    // reconnect race: both peers are sending reconnect messages
+    if (existing->peer_addrs->msgr2_addr() >
+            messenger->get_myaddrs().msgr2_addr() &&
+        !existing->policy.server) {
+      // the existing connection wins
+      ldout(cct, 1)
+          << __func__
+          << " reconnect race detected, this connection loses to existing="
+          << existing << dendl;
+
+      auto wait = WaitFrame::Encode();
+      return WRITE(wait, "wait", read_frame);
+    } else {
+      // this connection wins
+      ldout(cct, 1) << __func__
+                    << " reconnect race detected, replacing existing="
+                    << existing << " socket by this connection's socket"
+                    << dendl;
+    }
+  }
+
+  ldout(cct, 1) << __func__ << " reconnect to existing=" << existing << dendl;
+
+  reconnecting = true;
+
+  // everything looks good
+  exproto->connect_seq = reconnect.connect_seq();
+  exproto->message_seq = reconnect.msg_seq();
+
+  return reuse_connection(existing, exproto);
+}
+
+CtPtr ProtocolV2::handle_existing_connection(const AsyncConnectionRef& existing) {
+  ldout(cct, 20) << __func__ << " existing=" << existing << dendl;
+
+  std::lock_guard<std::mutex> l(existing->lock);
+
+  ProtocolV2 *exproto = dynamic_cast<ProtocolV2 *>(existing->protocol.get());
+  if (!exproto) {
+    ldout(cct, 1) << __func__ << " existing=" << existing << dendl;
+    ceph_assert(false);
+  }
+
+  if (exproto->state == CLOSED) {
+    ldout(cct, 1) << __func__ << " existing " << existing << " already closed."
+                  << dendl;
+    return send_server_ident();
+  }
+
+  if (exproto->replacing) {
+    ldout(cct, 1) << __func__
+                  << " existing racing replace happened while replacing."
+                  << " existing=" << existing << dendl;
+    auto wait = WaitFrame::Encode();
+    return WRITE(wait, "wait", read_frame);
+  }
+
+  if (exproto->peer_global_seq > peer_global_seq) {
+    ldout(cct, 1) << __func__ << " this is a stale connection, peer_global_seq="
+                  << peer_global_seq
+                  << " existing->peer_global_seq=" << exproto->peer_global_seq
+                  << ", stopping this connection." << dendl;
+    stop();
+    connection->dispatch_queue->queue_reset(connection);
+    return nullptr;
+  }
+
+  if (existing->policy.lossy) {
+    // existing connection can be thrown out in favor of this one
+    ldout(cct, 1)
+        << __func__ << " existing=" << existing
+        << " is a lossy channel. Stopping existing in favor of this connection"
+        << dendl;
+    existing->protocol->stop();
+    existing->dispatch_queue->queue_reset(existing.get());
+    return send_server_ident();
+  }
+
+  if (exproto->server_cookie && exproto->client_cookie &&
+      exproto->client_cookie != client_cookie) {
+    // Found previous session
+    // peer has reseted and we're going to reuse the existing connection
+    // by replacing the communication socket
+    ldout(cct, 1) << __func__ << " found previous session existing=" << existing
+                  << ", peer must have reseted." << dendl;
+    if (connection->policy.resetcheck) {
+      exproto->reset_session();
+    }
+    return reuse_connection(existing, exproto);
+  }
+
+  if (exproto->client_cookie == client_cookie) {
+    // session establishment interrupted between client_ident and server_ident,
+    // continuing...
+    ldout(cct, 1) << __func__ << " found previous session existing=" << existing
+                  << ", continuing session establishment." << dendl;
+    return reuse_connection(existing, exproto);
+  }
+
+  if (exproto->state == READY || exproto->state == STANDBY) {
+    ldout(cct, 1) << __func__ << " existing=" << existing
+                  << " is READY/STANDBY, lets reuse it" << dendl;
+    return reuse_connection(existing, exproto);
+  }
+
+  // Looks like a connection race: server and client are both connecting to
+  // each other at the same time.
+  if (connection->peer_addrs->msgr2_addr() <
+          messenger->get_myaddrs().msgr2_addr() ||
+      existing->policy.server) {
+    // this connection wins
+    ldout(cct, 1) << __func__
+                  << " connection race detected, replacing existing="
+                  << existing << " socket by this connection's socket" << dendl;
+    return reuse_connection(existing, exproto);
+  } else {
+    // the existing connection wins
+    ldout(cct, 1)
+        << __func__
+        << " connection race detected, this connection loses to existing="
+        << existing << dendl;
+    ceph_assert(connection->peer_addrs->msgr2_addr() >
+                messenger->get_myaddrs().msgr2_addr());
+
+    // make sure we follow through with opening the existing
+    // connection (if it isn't yet open) since we know the peer
+    // has something to send to us.
+    existing->send_keepalive();
+    auto wait = WaitFrame::Encode();
+    return WRITE(wait, "wait", read_frame);
+  }
+}
+
+CtPtr ProtocolV2::reuse_connection(const AsyncConnectionRef& existing,
+                                   ProtocolV2 *exproto) {
+  ldout(cct, 20) << __func__ << " existing=" << existing
+                 << " reconnect=" << reconnecting << dendl;
+
+  connection->inject_delay();
+
+  std::lock_guard<std::mutex> l(existing->write_lock);
+
+  connection->center->delete_file_event(connection->cs.fd(),
+                                        EVENT_READABLE | EVENT_WRITABLE);
+
+  if (existing->delay_state) {
+    existing->delay_state->flush();
+    ceph_assert(!connection->delay_state);
+  }
+  exproto->reset_recv_state();
+  exproto->pre_auth.enabled = false;
+
+  if (!reconnecting) {
+    exproto->peer_supported_features = peer_supported_features;
+    exproto->tx_frame_asm.set_is_rev1(tx_frame_asm.get_is_rev1());
+    exproto->rx_frame_asm.set_is_rev1(rx_frame_asm.get_is_rev1());
+
+    exproto->client_cookie = client_cookie;
+    exproto->peer_name = peer_name;
+    exproto->connection_features = connection_features;
+    existing->set_features(connection_features);
+  }
+  exproto->peer_global_seq = peer_global_seq;
+
+  ceph_assert(connection->center->in_thread());
+  auto temp_cs = std::move(connection->cs);
+  EventCenter *new_center = connection->center;
+  Worker *new_worker = connection->worker;
+  // we can steal the session_stream_handlers under the assumption
+  // this happens in the event center's thread as there should be
+  // no user outside its boundaries (simlarly to e.g. outgoing_bl).
+  auto temp_stream_handlers = std::move(session_stream_handlers);
+  exproto->auth_meta = auth_meta;
+
+  ldout(messenger->cct, 5) << __func__ << " stop myself to swap existing"
+                           << dendl;
+
+  // avoid _stop shutdown replacing socket
+  // queue a reset on the new connection, which we're dumping for the old
+  stop();
+
+  connection->dispatch_queue->queue_reset(connection);
+
+  exproto->can_write = false;
+  exproto->write_in_progress = false;
+  exproto->reconnecting = reconnecting;
+  exproto->replacing = true;
+  existing->state_offset = 0;
+  // avoid previous thread modify event
+  exproto->state = NONE;
+  existing->state = AsyncConnection::STATE_NONE;
+  // Discard existing prefetch buffer in `recv_buf`
+  existing->recv_start = existing->recv_end = 0;
+  // there shouldn't exist any buffer
+  ceph_assert(connection->recv_start == connection->recv_end);
+
+  auto deactivate_existing = std::bind(
+      [ existing,
+        new_worker,
+        new_center,
+        exproto,
+        temp_stream_handlers=std::move(temp_stream_handlers)
+      ](ConnectedSocket &cs) mutable {
+        // we need to delete time event in original thread
+        {
+          std::lock_guard<std::mutex> l(existing->lock);
+          existing->write_lock.lock();
+          exproto->requeue_sent();
+          // XXX: do we really need the locking for `outgoing_bl`? There is
+          // a comment just above its definition saying "lockfree, only used
+          // in own thread". I'm following lockfull schema just in the case.
+          // From performance point of view it should be fine  this happens
+          // far away from hot paths.
+          existing->outgoing_bl.clear();
+          existing->open_write = false;
+          exproto->session_stream_handlers = std::move(temp_stream_handlers);
+          existing->write_lock.unlock();
+          if (exproto->state == NONE) {
+            existing->shutdown_socket();
+            existing->cs = std::move(cs);
+            existing->worker->references--;
+            new_worker->references++;
+            existing->logger = new_worker->get_perf_counter();
+            existing->worker = new_worker;
+            existing->center = new_center;
+            if (existing->delay_state)
+              existing->delay_state->set_center(new_center);
+          } else if (exproto->state == CLOSED) {
+            auto back_to_close = std::bind(
+                [](ConnectedSocket &cs) mutable { cs.close(); }, std::move(cs));
+            new_center->submit_to(new_center->get_id(),
+                                  std::move(back_to_close), true);
+            return;
+          } else {
+            ceph_abort();
+          }
+        }
+
+        // Before changing existing->center, it may already exists some
+        // events in existing->center's queue. Then if we mark down
+        // `existing`, it will execute in another thread and clean up
+        // connection. Previous event will result in segment fault
+        auto transfer_existing = [existing, exproto]() mutable {
+          std::lock_guard<std::mutex> l(existing->lock);
+          if (exproto->state == CLOSED) return;
+          ceph_assert(exproto->state == NONE);
+
+          exproto->state = SESSION_ACCEPTING;
+          // we have called shutdown_socket above
+          ceph_assert(existing->last_tick_id == 0);
+          // restart timer since we are going to re-build connection
+          existing->last_connect_started = ceph::coarse_mono_clock::now();
+          existing->last_tick_id = existing->center->create_time_event(
+            existing->connect_timeout_us, existing->tick_handler);
+          existing->state = AsyncConnection::STATE_CONNECTION_ESTABLISHED;
+          existing->center->create_file_event(existing->cs.fd(), EVENT_READABLE,
+                                              existing->read_handler);
+          if (!exproto->reconnecting) {
+            exproto->run_continuation(exproto->send_server_ident());
+          } else {
+            exproto->run_continuation(exproto->send_reconnect_ok());
+          }
+        };
+        if (existing->center->in_thread())
+          transfer_existing();
+        else
+          existing->center->submit_to(existing->center->get_id(),
+                                      std::move(transfer_existing), true);
+      },
+      std::move(temp_cs));
+
+  existing->center->submit_to(existing->center->get_id(),
+                              std::move(deactivate_existing), true);
+  return nullptr;
+}
+
+CtPtr ProtocolV2::send_server_ident() {
+  ldout(cct, 20) << __func__ << dendl;
+
+  // this is required for the case when this connection is being replaced
+  out_seq = discard_requeued_up_to(out_seq, 0);
+  in_seq = 0;
+
+  if (!connection->policy.lossy) {
+    server_cookie = ceph::util::generate_random_number<uint64_t>(1, -1ll);
+  }
+
+  uint64_t flags = 0;
+  if (connection->policy.lossy) {
+    flags = flags | CEPH_MSG_CONNECT_LOSSY;
+  }
+
+  uint64_t gs = messenger->get_global_seq();
+  auto server_ident = ServerIdentFrame::Encode(
+          messenger->get_myaddrs(),
+          messenger->get_myname().num(),
+          gs,
+          connection->policy.features_supported,
+          connection->policy.features_required | msgr2_required,
+          flags,
+          server_cookie);
+
+  ldout(cct, 5) << __func__ << " sending identification:"
+                << " addrs=" << messenger->get_myaddrs()
+                << " gid=" << messenger->get_myname().num()
+                << " global_seq=" << gs << " features_supported=" << std::hex
+                << connection->policy.features_supported
+                << " features_required="
+		            << (connection->policy.features_required | msgr2_required)
+                << " flags=" << flags
+                << " cookie=" << server_cookie << std::dec << dendl;
+
+  connection->lock.unlock();
+  // Because "replacing" will prevent other connections preempt this addr,
+  // it's safe that here we don't acquire Connection's lock
+  ssize_t r = messenger->accept_conn(connection);
+
+  connection->inject_delay();
+
+  connection->lock.lock();
+
+  if (r < 0) {
+    ldout(cct, 1) << __func__ << " existing race replacing process for addr = "
+                  << connection->peer_addrs->msgr2_addr()
+                  << " just fail later one(this)" << dendl;
+    connection->inject_delay();
+    return _fault();
+  }
+  if (state != SESSION_ACCEPTING) {
+    ldout(cct, 1) << __func__
+                  << " state changed while accept_conn, it must be mark_down"
+                  << dendl;
+    ceph_assert(state == CLOSED || state == NONE);
+    messenger->unregister_conn(connection);
+    connection->inject_delay();
+    return _fault();
+  }
+
+  connection->set_features(connection_features);
+
+  // notify
+  connection->dispatch_queue->queue_accept(connection);
+  messenger->ms_deliver_handle_fast_accept(connection);
+
+  INTERCEPT(12);
+
+  return WRITE(server_ident, "server ident", server_ready);
+}
+
+CtPtr ProtocolV2::server_ready() {
+  ldout(cct, 20) << __func__ << dendl;
+
+  if (connection->delay_state) {
+    ceph_assert(connection->delay_state->ready());
+  }
+
+  return ready();
+}
+
+CtPtr ProtocolV2::send_reconnect_ok() {
+  ldout(cct, 20) << __func__ << dendl;
+
+  out_seq = discard_requeued_up_to(out_seq, message_seq);
+
+  uint64_t ms = in_seq;
+  auto reconnect_ok = ReconnectOkFrame::Encode(ms);
+
+  ldout(cct, 5) << __func__ << " sending reconnect_ok: msg_seq=" << ms << dendl;
+
+  connection->lock.unlock();
+  // Because "replacing" will prevent other connections preempt this addr,
+  // it's safe that here we don't acquire Connection's lock
+  ssize_t r = messenger->accept_conn(connection);
+
+  connection->inject_delay();
+
+  connection->lock.lock();
+
+  if (r < 0) {
+    ldout(cct, 1) << __func__ << " existing race replacing process for addr = "
+                  << connection->peer_addrs->msgr2_addr()
+                  << " just fail later one(this)" << dendl;
+    connection->inject_delay();
+    return _fault();
+  }
+  if (state != SESSION_ACCEPTING) {
+    ldout(cct, 1) << __func__
+                  << " state changed while accept_conn, it must be mark_down"
+                  << dendl;
+    ceph_assert(state == CLOSED || state == NONE);
+    messenger->unregister_conn(connection);
+    connection->inject_delay();
+    return _fault();
+  }
+
+  // notify
+  connection->dispatch_queue->queue_accept(connection);
+  messenger->ms_deliver_handle_fast_accept(connection);
+
+  INTERCEPT(14);
+
+  return WRITE(reconnect_ok, "reconnect ok", server_ready);
+}
diff --git a/src/msg/async/ProtocolV2.h b/src/msg/async/ProtocolV2.h
new file mode 100644
index 00000000000..d9a49d0e9d9
--- /dev/null
+++ b/src/msg/async/ProtocolV2.h
@@ -0,0 +1,259 @@
+// -*- mode:C++; tab-width:8; c-basic-offset:2; indent-tabs-mode:t -*-
+// vim: ts=8 sw=2 smarttab
+
+#ifndef _MSG_ASYNC_PROTOCOL_V2_
+#define _MSG_ASYNC_PROTOCOL_V2_
+
+#include "Protocol.h"
+#include "crypto_onwire.h"
+#include "frames_v2.h"
+
+class ProtocolV2 : public Protocol {
+private:
+  enum State {
+    NONE,
+    START_CONNECT,
+    BANNER_CONNECTING,
+    HELLO_CONNECTING,
+    AUTH_CONNECTING,
+    AUTH_CONNECTING_SIGN,
+    SESSION_CONNECTING,
+    SESSION_RECONNECTING,
+    START_ACCEPT,
+    BANNER_ACCEPTING,
+    HELLO_ACCEPTING,
+    AUTH_ACCEPTING,
+    AUTH_ACCEPTING_MORE,
+    AUTH_ACCEPTING_SIGN,
+    SESSION_ACCEPTING,
+    READY,
+    THROTTLE_MESSAGE,
+    THROTTLE_BYTES,
+    THROTTLE_DISPATCH_QUEUE,
+    THROTTLE_DONE,
+    READ_MESSAGE_COMPLETE,
+    STANDBY,
+    WAIT,
+    CLOSED
+  };
+
+  static const char *get_state_name(int state) {
+    const char *const statenames[] = {"NONE",
+                                      "START_CONNECT",
+                                      "BANNER_CONNECTING",
+                                      "HELLO_CONNECTING",
+                                      "AUTH_CONNECTING",
+                                      "AUTH_CONNECTING_SIGN",
+                                      "SESSION_CONNECTING",
+                                      "SESSION_RECONNECTING",
+                                      "START_ACCEPT",
+                                      "BANNER_ACCEPTING",
+                                      "HELLO_ACCEPTING",
+                                      "AUTH_ACCEPTING",
+                                      "AUTH_ACCEPTING_MORE",
+                                      "AUTH_ACCEPTING_SIGN",
+                                      "SESSION_ACCEPTING",
+                                      "READY",
+                                      "THROTTLE_MESSAGE",
+                                      "THROTTLE_BYTES",
+                                      "THROTTLE_DISPATCH_QUEUE",
+                                      "THROTTLE_DONE",
+                                      "READ_MESSAGE_COMPLETE",
+                                      "STANDBY",
+                                      "WAIT",
+                                      "CLOSED"};
+    return statenames[state];
+  }
+
+  // TODO: move into auth_meta?
+  ceph::crypto::onwire::rxtx_t session_stream_handlers;
+
+  entity_name_t peer_name;
+  State state;
+  uint64_t peer_supported_features;  // CEPH_MSGR2_FEATURE_*
+
+  uint64_t client_cookie;
+  uint64_t server_cookie;
+  uint64_t global_seq;
+  uint64_t connect_seq;
+  uint64_t peer_global_seq;
+  uint64_t message_seq;
+  bool reconnecting;
+  bool replacing;
+  bool can_write;
+  struct out_queue_entry_t {
+    bool is_prepared {false};
+    Message* m {nullptr};
+  };
+  std::map<int, std::list<out_queue_entry_t>> out_queue;
+  std::list<Message *> sent;
+  std::atomic<uint64_t> out_seq{0};
+  std::atomic<uint64_t> in_seq{0};
+  std::atomic<uint64_t> ack_left{0};
+
+  using ProtFuncPtr = void (ProtocolV2::*)();
+  Ct<ProtocolV2> *bannerExchangeCallback;
+
+  ceph::msgr::v2::FrameAssembler tx_frame_asm;
+  ceph::msgr::v2::FrameAssembler rx_frame_asm;
+
+  ceph::bufferlist rx_preamble;
+  ceph::bufferlist rx_epilogue;
+  ceph::msgr::v2::segment_bls_t rx_segments_data;
+  ceph::msgr::v2::Tag next_tag;
+  utime_t backoff;  // backoff time
+  utime_t recv_stamp;
+  utime_t throttle_stamp;
+
+  struct {
+    ceph::bufferlist rxbuf;
+    ceph::bufferlist txbuf;
+    bool enabled {true};
+  } pre_auth;
+
+  bool keepalive;
+  bool write_in_progress = false;
+
+  ostream &_conn_prefix(std::ostream *_dout);
+  void run_continuation(Ct<ProtocolV2> *pcontinuation);
+  void run_continuation(Ct<ProtocolV2> &continuation);
+
+  Ct<ProtocolV2> *read(CONTINUATION_RXBPTR_TYPE<ProtocolV2> &next,
+                       rx_buffer_t&& buffer);
+  template <class F>
+  Ct<ProtocolV2> *write(const std::string &desc,
+                        CONTINUATION_TYPE<ProtocolV2> &next,
+			F &frame);
+  Ct<ProtocolV2> *write(const std::string &desc,
+                        CONTINUATION_TYPE<ProtocolV2> &next,
+                        bufferlist &buffer);
+
+  template <class F>
+  bool append_frame(F& frame);
+
+  void requeue_sent();
+  uint64_t discard_requeued_up_to(uint64_t out_seq, uint64_t seq);
+  void reset_recv_state();
+  void reset_security();
+  void reset_throttle();
+  Ct<ProtocolV2> *_fault();
+  void discard_out_queue();
+  void reset_session();
+  void prepare_send_message(uint64_t features, Message *m);
+  out_queue_entry_t _get_next_outgoing();
+  ssize_t write_message(Message *m, bool more);
+  void handle_message_ack(uint64_t seq);
+
+  CONTINUATION_DECL(ProtocolV2, _wait_for_peer_banner);
+  READ_BPTR_HANDLER_CONTINUATION_DECL(ProtocolV2, _handle_peer_banner);
+  READ_BPTR_HANDLER_CONTINUATION_DECL(ProtocolV2, _handle_peer_banner_payload);
+
+  Ct<ProtocolV2> *_banner_exchange(Ct<ProtocolV2> &callback);
+  Ct<ProtocolV2> *_wait_for_peer_banner();
+  Ct<ProtocolV2> *_handle_peer_banner(rx_buffer_t &&buffer, int r);
+  Ct<ProtocolV2> *_handle_peer_banner_payload(rx_buffer_t &&buffer, int r);
+  Ct<ProtocolV2> *handle_hello(ceph::bufferlist &payload);
+
+  CONTINUATION_DECL(ProtocolV2, read_frame);
+  CONTINUATION_DECL(ProtocolV2, finish_auth);
+  READ_BPTR_HANDLER_CONTINUATION_DECL(ProtocolV2, handle_read_frame_preamble_main);
+  READ_BPTR_HANDLER_CONTINUATION_DECL(ProtocolV2, handle_read_frame_segment);
+  READ_BPTR_HANDLER_CONTINUATION_DECL(ProtocolV2, handle_read_frame_epilogue_main);
+  CONTINUATION_DECL(ProtocolV2, throttle_message);
+  CONTINUATION_DECL(ProtocolV2, throttle_bytes);
+  CONTINUATION_DECL(ProtocolV2, throttle_dispatch_queue);
+
+  Ct<ProtocolV2> *read_frame();
+  Ct<ProtocolV2> *finish_auth();
+  Ct<ProtocolV2> *finish_client_auth();
+  Ct<ProtocolV2> *handle_read_frame_preamble_main(rx_buffer_t &&buffer, int r);
+  Ct<ProtocolV2> *read_frame_segment();
+  Ct<ProtocolV2> *handle_read_frame_segment(rx_buffer_t &&rx_buffer, int r);
+  Ct<ProtocolV2> *_handle_read_frame_segment();
+  Ct<ProtocolV2> *handle_read_frame_epilogue_main(rx_buffer_t &&buffer, int r);
+  Ct<ProtocolV2> *_handle_read_frame_epilogue_main();
+  Ct<ProtocolV2> *handle_read_frame_dispatch();
+  Ct<ProtocolV2> *handle_frame_payload();
+
+  Ct<ProtocolV2> *ready();
+
+  Ct<ProtocolV2> *handle_message();
+  Ct<ProtocolV2> *throttle_message();
+  Ct<ProtocolV2> *throttle_bytes();
+  Ct<ProtocolV2> *throttle_dispatch_queue();
+  Ct<ProtocolV2> *read_message_data_prepare();
+
+  Ct<ProtocolV2> *handle_keepalive2(ceph::bufferlist &payload);
+  Ct<ProtocolV2> *handle_keepalive2_ack(ceph::bufferlist &payload);
+
+  Ct<ProtocolV2> *handle_message_ack(ceph::bufferlist &payload);
+
+public:
+  uint64_t connection_features;
+
+  ProtocolV2(AsyncConnection *connection);
+  virtual ~ProtocolV2();
+
+  virtual void connect() override;
+  virtual void accept() override;
+  virtual bool is_connected() override;
+  virtual void stop() override;
+  virtual void fault() override;
+  virtual void send_message(Message *m) override;
+  virtual void send_keepalive() override;
+
+  virtual void read_event() override;
+  virtual void write_event() override;
+  virtual bool is_queued() override;
+
+private:
+  // Client Protocol
+  CONTINUATION_DECL(ProtocolV2, start_client_banner_exchange);
+  CONTINUATION_DECL(ProtocolV2, post_client_banner_exchange);
+
+  Ct<ProtocolV2> *start_client_banner_exchange();
+  Ct<ProtocolV2> *post_client_banner_exchange();
+  inline Ct<ProtocolV2> *send_auth_request() {
+    std::vector<uint32_t> empty;
+    return send_auth_request(empty);
+  }
+  Ct<ProtocolV2> *send_auth_request(std::vector<uint32_t> &allowed_methods);
+  Ct<ProtocolV2> *handle_auth_bad_method(ceph::bufferlist &payload);
+  Ct<ProtocolV2> *handle_auth_reply_more(ceph::bufferlist &payload);
+  Ct<ProtocolV2> *handle_auth_done(ceph::bufferlist &payload);
+  Ct<ProtocolV2> *handle_auth_signature(ceph::bufferlist &payload);
+  Ct<ProtocolV2> *send_client_ident();
+  Ct<ProtocolV2> *send_reconnect();
+  Ct<ProtocolV2> *handle_ident_missing_features(ceph::bufferlist &payload);
+  Ct<ProtocolV2> *handle_session_reset(ceph::bufferlist &payload);
+  Ct<ProtocolV2> *handle_session_retry(ceph::bufferlist &payload);
+  Ct<ProtocolV2> *handle_session_retry_global(ceph::bufferlist &payload);
+  Ct<ProtocolV2> *handle_wait(ceph::bufferlist &payload);
+  Ct<ProtocolV2> *handle_reconnect_ok(ceph::bufferlist &payload);
+  Ct<ProtocolV2> *handle_server_ident(ceph::bufferlist &payload);
+
+  // Server Protocol
+  CONTINUATION_DECL(ProtocolV2, start_server_banner_exchange);
+  CONTINUATION_DECL(ProtocolV2, post_server_banner_exchange);
+  CONTINUATION_DECL(ProtocolV2, server_ready);
+
+  Ct<ProtocolV2> *start_server_banner_exchange();
+  Ct<ProtocolV2> *post_server_banner_exchange();
+  Ct<ProtocolV2> *handle_auth_request(ceph::bufferlist &payload);
+  Ct<ProtocolV2> *handle_auth_request_more(ceph::bufferlist &payload);
+  Ct<ProtocolV2> *_handle_auth_request(bufferlist& auth_payload, bool more);
+  Ct<ProtocolV2> *_auth_bad_method(int r);
+  Ct<ProtocolV2> *handle_client_ident(ceph::bufferlist &payload);
+  Ct<ProtocolV2> *handle_ident_missing_features_write(int r);
+  Ct<ProtocolV2> *handle_reconnect(ceph::bufferlist &payload);
+  Ct<ProtocolV2> *handle_existing_connection(const AsyncConnectionRef& existing);
+  Ct<ProtocolV2> *reuse_connection(const AsyncConnectionRef& existing,
+                                   ProtocolV2 *exproto);
+  Ct<ProtocolV2> *send_server_ident();
+  Ct<ProtocolV2> *send_reconnect_ok();
+  Ct<ProtocolV2> *server_ready();
+
+  size_t get_current_msg_size() const;
+};
+
+#endif /* _MSG_ASYNC_PROTOCOL_V2_ */
diff --git a/src/msg/async/Stack.cc b/src/msg/async/Stack.cc
index 807db1136e3..eb5b628a79e 100644
--- a/src/msg/async/Stack.cc
+++ b/src/msg/async/Stack.cc
@@ -14,6 +14,8 @@
  *
  */
 
+#include <mutex>
+
 #include "include/compat.h"
 #include "common/Cond.h"
 #include "common/errno.h"
@@ -26,20 +28,20 @@
 #endif
 
 #include "common/dout.h"
-#include "include/assert.h"
+#include "include/ceph_assert.h"
 
 #define dout_subsys ceph_subsys_ms
 #undef dout_prefix
 #define dout_prefix *_dout << "stack "
 
-std::function<void ()> NetworkStack::add_thread(unsigned i)
+std::function<void ()> NetworkStack::add_thread(unsigned worker_id)
 {
-  Worker *w = workers[i];
+  Worker *w = workers[worker_id];
   return [this, w]() {
       char tp_name[16];
-      sprintf(tp_name, "msgr-worker-%d", w->id);
+      sprintf(tp_name, "msgr-worker-%u", w->id);
       ceph_pthread_setname(pthread_self(), tp_name);
-      const uint64_t EventMaxWaitUs = 30000000;
+      const unsigned EventMaxWaitUs = 30000000;
       w->center.set_owner();
       ldout(cct, 10) << __func__ << " starting" << dendl;
       w->initialize();
@@ -80,17 +82,17 @@ std::shared_ptr<NetworkStack> NetworkStack::create(CephContext *c, const string
   return nullptr;
 }
 
-Worker* NetworkStack::create_worker(CephContext *c, const string &type, unsigned i)
+Worker* NetworkStack::create_worker(CephContext *c, const string &type, unsigned worker_id)
 {
   if (type == "posix")
-    return new PosixWorker(c, i);
+    return new PosixWorker(c, worker_id);
 #ifdef HAVE_RDMA
   else if (type == "rdma")
-    return new RDMAWorker(c, i);
+    return new RDMAWorker(c, worker_id);
 #endif
 #ifdef HAVE_DPDK
   else if (type == "dpdk")
-    return new DPDKWorker(c, i);
+    return new DPDKWorker(c, worker_id);
 #endif
 
   lderr(c) << __func__ << " ms_async_transport_type " << type <<
@@ -101,9 +103,9 @@ Worker* NetworkStack::create_worker(CephContext *c, const string &type, unsigned
 
 NetworkStack::NetworkStack(CephContext *c, const string &t): type(t), started(false), cct(c)
 {
-  assert(cct->_conf->ms_async_op_threads > 0);
+  ceph_assert(cct->_conf->ms_async_op_threads > 0);
 
-  const uint64_t InitEventNumber = 5000;
+  const int InitEventNumber = 5000;
   num_workers = cct->_conf->ms_async_op_threads;
   if (num_workers >= EventCenter::MAX_EVENTCENTER) {
     ldout(cct, 0) << __func__ << " max thread limit is "
@@ -113,19 +115,20 @@ NetworkStack::NetworkStack(CephContext *c, const string &t): type(t), started(fa
     num_workers = EventCenter::MAX_EVENTCENTER;
   }
 
-  for (unsigned i = 0; i < num_workers; ++i) {
-    Worker *w = create_worker(cct, type, i);
-    w->center.init(InitEventNumber, i, type);
+  for (unsigned worker_id = 0; worker_id < num_workers; ++worker_id) {
+    Worker *w = create_worker(cct, type, worker_id);
+    int ret = w->center.init(InitEventNumber, worker_id, type);
+    if (ret)
+      throw std::system_error(-ret, std::generic_category());
     workers.push_back(w);
   }
-  cct->register_fork_watcher(this);
 }
 
 void NetworkStack::start()
 {
-  pool_spin.lock();
+  std::unique_lock<decltype(pool_spin)> lk(pool_spin);
+
   if (started) {
-    pool_spin.unlock();
     return ;
   }
 
@@ -136,7 +139,7 @@ void NetworkStack::start()
     spawn_worker(i, std::move(thread));
   }
   started = true;
-  pool_spin.unlock();
+  lk.unlock();
 
   for (unsigned i = 0; i < num_workers; ++i)
     workers[i]->wait_for_init();
@@ -163,14 +166,14 @@ Worker* NetworkStack::get_worker()
   }
 
   pool_spin.unlock();
-  assert(current_best);
+  ceph_assert(current_best);
   ++current_best->references;
   return current_best;
 }
 
 void NetworkStack::stop()
 {
-  Spinlock::Locker l(pool_spin);
+  std::lock_guard lk(pool_spin);
   for (unsigned i = 0; i < num_workers; ++i) {
     workers[i]->done = true;
     workers[i]->center.wakeup();
@@ -180,23 +183,21 @@ void NetworkStack::stop()
 }
 
 class C_drain : public EventCallback {
-  Mutex drain_lock;
-  Cond drain_cond;
+  ceph::mutex drain_lock = ceph::make_mutex("C_drain::drain_lock");
+  ceph::condition_variable drain_cond;
   unsigned drain_count;
 
  public:
   explicit C_drain(size_t c)
-      : drain_lock("C_drain::drain_lock"),
-        drain_count(c) {}
-  void do_request(int id) override {
-    Mutex::Locker l(drain_lock);
+      : drain_count(c) {}
+  void do_request(uint64_t id) override {
+    std::lock_guard l{drain_lock};
     drain_count--;
-    if (drain_count == 0) drain_cond.Signal();
+    if (drain_count == 0) drain_cond.notify_all();
   }
   void wait() {
-    Mutex::Locker l(drain_lock);
-    while (drain_count)
-      drain_cond.Wait(drain_lock);
+    std::unique_lock l{drain_lock};
+    drain_cond.wait(l, [this] { return drain_count == 0; });
   }
 };
 
@@ -207,7 +208,7 @@ void NetworkStack::drain()
   pool_spin.lock();
   C_drain drain(num_workers);
   for (unsigned i = 0; i < num_workers; ++i) {
-    assert(cur != workers[i]->center.get_owner());
+    ceph_assert(cur != workers[i]->center.get_owner());
     workers[i]->center.dispatch_event_external(EventCallbackRef(&drain));
   }
   pool_spin.unlock();
diff --git a/src/msg/async/Stack.h b/src/msg/async/Stack.h
index a201406016e..37a33163d6b 100644
--- a/src/msg/async/Stack.h
+++ b/src/msg/async/Stack.h
@@ -17,9 +17,8 @@
 #ifndef CEPH_MSG_ASYNC_STACK_H
 #define CEPH_MSG_ASYNC_STACK_H
 
-#include "include/Spinlock.h"
+#include "include/spinlock.h"
 #include "common/perf_counters.h"
-#include "common/simple_spin.h"
 #include "msg/msg_types.h"
 #include "msg/async/Event.h"
 
@@ -29,7 +28,6 @@ class ConnectedSocketImpl {
   virtual ~ConnectedSocketImpl() {}
   virtual int is_connected() = 0;
   virtual ssize_t read(char*, size_t) = 0;
-  virtual ssize_t zero_copy_read(bufferptr&) = 0;
   virtual ssize_t send(bufferlist &bl, bool more) = 0;
   virtual void shutdown() = 0;
   virtual void close() = 0;
@@ -48,6 +46,10 @@ struct SocketOptions {
 /// \cond internal
 class ServerSocketImpl {
  public:
+  unsigned addr_type; ///< entity_addr_t::TYPE_*
+  unsigned addr_slot; ///< position of our addr in myaddrs().v
+  ServerSocketImpl(unsigned type, unsigned slot)
+    : addr_type(type), addr_slot(slot) {}
   virtual ~ServerSocketImpl() {}
   virtual int accept(ConnectedSocket *sock, const SocketOptions &opt, entity_addr_t *out, Worker *w) = 0;
   virtual void abort_accept() = 0;
@@ -91,12 +93,6 @@ class ConnectedSocket {
   ssize_t read(char* buf, size_t len) {
     return _csi->read(buf, len);
   }
-  /// Gets the input stream.
-  ///
-  /// Gets an object returning data sent from the remote endpoint.
-  ssize_t zero_copy_read(bufferptr &data) {
-    return _csi->zero_copy_read(data);
-  }
   /// Gets the output stream.
   ///
   /// Gets an object that sends data to the remote endpoint.
@@ -177,6 +173,11 @@ class ServerSocket {
     return _ssi->fd();
   }
 
+  /// get listen/bind addr
+  unsigned get_addr_slot() {
+    return _ssi->addr_slot;
+  }
+
   explicit operator bool() const {
     return _ssi.get();
   }
@@ -199,6 +200,9 @@ enum {
   l_msgr_running_recv_time,
   l_msgr_running_fast_dispatch_time,
 
+  l_msgr_send_messages_queue_lat,
+  l_msgr_handle_ack_lat,
+
   l_msgr_last,
 };
 
@@ -220,8 +224,8 @@ class Worker {
   Worker(const Worker&) = delete;
   Worker& operator=(const Worker&) = delete;
 
-  Worker(CephContext *c, unsigned i)
-    : cct(c), perf_logger(NULL), id(i), references(0), center(c) {
+  Worker(CephContext *c, unsigned worker_id)
+    : cct(c), perf_logger(NULL), id(worker_id), references(0), center(c) {
     char name[128];
     sprintf(name, "AsyncMessenger::Worker-%u", id);
     // initialize perf_logger
@@ -229,8 +233,8 @@ class Worker {
 
     plb.add_u64_counter(l_msgr_recv_messages, "msgr_recv_messages", "Network received messages");
     plb.add_u64_counter(l_msgr_send_messages, "msgr_send_messages", "Network sent messages");
-    plb.add_u64_counter(l_msgr_recv_bytes, "msgr_recv_bytes", "Network received bytes");
-    plb.add_u64_counter(l_msgr_send_bytes, "msgr_send_bytes", "Network sent bytes");
+    plb.add_u64_counter(l_msgr_recv_bytes, "msgr_recv_bytes", "Network received bytes", NULL, 0, unit_t(UNIT_BYTES));
+    plb.add_u64_counter(l_msgr_send_bytes, "msgr_send_bytes", "Network sent bytes", NULL, 0, unit_t(UNIT_BYTES));
     plb.add_u64_counter(l_msgr_active_connections, "msgr_active_connections", "Active connection number");
     plb.add_u64_counter(l_msgr_created_connections, "msgr_created_connections", "Created connection number");
 
@@ -239,6 +243,9 @@ class Worker {
     plb.add_time(l_msgr_running_recv_time, "msgr_running_recv_time", "The total time of message receiving");
     plb.add_time(l_msgr_running_fast_dispatch_time, "msgr_running_fast_dispatch_time", "The total time of fast dispatch");
 
+    plb.add_time_avg(l_msgr_send_messages_queue_lat, "msgr_send_messages_queue_lat", "Network sent messages lat");
+    plb.add_time_avg(l_msgr_handle_ack_lat, "msgr_handle_ack_lat", "Connection handle ack lat");
+
     perf_logger = plb.create_perf_counters();
     cct->get_perfcounters_collection()->add(perf_logger);
   }
@@ -249,7 +256,7 @@ class Worker {
     }
   }
 
-  virtual int listen(entity_addr_t &addr,
+  virtual int listen(entity_addr_t &addr, unsigned addr_slot,
                      const SocketOptions &opts, ServerSocket *) = 0;
   virtual int connect(const entity_addr_t &addr,
                       const SocketOptions &opts, ConnectedSocket *socket) = 0;
@@ -259,7 +266,7 @@ class Worker {
   PerfCounters *get_perf_counter() { return perf_logger; }
   void release_worker() {
     int oldref = references.fetch_sub(1);
-    assert(oldref > 0);
+    ceph_assert(oldref > 0);
   }
   void init_done() {
     init_lock.lock();
@@ -285,10 +292,10 @@ class Worker {
   }
 };
 
-class NetworkStack : public CephContext::ForkWatcher {
+class NetworkStack {
   std::string type;
   unsigned num_workers = 0;
-  Spinlock pool_spin;
+  ceph::spinlock pool_spin;
   bool started = false;
 
   std::function<void ()> add_thread(unsigned i);
@@ -301,7 +308,7 @@ class NetworkStack : public CephContext::ForkWatcher {
  public:
   NetworkStack(const NetworkStack &) = delete;
   NetworkStack& operator=(const NetworkStack &) = delete;
-  ~NetworkStack() override {
+  virtual ~NetworkStack() {
     for (auto &&w : workers)
       delete w;
   }
@@ -311,8 +318,6 @@ class NetworkStack : public CephContext::ForkWatcher {
 
   static Worker* create_worker(
           CephContext *c, const string &t, unsigned i);
-  // backend need to override this method if supports zero copy read
-  virtual bool support_zero_copy_read() const { return false; }
   // backend need to override this method if backend doesn't support shared
   // listen table.
   // For example, posix backend has in kernel global listen table. If one
@@ -325,8 +330,8 @@ class NetworkStack : public CephContext::ForkWatcher {
   void start();
   void stop();
   virtual Worker *get_worker();
-  Worker *get_worker(unsigned i) {
-    return workers[i];
+  Worker *get_worker(unsigned worker_id) {
+    return workers[worker_id];
   }
   void drain();
   unsigned get_num_worker() const {
@@ -337,14 +342,6 @@ class NetworkStack : public CephContext::ForkWatcher {
   virtual void spawn_worker(unsigned i, std::function<void ()> &&) = 0;
   virtual void join_worker(unsigned i) = 0;
 
-  void handle_pre_fork() override {
-    stop();
-  }
-
-  void handle_post_fork() override {
-    start();
-  }
-
   virtual bool is_ready() { return true; };
   virtual void ready() { };
 };
diff --git a/src/msg/async/crypto_onwire.cc b/src/msg/async/crypto_onwire.cc
new file mode 100644
index 00000000000..4e42340603e
--- /dev/null
+++ b/src/msg/async/crypto_onwire.cc
@@ -0,0 +1,311 @@
+// -*- mode:C++; tab-width:8; c-basic-offset:2; indent-tabs-mode:t -*-
+// vim: ts=8 sw=2 smarttab
+
+#include <array>
+#include <openssl/evp.h>
+
+#include "crypto_onwire.h"
+
+#include "common/debug.h"
+#include "common/ceph_crypto.h"
+#include "include/types.h"
+
+#define dout_subsys ceph_subsys_ms
+
+namespace ceph::crypto::onwire {
+
+static constexpr const std::size_t AESGCM_KEY_LEN{16};
+static constexpr const std::size_t AESGCM_IV_LEN{12};
+static constexpr const std::size_t AESGCM_TAG_LEN{16};
+static constexpr const std::size_t AESGCM_BLOCK_LEN{16};
+
+struct nonce_t {
+  ceph_le32 fixed;
+  ceph_le64 counter;
+
+  bool operator==(const nonce_t& rhs) const {
+    return !memcmp(this, &rhs, sizeof(*this));
+  }
+} __attribute__((packed));
+static_assert(sizeof(nonce_t) == AESGCM_IV_LEN);
+
+using key_t = std::array<std::uint8_t, AESGCM_KEY_LEN>;
+
+// http://www.mindspring.com/~dmcgrew/gcm-nist-6.pdf
+// https://www.openssl.org/docs/man1.0.2/crypto/EVP_aes_128_gcm.html#GCM-mode
+// https://wiki.openssl.org/index.php/EVP_Authenticated_Encryption_and_Decryption
+// https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-38d.pdf
+class AES128GCM_OnWireTxHandler : public ceph::crypto::onwire::TxHandler {
+  CephContext* const cct;
+  std::unique_ptr<EVP_CIPHER_CTX, decltype(&::EVP_CIPHER_CTX_free)> ectx;
+  ceph::bufferlist buffer;
+  nonce_t nonce, initial_nonce;
+  bool used_initial_nonce;
+  bool new_nonce_format;  // 64-bit counter?
+  static_assert(sizeof(nonce) == AESGCM_IV_LEN);
+
+public:
+  AES128GCM_OnWireTxHandler(CephContext* const cct,
+			    const key_t& key,
+			    const nonce_t& nonce,
+			    bool new_nonce_format)
+    : cct(cct),
+      ectx(EVP_CIPHER_CTX_new(), EVP_CIPHER_CTX_free),
+      nonce(nonce), initial_nonce(nonce), used_initial_nonce(false),
+      new_nonce_format(new_nonce_format) {
+    ceph_assert_always(ectx);
+    ceph_assert_always(key.size() * CHAR_BIT == 128);
+
+    if (1 != EVP_EncryptInit_ex(ectx.get(), EVP_aes_128_gcm(),
+			        nullptr, nullptr, nullptr)) {
+      throw std::runtime_error("EVP_EncryptInit_ex failed");
+    }
+
+    if(1 != EVP_EncryptInit_ex(ectx.get(), nullptr, nullptr,
+			       key.data(), nullptr)) {
+      throw std::runtime_error("EVP_EncryptInit_ex failed");
+    }
+  }
+
+  ~AES128GCM_OnWireTxHandler() override {
+    ::ceph::crypto::zeroize_for_security(&nonce, sizeof(nonce));
+    ::ceph::crypto::zeroize_for_security(&initial_nonce, sizeof(initial_nonce));
+  }
+
+  void reset_tx_handler(const uint32_t* first, const uint32_t* last) override;
+
+  void authenticated_encrypt_update(const ceph::bufferlist& plaintext) override;
+  ceph::bufferlist authenticated_encrypt_final() override;
+};
+
+void AES128GCM_OnWireTxHandler::reset_tx_handler(const uint32_t* first,
+                                                 const uint32_t* last)
+{
+  if (nonce == initial_nonce) {
+    if (used_initial_nonce) {
+      throw ceph::crypto::onwire::TxHandlerError("out of nonces");
+    }
+    used_initial_nonce = true;
+  }
+
+  if(1 != EVP_EncryptInit_ex(ectx.get(), nullptr, nullptr, nullptr,
+      reinterpret_cast<const unsigned char*>(&nonce))) {
+    throw std::runtime_error("EVP_EncryptInit_ex failed");
+  }
+
+  ceph_assert(buffer.get_append_buffer_unused_tail_length() == 0);
+  buffer.reserve(std::accumulate(first, last, AESGCM_TAG_LEN));
+
+  if (!new_nonce_format) {
+    // msgr2.0: 32-bit counter followed by 64-bit fixed field,
+    // susceptible to overflow!
+    nonce.fixed = nonce.fixed + 1;
+  } else {
+    nonce.counter = nonce.counter + 1;
+  }
+}
+
+void AES128GCM_OnWireTxHandler::authenticated_encrypt_update(
+  const ceph::bufferlist& plaintext)
+{
+  ceph_assert(buffer.get_append_buffer_unused_tail_length() >=
+              plaintext.length());
+  auto filler = buffer.append_hole(plaintext.length());
+
+  for (const auto& plainbuf : plaintext.buffers()) {
+    int update_len = 0;
+
+    if(1 != EVP_EncryptUpdate(ectx.get(),
+	reinterpret_cast<unsigned char*>(filler.c_str()),
+	&update_len,
+	reinterpret_cast<const unsigned char*>(plainbuf.c_str()),
+	plainbuf.length())) {
+      throw std::runtime_error("EVP_EncryptUpdate failed");
+    }
+    ceph_assert_always(update_len >= 0);
+    ceph_assert(static_cast<unsigned>(update_len) == plainbuf.length());
+    filler.advance(update_len);
+  }
+
+  ldout(cct, 15) << __func__
+		 << " plaintext.length()=" << plaintext.length()
+		 << " buffer.length()=" << buffer.length()
+		 << dendl;
+}
+
+ceph::bufferlist AES128GCM_OnWireTxHandler::authenticated_encrypt_final()
+{
+  int final_len = 0;
+  ceph_assert(buffer.get_append_buffer_unused_tail_length() ==
+              AESGCM_BLOCK_LEN);
+  auto filler = buffer.append_hole(AESGCM_BLOCK_LEN);
+  if(1 != EVP_EncryptFinal_ex(ectx.get(),
+	reinterpret_cast<unsigned char*>(filler.c_str()),
+	&final_len)) {
+    throw std::runtime_error("EVP_EncryptFinal_ex failed");
+  }
+  ceph_assert_always(final_len == 0);
+
+  static_assert(AESGCM_BLOCK_LEN == AESGCM_TAG_LEN);
+  if(1 != EVP_CIPHER_CTX_ctrl(ectx.get(),
+	EVP_CTRL_GCM_GET_TAG, AESGCM_TAG_LEN,
+	filler.c_str())) {
+    throw std::runtime_error("EVP_CIPHER_CTX_ctrl failed");
+  }
+
+  ldout(cct, 15) << __func__
+		 << " buffer.length()=" << buffer.length()
+		 << " final_len=" << final_len
+		 << dendl;
+  return std::move(buffer);
+}
+
+// RX PART
+class AES128GCM_OnWireRxHandler : public ceph::crypto::onwire::RxHandler {
+  CephContext* const cct;
+  std::unique_ptr<EVP_CIPHER_CTX, decltype(&::EVP_CIPHER_CTX_free)> ectx;
+  nonce_t nonce;
+  bool new_nonce_format;  // 64-bit counter?
+  static_assert(sizeof(nonce) == AESGCM_IV_LEN);
+
+public:
+  AES128GCM_OnWireRxHandler(CephContext* const cct,
+			    const key_t& key,
+			    const nonce_t& nonce,
+			    bool new_nonce_format)
+    : cct(cct),
+      ectx(EVP_CIPHER_CTX_new(), EVP_CIPHER_CTX_free),
+      nonce(nonce), new_nonce_format(new_nonce_format) {
+    ceph_assert_always(ectx);
+    ceph_assert_always(key.size() * CHAR_BIT == 128);
+
+    if (1 != EVP_DecryptInit_ex(ectx.get(), EVP_aes_128_gcm(),
+			        nullptr, nullptr, nullptr)) {
+      throw std::runtime_error("EVP_DecryptInit_ex failed");
+    }
+
+    if(1 != EVP_DecryptInit_ex(ectx.get(), nullptr, nullptr,
+			       key.data(), nullptr)) {
+      throw std::runtime_error("EVP_DecryptInit_ex failed");
+    }
+  }
+
+  ~AES128GCM_OnWireRxHandler() override {
+    ::ceph::crypto::zeroize_for_security(&nonce, sizeof(nonce));
+  }
+
+  std::uint32_t get_extra_size_at_final() override {
+    return AESGCM_TAG_LEN;
+  }
+  void reset_rx_handler() override;
+  void authenticated_decrypt_update(ceph::bufferlist& bl) override;
+  void authenticated_decrypt_update_final(ceph::bufferlist& bl) override;
+};
+
+void AES128GCM_OnWireRxHandler::reset_rx_handler()
+{
+  if(1 != EVP_DecryptInit_ex(ectx.get(), nullptr, nullptr, nullptr,
+	reinterpret_cast<const unsigned char*>(&nonce))) {
+    throw std::runtime_error("EVP_DecryptInit_ex failed");
+  }
+
+  if (!new_nonce_format) {
+    // msgr2.0: 32-bit counter followed by 64-bit fixed field,
+    // susceptible to overflow!
+    nonce.fixed = nonce.fixed + 1;
+  } else {
+    nonce.counter = nonce.counter + 1;
+  }
+}
+
+void AES128GCM_OnWireRxHandler::authenticated_decrypt_update(
+  ceph::bufferlist& bl)
+{
+  // discard cached crcs as we will be writing through c_str()
+  bl.invalidate_crc();
+  for (auto& buf : bl.buffers()) {
+    auto p = reinterpret_cast<unsigned char*>(const_cast<char*>(buf.c_str()));
+    int update_len = 0;
+
+    if (1 != EVP_DecryptUpdate(ectx.get(), p, &update_len, p, buf.length())) {
+      throw std::runtime_error("EVP_DecryptUpdate failed");
+    }
+    ceph_assert_always(update_len >= 0);
+    ceph_assert(static_cast<unsigned>(update_len) == buf.length());
+  }
+}
+
+void AES128GCM_OnWireRxHandler::authenticated_decrypt_update_final(
+  ceph::bufferlist& bl)
+{
+  unsigned orig_len = bl.length();
+  ceph_assert(orig_len >= AESGCM_TAG_LEN);
+
+  // decrypt optional data. Caller is obliged to provide only signature but it
+  // may supply ciphertext as well. Combining the update + final is reflected
+  // combined together.
+  ceph::bufferlist auth_tag;
+  bl.splice(orig_len - AESGCM_TAG_LEN, AESGCM_TAG_LEN, &auth_tag);
+  if (bl.length() > 0) {
+    authenticated_decrypt_update(bl);
+  }
+
+  // we need to ensure the tag is stored in continuous memory.
+  if (1 != EVP_CIPHER_CTX_ctrl(ectx.get(), EVP_CTRL_GCM_SET_TAG,
+	AESGCM_TAG_LEN, auth_tag.c_str())) {
+    throw std::runtime_error("EVP_CIPHER_CTX_ctrl failed");
+  }
+
+  // I expect that 0 bytes will be appended. The call is supposed solely to
+  // authenticate the message.
+  {
+    int final_len = 0;
+    if (0 >= EVP_DecryptFinal_ex(ectx.get(), nullptr, &final_len)) {
+      throw MsgAuthError();
+    }
+    ceph_assert_always(final_len == 0);
+    ceph_assert(bl.length() + AESGCM_TAG_LEN == orig_len);
+  }
+}
+
+ceph::crypto::onwire::rxtx_t ceph::crypto::onwire::rxtx_t::create_handler_pair(
+  CephContext* cct,
+  const AuthConnectionMeta& auth_meta,
+  bool new_nonce_format,
+  bool crossed)
+{
+  if (auth_meta.is_mode_secure()) {
+    ceph_assert_always(auth_meta.connection_secret.length() >= \
+      sizeof(key_t) + 2 * sizeof(nonce_t));
+    const char* secbuf = auth_meta.connection_secret.c_str();
+
+    key_t key;
+    {
+      ::memcpy(key.data(), secbuf, sizeof(key));
+      secbuf += sizeof(key);
+    }
+
+    nonce_t rx_nonce;
+    {
+      ::memcpy(&rx_nonce, secbuf, sizeof(rx_nonce));
+      secbuf += sizeof(rx_nonce);
+    }
+
+    nonce_t tx_nonce;
+    {
+      ::memcpy(&tx_nonce, secbuf, sizeof(tx_nonce));
+      secbuf += sizeof(tx_nonce);
+    }
+
+    return {
+      std::make_unique<AES128GCM_OnWireRxHandler>(
+	cct, key, crossed ? tx_nonce : rx_nonce, new_nonce_format),
+      std::make_unique<AES128GCM_OnWireTxHandler>(
+	cct, key, crossed ? rx_nonce : tx_nonce, new_nonce_format)
+    };
+  } else {
+    return { nullptr, nullptr };
+  }
+}
+
+} // namespace ceph::crypto::onwire
diff --git a/src/msg/async/crypto_onwire.h b/src/msg/async/crypto_onwire.h
new file mode 100644
index 00000000000..55f7550868f
--- /dev/null
+++ b/src/msg/async/crypto_onwire.h
@@ -0,0 +1,130 @@
+// -*- mode:C++; tab-width:8; c-basic-offset:2; indent-tabs-mode:t -*-
+// vim: ts=8 sw=2 smarttab
+/*
+ * Ceph - scalable distributed file system
+ *
+ * Copyright (C) 2004-2009 Sage Weil <sage@newdream.net>
+ *
+ * This is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License version 2.1, as published by the Free Software
+ * Foundation.  See file COPYING.
+ *
+ */
+
+
+#ifndef CEPH_CRYPTO_ONWIRE_H
+#define CEPH_CRYPTO_ONWIRE_H
+
+#include <cstdint>
+#include <memory>
+
+#include "auth/Auth.h"
+#include "include/buffer.h"
+
+namespace ceph::math {
+
+// TODO
+template <typename T>
+class always_aligned_t {
+  T val;
+
+  template <class... Args>
+  always_aligned_t(Args&&... args)
+    : val(std::forward<Args>(args)...) {
+  }
+};
+
+} // namespace ceph::math
+
+namespace ceph::crypto::onwire {
+
+struct MsgAuthError : public std::runtime_error {
+  MsgAuthError()
+    : runtime_error("message signature mismatch") {
+  }
+};
+
+struct TxHandlerError : public std::runtime_error {
+  TxHandlerError(const char* what)
+    : std::runtime_error(std::string("tx handler error: ") + what) {}
+};
+
+struct TxHandler {
+  virtual ~TxHandler() = default;
+
+  // Instance of TxHandler must be reset before doing any encrypt-update
+  // step. This applies also to situation when encrypt-final was already
+  // called and another round of update-...-update-final will take place.
+  //
+  // The input parameter informs implementation how the -update sequence
+  // is fragmented and allows to make concious decision about allocation
+  // or reusage of provided memory. One implementation could do in-place
+  // encryption while other might prefer one huge output buffer.
+  //
+  // It's undefined what will happen if client doesn't follow the order.
+  //
+  // TODO: switch to always_aligned_t
+  virtual void reset_tx_handler(const uint32_t* first,
+                                const uint32_t* last) = 0;
+
+  void reset_tx_handler(std::initializer_list<uint32_t> update_size_sequence) {
+    if (update_size_sequence.size() > 0) {
+      const uint32_t* first = &*update_size_sequence.begin();
+      reset_tx_handler(first, first + update_size_sequence.size());
+    } else {
+      reset_tx_handler(nullptr, nullptr);
+    }
+  }
+
+  // Perform encryption. Client gives full ownership right to provided
+  // bufferlist. The method MUST NOT be called after _final() if there
+  // was no call to _reset().
+  virtual void authenticated_encrypt_update(
+    const ceph::bufferlist& plaintext) = 0;
+
+  // Generates authentication signature and returns bufferlist crafted
+  // basing on plaintext from preceding call to _update().
+  virtual ceph::bufferlist authenticated_encrypt_final() = 0;
+};
+
+class RxHandler {
+public:
+  virtual ~RxHandler() = default;
+
+  // Transmitter can append extra bytes of ciphertext at the -final step.
+  // This method return how much was added, and thus let client translate
+  // plaintext size into ciphertext size to grab from wire.
+  virtual std::uint32_t get_extra_size_at_final() = 0;
+
+  // Instance of RxHandler must be reset before doing any decrypt-update
+  // step. This applies also to situation when decrypt-final was already
+  // called and another round of update-...-update-final will take place.
+  virtual void reset_rx_handler() = 0;
+
+  // Perform decryption ciphertext must be ALWAYS aligned to 16 bytes.
+  virtual void authenticated_decrypt_update(ceph::bufferlist& bl) = 0;
+
+  // Perform decryption of last cipertext's portion and verify signature
+  // for overall decryption sequence.
+  // Throws on integrity/authenticity checks
+  virtual void authenticated_decrypt_update_final(ceph::bufferlist& bl) = 0;
+};
+
+struct rxtx_t {
+  //rxtx_t(rxtx_t&& r) : rx(std::move(rx)), tx(std::move(tx)) {}
+  // Each peer can use different handlers.
+  // Hmm, isn't that too much flexbility?
+  std::unique_ptr<RxHandler> rx;
+  std::unique_ptr<TxHandler> tx;
+
+  static rxtx_t create_handler_pair(
+    CephContext* ctx,
+    const class AuthConnectionMeta& auth_meta,
+    bool new_nonce_format,
+    bool crossed);
+};
+
+} // namespace ceph::crypto::onwire
+
+#endif // CEPH_CRYPTO_ONWIRE_H
diff --git a/src/msg/async/dpdk/ARP.cc b/src/msg/async/dpdk/ARP.cc
index 73958cfb83b..dedc9e3c7aa 100644
--- a/src/msg/async/dpdk/ARP.cc
+++ b/src/msg/async/dpdk/ARP.cc
@@ -19,19 +19,6 @@
 /*
  * Copyright (C) 2014 Cloudius Systems, Ltd.
  */
-/*
- * Ceph - scalable distributed file system
- *
- * Copyright (C) 2015 XSky <haomai@xsky.com>
- *
- * Author: Haomai Wang <haomaiwang@gmail.com>
- *
- * This is free software; you can redistribute it and/or
- * modify it under the terms of the GNU Lesser General Public
- * License version 2.1, as published by the Free Software
- * Foundation.  See file COPYING.
- *
- */
 
 #include "ARP.h"
 
diff --git a/src/msg/async/dpdk/ARP.h b/src/msg/async/dpdk/ARP.h
index 2b34ebf1f23..5456956480a 100644
--- a/src/msg/async/dpdk/ARP.h
+++ b/src/msg/async/dpdk/ARP.h
@@ -20,19 +20,6 @@
  * Copyright (C) 2014 Cloudius Systems, Ltd.
  *
  */
-/*
- * Ceph - scalable distributed file system
- *
- * Copyright (C) 2015 XSky <haomai@xsky.com>
- *
- * Author: Haomai Wang <haomaiwang@gmail.com>
- *
- * This is free software; you can redistribute it and/or
- * modify it under the terms of the GNU Lesser General Public
- * License version 2.1, as published by the Free Software
- * Foundation.  See file COPYING.
- *
- */
 
 #ifndef CEPH_MSG_ARP_H_
 #define CEPH_MSG_ARP_H_
@@ -161,7 +148,7 @@ class arp_for : public arp_for_protocol {
    public:
     C_handle_arp_timeout(arp_for *a, l3addr addr, bool first):
         arp(a), paddr(addr), first_request(first) {}
-    void do_request(int r) {
+    void do_request(uint64_t r) {
       arp->send_query(paddr);
       auto &res = arp->_in_progress[paddr];
 
diff --git a/src/msg/async/dpdk/DPDK.cc b/src/msg/async/dpdk/DPDK.cc
index 8da12c3fd35..ff496788810 100644
--- a/src/msg/async/dpdk/DPDK.cc
+++ b/src/msg/async/dpdk/DPDK.cc
@@ -19,19 +19,6 @@
 /*
  * Copyright (C) 2014 Cloudius Systems, Ltd.
  */
-/*
- * Ceph - scalable distributed file system
- *
- * Copyright (C) 2015 XSky <haomai@xsky.com>
- *
- * Author: Haomai Wang <haomaiwang@gmail.com>
- *
- * This is free software; you can redistribute it and/or
- * modify it under the terms of the GNU Lesser General Public
- * License version 2.1, as published by the Free Software
- * Foundation.  See file COPYING.
- *
- */
 
 #include <atomic>
 #include <vector>
@@ -56,7 +43,7 @@
 #include "common/Cycles.h"
 #include "common/dout.h"
 #include "common/errno.h"
-#include "include/assert.h"
+#include "include/ceph_assert.h"
 
 #define dout_subsys ceph_subsys_dpdk
 #undef dout_prefix
@@ -88,8 +75,17 @@ static constexpr uint16_t rx_gc_thresh           = 64;
 static constexpr uint16_t mbufs_per_queue_tx     = 2 * default_ring_size;
 
 static constexpr uint16_t mbuf_cache_size        = 512;
+//
+// Size of the data buffer in the non-inline case.
+//
+// We may want to change (increase) this value in future, while the
+// inline_mbuf_data_size value will unlikely change due to reasons described
+// above.
+//
+static constexpr size_t mbuf_data_size = 4096;
+
 static constexpr uint16_t mbuf_overhead          =
-sizeof(struct rte_mbuf) + RTE_PKTMBUF_HEADROOM;
+                          sizeof(struct rte_mbuf) + RTE_PKTMBUF_HEADROOM;
 //
 // We'll allocate 2K data buffers for an inline case because this would require
 // a single page per mbuf. If we used 4K data buffers here it would require 2
@@ -98,14 +94,6 @@ sizeof(struct rte_mbuf) + RTE_PKTMBUF_HEADROOM;
 //
 static constexpr size_t inline_mbuf_data_size = 2048;
 
-//
-// Size of the data buffer in the non-inline case.
-//
-// We may want to change (increase) this value in future, while the
-// inline_mbuf_data_size value will unlikely change due to reasons described
-// above.
-//
-static constexpr size_t mbuf_data_size = 4096;
 
 // (INLINE_MBUF_DATA_SIZE(2K)*32 = 64K = Max TSO/LRO size) + 1 mbuf for headers
 static constexpr uint8_t max_frags = 32 + 1;
@@ -163,7 +151,7 @@ static constexpr uint8_t packet_read_size        = 32;
 
 int DPDKDevice::init_port_start()
 {
-  assert(_port_idx < rte_eth_dev_count());
+  ceph_assert(_port_idx < rte_eth_dev_count_avail());
 
   rte_eth_dev_info_get(_port_idx, &_dev_info);
 
@@ -200,41 +188,35 @@ int DPDKDevice::init_port_start()
     _dev_info.max_rx_queues = std::min(_dev_info.max_rx_queues, (uint16_t)16);
   }
 
-  // Clear txq_flags - we want to support all available offload features
-  // except for multi-mempool and refcnt'ing which we don't need
-  _dev_info.default_txconf.txq_flags =
-      ETH_TXQ_FLAGS_NOMULTMEMP | ETH_TXQ_FLAGS_NOREFCOUNT;
-
-  //
-  // Disable features that are not supported by port's HW
-  //
-  if (!(_dev_info.tx_offload_capa & DEV_TX_OFFLOAD_UDP_CKSUM)) {
-    _dev_info.default_txconf.txq_flags |= ETH_TXQ_FLAGS_NOXSUMUDP;
-  }
-
-  if (!(_dev_info.tx_offload_capa & DEV_TX_OFFLOAD_TCP_CKSUM)) {
-    _dev_info.default_txconf.txq_flags |= ETH_TXQ_FLAGS_NOXSUMTCP;
-  }
-
-  if (!(_dev_info.tx_offload_capa & DEV_TX_OFFLOAD_SCTP_CKSUM)) {
-    _dev_info.default_txconf.txq_flags |= ETH_TXQ_FLAGS_NOXSUMSCTP;
-  }
-
-  if (!(_dev_info.tx_offload_capa & DEV_TX_OFFLOAD_VLAN_INSERT)) {
-    _dev_info.default_txconf.txq_flags |= ETH_TXQ_FLAGS_NOVLANOFFL;
-  }
-
-  if (!(_dev_info.tx_offload_capa & DEV_TX_OFFLOAD_VLAN_INSERT)) {
-    _dev_info.default_txconf.txq_flags |= ETH_TXQ_FLAGS_NOVLANOFFL;
-  }
-
-  if (!(_dev_info.tx_offload_capa & DEV_TX_OFFLOAD_TCP_TSO)) {
-    _dev_info.default_txconf.txq_flags |= ETH_TXQ_FLAGS_NOMULTSEGS;
-  }
+  // Hardware offload capabilities
+  // https://github.com/DPDK/dpdk/blob/v19.05/lib/librte_ethdev/rte_ethdev.h#L993-L1074
+  // We want to support all available offload features
+  // TODO: below features are implemented in 17.05, should support new ones
+  const uint64_t tx_offloads_wanted =
+    DEV_TX_OFFLOAD_VLAN_INSERT      |
+    DEV_TX_OFFLOAD_IPV4_CKSUM       |
+    DEV_TX_OFFLOAD_UDP_CKSUM        |
+    DEV_TX_OFFLOAD_TCP_CKSUM        |
+    DEV_TX_OFFLOAD_SCTP_CKSUM       |
+    DEV_TX_OFFLOAD_TCP_TSO          |
+    DEV_TX_OFFLOAD_UDP_TSO          |
+    DEV_TX_OFFLOAD_OUTER_IPV4_CKSUM |
+    DEV_TX_OFFLOAD_QINQ_INSERT      |
+    DEV_TX_OFFLOAD_VXLAN_TNL_TSO    |
+    DEV_TX_OFFLOAD_GRE_TNL_TSO      |
+    DEV_TX_OFFLOAD_IPIP_TNL_TSO     |
+    DEV_TX_OFFLOAD_GENEVE_TNL_TSO   |
+    DEV_TX_OFFLOAD_MACSEC_INSERT;
+
+  _dev_info.default_txconf.offloads =
+    _dev_info.tx_offload_capa & tx_offloads_wanted;
 
   /* for port configuration all features are off by default */
   rte_eth_conf port_conf = { 0 };
 
+  /* setting tx offloads for port */
+  port_conf.txmode.offloads = _dev_info.default_txconf.offloads;
+
   ldout(cct, 5) << __func__ << " Port " << int(_port_idx) << ": max_rx_queues "
                 << _dev_info.max_rx_queues << "  max_tx_queues "
                 << _dev_info.max_tx_queues << dendl;
@@ -242,7 +224,7 @@ int DPDKDevice::init_port_start()
   _num_queues = std::min({_num_queues, _dev_info.max_rx_queues, _dev_info.max_tx_queues});
 
   ldout(cct, 5) << __func__ << " Port " << int(_port_idx) << ": using "
-                << _num_queues << " queues" << dendl;;
+                << _num_queues << " queues" << dendl;
 
   // Set RSS mode: enable RSS if seastar is configured with more than 1 CPU.
   // Even if port has a single queue we still want the RSS feature to be
@@ -253,7 +235,6 @@ int DPDKDevice::init_port_start()
     } else if (_dev_info.hash_key_size == 52) {
       _rss_key = default_rsskey_52bytes;
     } else if (_dev_info.hash_key_size != 0) {
-      // WTF?!!
       rte_exit(EXIT_FAILURE,
                "Port %d: We support only 40 or 52 bytes RSS hash keys, %d bytes key requested",
                _port_idx, _dev_info.hash_key_size);
@@ -263,7 +244,8 @@ int DPDKDevice::init_port_start()
     }
 
     port_conf.rxmode.mq_mode = ETH_MQ_RX_RSS;
-    port_conf.rx_adv_conf.rss_conf.rss_hf = ETH_RSS_PROTO_MASK;
+    /* enable all supported rss offloads */
+    port_conf.rx_adv_conf.rss_conf.rss_hf = _dev_info.flow_type_rss_offloads;
     if (_dev_info.hash_key_size) {
       port_conf.rx_adv_conf.rss_conf.rss_key = const_cast<uint8_t *>(_rss_key.data());
       port_conf.rx_adv_conf.rss_conf.rss_key_len = _dev_info.hash_key_size;
@@ -275,7 +257,7 @@ int DPDKDevice::init_port_start()
   if (_num_queues > 1) {
     if (_dev_info.reta_size) {
       // RETA size should be a power of 2
-      assert((_dev_info.reta_size & (_dev_info.reta_size - 1)) == 0);
+      ceph_assert((_dev_info.reta_size & (_dev_info.reta_size - 1)) == 0);
 
       // Set the RSS table to the correct size
       _redir_table.resize(_dev_info.reta_size);
@@ -293,17 +275,14 @@ int DPDKDevice::init_port_start()
 
   // Set Rx VLAN stripping
   if (_dev_info.rx_offload_capa & DEV_RX_OFFLOAD_VLAN_STRIP) {
-    port_conf.rxmode.hw_vlan_strip = 1;
+    port_conf.rxmode.offloads |= DEV_RX_OFFLOAD_VLAN_STRIP;
   }
 
-  // Enable HW CRC stripping
-  port_conf.rxmode.hw_strip_crc = 1;
-
 #ifdef RTE_ETHDEV_HAS_LRO_SUPPORT
   // Enable LRO
   if (_use_lro && (_dev_info.rx_offload_capa & DEV_RX_OFFLOAD_TCP_LRO)) {
     ldout(cct, 1) << __func__ << " LRO is on" << dendl;
-    port_conf.rxmode.enable_lro = 1;
+    port_conf.rxmode.offloads |= DEV_RX_OFFLOAD_TCP_LRO;
     _hw_features.rx_lro = true;
   } else
 #endif
@@ -313,7 +292,7 @@ int DPDKDevice::init_port_start()
   // all together. If this assumption breaks we need to rework the below logic
   // by splitting the csum offload feature bit into separate bits for IPv4,
   // TCP.
-  assert(((_dev_info.rx_offload_capa & DEV_RX_OFFLOAD_IPV4_CKSUM) &&
+  ceph_assert(((_dev_info.rx_offload_capa & DEV_RX_OFFLOAD_IPV4_CKSUM) &&
           (_dev_info.rx_offload_capa & DEV_RX_OFFLOAD_TCP_CKSUM)) ||
          (!(_dev_info.rx_offload_capa & DEV_RX_OFFLOAD_IPV4_CKSUM) &&
           !(_dev_info.rx_offload_capa & DEV_RX_OFFLOAD_TCP_CKSUM)));
@@ -322,7 +301,7 @@ int DPDKDevice::init_port_start()
   if ((_dev_info.rx_offload_capa & DEV_RX_OFFLOAD_IPV4_CKSUM) &&
       (_dev_info.rx_offload_capa & DEV_RX_OFFLOAD_TCP_CKSUM)) {
     ldout(cct, 1) << __func__ << " RX checksum offload supported" << dendl;
-    port_conf.rxmode.hw_ip_checksum = 1;
+    port_conf.rxmode.offloads |= DEV_RX_OFFLOAD_CHECKSUM;
     _hw_features.rx_csum_offload = 1;
   }
 
@@ -341,7 +320,7 @@ int DPDKDevice::init_port_start()
   // or not set all together. If this assumption breaks we need to rework the
   // below logic by splitting the csum offload feature bit into separate bits
   // for TCP.
-  assert((_dev_info.tx_offload_capa & DEV_TX_OFFLOAD_TCP_CKSUM) ||
+  ceph_assert((_dev_info.tx_offload_capa & DEV_TX_OFFLOAD_TCP_CKSUM) ||
           !(_dev_info.tx_offload_capa & DEV_TX_OFFLOAD_TCP_CKSUM));
 
   if (_dev_info.tx_offload_capa & DEV_TX_OFFLOAD_TCP_CKSUM) {
@@ -424,25 +403,8 @@ int DPDKDevice::init_port_fini()
     return -1;
   }
 
-  if (_num_queues > 1) {
-    if (!rte_eth_dev_filter_supported(_port_idx, RTE_ETH_FILTER_HASH)) {
-      ldout(cct, 5) << __func__ << " Port " << _port_idx << ": HASH FILTER configuration is supported" << dendl;
-
-      // Setup HW touse the TOEPLITZ hash function as an RSS hash function
-      struct rte_eth_hash_filter_info info = {};
-
-      info.info_type = RTE_ETH_HASH_FILTER_GLOBAL_CONFIG;
-      info.info.global_conf.hash_func = RTE_ETH_HASH_FUNCTION_TOEPLITZ;
-
-      if (rte_eth_dev_filter_ctrl(_port_idx, RTE_ETH_FILTER_HASH,
-                                  RTE_ETH_FILTER_SET, &info) < 0) {
-        lderr(cct) << __func__ << " cannot set hash function on a port " << _port_idx << dendl;
-        return -1;
-      }
-    }
-
+  if (_num_queues > 1)
     set_rss_table();
-  }
 
   // Wait for a link
   if (check_port_link_status() < 0) {
@@ -454,8 +416,52 @@ int DPDKDevice::init_port_fini()
   return 0;
 }
 
+void DPDKDevice::set_rss_table()
+{
+  struct rte_flow_attr attr;
+  struct rte_flow_item pattern[1];
+  struct rte_flow_action action[2];
+  struct rte_flow_action_rss rss_conf;
+
+  /*
+   * set the rule attribute.
+   * in this case only ingress packets will be checked.
+   */
+  memset(&attr, 0, sizeof(struct rte_flow_attr));
+  attr.ingress = 1;
+
+  /* the final level must be always type end */
+  pattern[0].type = RTE_FLOW_ITEM_TYPE_END;
+
+  /*
+   * create the action sequence.
+   * one action only,  set rss hash func to toeplitz.
+   */
+  uint16_t i = 0;
+  for (auto& r : _redir_table) {
+    r = i++ % _num_queues;
+  }
+  rss_conf.func = RTE_ETH_HASH_FUNCTION_TOEPLITZ;
+  rss_conf.types = ETH_RSS_FRAG_IPV4 | ETH_RSS_NONFRAG_IPV4_TCP;
+  rss_conf.queue_num = _num_queues;
+  rss_conf.queue = const_cast<uint16_t *>(_redir_table.data());
+  rss_conf.key_len = _dev_info.hash_key_size;
+  rss_conf.key = const_cast<uint8_t *>(_rss_key.data());
+  rss_conf.level = 0;
+  action[0].type = RTE_FLOW_ACTION_TYPE_RSS;
+  action[0].conf = &rss_conf;
+  action[1].type = RTE_FLOW_ACTION_TYPE_END;
+
+  if (rte_flow_validate(_port_idx, &attr, pattern, action, nullptr) == 0)
+    _flow = rte_flow_create(_port_idx, &attr, pattern, action, nullptr);
+  else
+    ldout(cct, 0) << __func__ << " Port " << _port_idx
+                  << ": flow rss func configuration is unsupported"
+                  << dendl;
+}
+
 void DPDKQueuePair::configure_proxies(const std::map<unsigned, float>& cpu_weights) {
-  assert(!cpu_weights.empty());
+  ceph_assert(!cpu_weights.empty());
   if (cpu_weights.size() == 1 && cpu_weights.begin()->first == _qid) {
     // special case queue sending to self only, to avoid requiring a hash value
     return;
@@ -512,7 +518,7 @@ bool DPDKQueuePair::init_rx_mbuf_pool()
     roomsz.mbuf_data_room_size = mbuf_data_size + RTE_PKTMBUF_HEADROOM;
     _pktmbuf_pool_rx = rte_mempool_create(
         name.c_str(),
-        mbufs_per_queue_rx, mbuf_overhead,
+        mbufs_per_queue_rx, mbuf_overhead + mbuf_data_size,
         mbuf_cache_size,
         sizeof(struct rte_pktmbuf_pool_private),
         rte_pktmbuf_pool_init, as_cookie(roomsz),
@@ -524,34 +530,20 @@ bool DPDKQueuePair::init_rx_mbuf_pool()
     }
 
     //
-    // 1) Pull all entries from the pool.
-    // 2) Bind data buffers to each of them.
-    // 3) Return them back to the pool.
-    //
-    for (int i = 0; i < mbufs_per_queue_rx; i++) {
-      rte_mbuf* m = rte_pktmbuf_alloc(_pktmbuf_pool_rx);
-      assert(m);
-      _rx_free_bufs.push_back(m);
-    }
-
-    for (int i = 0; i < cct->_conf->ms_dpdk_rx_buffer_count_per_core; i++) {
-      void* m = rte_malloc(NULL, mbuf_data_size, mbuf_data_size);
-      assert(m);
+    // allocate more data buffer
+    int bufs_count =  cct->_conf->ms_dpdk_rx_buffer_count_per_core - mbufs_per_queue_rx;
+    int mz_flags = RTE_MEMZONE_1GB|RTE_MEMZONE_SIZE_HINT_ONLY;
+    std::string mz_name = "rx_buffer_data" + std::to_string(_qid);
+    const struct rte_memzone *mz = rte_memzone_reserve_aligned(mz_name.c_str(),
+          mbuf_data_size*bufs_count, _pktmbuf_pool_rx->socket_id, mz_flags, mbuf_data_size);
+    ceph_assert(mz);
+    void* m = mz->addr;
+    for (int i = 0; i < bufs_count; i++) {
+      ceph_assert(m);
       _alloc_bufs.push_back(m);
+      m += mbuf_data_size;
     }
 
-    for (auto&& m : _rx_free_bufs) {
-      if (!init_noninline_rx_mbuf(m, mbuf_data_size, _alloc_bufs)) {
-        lderr(cct) << __func__ << " Failed to allocate data buffers for Rx ring. "
-                   "Consider increasing the amount of memory." << dendl;
-        return false;
-      }
-    }
-
-    rte_mempool_put_bulk(_pktmbuf_pool_rx, (void**)_rx_free_bufs.data(),
-                         _rx_free_bufs.size());
-
-    _rx_free_bufs.clear();
     if (rte_eth_rx_queue_setup(_dev_port_idx, _qid, default_ring_size,
                                rte_eth_dev_socket_id(_dev_port_idx),
                                _dev->def_rx_conf(), _pktmbuf_pool_rx) < 0) {
@@ -560,7 +552,6 @@ bool DPDKQueuePair::init_rx_mbuf_pool()
     }
   }
 
-  ldout(cct, 20) << __func__ << " count " << rte_mempool_count(_pktmbuf_pool_rx) << " free count " << rte_mempool_free_count(_pktmbuf_pool_rx) << dendl;
   return _pktmbuf_pool_rx != nullptr;
 }
 
@@ -589,7 +580,7 @@ int DPDKDevice::check_port_link_status()
         ldout(cct, 20) << __func__ << " not ready, continue to wait." << dendl;
         usleep(sleep_time);
       } else {
-        lderr(cct) << __func__ << "done port " << _port_idx << " link down" << dendl;
+        lderr(cct) << __func__ << " done port " << _port_idx << " link down" << dendl;
         return -1;
       }
     }
@@ -601,7 +592,7 @@ class C_handle_dev_stats : public EventCallback {
   DPDKQueuePair *_qp;
  public:
   C_handle_dev_stats(DPDKQueuePair *qp): _qp(qp) { }
-  void do_request(int id) {
+  void do_request(uint64_t id) {
     _qp->handle_stats();
   }
 };
@@ -633,16 +624,16 @@ DPDKQueuePair::DPDKQueuePair(CephContext *c, EventCenter *cen, DPDKDevice* dev,
   plb.add_u64_counter(l_dpdk_qp_tx_packets, "dpdk_send_packets", "DPDK sendd packets");
   plb.add_u64_counter(l_dpdk_qp_rx_bad_checksum_errors, "dpdk_receive_bad_checksum_errors", "DPDK received bad checksum packets");
   plb.add_u64_counter(l_dpdk_qp_rx_no_memory_errors, "dpdk_receive_no_memory_errors", "DPDK received no memory packets");
-  plb.add_u64_counter(l_dpdk_qp_rx_bytes, "dpdk_receive_bytes", "DPDK received bytes");
-  plb.add_u64_counter(l_dpdk_qp_tx_bytes, "dpdk_send_bytes", "DPDK sendd bytes");
+  plb.add_u64_counter(l_dpdk_qp_rx_bytes, "dpdk_receive_bytes", "DPDK received bytes", NULL, 0, unit_t(UNIT_BYTES));
+  plb.add_u64_counter(l_dpdk_qp_tx_bytes, "dpdk_send_bytes", "DPDK sendd bytes", NULL, 0, unit_t(UNIT_BYTES));
   plb.add_u64_counter(l_dpdk_qp_rx_last_bunch, "dpdk_receive_last_bunch", "DPDK last received bunch");
   plb.add_u64_counter(l_dpdk_qp_tx_last_bunch, "dpdk_send_last_bunch", "DPDK last send bunch");
   plb.add_u64_counter(l_dpdk_qp_rx_fragments, "dpdk_receive_fragments", "DPDK received total fragments");
   plb.add_u64_counter(l_dpdk_qp_tx_fragments, "dpdk_send_fragments", "DPDK sendd total fragments");
   plb.add_u64_counter(l_dpdk_qp_rx_copy_ops, "dpdk_receive_copy_ops", "DPDK received copy operations");
   plb.add_u64_counter(l_dpdk_qp_tx_copy_ops, "dpdk_send_copy_ops", "DPDK sendd copy operations");
-  plb.add_u64_counter(l_dpdk_qp_rx_copy_bytes, "dpdk_receive_copy_bytes", "DPDK received copy bytes");
-  plb.add_u64_counter(l_dpdk_qp_tx_copy_bytes, "dpdk_send_copy_bytes", "DPDK send copy bytes");
+  plb.add_u64_counter(l_dpdk_qp_rx_copy_bytes, "dpdk_receive_copy_bytes", "DPDK received copy bytes", NULL, 0, unit_t(UNIT_BYTES));
+  plb.add_u64_counter(l_dpdk_qp_tx_copy_bytes, "dpdk_send_copy_bytes", "DPDK send copy bytes", NULL, 0, unit_t(UNIT_BYTES));
   plb.add_u64_counter(l_dpdk_qp_rx_linearize_ops, "dpdk_receive_linearize_ops", "DPDK received linearize operations");
   plb.add_u64_counter(l_dpdk_qp_tx_linearize_ops, "dpdk_send_linearize_ops", "DPDK send linearize operations");
   plb.add_u64_counter(l_dpdk_qp_tx_queue_length, "dpdk_send_queue_length", "DPDK send queue length");
@@ -783,8 +774,6 @@ bool DPDKQueuePair::rx_gc(bool force)
     ldout(cct, 10) << __func__ << " free segs " << _num_rx_free_segs
                    << " thresh " << rx_gc_thresh
                    << " free pkts " << _rx_free_pkts.size()
-                   << " pool count " << rte_mempool_count(_pktmbuf_pool_rx)
-                   << " free pool count " << rte_mempool_free_count(_pktmbuf_pool_rx)
                    << dendl;
 
     while (!_rx_free_pkts.empty()) {
@@ -801,20 +790,23 @@ bool DPDKQueuePair::rx_gc(bool force)
         break;
       }
     }
+    for (auto&& m : _rx_free_bufs) {
+      rte_pktmbuf_prefree_seg(m);
+    }
 
     if (_rx_free_bufs.size()) {
       rte_mempool_put_bulk(_pktmbuf_pool_rx,
                            (void **)_rx_free_bufs.data(),
                            _rx_free_bufs.size());
 
-      // TODO: assert() in a fast path! Remove me ASAP!
-      assert(_num_rx_free_segs >= _rx_free_bufs.size());
+      // TODO: ceph_assert() in a fast path! Remove me ASAP!
+      ceph_assert(_num_rx_free_segs >= _rx_free_bufs.size());
 
       _num_rx_free_segs -= _rx_free_bufs.size();
       _rx_free_bufs.clear();
 
-      // TODO: assert() in a fast path! Remove me ASAP!
-      assert((_rx_free_pkts.empty() && !_num_rx_free_segs) ||
+      // TODO: ceph_assert() in a fast path! Remove me ASAP!
+      ceph_assert((_rx_free_pkts.empty() && !_num_rx_free_segs) ||
              (!_rx_free_pkts.empty() && _num_rx_free_segs));
     }
   }
@@ -846,7 +838,7 @@ void DPDKQueuePair::process_packets(
 
     // Set stipped VLAN value if available
     if ((_dev->_dev_info.rx_offload_capa & DEV_RX_OFFLOAD_VLAN_STRIP) &&
-        (m->ol_flags & PKT_RX_VLAN_PKT)) {
+        (m->ol_flags & PKT_RX_VLAN_STRIPPED)) {
       oi.vlan_tci = m->vlan_tci;
     }
 
@@ -1046,7 +1038,7 @@ void DPDKQueuePair::tx_buf::set_cluster_offload_info(const Packet& p, const DPDK
       head->l3_len = oi.ip_hdr_len;
 
       if (oi.tso_seg_size) {
-        assert(oi.needs_ip_csum);
+        ceph_assert(oi.needs_ip_csum);
         head->ol_flags |= PKT_TX_TCP_SEG;
         head->l4_len = oi.tcp_hdr_len;
         head->tso_segsz = oi.tso_seg_size;
@@ -1102,6 +1094,9 @@ DPDKQueuePair::tx_buf* DPDKQueuePair::tx_buf::from_packet_zc(
   // Update the HEAD buffer with the packet info
   head->pkt_len = p.len();
   head->nb_segs = total_nsegs;
+  // tx_pkt_burst loops until the next pointer is null, so last_seg->next must
+  // be null.
+  last_seg->next = nullptr;
 
   set_cluster_offload_info(p, qp, head);
 
@@ -1165,7 +1160,7 @@ void DPDKQueuePair::tx_buf::copy_packet_to_cluster(const Packet& p, rte_mbuf* he
       cur_seg_offset = 0;
 
       // FIXME: assert in a fast-path - remove!!!
-      assert(cur_seg);
+      ceph_assert(cur_seg);
     }
   }
 }
@@ -1213,6 +1208,9 @@ DPDKQueuePair::tx_buf* DPDKQueuePair::tx_buf::from_packet_copy(Packet&& p, DPDKQ
   //
   head->pkt_len = p.len();
   head->nb_segs = nsegs;
+  // tx_pkt_burst loops until the next pointer is null, so last_seg->next must
+  // be null.
+  last_seg->next = nullptr;
 
   copy_packet_to_cluster(p, head);
   set_cluster_offload_info(p, qp, head);
@@ -1244,34 +1242,6 @@ size_t DPDKQueuePair::tx_buf::copy_one_data_buf(
   return len;
 }
 
-void DPDKDevice::set_rss_table()
-{
-  // always fill our local indirection table.
-  unsigned i = 0;
-  for (auto& r : _redir_table) {
-    r = i++ % _num_queues;
-  }
-
-  if (_dev_info.reta_size == 0)
-    return;
-
-  int reta_conf_size = std::max(1, _dev_info.reta_size / RTE_RETA_GROUP_SIZE);
-  rte_eth_rss_reta_entry64 reta_conf[reta_conf_size];
-
-  // Configure the HW indirection table
-  i = 0;
-  for (auto& x : reta_conf) {
-    x.mask = ~0ULL;
-    for (auto& r: x.reta) {
-      r = i++ % _num_queues;
-    }
-  }
-
-  if (rte_eth_dev_rss_reta_update(_port_idx, reta_conf, _dev_info.reta_size)) {
-    rte_exit(EXIT_FAILURE, "Port %d: Failed to update an RSS indirection table", _port_idx);
-  }
-}
-
 /******************************** Interface functions *************************/
 
 std::unique_ptr<DPDKDevice> create_dpdk_net_device(
@@ -1282,10 +1252,10 @@ std::unique_ptr<DPDKDevice> create_dpdk_net_device(
     bool enable_fc)
 {
   // Check that we have at least one DPDK-able port
-  if (rte_eth_dev_count() == 0) {
+  if (rte_eth_dev_count_avail() == 0) {
     rte_exit(EXIT_FAILURE, "No Ethernet ports - bye\n");
   } else {
-    ldout(cct, 10) << __func__ << " ports number: " << int(rte_eth_dev_count()) << dendl;
+    ldout(cct, 10) << __func__ << " ports number: " << int(rte_eth_dev_count_avail()) << dendl;
   }
 
   return std::unique_ptr<DPDKDevice>(
diff --git a/src/msg/async/dpdk/DPDK.h b/src/msg/async/dpdk/DPDK.h
index 3d65a4fe0af..78a1a076932 100644
--- a/src/msg/async/dpdk/DPDK.h
+++ b/src/msg/async/dpdk/DPDK.h
@@ -19,19 +19,6 @@
 /*
  * Copyright (C) 2014 Cloudius Systems, Ltd.
  */
-/*
- * Ceph - scalable distributed file system
- *
- * Copyright (C) 2015 XSky <haomai@xsky.com>
- *
- * Author: Haomai Wang <haomaiwang@gmail.com>
- *
- * This is free software; you can redistribute it and/or
- * modify it under the terms of the GNU Lesser General Public
- * License version 2.1, as published by the Free Software
- * Foundation.  See file COPYING.
- *
- */
 
 #ifndef CEPH_DPDK_DEV_H
 #define CEPH_DPDK_DEV_H
@@ -51,7 +38,6 @@
 #include "const.h"
 #include "circular_buffer.h"
 #include "ethernet.h"
-#include "memory.h"
 #include "Packet.h"
 #include "stream.h"
 #include "net.h"
@@ -201,8 +187,8 @@ class DPDKQueuePair {
 
       rte_mbuf* m;
 
-      // TODO: assert() in a fast path! Remove me ASAP!
-      assert(frag.size);
+      // TODO: ceph_assert() in a fast path! Remove me ASAP!
+      ceph_assert(frag.size);
 
       // Create a HEAD of mbufs' cluster and set the first bytes into it
       len = do_one_buf(qp, head, base, left_to_set);
@@ -294,18 +280,18 @@ class DPDKQueuePair {
         DPDKQueuePair& qp, rte_mbuf*& m, char* va, size_t buf_len) {
       static constexpr size_t max_frag_len = 15 * 1024; // 15K
 
-      // FIXME: current all tx buf is alloced without rte_malloc
+      // FIXME: current all tx buf is allocated without rte_malloc
       return copy_one_data_buf(qp, m, va, buf_len);
       //
       // Currently we break a buffer on a 15K boundary because 82599
       // devices have a 15.5K limitation on a maximum single fragment
       // size.
       //
-      phys_addr_t pa = rte_malloc_virt2phy(va);
+      rte_iova_t pa = rte_malloc_virt2iova(va);
       if (!pa)
         return copy_one_data_buf(qp, m, va, buf_len);
 
-      assert(buf_len);
+      ceph_assert(buf_len);
       tx_buf* buf = qp.get_tx_buf();
       if (!buf) {
         return 0;
@@ -564,8 +550,8 @@ class DPDKQueuePair {
   uint32_t _send(circular_buffer<Packet>& pb, Func &&packet_to_tx_buf_p) {
     if (_tx_burst.size() == 0) {
       for (auto&& p : pb) {
-        // TODO: assert() in a fast path! Remove me ASAP!
-        assert(p.len());
+        // TODO: ceph_assert() in a fast path! Remove me ASAP!
+        ceph_assert(p.len());
 
         tx_buf* buf = packet_to_tx_buf_p(std::move(p));
         if (!buf) {
@@ -626,18 +612,7 @@ class DPDKQueuePair {
     // actual data buffer.
     //
     m->buf_addr      = (char*)data - RTE_PKTMBUF_HEADROOM;
-    m->buf_physaddr  = rte_malloc_virt2phy(data) - RTE_PKTMBUF_HEADROOM;
-    return true;
-  }
-
-  static bool init_noninline_rx_mbuf(rte_mbuf* m, size_t size,
-                                     std::vector<void*> &datas) {
-    if (!refill_rx_mbuf(m, size, datas)) {
-      return false;
-    }
-    // The below fields stay constant during the execution.
-    m->buf_len       = size + RTE_PKTMBUF_HEADROOM;
-    m->data_off      = RTE_PKTMBUF_HEADROOM;
+    m->buf_physaddr  = rte_mem_virt2phy(data) - RTE_PKTMBUF_HEADROOM;
     return true;
   }
 
@@ -773,8 +748,9 @@ class DPDKDevice {
   unsigned _home_cpu;
   bool _use_lro;
   bool _enable_fc;
-  std::vector<uint8_t> _redir_table;
+  std::vector<uint16_t> _redir_table;
   rss_key_type _rss_key;
+  struct rte_flow *_flow = nullptr;
   bool _is_i40e_device = false;
   bool _is_vmxnet3_device = false;
 
@@ -848,6 +824,8 @@ class DPDKDevice {
   }
 
   ~DPDKDevice() {
+    if (_flow)
+       rte_flow_destroy(_port_idx, _flow, nullptr);
     rte_eth_dev_stop(_port_idx);
   }
 
@@ -858,7 +836,7 @@ class DPDKDevice {
   subscription<Packet> receive(unsigned cpuid, std::function<int (Packet)> next_packet) {
     auto sub = _queues[cpuid]->_rx_stream.listen(std::move(next_packet));
     _queues[cpuid]->rx_start();
-    return std::move(sub);
+    return sub;
   }
   ethernet_address hw_address() {
     struct ether_addr mac;
@@ -874,18 +852,18 @@ class DPDKDevice {
   std::unique_ptr<DPDKQueuePair> init_local_queue(CephContext *c, EventCenter *center, string hugepages, uint16_t qid) {
     std::unique_ptr<DPDKQueuePair> qp;
     qp = std::unique_ptr<DPDKQueuePair>(new DPDKQueuePair(c, center, this, qid));
-    return std::move(qp);
+    return qp;
   }
   unsigned hash2qid(uint32_t hash) {
     // return hash % hw_queues_count();
     return _redir_table[hash & (_redir_table.size() - 1)];
   }
   void set_local_queue(unsigned i, std::unique_ptr<DPDKQueuePair> qp) {
-    assert(!_queues[i]);
+    ceph_assert(!_queues[i]);
     _queues[i] = std::move(qp);
   }
   void unset_local_queue(unsigned i) {
-    assert(_queues[i]);
+    ceph_assert(_queues[i]);
     _queues[i].reset();
   }
   template <typename Func>
@@ -894,7 +872,7 @@ class DPDKDevice {
     if (!qp._sw_reta)
       return src_cpuid;
 
-    assert(!qp._sw_reta);
+    ceph_assert(!qp._sw_reta);
     auto hash = hashfn() >> _rss_table_bits;
     auto& reta = *qp._sw_reta;
     return reta[hash % reta.size()];
diff --git a/src/msg/async/dpdk/DPDKStack.cc b/src/msg/async/dpdk/DPDKStack.cc
index e828789a51d..9a73dac5db1 100644
--- a/src/msg/async/dpdk/DPDKStack.cc
+++ b/src/msg/async/dpdk/DPDKStack.cc
@@ -41,13 +41,15 @@
 #include <tuple>
 
 #include "common/ceph_argparse.h"
+#include "dpdk_rte.h"
 #include "DPDKStack.h"
 #include "DPDK.h"
 #include "IP.h"
 #include "TCP-Stack.h"
 
 #include "common/dout.h"
-#include "include/assert.h"
+#include "include/ceph_assert.h"
+#include "common/Cond.h"
 
 #define dout_subsys ceph_subsys_dpdk
 #undef dout_prefix
@@ -66,8 +68,8 @@ void DPDKWorker::initialize()
     WAIT_PORT_FIN_STAGE,
     DONE
   } create_stage = WAIT_DEVICE_STAGE;
-  static Mutex lock("DPDKStack::lock");
-  static Cond cond;
+  static ceph::mutex lock = ceph::make_mutex("DPDKStack::lock");
+  static ceph::condition_variable cond;
   static unsigned queue_init_done = 0;
   static unsigned cores = 0;
   static std::shared_ptr<DPDKDevice> sdev;
@@ -85,15 +87,14 @@ void DPDKWorker::initialize()
     sdev->workers.resize(cores);
     ldout(cct, 1) << __func__ << " using " << cores << " cores " << dendl;
 
-    Mutex::Locker l(lock);
+    std::lock_guard l{lock};
     create_stage = WAIT_PORT_FIN_STAGE;
-    cond.Signal();
+    cond.notify_all();
   } else {
-    Mutex::Locker l(lock);
-    while (create_stage <= WAIT_DEVICE_STAGE)
-      cond.Wait(lock);
+    std::unique_lock l{lock};
+    cond.wait(l, [] { return create_stage > WAIT_DEVICE_STAGE; });
   }
-  assert(sdev);
+  ceph_assert(sdev);
   if (i < sdev->hw_queues_count()) {
     auto qp = sdev->init_local_queue(cct, &center, cct->_conf->ms_dpdk_hugepages, i);
     std::map<unsigned, float> cpu_weights;
@@ -103,9 +104,9 @@ void DPDKWorker::initialize()
     cpu_weights[i] = cct->_conf->ms_dpdk_hw_queue_weight;
     qp->configure_proxies(cpu_weights);
     sdev->set_local_queue(i, std::move(qp));
-    Mutex::Locker l(lock);
+    std::lock_guard l{lock};
     ++queue_init_done;
-    cond.Signal();
+    cond.notify_all();
   } else {
     // auto master = qid % sdev->hw_queues_count();
     // sdev->set_local_queue(create_proxy_net_device(master, sdev.get()));
@@ -113,29 +114,27 @@ void DPDKWorker::initialize()
   }
   if (i == 0) {
     {
-      Mutex::Locker l(lock);
-      while (queue_init_done < cores)
-        cond.Wait(lock);
+      std::unique_lock l{lock};
+      cond.wait(l, [] { return queue_init_done >= cores; });
     }
 
     if (sdev->init_port_fini() < 0) {
       lderr(cct) << __func__ << " init_port_fini failed " << dendl;
       ceph_abort();
     }
-    Mutex::Locker l(lock);
+    std::lock_guard l{lock};
     create_stage = DONE;
-    cond.Signal();
+    cond.notify_all();
   } else {
-    Mutex::Locker l(lock);
-    while (create_stage <= WAIT_PORT_FIN_STAGE)
-      cond.Wait(lock);
+    std::unique_lock  l{lock};
+    cond.wait(l, [&] { return create_stage > WAIT_PORT_FIN_STAGE; });
   }
 
   sdev->workers[i] = this;
   _impl = std::unique_ptr<DPDKWorker::Impl>(
           new DPDKWorker::Impl(cct, i, &center, sdev));
   {
-    Mutex::Locker l(lock);
+    std::lock_guard l{lock};
     if (!--queue_init_done) {
       create_stage = WAIT_DEVICE_STAGE;
       sdev.reset();
@@ -180,14 +179,14 @@ DPDKWorker::Impl::Impl(CephContext *cct, unsigned i, EventCenter *c, std::shared
     : id(i), _netif(cct, dev, c), _dev(dev), _inet(cct, c, &_netif)
 {
   vector<AvailableIPAddress> tuples;
-  bool parsed = parse_available_address(cct->_conf->get_val<std::string>("ms_dpdk_host_ipv4_addr"),
-                                        cct->_conf->get_val<std::string>("ms_dpdk_gateway_ipv4_addr"),
-                                        cct->_conf->get_val<std::string>("ms_dpdk_netmask_ipv4_addr"), tuples);
+  bool parsed = parse_available_address(cct->_conf.get_val<std::string>("ms_dpdk_host_ipv4_addr"),
+                                        cct->_conf.get_val<std::string>("ms_dpdk_gateway_ipv4_addr"),
+                                        cct->_conf.get_val<std::string>("ms_dpdk_netmask_ipv4_addr"), tuples);
   if (!parsed) {
     lderr(cct) << __func__ << " no available address "
-               << cct->_conf->get_val<std::string>("ms_dpdk_host_ipv4_addr") << ", "
-               << cct->_conf->get_val<std::string>("ms_dpdk_gateway_ipv4_addr") << ", "
-               << cct->_conf->get_val<std::string>("ms_dpdk_netmask_ipv4_addr") << ", "
+               << cct->_conf.get_val<std::string>("ms_dpdk_host_ipv4_addr") << ", "
+               << cct->_conf.get_val<std::string>("ms_dpdk_gateway_ipv4_addr") << ", "
+               << cct->_conf.get_val<std::string>("ms_dpdk_netmask_ipv4_addr") << ", "
                << dendl;
     ceph_abort();
   }
@@ -196,11 +195,18 @@ DPDKWorker::Impl::Impl(CephContext *cct, unsigned i, EventCenter *c, std::shared
   _inet.set_netmask_address(ipv4_address(std::get<2>(tuples[0])));
 }
 
-int DPDKWorker::listen(entity_addr_t &sa, const SocketOptions &opt,
+DPDKWorker::Impl::~Impl()
+{
+  _dev->unset_local_queue(id);
+}
+
+int DPDKWorker::listen(entity_addr_t &sa,
+		       unsigned addr_slot,
+		       const SocketOptions &opt,
                        ServerSocket *sock)
 {
-  assert(sa.get_family() == AF_INET);
-  assert(sock);
+  ceph_assert(sa.get_family() == AF_INET);
+  ceph_assert(sock);
 
   ldout(cct, 10) << __func__ << " addr " << sa << dendl;
   // vector<AvailableIPAddress> tuples;
@@ -224,12 +230,13 @@ int DPDKWorker::listen(entity_addr_t &sa, const SocketOptions &opt,
   // _inet.set_host_address(ipv4_address(std::get<0>(tuples[idx])));
   // _inet.set_gw_address(ipv4_address(std::get<1>(tuples[idx])));
   // _inet.set_netmask_address(ipv4_address(std::get<2>(tuples[idx])));
-  return tcpv4_listen(_impl->_inet.get_tcp(), sa.get_port(), opt, sock);
+  return tcpv4_listen(_impl->_inet.get_tcp(), sa.get_port(), opt, sa.get_type(),
+		      addr_slot, sock);
 }
 
 int DPDKWorker::connect(const entity_addr_t &addr, const SocketOptions &opts, ConnectedSocket *socket)
 {
-  // assert(addr.get_family() == AF_INET);
+  // ceph_assert(addr.get_family() == AF_INET);
   int r =  tcpv4_connect(_impl->_inet.get_tcp(), addr, socket);
   ldout(cct, 10) << __func__ << " addr " << addr << dendl;
   return r;
@@ -248,12 +255,26 @@ void DPDKStack::spawn_worker(unsigned i, std::function<void ()> &&func)
   }
   // if dpdk::eal::init already called by NVMEDevice, we will select 1..n
   // cores
-  assert(rte_lcore_count() >= i + 1);
+  ceph_assert(rte_lcore_count() >= i + 1);
+  unsigned core_id;
+  int j = i;
+  RTE_LCORE_FOREACH_SLAVE(core_id) {
+    if (i-- == 0) {
+      break;
+    }
+  }
   dpdk::eal::execute_on_master([&]() {
-    r = rte_eal_remote_launch(dpdk_thread_adaptor, static_cast<void*>(&funcs[i]), i+1);
+    r = rte_eal_remote_launch(dpdk_thread_adaptor, static_cast<void*>(&funcs[j]), core_id);
     if (r < 0) {
       lderr(cct) << __func__ << " remote launch failed, r=" << r << dendl;
       ceph_abort();
     }
   });
 }
+
+void DPDKStack::join_worker(unsigned i)
+{
+  dpdk::eal::execute_on_master([&]() {
+    rte_eal_wait_lcore(i+1);
+  });
+}
diff --git a/src/msg/async/dpdk/DPDKStack.h b/src/msg/async/dpdk/DPDKStack.h
index af5a5fd2400..37626bee492 100644
--- a/src/msg/async/dpdk/DPDKStack.h
+++ b/src/msg/async/dpdk/DPDKStack.h
@@ -21,8 +21,6 @@
 #include "common/Tub.h"
 
 #include "msg/async/Stack.h"
-#include "dpdk_rte.h"
-#include "DPDK.h"
 #include "net.h"
 #include "const.h"
 #include "IP.h"
@@ -38,7 +36,8 @@ template <typename Protocol>
 class DPDKServerSocketImpl : public ServerSocketImpl {
   typename Protocol::listener _listener;
  public:
-  DPDKServerSocketImpl(Protocol& proto, uint16_t port, const SocketOptions &opt);
+  DPDKServerSocketImpl(Protocol& proto, uint16_t port, const SocketOptions &opt,
+		       int type, unsigned addr_slot);
   int listen() {
     return _listener.listen();
   }
@@ -98,7 +97,8 @@ class NativeConnectedSocketImpl : public ConnectedSocketImpl {
     return len - left ? len - left : -EAGAIN;
   }
 
-  virtual ssize_t zero_copy_read(bufferptr &data) override {
+private:
+  ssize_t zero_copy_read(bufferptr &data) {
     auto err = _conn.get_errno();
     if (err <= 0)
       return err;
@@ -122,7 +122,7 @@ class NativeConnectedSocketImpl : public ConnectedSocketImpl {
     } else {
       _cur_off += f.size;
     }
-    assert(data.length());
+    ceph_assert(data.length());
     return data.length();
   }
   virtual ssize_t send(bufferlist &bl, bool more) override {
@@ -136,18 +136,22 @@ class NativeConnectedSocketImpl : public ConnectedSocketImpl {
     }
 
     std::vector<fragment> frags;
-    std::list<bufferptr>::const_iterator pb = bl.buffers().begin();
-    uint64_t left_pbrs = bl.buffers().size();
+    auto pb = bl.buffers().begin();
     uint64_t len = 0;
     uint64_t seglen = 0;
-    while (len < available && left_pbrs--) {
+    while (len < available && pb != bl.buffers().end()) {
       seglen = pb->length();
+      // Buffer length is zero, no need to send, so skip it
+      if (seglen == 0) {
+        ++pb;
+        continue;
+      }
       if (len + seglen > available) {
         // don't continue if we enough at least 1 fragment since no available
         // space for next ptr.
         if (len > 0)
           break;
-        seglen = MIN(seglen, available);
+        seglen = std::min(seglen, available);
       }
       len += seglen;
       frags.push_back(fragment{(char*)pb->c_str(), seglen});
@@ -167,6 +171,8 @@ class NativeConnectedSocketImpl : public ConnectedSocketImpl {
       return _conn.send(Packet(std::move(frags), make_deleter(std::move(del))));
     }
   }
+
+public:
   virtual void shutdown() override {
     _conn.close_write();
   }
@@ -181,8 +187,9 @@ class NativeConnectedSocketImpl : public ConnectedSocketImpl {
 
 template <typename Protocol>
 DPDKServerSocketImpl<Protocol>::DPDKServerSocketImpl(
-        Protocol& proto, uint16_t port, const SocketOptions &opt)
-        : _listener(proto.listen(port)) {}
+  Protocol& proto, uint16_t port, const SocketOptions &opt,
+  int type, unsigned addr_slot)
+  : ServerSocketImpl(type, addr_slot), _listener(proto.listen(port)) {}
 
 template <typename Protocol>
 int DPDKServerSocketImpl<Protocol>::accept(ConnectedSocket *s, const SocketOptions &options, entity_addr_t *out, Worker *w) {
@@ -192,8 +199,10 @@ int DPDKServerSocketImpl<Protocol>::accept(ConnectedSocket *s, const SocketOptio
   if (!c)
     return -EAGAIN;
 
-  if (out)
+  if (out) {
     *out = c->remote_addr();
+    out->set_type(addr_type);
+  }
   std::unique_ptr<NativeConnectedSocketImpl<Protocol>> csi(
           new NativeConnectedSocketImpl<Protocol>(std::move(*c)));
   *s = ConnectedSocket(std::move(csi));
@@ -212,13 +221,11 @@ class DPDKWorker : public Worker {
     std::shared_ptr<DPDKDevice> _dev;
     ipv4 _inet;
     Impl(CephContext *cct, unsigned i, EventCenter *c, std::shared_ptr<DPDKDevice> dev);
-    ~Impl() {
-      _dev->unset_local_queue(id);
-    }
+    ~Impl();
   };
   std::unique_ptr<Impl> _impl;
 
-  virtual void initialize();
+  virtual void initialize() override;
   void set_ipv4_packet_filter(ip_packet_filter* filter) {
     _impl->_inet.set_packet_filter(filter);
   }
@@ -226,7 +233,8 @@ class DPDKWorker : public Worker {
 
  public:
   explicit DPDKWorker(CephContext *c, unsigned i): Worker(c, i) {}
-  virtual int listen(entity_addr_t &addr, const SocketOptions &opts, ServerSocket *) override;
+  virtual int listen(entity_addr_t &addr, unsigned addr_slot,
+		     const SocketOptions &opts, ServerSocket *) override;
   virtual int connect(const entity_addr_t &addr, const SocketOptions &opts, ConnectedSocket *socket) override;
   void arp_learn(ethernet_address l2, ipv4_address l3) {
     _impl->_inet.learn(l2, l3);
@@ -244,15 +252,10 @@ class DPDKStack : public NetworkStack {
   explicit DPDKStack(CephContext *cct, const string &t): NetworkStack(cct, t) {
     funcs.resize(cct->_conf->ms_async_max_op_threads);
   }
-  virtual bool support_zero_copy_read() const override { return true; }
-  virtual bool support_local_listen_table() const { return true; }
+  virtual bool support_local_listen_table() const override { return true; }
 
   virtual void spawn_worker(unsigned i, std::function<void ()> &&func) override;
-  virtual void join_worker(unsigned i) override {
-    dpdk::eal::execute_on_master([&]() {
-      rte_eal_wait_lcore(i+1);
-    });
-  }
+  virtual void join_worker(unsigned i) override;
 };
 
 #endif
diff --git a/src/msg/async/dpdk/EventDPDK.cc b/src/msg/async/dpdk/EventDPDK.cc
index 0a80bc7a02d..5d291716c71 100644
--- a/src/msg/async/dpdk/EventDPDK.cc
+++ b/src/msg/async/dpdk/EventDPDK.cc
@@ -19,7 +19,7 @@
 #include "EventDPDK.h"
 
 #include "common/dout.h"
-#include "include/assert.h"
+#include "include/ceph_assert.h"
 
 #define dout_subsys ceph_subsys_ms
 
diff --git a/src/msg/async/dpdk/EventDPDK.h b/src/msg/async/dpdk/EventDPDK.h
index 3e2aeb0e39e..541c2210e01 100644
--- a/src/msg/async/dpdk/EventDPDK.h
+++ b/src/msg/async/dpdk/EventDPDK.h
@@ -26,7 +26,7 @@ class DPDKDriver : public EventDriver {
  public:
   UserspaceEventManager manager;
 
-  DPDKDriver(CephContext *c): cct(c), manager(c) {}
+  explicit DPDKDriver(CephContext *c): cct(c), manager(c) {}
   virtual ~DPDKDriver() { }
 
   int init(EventCenter *c, int nevent) override;
@@ -34,7 +34,7 @@ class DPDKDriver : public EventDriver {
   int del_event(int fd, int cur_mask, int del_mask) override;
   int resize_events(int newsize) override;
   int event_wait(vector<FiredFileEvent> &fired_events, struct timeval *tp) override;
-  bool need_wakeup() { return false; }
+  bool need_wakeup() override { return false; }
 };
 
 #endif //CEPH_EVENTDPDK_H
diff --git a/src/msg/async/dpdk/IP.cc b/src/msg/async/dpdk/IP.cc
index 9f36d2eafff..fab534bb25b 100644
--- a/src/msg/async/dpdk/IP.cc
+++ b/src/msg/async/dpdk/IP.cc
@@ -38,11 +38,10 @@
 
 #include "capture.h"
 #include "IP.h"
-#include "shared_ptr.h"
 #include "toeplitz.h"
 
 #include "common/dout.h"
-#include "include/assert.h"
+#include "include/ceph_assert.h"
 
 #define dout_subsys ceph_subsys_dpdk
 #undef dout_prefix
@@ -63,7 +62,7 @@ class C_handle_frag_timeout : public EventCallback {
 
  public:
   C_handle_frag_timeout(ipv4 *i): _ipv4(i) {}
-  void do_request(int fd_or_id) {
+  void do_request(uint64_t fd_or_id) {
     _ipv4->frag_timeout();
   }
 };
@@ -74,6 +73,17 @@ enum {
   l_dpdk_qp_last
 };
 
+struct icmp_hdr {
+  enum class msg_type : uint8_t {
+    echo_reply = 0,
+    echo_request = 8,
+  };
+  msg_type type;
+  uint8_t code;
+  uint16_t csum;
+  uint32_t rest;
+} __attribute__((packed));
+
 ipv4::ipv4(CephContext *c, EventCenter *cen, interface* netif)
   : cct(c), center(cen), _netif(netif), _global_arp(netif),
     _arp(c, _global_arp, cen),
diff --git a/src/msg/async/dpdk/IP.h b/src/msg/async/dpdk/IP.h
index d5a4d305481..1fc60658235 100644
--- a/src/msg/async/dpdk/IP.h
+++ b/src/msg/async/dpdk/IP.h
@@ -20,19 +20,6 @@
  * Copyright (C) 2014 Cloudius Systems, Ltd.
  *
  */
-/*
- * Ceph - scalable distributed file system
- *
- * Copyright (C) 2015 XSky <haomai@xsky.com>
- *
- * Author: Haomai Wang <haomaiwang@gmail.com>
- *
- * This is free software; you can redistribute it and/or
- * modify it under the terms of the GNU Lesser General Public
- * License version 2.1, as published by the Free Software
- * Foundation.  See file COPYING.
- *
- */
 
 #ifndef CEPH_MSG_IP_H_
 #define CEPH_MSG_IP_H_
@@ -131,22 +118,11 @@ class ipv4_tcp final : public ip_protocol {
  public:
   ipv4_tcp(ipv4& inet, EventCenter *c);
   ~ipv4_tcp();
-  virtual void received(Packet p, ipv4_address from, ipv4_address to);
+  virtual void received(Packet p, ipv4_address from, ipv4_address to) override;
   virtual bool forward(forward_hash& out_hash_data, Packet& p, size_t off) override;
   friend class ipv4;
 };
 
-struct icmp_hdr {
-  enum class msg_type : uint8_t {
-    echo_reply = 0,
-    echo_request = 8,
-  };
-  msg_type type;
-  uint8_t code;
-  uint16_t csum;
-  uint32_t rest;
-} __attribute__((packed));
-
 
 class icmp {
  public:
@@ -180,7 +156,7 @@ class ipv4_icmp final : public ip_protocol {
   icmp _icmp;
  public:
   ipv4_icmp(CephContext *c, ipv4& inet) : cct(c), _inet_l4(inet), _icmp(c, _inet_l4) {}
-  virtual void received(Packet p, ipv4_address from, ipv4_address to) {
+  virtual void received(Packet p, ipv4_address from, ipv4_address to) override {
     _icmp.received(std::move(p), from, to);
   }
   friend class ipv4;
diff --git a/src/msg/async/dpdk/IPChecksum.cc b/src/msg/async/dpdk/IPChecksum.cc
index 5048a6902b2..7a3253c1e9b 100644
--- a/src/msg/async/dpdk/IPChecksum.cc
+++ b/src/msg/async/dpdk/IPChecksum.cc
@@ -19,19 +19,6 @@
 /*
  * Copyright (C) 2014 Cloudius Systems, Ltd.
  */
-/*
- * Ceph - scalable distributed file system
- *
- * Copyright (C) 2015 XSky <haomai@xsky.com>
- *
- * Author: Haomai Wang <haomaiwang@gmail.com>
- *
- * This is free software; you can redistribute it and/or
- * modify it under the terms of the GNU Lesser General Public
- * License version 2.1, as published by the Free Software
- * Foundation.  See file COPYING.
- *
- */
 
 #include <arpa/inet.h>
 #include "net.h"
diff --git a/src/msg/async/dpdk/IPChecksum.h b/src/msg/async/dpdk/IPChecksum.h
index a1826dfdfe7..9af4a86b9c2 100644
--- a/src/msg/async/dpdk/IPChecksum.h
+++ b/src/msg/async/dpdk/IPChecksum.h
@@ -19,19 +19,6 @@
 /*
  * Copyright (C) 2014 Cloudius Systems, Ltd.
  */
-/*
- * Ceph - scalable distributed file system
- *
- * Copyright (C) 2015 XSky <haomai@xsky.com>
- *
- * Author: Haomai Wang <haomaiwang@gmail.com>
- *
- * This is free software; you can redistribute it and/or
- * modify it under the terms of the GNU Lesser General Public
- * License version 2.1, as published by the Free Software
- * Foundation.  See file COPYING.
- *
- */
 
 #ifndef CEPH_MSG_CHECKSUM_H_
 #define CEPH_MSG_CHECKSUM_H_
diff --git a/src/msg/async/dpdk/Packet.cc b/src/msg/async/dpdk/Packet.cc
index eb4a5dc4ab1..6c2320a01ae 100644
--- a/src/msg/async/dpdk/Packet.cc
+++ b/src/msg/async/dpdk/Packet.cc
@@ -79,7 +79,7 @@ class C_free_on_cpu : public EventCallback {
  public:
   C_free_on_cpu(deleter &&d, std::function<void()> &&c):
       del(std::move(d)), cb(std::move(c)) {}
-  void do_request(int fd) {
+  void do_request(uint64_t fd) {
     // deleter needs to be moved from lambda capture to be destroyed here
     // otherwise deleter destructor will be called on a cpu that called
     // create_external_event when work_item is destroyed.
diff --git a/src/msg/async/dpdk/Packet.h b/src/msg/async/dpdk/Packet.h
index b8134573fa3..f929da31786 100644
--- a/src/msg/async/dpdk/Packet.h
+++ b/src/msg/async/dpdk/Packet.h
@@ -19,20 +19,6 @@
 /*
  * Copyright (C) 2014 Cloudius Systems, Ltd.
  */
-/*
- * Ceph - scalable distributed file system
- *
- * Copyright (C) 2015 XSky <haomai@xsky.com>
- *
- * Author: Haomai Wang <haomaiwang@gmail.com>
- *
- * This is free software; you can redistribute it and/or
- * modify it under the terms of the GNU Lesser General Public
- * License version 2.1, as published by the Free Software
- * Foundation.  See file COPYING.
- *
- */
-
 
 #ifndef CEPH_MSG_PACKET_H_
 #define CEPH_MSG_PACKET_H_
@@ -118,14 +104,14 @@ class Packet {
 
     fragment frags[];
 
-    impl(size_t nr_frags = default_nr_frags);
+    explicit impl(size_t nr_frags = default_nr_frags);
     impl(const impl&) = delete;
     impl(fragment frag, size_t nr_frags = default_nr_frags);
 
     pseudo_vector fragments() { return { frags, _nr_frags }; }
 
     static std::unique_ptr<impl> allocate(size_t nr_frags) {
-      nr_frags = MAX(nr_frags, default_nr_frags);
+      nr_frags = std::max(nr_frags, default_nr_frags);
       return std::unique_ptr<impl>(new (nr_frags) impl(nr_frags));
     }
 
@@ -139,7 +125,7 @@ class Packet {
       n->rss_hash.construct(old->rss_hash);
       std::copy(old->frags, old->frags + old->_nr_frags, n->frags);
       old->copy_internal_fragment_to(n.get());
-      return std::move(n);
+      return n;
     }
 
     static std::unique_ptr<impl> copy(impl* old) {
@@ -148,12 +134,12 @@ class Packet {
 
     static std::unique_ptr<impl> allocate_if_needed(std::unique_ptr<impl> old, size_t extra_frags) {
       if (old->_allocated_frags >= old->_nr_frags + extra_frags) {
-        return std::move(old);
+        return old;
       }
       return copy(old.get(), std::max<size_t>(old->_nr_frags + extra_frags, 2 * old->_nr_frags));
     }
     void* operator new(size_t size, size_t nr_frags = default_nr_frags) {
-      assert(nr_frags == uint16_t(nr_frags));
+      ceph_assert(nr_frags == uint16_t(nr_frags));
       return ::operator new(size + nr_frags * sizeof(fragment));
     }
     // Matching the operator new above
@@ -194,7 +180,7 @@ class Packet {
               to->frags[0].base);
     }
   };
-  Packet(std::unique_ptr<impl>&& impl) : _impl(std::move(impl)) {}
+  explicit Packet(std::unique_ptr<impl>&& impl) : _impl(std::move(impl)) {}
   std::unique_ptr<impl> _impl;
 public:
   static Packet from_static_data(const char* data, size_t len) {
@@ -204,13 +190,13 @@ public:
   // build empty Packet
   Packet();
   // build empty Packet with nr_frags allocated
-  Packet(size_t nr_frags);
+  explicit Packet(size_t nr_frags);
   // move existing Packet
   Packet(Packet&& x) noexcept;
   // copy data into Packet
   Packet(const char* data, size_t len);
   // copy data into Packet
-  Packet(fragment frag);
+  explicit Packet(fragment frag);
   // zero-copy single fragment
   Packet(fragment frag, deleter del);
   // zero-copy multiple fragments
@@ -309,7 +295,7 @@ inline Packet::impl::impl(size_t nr_frags)
 
 inline Packet::impl::impl(fragment frag, size_t nr_frags)
     : _len(frag.size), _allocated_frags(nr_frags) {
-    assert(_allocated_frags > _nr_frags);
+    ceph_assert(_allocated_frags > _nr_frags);
   if (frag.size <= internal_data_size) {
     headroom -= frag.size;
     frags[0] = { data + headroom, frag.size };
@@ -472,7 +458,7 @@ inline Header* Packet::get_header(size_t offset) {
 }
 
 inline void Packet::trim_front(size_t how_much) {
-  assert(how_much <= _impl->_len);
+  ceph_assert(how_much <= _impl->_len);
   _impl->_len -= how_much;
   size_t i = 0;
   while (how_much && how_much >= _impl->frags[i].size) {
@@ -493,7 +479,7 @@ inline void Packet::trim_front(size_t how_much) {
 }
 
 inline void Packet::trim_back(size_t how_much) {
-  assert(how_much <= _impl->_len);
+  ceph_assert(how_much <= _impl->_len);
   _impl->_len -= how_much;
   size_t i = _impl->_nr_frags - 1;
   while (how_much && how_much >= _impl->frags[i].size) {
@@ -556,7 +542,7 @@ inline Packet Packet::share(size_t offset, size_t len) {
     offset = 0;
   }
   n._impl->_offload_info = _impl->_offload_info;
-  assert(!n._impl->_deleter);
+  ceph_assert(!n._impl->_deleter);
   n._impl->_deleter = _impl->_deleter.share();
   return n;
 }
diff --git a/src/msg/async/dpdk/PacketUtil.h b/src/msg/async/dpdk/PacketUtil.h
index db0ea143bb2..118218e6633 100644
--- a/src/msg/async/dpdk/PacketUtil.h
+++ b/src/msg/async/dpdk/PacketUtil.h
@@ -19,19 +19,6 @@
 /*
  * Copyright (C) 2014 Cloudius Systems, Ltd.
  */
-/*
- * Ceph - scalable distributed file system
- *
- * Copyright (C) 2015 XSky <haomai@xsky.com>
- *
- * Author: Haomai Wang <haomaiwang@gmail.com>
- *
- * This is free software; you can redistribute it and/or
- * modify it under the terms of the GNU Lesser General Public
- * License version 2.1, as published by the Free Software
- * Foundation.  See file COPYING.
- *
- */
 
 #ifndef CEPH_MSG_PACKET_UTIL_H_
 #define CEPH_MSG_PACKET_UTIL_H_
@@ -59,7 +46,7 @@ class packet_merger {
     bool insert = true;
     auto beg = offset;
     auto end = beg + p.len();
-    // Fisrt, try to merge the packet with existing segment
+    // First, try to merge the packet with existing segment
     for (auto it = map.begin(); it != map.end();) {
       auto& seg_pkt = it->second;
       auto seg_beg = it->first;
@@ -114,7 +101,7 @@ class packet_merger {
     }
 
     // Second, merge adjacent segments after this packet has been merged,
-    // becasue this packet might fill a "whole" and make two adjacent
+    // because this packet might fill a "whole" and make two adjacent
     // segments mergable
     for (auto it = map.begin(); it != map.end();) {
       // The first segment
@@ -159,7 +146,6 @@ class packet_merger {
         // If we reach here, we have a bug with merge.
         std::cout << "packet_merger: merge error\n";
         abort();
-        break;
       }
     }
   }
diff --git a/src/msg/async/dpdk/TCP-Stack.h b/src/msg/async/dpdk/TCP-Stack.h
index 86872560c6e..edcf4d80344 100644
--- a/src/msg/async/dpdk/TCP-Stack.h
+++ b/src/msg/async/dpdk/TCP-Stack.h
@@ -32,7 +32,7 @@ template <typename InetTraits>
 class tcp;
 
 int tcpv4_listen(tcp<ipv4_traits>& tcpv4, uint16_t port, const SocketOptions &opts,
-                 ServerSocket *sa);
+                 int type, unsigned addr_slot, ServerSocket *sa);
 
 int tcpv4_connect(tcp<ipv4_traits>& tcpv4, const entity_addr_t &addr,
                   ConnectedSocket *sa);
diff --git a/src/msg/async/dpdk/TCP.cc b/src/msg/async/dpdk/TCP.cc
index 3fd8341e751..26f29e10f75 100644
--- a/src/msg/async/dpdk/TCP.cc
+++ b/src/msg/async/dpdk/TCP.cc
@@ -26,7 +26,7 @@
 #include "DPDKStack.h"
 
 #include "common/dout.h"
-#include "include/assert.h"
+#include "include/ceph_assert.h"
 
 #define dout_subsys ceph_subsys_dpdk
 #undef dout_prefix
@@ -112,7 +112,7 @@ uint8_t tcp_option::fill(tcp_hdr* th, uint8_t options_size)
     new (off) tcp_option::eol;
     size += option_len::eol;
   }
-  assert(size == options_size);
+  ceph_assert(size == options_size);
 
   return size;
 }
@@ -153,9 +153,10 @@ bool ipv4_tcp::forward(forward_hash& out_hash_data, Packet& p, size_t off)
 }
 
 int tcpv4_listen(tcp<ipv4_traits>& tcpv4, uint16_t port, const SocketOptions &opts,
-                 ServerSocket *sock)
+                 int type, unsigned addr_slot, ServerSocket *sock)
 {
-  auto p = new DPDKServerSocketImpl<tcp<ipv4_traits>>(tcpv4, port, opts);
+  auto p = new DPDKServerSocketImpl<tcp<ipv4_traits>>(tcpv4, port, opts,
+						      type, addr_slot);
   int r = p->listen();
   if (r < 0) {
     delete p;
diff --git a/src/msg/async/dpdk/TCP.h b/src/msg/async/dpdk/TCP.h
index 9923fab544d..a0104fb44f4 100644
--- a/src/msg/async/dpdk/TCP.h
+++ b/src/msg/async/dpdk/TCP.h
@@ -29,18 +29,15 @@
 #include <functional>
 #include <deque>
 #include <chrono>
-#include <random>
 #include <stdexcept>
 #include <system_error>
 
-#define CRYPTOPP_ENABLE_NAMESPACE_WEAK 1
-#include <cryptopp/md5.h>
-
 #include "msg/async/dpdk/EventDPDK.h"
 
 #include "include/utime.h"
 #include "common/Throttle.h"
 #include "common/ceph_time.h"
+#include "common/ceph_crypto.h"
 #include "msg/async/Event.h"
 #include "IPChecksum.h"
 #include "IP.h"
@@ -49,6 +46,8 @@
 #include "shared_ptr.h"
 #include "PacketUtil.h"
 
+#include "include/random.h"
+
 struct tcp_hdr;
 
 enum class tcp_state : uint16_t {
@@ -234,7 +233,8 @@ class tcp {
 
    public:
     C_handle_delayed_ack(tcb *t): tc(t) { }
-    void do_request(int r) {
+    void do_request(uint64_t r) {
+      tc->_delayed_ack_fd.destroy();
       tc->_nr_full_seg_received = 0;
       tc->output();
     }
@@ -245,7 +245,8 @@ class tcp {
 
    public:
     C_handle_retransmit(tcb *t): tc(t) { }
-    void do_request(int r) {
+    void do_request(uint64_t r) {
+      tc->retransmit_fd.destroy();
       tc->retransmit();
     }
   };
@@ -255,7 +256,8 @@ class tcp {
 
    public:
     C_handle_persist(tcb *t): tc(t) { }
-    void do_request(int r) {
+    void do_request(uint64_t r) {
+      tc->persist_fd.destroy();
       tc->persist();
     }
   };
@@ -265,7 +267,7 @@ class tcp {
 
    public:
     C_all_data_acked(tcb *t): tc(t) {}
-    void do_request(int fd_or_id) {
+    void do_request(uint64_t fd_or_id) {
       tc->close_final_cleanup();
     }
   };
@@ -274,7 +276,7 @@ class tcp {
     lw_shared_ptr<tcb> tc;
    public:
     C_actual_remove_tcb(tcb *t): tc(t->shared_from_this()) {}
-    void do_request(int r) {
+    void do_request(uint64_t r) {
       delete this;
     }
   };
@@ -381,11 +383,8 @@ class tcp {
       // 512 bits secretkey for ISN generating
       uint32_t key[16];
       isn_secret () {
-        std::random_device rd;
-        std::default_random_engine e(rd());
-        std::uniform_int_distribution<uint32_t> dist{};
         for (auto& k : key) {
-          k = dist(e);
+          k = ceph::util::generate_random_number<uint32_t>(0, std::numeric_limits<uint32_t>::max());
         }
       }
     };
@@ -1067,7 +1066,7 @@ Packet tcp<InetTraits>::tcb::get_transmit_packet() {
   // Max number of TCP payloads we can pass to NIC
   uint32_t len;
   if (_tcp.get_hw_features().tx_tso) {
-    // FIXME: Info tap device the size of the splitted packet
+    // FIXME: Info tap device the size of the split packet
     len = _tcp.get_hw_features().max_packet_len - tcp_hdr_len_min - InetTraits::ip_hdr_len_min;
   } else {
     len = std::min(uint16_t(_tcp.get_hw_features().mtu - tcp_hdr_len_min - InetTraits::ip_hdr_len_min), _snd.mss);
@@ -1237,7 +1236,7 @@ Tub<Packet> tcp<InetTraits>::tcb::read() {
 template <typename InetTraits>
 int tcp<InetTraits>::tcb::send(Packet p) {
   // We can not send after the connection is closed
-  assert(!_snd.closed);
+  ceph_assert(!_snd.closed);
 
   if (in_state(CLOSED))
     return -ECONNRESET;
@@ -1443,7 +1442,9 @@ tcp_sequence tcp<InetTraits>::tcb::get_isn() {
   hash[1] = _foreign_ip.ip;
   hash[2] = (_local_port << 16) + _foreign_port;
   hash[3] = _isn_secret.key[15];
-  CryptoPP::Weak::MD5::Transform(hash, _isn_secret.key);
+  ceph::crypto::MD5 md5;
+  md5.Update((const unsigned char*)_isn_secret.key, sizeof(_isn_secret.key));
+  md5.Final((unsigned char*)hash);
   auto seq = hash[0];
   auto m = duration_cast<microseconds>(clock_type::now().time_since_epoch());
   seq += m.count() / 4;
@@ -1462,7 +1463,7 @@ Tub<typename InetTraits::l4packet> tcp<InetTraits>::tcb::get_packet() {
     return p;
   }
 
-  assert(!_packetq.empty());
+  ceph_assert(!_packetq.empty());
 
   p = std::move(_packetq.front());
   _packetq.pop_front();
diff --git a/src/msg/async/dpdk/UserspaceEvent.cc b/src/msg/async/dpdk/UserspaceEvent.cc
index 25b082d613b..282dcef12f6 100644
--- a/src/msg/async/dpdk/UserspaceEvent.cc
+++ b/src/msg/async/dpdk/UserspaceEvent.cc
@@ -16,7 +16,7 @@
 #include "UserspaceEvent.h"
 
 #include "common/dout.h"
-#include "include/assert.h"
+#include "include/ceph_assert.h"
 
 #define dout_subsys ceph_subsys_dpdk
 #undef dout_prefix
@@ -34,7 +34,7 @@ int UserspaceEventManager::get_eventfd()
   }
 
   Tub<UserspaceFDImpl> &impl = fds[fd];
-  assert(!impl);
+  ceph_assert(!impl);
   impl.construct();
   ldout(cct, 20) << __func__ << " fd=" << fd << dendl;
   return fd;
@@ -88,7 +88,7 @@ void UserspaceEventManager::close(int fd)
 
   if (impl->activating_mask) {
     if (waiting_fds[max_wait_idx] == fd) {
-      assert(impl->waiting_idx == max_wait_idx);
+      ceph_assert(impl->waiting_idx == max_wait_idx);
       --max_wait_idx;
     }
     waiting_fds[impl->waiting_idx] = -1;
@@ -101,7 +101,7 @@ int UserspaceEventManager::poll(int *events, int *masks, int num_events, struct
   int fd;
   uint32_t i = 0;
   int count = 0;
-  assert(num_events);
+  ceph_assert(num_events);
   // leave zero slot for waiting_fds
   while (i < max_wait_idx) {
     fd = waiting_fds[++i];
@@ -110,9 +110,9 @@ int UserspaceEventManager::poll(int *events, int *masks, int num_events, struct
 
     events[count] = fd;
     Tub<UserspaceFDImpl> &impl = fds[fd];
-    assert(impl);
+    ceph_assert(impl);
     masks[count] = impl->listening_mask & impl->activating_mask;
-    assert(masks[count]);
+    ceph_assert(masks[count]);
     ldout(cct, 20) << __func__ << " fd=" << fd << " mask=" << masks[count] << dendl;
     impl->activating_mask &= (~masks[count]);
     impl->waiting_idx = 0;
diff --git a/src/msg/async/dpdk/UserspaceEvent.h b/src/msg/async/dpdk/UserspaceEvent.h
index 1a725a667d8..7e89517df87 100644
--- a/src/msg/async/dpdk/UserspaceEvent.h
+++ b/src/msg/async/dpdk/UserspaceEvent.h
@@ -23,7 +23,7 @@
 #include <vector>
 #include <list>
 
-#include "include/assert.h"
+#include "include/ceph_assert.h"
 #include "include/int_types.h"
 #include "common/Tub.h"
 
@@ -46,7 +46,7 @@ class UserspaceEventManager {
   std::list<uint32_t> unused_fds;
 
  public:
-  UserspaceEventManager(CephContext *c): cct(c) {
+  explicit UserspaceEventManager(CephContext *c): cct(c) {
     waiting_fds.resize(1024);
   }
 
@@ -81,7 +81,7 @@ class UserspaceEventManager {
     impl->listening_mask &= (~mask);
     if (!(impl->activating_mask & impl->listening_mask) && impl->waiting_idx) {
       if (waiting_fds[max_wait_idx] == fd) {
-        assert(impl->waiting_idx == max_wait_idx);
+        ceph_assert(impl->waiting_idx == max_wait_idx);
         --max_wait_idx;
       }
       waiting_fds[impl->waiting_idx] = -1;
diff --git a/src/msg/async/dpdk/byteorder.h b/src/msg/async/dpdk/byteorder.h
index 2c198a2b675..a996ec0778e 100644
--- a/src/msg/async/dpdk/byteorder.h
+++ b/src/msg/async/dpdk/byteorder.h
@@ -19,19 +19,6 @@
 /*
  * Copyright (C) 2014 Cloudius Systems, Ltd.
  */
-/*
- * Ceph - scalable distributed file system
- *
- * Copyright (C) 2015 XSky <haomai@xsky.com>
- *
- * Author: Haomai Wang <haomaiwang@gmail.com>
- *
- * This is free software; you can redistribute it and/or
- * modify it under the terms of the GNU Lesser General Public
- * License version 2.1, as published by the Free Software
- * Foundation.  See file COPYING.
- *
- */
 
 #ifndef CEPH_MSG_BYTEORDER_H_
 #define CEPH_MSG_BYTEORDER_H_
diff --git a/src/msg/async/dpdk/const.h b/src/msg/async/dpdk/const.h
index 42af94a391e..ea5dc49e5b5 100644
--- a/src/msg/async/dpdk/const.h
+++ b/src/msg/async/dpdk/const.h
@@ -19,19 +19,6 @@
 /*
  * Copyright (C) 2014 Cloudius Systems, Ltd.
  */
-/*
- * Ceph - scalable distributed file system
- *
- * Copyright (C) 2015 XSky <haomai@xsky.com>
- *
- * Author: Haomai Wang <haomaiwang@gmail.com>
- *
- * This is free software; you can redistribute it and/or
- * modify it under the terms of the GNU Lesser General Public
- * License version 2.1, as published by the Free Software
- * Foundation.  See file COPYING.
- *
- */
 
 #ifndef CEPH_MSG_CONST_H_
 #define CEPH_MSG_CONST_H_
diff --git a/src/msg/async/dpdk/dpdk_rte.cc b/src/msg/async/dpdk/dpdk_rte.cc
index 6510df103b2..96cf896f823 100644
--- a/src/msg/async/dpdk/dpdk_rte.cc
+++ b/src/msg/async/dpdk/dpdk_rte.cc
@@ -16,6 +16,8 @@
  * under the License.
  */
 
+#include <bitset>
+
 #include <rte_config.h>
 #include <rte_common.h>
 #include <rte_ethdev.h>
@@ -38,12 +40,39 @@ namespace dpdk {
   std::condition_variable eal::cond;
   std::list<std::function<void()>> eal::funcs;
 
-  static int bitcount(unsigned n)
+  static int bitcount(unsigned long long n)
+  {
+    return std::bitset<CHAR_BIT * sizeof(n)>{n}.count();
+  }
+
+  static int hex2bitcount(unsigned char c)
+  {
+    int val;
+
+    if (isdigit(c))
+      val = c - '0';
+    else if (isupper(c))
+      val = c - 'A' + 10;
+    else
+      val = c - 'a' + 10;
+    return bitcount(val);
+  }
+
+  static int coremask_bitcount(const char *buf)
   {
-    unsigned int c =0 ;
-    for (c = 0; n; ++c)
-      n &= (n -1);
-    return c;
+    int count = 0;
+
+    if (buf[0] == '0' && 
+        ((buf[1] == 'x') || (buf[1] == 'X')))
+      buf += 2;
+
+    for (int i = 0; buf[i] != '\0'; i++) {
+      char c = buf[i];
+      if (isxdigit(c) == 0)
+        return -EINVAL;
+      count += hex2bitcount(c);
+    }
+    return count;
   }
 
   int eal::init(CephContext *c)
@@ -53,11 +82,18 @@ namespace dpdk {
     }
 
     bool done = false;
+    auto coremask = c->_conf.get_val<std::string>("ms_dpdk_coremask");
+    int coremaskbit = coremask_bitcount(coremask.c_str());
+
+    if (coremaskbit <= 0
+        || static_cast<uint64_t>(coremaskbit) < c->_conf->ms_async_op_threads)
+      return -EINVAL;
+
     t = std::thread([&]() {
       // TODO: Inherit these from the app parameters - "opts"
       std::vector<std::vector<char>> args {
           string2vector(string("ceph")),
-          string2vector("-c"), string2vector(c->_conf->get_val<std::string>("ms_dpdk_coremask")),
+          string2vector("-c"), string2vector(c->_conf.get_val<std::string>("ms_dpdk_coremask")),
           string2vector("-n"), string2vector(c->_conf->ms_dpdk_memory_channel),
       };
 
diff --git a/src/msg/async/dpdk/ethernet.h b/src/msg/async/dpdk/ethernet.h
index 17546d73a10..b007425fe92 100644
--- a/src/msg/async/dpdk/ethernet.h
+++ b/src/msg/async/dpdk/ethernet.h
@@ -26,7 +26,7 @@
 #include <array>
 #include <sstream>
 
-#include "include/assert.h"
+#include "include/ceph_assert.h"
 #include "byteorder.h"
 
 struct ethernet_address {
@@ -37,7 +37,7 @@ struct ethernet_address {
   }
 
   ethernet_address(std::initializer_list<uint8_t> eaddr) {
-    assert(eaddr.size() == mac.size());
+    ceph_assert(eaddr.size() == mac.size());
     std::copy(eaddr.begin(), eaddr.end(), mac.begin());
   }
 
@@ -58,7 +58,7 @@ std::ostream& operator<<(std::ostream& os, const ethernet_address& ea);
 struct ethernet {
   using address = ethernet_address;
   static address broadcast_address() {
-      return  {0xff, 0xff, 0xff, 0xff, 0xff, 0xff};
+      return {0xff, 0xff, 0xff, 0xff, 0xff, 0xff};
   }
   static constexpr uint16_t arp_hardware_type() { return 1; }
 };
diff --git a/src/msg/async/dpdk/net.cc b/src/msg/async/dpdk/net.cc
index d09dfaa576c..6e361f182d1 100644
--- a/src/msg/async/dpdk/net.cc
+++ b/src/msg/async/dpdk/net.cc
@@ -26,11 +26,6 @@
  *
  * Author: Haomai Wang <haomaiwang@gmail.com>
  *
- * This is free software; you can redistribute it and/or
- * modify it under the terms of the GNU Lesser General Public
- * License version 2.1, as published by the Free Software
- * Foundation.  See file COPYING.
- *
  */
 
 #include "net.h"
@@ -38,14 +33,14 @@
 #include "DPDKStack.h"
 
 #include "common/dout.h"
-#include "include/assert.h"
+#include "include/ceph_assert.h"
 
 #define dout_subsys ceph_subsys_dpdk
 #undef dout_prefix
 #define dout_prefix *_dout << "net "
 
-interface::interface(CephContext *c, std::shared_ptr<DPDKDevice> dev, EventCenter *center)
-    : cct(c), _dev(dev),
+interface::interface(CephContext *cct, std::shared_ptr<DPDKDevice> dev, EventCenter *center)
+    : cct(cct), _dev(dev),
       _rx(_dev->receive(
           center->get_id(),
           [center, this] (Packet p) {
@@ -69,7 +64,7 @@ interface::interface(CephContext *c, std::shared_ptr<DPDKDevice> dev, EventCente
         eh->src_mac = _hw_address;
         eh->eth_proto = uint16_t(l3pv.proto_num);
         *eh = eh->hton();
-        ldout(cct, 10) << "=== tx === proto " << std::hex << uint16_t(l3pv.proto_num)
+        ldout(this->cct, 10) << "=== tx === proto " << std::hex << uint16_t(l3pv.proto_num)
                        << " " << _hw_address << " -> " << l3pv.to
                        << " length " << std::dec << l3pv.p.len() << dendl;
         p = std::move(l3pv.p);
@@ -86,7 +81,7 @@ subscription<Packet, ethernet_address> interface::register_l3(
     std::function<bool (forward_hash&, Packet& p, size_t)> forward)
 {
   auto i = _proto_map.emplace(std::piecewise_construct, std::make_tuple(uint16_t(proto_num)), std::forward_as_tuple(std::move(forward)));
-  assert(i.second);
+  ceph_assert(i.second);
   l3_rx_stream& l3_rx = i.first->second;
   return l3_rx.packet_stream.listen(std::move(next));
 }
@@ -112,7 +107,7 @@ class C_handle_l2forward : public EventCallback {
  public:
   C_handle_l2forward(std::shared_ptr<DPDKDevice> &p, unsigned &qd, Packet pkt, unsigned target)
       : sdev(p), queue_depth(qd), p(std::move(pkt)), dst(target) {}
-  void do_request(int fd) {
+  void do_request(uint64_t fd) {
     sdev->l2receive(dst, std::move(p));
     queue_depth--;
     delete this;
@@ -184,7 +179,7 @@ class C_arp_learn : public EventCallback {
  public:
   C_arp_learn(DPDKWorker *w, ethernet_address l2, ipv4_address l3)
       : worker(w), l2_addr(l2), l3_addr(l3) {}
-  void do_request(int id) {
+  void do_request(uint64_t id) {
     worker->arp_learn(l2_addr, l3_addr);
     delete this;
   }
diff --git a/src/msg/async/dpdk/net.h b/src/msg/async/dpdk/net.h
index b744422fdad..63f0422b72c 100644
--- a/src/msg/async/dpdk/net.h
+++ b/src/msg/async/dpdk/net.h
@@ -1,17 +1,24 @@
-// -*- mode:C++; tab-width:8; c-basic-offset:2; indent-tabs-mode:t -*-
 /*
- * Ceph - scalable distributed file system
+ * This file is open source software, licensed to you under the terms
+ * of the Apache License, Version 2.0 (the "License").  See the NOTICE file
+ * distributed with this work for additional information regarding copyright
+ * ownership.  You may not use this file except in compliance with the License.
  *
- * Copyright (C) 2015 XSky <haomai@xsky.com>
+ * You may obtain a copy of the License at
  *
- * Author: Haomai Wang <haomaiwang@gmail.com>
- *
- * This is free software; you can redistribute it and/or
- * modify it under the terms of the GNU Lesser General Public
- * License version 2.1, as published by the Free Software
- * Foundation.  See file COPYING.
+ *   http://www.apache.org/licenses/LICENSE-2.0
  *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+/*
+ * Copyright (C) 2014 Cloudius Systems, Ltd.
  */
+
 #ifndef CEPH_MSG_DPDK_NET_H
 #define CEPH_MSG_DPDK_NET_H
 
@@ -48,7 +55,7 @@ class forward_hash {
     return end_idx;
   }
   void push_back(uint8_t b) {
-    assert(end_idx < sizeof(data));
+    ceph_assert(end_idx < sizeof(data));
     data[end_idx++] = b;
   }
   void push_back(uint16_t b) {
@@ -98,7 +105,7 @@ class interface {
     stream<Packet, ethernet_address> packet_stream;
     std::function<bool (forward_hash&, Packet&, size_t)> forward;
     bool ready() { return packet_stream.started(); }
-    l3_rx_stream(std::function<bool (forward_hash&, Packet&, size_t)>&& fw) : forward(fw) {}
+    explicit l3_rx_stream(std::function<bool (forward_hash&, Packet&, size_t)>&& fw) : forward(fw) {}
   };
   std::unordered_map<uint16_t, l3_rx_stream> _proto_map;
   std::shared_ptr<DPDKDevice> _dev;
@@ -110,7 +117,7 @@ class interface {
  private:
   int dispatch_packet(EventCenter *c, Packet p);
  public:
-  explicit interface(CephContext *cct, std::shared_ptr<DPDKDevice> dev, EventCenter *c);
+  explicit interface(CephContext *cct, std::shared_ptr<DPDKDevice> dev, EventCenter *center);
   ethernet_address hw_address() { return _hw_address; }
   const struct hw_features& get_hw_features() const { return _hw_features; }
   subscription<Packet, ethernet_address> register_l3(
diff --git a/src/msg/async/dpdk/shared_ptr.h b/src/msg/async/dpdk/shared_ptr.h
index 11fbf2c9058..d078063b33f 100644
--- a/src/msg/async/dpdk/shared_ptr.h
+++ b/src/msg/async/dpdk/shared_ptr.h
@@ -1,4 +1,4 @@
-// -*- mode:C++; tab-width:8; c-basic-offset:2; indent-tabs-mode:t -*- 
+// -*- mode:C++; tab-width:8; c-basic-offset:4; indent-tabs-mode:nil -*-
 /*
  * This file is open source software, licensed to you under the terms
  * of the Apache License, Version 2.0 (the "License").  See the NOTICE file
@@ -28,10 +28,10 @@
 #include <functional>
 #include <iostream>
 
-// This header defines two shared pointer facilities, lw_shared_ptr<>
+// This header defines a shared pointer facility, lw_shared_ptr<>,
 // modeled after std::shared_ptr<>.
 //
-// Unlike std::shared_ptr<>, neither of these implementations are thread
+// Unlike std::shared_ptr<>, this implementation is thread
 // safe, and two pointers sharing the same object must not be used in
 // different threads.
 //
@@ -39,16 +39,19 @@
 // occupying just one machine word, and adding just one word to the shared
 // object.  However, it does not support polymorphism.
 //
-// Both variants support shared_from_this() via enable_shared_from_this<>
+// It supports shared_from_this() via enable_shared_from_this<>
 // and lw_enable_shared_from_this<>().
 //
 
-template< class T >
-using remove_const_t = typename remove_const<T>::type;
-
 template <typename T>
 class lw_shared_ptr;
 
+template <typename T>
+class enable_lw_shared_from_this;
+
+template <typename T>
+class enable_shared_from_this;
+
 template <typename T, typename... A>
 lw_shared_ptr<T> make_lw_shared(A&&... a);
 
@@ -58,157 +61,279 @@ lw_shared_ptr<T> make_lw_shared(T&& a);
 template <typename T>
 lw_shared_ptr<T> make_lw_shared(T& a);
 
+struct lw_shared_ptr_counter_base {
+    long _count = 0;
+};
+
+
+namespace internal {
+
+template <class T, class U>
+struct lw_shared_ptr_accessors;
+
+template <class T>
+struct lw_shared_ptr_accessors_esft;
+
+template <class T>
+struct lw_shared_ptr_accessors_no_esft;
+
+}
+
+
+// We want to support two use cases for shared_ptr<T>:
+//
+//   1. T is any type (primitive or class type)
+//
+//   2. T is a class type that inherits from enable_shared_from_this<T>.
+//
+// In the first case, we must wrap T in an object containing the counter,
+// since T may be a primitive type and cannot be a base class.
+//
+// In the second case, we want T to reach the counter through its
+// enable_shared_from_this<> base class, so that we can implement
+// shared_from_this().
+//
+// To implement those two conflicting requirements (T alongside its counter;
+// T inherits from an object containing the counter) we use std::conditional<>
+// and some accessor functions to select between two implementations.
+
+
 // CRTP from this to enable shared_from_this:
 template <typename T>
-class enable_lw_shared_from_this {
-  long _count = 0;
-  using ctor = T;
-  T* to_value() { return static_cast<T*>(this); }
-  T* to_internal_object() { return static_cast<T*>(this); }
- protected:
-  enable_lw_shared_from_this() noexcept {}
-  enable_lw_shared_from_this(enable_lw_shared_from_this&&) noexcept {}
-  enable_lw_shared_from_this(const enable_lw_shared_from_this&) noexcept {}
-  enable_lw_shared_from_this& operator=(const enable_lw_shared_from_this&) noexcept { return *this; }
-  enable_lw_shared_from_this& operator=(enable_lw_shared_from_this&&) noexcept { return *this; }
- public:
-  lw_shared_ptr<T> shared_from_this();
-  lw_shared_ptr<const T> shared_from_this() const;
-  template <typename X>
-  friend class lw_shared_ptr;
+class enable_lw_shared_from_this : private lw_shared_ptr_counter_base {
+    using ctor = T;
+protected:
+    enable_lw_shared_from_this() noexcept {}
+    enable_lw_shared_from_this(enable_lw_shared_from_this&&) noexcept {}
+    enable_lw_shared_from_this(const enable_lw_shared_from_this&) noexcept {}
+    enable_lw_shared_from_this& operator=(const enable_lw_shared_from_this&) noexcept { return *this; }
+    enable_lw_shared_from_this& operator=(enable_lw_shared_from_this&&) noexcept { return *this; }
+public:
+    lw_shared_ptr<T> shared_from_this();
+    lw_shared_ptr<const T> shared_from_this() const;
+
+    template <typename X>
+    friend class lw_shared_ptr;
+    template <typename X>
+    friend class ::internal::lw_shared_ptr_accessors_esft;
+    template <typename X, class Y>
+    friend class ::internal::lw_shared_ptr_accessors;
 };
 
 template <typename T>
-struct shared_ptr_no_esft {
-  T _value;
-  using ctor = shared_ptr_no_esft;
-
-  T* to_value() { return &_value; }
-  shared_ptr_no_esft* to_internal_object() { return this; }
-  shared_ptr_no_esft() = default;
-  shared_ptr_no_esft(const T& x) : _value(x) {}
-  shared_ptr_no_esft(T&& x) : _value(std::move(x)) {}
-  template <typename... A>
-  shared_ptr_no_esft(A&&... a) : _value(std::forward<A>(a)...) {}
-  template <typename X>
-  friend class lw_shared_ptr;
+struct shared_ptr_no_esft : private lw_shared_ptr_counter_base {
+    T _value;
+
+    shared_ptr_no_esft() = default;
+    shared_ptr_no_esft(const T& x) : _value(x) {}
+    shared_ptr_no_esft(T&& x) : _value(std::move(x)) {}
+    template <typename... A>
+    shared_ptr_no_esft(A&&... a) : _value(std::forward<A>(a)...) {}
+
+    template <typename X>
+    friend class lw_shared_ptr;
+    template <typename X>
+    friend class ::internal::lw_shared_ptr_accessors_no_esft;
+    template <typename X, class Y>
+    friend class ::internal::lw_shared_ptr_accessors;
 };
 
+
+/// Extension point: the user may override this to change how \ref lw_shared_ptr objects are destroyed,
+/// primarily so that incomplete classes can be used.
+///
+/// Customizing the deleter requires that \c T be derived from \c enable_lw_shared_from_this<T>.
+/// The specialization must be visible for all uses of \c lw_shared_ptr<T>.
+///
+/// To customize, the template must have a `static void dispose(T*)` operator that disposes of
+/// the object.
+template <typename T>
+struct lw_shared_ptr_deleter;  // No generic implementation
+
+namespace internal {
+
 template <typename T>
-using shared_ptr_impl = typename std::conditional<
-        std::is_base_of<enable_lw_shared_from_this<remove_const_t<T>>, T>::value,
-        enable_lw_shared_from_this<remove_const_t<T>>,
-        shared_ptr_no_esft<remove_const_t<T>> >::type;
+struct lw_shared_ptr_accessors_esft {
+    using concrete_type = std::remove_const_t<T>;
+    static T* to_value(lw_shared_ptr_counter_base* counter) {
+        return static_cast<T*>(counter);
+    }
+    static void dispose(lw_shared_ptr_counter_base* counter) {
+        delete static_cast<T*>(counter);
+    }
+    static void instantiate_to_value(lw_shared_ptr_counter_base* p) {
+        // since to_value() is defined above, we don't need to do anything special
+        // to force-instantiate it
+    }
+};
+
+template <typename T>
+struct lw_shared_ptr_accessors_no_esft {
+    using concrete_type = shared_ptr_no_esft<T>;
+    static T* to_value(lw_shared_ptr_counter_base* counter) {
+        return &static_cast<concrete_type*>(counter)->_value;
+    }
+    static void dispose(lw_shared_ptr_counter_base* counter) {
+        delete static_cast<concrete_type*>(counter);
+    }
+    static void instantiate_to_value(lw_shared_ptr_counter_base* p) {
+        // since to_value() is defined above, we don't need to do anything special
+        // to force-instantiate it
+    }
+};
+
+// Generic case: lw_shared_ptr_deleter<T> is not specialized, select
+// implementation based on whether T inherits from enable_lw_shared_from_this<T>.
+template <typename T, typename U = void>
+struct lw_shared_ptr_accessors : std::conditional_t<
+         std::is_base_of<enable_lw_shared_from_this<T>, T>::value,
+         lw_shared_ptr_accessors_esft<T>,
+         lw_shared_ptr_accessors_no_esft<T>> {
+};
+
+// Overload when lw_shared_ptr_deleter<T> specialized
+template <typename T>
+struct lw_shared_ptr_accessors<T, std::void_t<decltype(lw_shared_ptr_deleter<T>{})>> {
+    using concrete_type = T;
+    static T* to_value(lw_shared_ptr_counter_base* counter);
+    static void dispose(lw_shared_ptr_counter_base* counter) {
+        lw_shared_ptr_deleter<T>::dispose(to_value(counter));
+    }
+    static void instantiate_to_value(lw_shared_ptr_counter_base* p) {
+        // instantiate to_value(); must be defined by shared_ptr_incomplete.hh
+        to_value(p);
+    }
+};
+
+}
 
 template <typename T>
 class lw_shared_ptr {
-  mutable shared_ptr_impl<T>* _p = nullptr;
- private:
-  lw_shared_ptr(shared_ptr_impl<T>* p) noexcept : _p(p) {
-    if (_p) {
-      ++_p->_count;
+    using accessors = ::internal::lw_shared_ptr_accessors<std::remove_const_t<T>>;
+    using concrete_type = typename accessors::concrete_type;
+    mutable lw_shared_ptr_counter_base* _p = nullptr;
+private:
+    lw_shared_ptr(lw_shared_ptr_counter_base* p) noexcept : _p(p) {
+        if (_p) {
+            ++_p->_count;
+        }
+    }
+    template <typename... A>
+    static lw_shared_ptr make(A&&... a) {
+        auto p = new concrete_type(std::forward<A>(a)...);
+        accessors::instantiate_to_value(p);
+        return lw_shared_ptr(p);
+    }
+public:
+    using element_type = T;
+
+    lw_shared_ptr() noexcept = default;
+    lw_shared_ptr(std::nullptr_t) noexcept : lw_shared_ptr() {}
+    lw_shared_ptr(const lw_shared_ptr& x) noexcept : _p(x._p) {
+        if (_p) {
+            ++_p->_count;
+        }
+    }
+    lw_shared_ptr(lw_shared_ptr&& x) noexcept  : _p(x._p) {
+        x._p = nullptr;
+    }
+    [[gnu::always_inline]]
+    ~lw_shared_ptr() {
+        if (_p && !--_p->_count) {
+            accessors::dispose(_p);
+        }
     }
-  }
-  template <typename... A>
-  static lw_shared_ptr make(A&&... a) {
-      return lw_shared_ptr(new typename shared_ptr_impl<T>::ctor(std::forward<A>(a)...));
-  }
- public:
-  using element_type = T;
-
-  lw_shared_ptr() noexcept = default;
-  lw_shared_ptr(std::nullptr_t) noexcept : lw_shared_ptr() {}
-  lw_shared_ptr(const lw_shared_ptr& x) noexcept : _p(x._p) {
-    if (_p) {
-      ++_p->_count;
+    lw_shared_ptr& operator=(const lw_shared_ptr& x) noexcept {
+        if (_p != x._p) {
+            this->~lw_shared_ptr();
+            new (this) lw_shared_ptr(x);
+        }
+        return *this;
     }
-  }
-  lw_shared_ptr(lw_shared_ptr&& x) noexcept  : _p(x._p) {
-      x._p = nullptr;
-  }
-  [[gnu::always_inline]]
-  ~lw_shared_ptr() {
-    if (_p && !--_p->_count) {
-      delete _p->to_internal_object();
+    lw_shared_ptr& operator=(lw_shared_ptr&& x) noexcept {
+        if (_p != x._p) {
+            this->~lw_shared_ptr();
+            new (this) lw_shared_ptr(std::move(x));
+        }
+        return *this;
     }
-  }
-  lw_shared_ptr& operator=(const lw_shared_ptr& x) noexcept {
-      if (_p != x._p) {
-          this->~lw_shared_ptr();
-          new (this) lw_shared_ptr(x);
-      }
-      return *this;
-  }
-  lw_shared_ptr& operator=(lw_shared_ptr&& x) noexcept {
-      if (_p != x._p) {
-          this->~lw_shared_ptr();
-          new (this) lw_shared_ptr(std::move(x));
-      }
-      return *this;
-  }
-  lw_shared_ptr& operator=(std::nullptr_t) noexcept {
-      return *this = lw_shared_ptr();
-  }
-  lw_shared_ptr& operator=(T&& x) noexcept {
-      this->~lw_shared_ptr();
-      new (this) lw_shared_ptr(make_lw_shared<T>(std::move(x)));
-      return *this;
-  }
-
-  T& operator*() const noexcept { return *_p->to_value(); }
-  T* operator->() const noexcept { return _p->to_value(); }
-  T* get() const noexcept { return _p->to_value(); }
-
-  long int use_count() const noexcept {
-    if (_p) {
-      return _p->_count;
-    } else {
-      return 0;
+    lw_shared_ptr& operator=(std::nullptr_t) noexcept {
+        return *this = lw_shared_ptr();
+    }
+    lw_shared_ptr& operator=(T&& x) noexcept {
+        this->~lw_shared_ptr();
+        new (this) lw_shared_ptr(make_lw_shared<T>(std::move(x)));
+        return *this;
+    }
+
+    T& operator*() const noexcept { return *accessors::to_value(_p); }
+    T* operator->() const noexcept { return accessors::to_value(_p); }
+    T* get() const noexcept {
+        if (_p) {
+            return accessors::to_value(_p);
+        } else {
+            return nullptr;
+        }
+    }
+
+    long int use_count() const noexcept {
+        if (_p) {
+            return _p->_count;
+        } else {
+            return 0;
+        }
+    }
+
+    operator lw_shared_ptr<const T>() const noexcept {
+        return lw_shared_ptr<const T>(_p);
+    }
+
+    explicit operator bool() const noexcept {
+        return _p;
     }
-  }
 
-  operator lw_shared_ptr<const T>() const noexcept {
-      return lw_shared_ptr<const T>(_p);
-  }
+    bool owned() const noexcept {
+        return _p->_count == 1;
+    }
 
-  explicit operator bool() const noexcept {
-      return _p;
-  }
+    bool operator==(const lw_shared_ptr<const T>& x) const {
+        return _p == x._p;
+    }
 
-  bool owned() const noexcept {
-    return _p->_count == 1;
-  }
+    bool operator!=(const lw_shared_ptr<const T>& x) const {
+        return !operator==(x);
+    }
 
-  bool operator==(const lw_shared_ptr<const T>& x) const {
-      return _p == x._p;
-  }
+    bool operator==(const lw_shared_ptr<std::remove_const_t<T>>& x) const {
+        return _p == x._p;
+    }
 
-  bool operator!=(const lw_shared_ptr<const T>& x) const {
-      return !operator==(x);
-  }
+    bool operator!=(const lw_shared_ptr<std::remove_const_t<T>>& x) const {
+        return !operator==(x);
+    }
 
-  bool operator==(const lw_shared_ptr<remove_const_t<T>>& x) const {
-      return _p == x._p;
-  }
+    bool operator<(const lw_shared_ptr<const T>& x) const {
+        return _p < x._p;
+    }
 
-  bool operator!=(const lw_shared_ptr<remove_const_t<T>>& x) const {
-      return !operator==(x);
-  }
+    bool operator<(const lw_shared_ptr<std::remove_const_t<T>>& x) const {
+        return _p < x._p;
+    }
 
-  template <typename U>
-  friend class lw_shared_ptr;
+    template <typename U>
+    friend class lw_shared_ptr;
 
-  template <typename X, typename... A>
-  friend lw_shared_ptr<X> make_lw_shared(A&&...);
+    template <typename X, typename... A>
+    friend lw_shared_ptr<X> make_lw_shared(A&&...);
 
-  template <typename U>
-  friend lw_shared_ptr<U> make_lw_shared(U&&);
+    template <typename U>
+    friend lw_shared_ptr<U> make_lw_shared(U&&);
 
-  template <typename U>
-  friend lw_shared_ptr<U> make_lw_shared(U&);
+    template <typename U>
+    friend lw_shared_ptr<U> make_lw_shared(U&);
 
-  template <typename U>
-  friend class enable_lw_shared_from_this;
+    template <typename U>
+    friend class enable_lw_shared_from_this;
 };
 
 template <typename T, typename... A>
diff --git a/src/msg/async/dpdk/stream.h b/src/msg/async/dpdk/stream.h
index 426bc64f5e0..1898e8f8628 100644
--- a/src/msg/async/dpdk/stream.h
+++ b/src/msg/async/dpdk/stream.h
@@ -19,19 +19,6 @@
 /*
  * Copyright (C) 2014 Cloudius Systems, Ltd.
  */
-/*
- * Ceph - scalable distributed file system
- *
- * Copyright (C) 2015 XSky <haomai@xsky.com>
- *
- * Author: Haomai Wang <haomaiwang@gmail.com>
- *
- * This is free software; you can redistribute it and/or
- * modify it under the terms of the GNU Lesser General Public
- * License version 2.1, as published by the Free Software
- * Foundation.  See file COPYING.
- *
- */
 
 #ifndef CEPH_MSG_STREAM_H_
 #define CEPH_MSG_STREAM_H_
@@ -130,7 +117,7 @@ class subscription {
   next_fn _next;
  private:
   explicit subscription(stream<T...>* s): _stream(s) {
-    assert(!_stream->_sub);
+    ceph_assert(!_stream->_sub);
     _stream->_sub = this;
   }
 
diff --git a/src/msg/async/frames_v2.cc b/src/msg/async/frames_v2.cc
new file mode 100644
index 00000000000..8fdded42d87
--- /dev/null
+++ b/src/msg/async/frames_v2.cc
@@ -0,0 +1,478 @@
+// -*- mode:C++; tab-width:8; c-basic-offset:2; indent-tabs-mode:t -*-
+// vim: ts=8 sw=2 smarttab
+/*
+ * Ceph - scalable distributed file system
+ *
+ * Copyright (C) 2020 Red Hat
+ *
+ * This is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License version 2.1, as published by the Free Software
+ * Foundation. See file COPYING.
+ *
+ */
+
+#include "frames_v2.h"
+
+#include <ostream>
+
+#include <fmt/format.h>
+
+namespace ceph::msgr::v2 {
+
+// Unpads bufferlist to unpadded_len.
+static void unpad_zero(bufferlist& bl, uint32_t unpadded_len) {
+  ceph_assert(bl.length() >= unpadded_len);
+  if (bl.length() > unpadded_len) {
+    bl.splice(unpadded_len, bl.length() - unpadded_len);
+  }
+}
+
+// Discards trailing empty segments, unless there is just one segment.
+// A frame always has at least one (possibly empty) segment.
+static size_t calc_num_segments(const bufferlist segment_bls[],
+                                size_t segment_count) {
+  ceph_assert(segment_count > 0 && segment_count <= MAX_NUM_SEGMENTS);
+  for (size_t i = segment_count; i-- > 0; ) {
+    if (segment_bls[i].length() > 0) {
+      return i + 1;
+    }
+  }
+  return 1;
+}
+
+static void check_segment_crc(const bufferlist& segment_bl,
+                              uint32_t expected_crc) {
+  uint32_t crc = segment_bl.crc32c(-1);
+  if (crc != expected_crc) {
+    throw FrameError(fmt::format(
+        "bad segment crc calculated={} expected={}", crc, expected_crc));
+  }
+}
+
+// Returns true if the frame is ready for dispatching, or false if
+// it was aborted by the sender and must be dropped.
+static bool check_epilogue_late_status(__u8 late_status) {
+  __u8 aborted = late_status & FRAME_LATE_STATUS_ABORTED_MASK;
+  if (aborted != FRAME_LATE_STATUS_ABORTED &&
+      aborted != FRAME_LATE_STATUS_COMPLETE) {
+    throw FrameError(fmt::format("bad late_status"));
+  }
+  return aborted == FRAME_LATE_STATUS_COMPLETE;
+}
+
+void FrameAssembler::fill_preamble(Tag tag,
+                                   preamble_block_t& preamble) const {
+  // FIPS zeroization audit 20191115: this memset is not security related.
+  ::memset(&preamble, 0, sizeof(preamble));
+
+  preamble.tag = static_cast<__u8>(tag);
+  for (size_t i = 0; i < m_descs.size(); i++) {
+    preamble.segments[i].length = m_descs[i].logical_len;
+    preamble.segments[i].alignment = m_descs[i].align;
+  }
+  preamble.num_segments = m_descs.size();
+  preamble.crc = ceph_crc32c(
+      0, reinterpret_cast<const unsigned char*>(&preamble),
+      sizeof(preamble) - sizeof(preamble.crc));
+}
+
+uint64_t FrameAssembler::get_frame_logical_len() const {
+  ceph_assert(!m_descs.empty());
+  uint64_t logical_len = 0;
+  for (size_t i = 0; i < m_descs.size(); i++) {
+    logical_len += m_descs[i].logical_len;
+  }
+  return logical_len;
+}
+
+uint64_t FrameAssembler::get_frame_onwire_len() const {
+  ceph_assert(!m_descs.empty());
+  uint64_t onwire_len = get_preamble_onwire_len();
+  for (size_t i = 0; i < m_descs.size(); i++) {
+    onwire_len += get_segment_onwire_len(i);
+  }
+  onwire_len += get_epilogue_onwire_len();
+  return onwire_len;
+}
+
+bufferlist FrameAssembler::asm_crc_rev0(const preamble_block_t& preamble,
+                                        bufferlist segment_bls[]) const {
+  epilogue_crc_rev0_block_t epilogue;
+  // FIPS zeroization audit 20191115: this memset is not security related.
+  ::memset(&epilogue, 0, sizeof(epilogue));
+
+  bufferlist frame_bl(sizeof(preamble) + sizeof(epilogue));
+  frame_bl.append(reinterpret_cast<const char*>(&preamble), sizeof(preamble));
+  for (size_t i = 0; i < m_descs.size(); i++) {
+    ceph_assert(segment_bls[i].length() == m_descs[i].logical_len);
+    epilogue.crc_values[i] = segment_bls[i].crc32c(-1);
+    if (segment_bls[i].length() > 0) {
+      frame_bl.claim_append(segment_bls[i]);
+    }
+  }
+  frame_bl.append(reinterpret_cast<const char*>(&epilogue), sizeof(epilogue));
+  return frame_bl;
+}
+
+bufferlist FrameAssembler::asm_secure_rev0(const preamble_block_t& preamble,
+                                           bufferlist segment_bls[]) const {
+  bufferlist preamble_bl(sizeof(preamble));
+  preamble_bl.append(reinterpret_cast<const char*>(&preamble),
+                     sizeof(preamble));
+
+  epilogue_secure_rev0_block_t epilogue;
+  // FIPS zeroization audit 20191115: this memset is not security related.
+  ::memset(&epilogue, 0, sizeof(epilogue));
+  bufferlist epilogue_bl(sizeof(epilogue));
+  epilogue_bl.append(reinterpret_cast<const char*>(&epilogue),
+                     sizeof(epilogue));
+
+  // preamble + MAX_NUM_SEGMENTS + epilogue
+  uint32_t onwire_lens[MAX_NUM_SEGMENTS + 2];
+  onwire_lens[0] = preamble_bl.length();
+  for (size_t i = 0; i < m_descs.size(); i++) {
+    onwire_lens[i + 1] = segment_bls[i].length();  // already padded
+  }
+  onwire_lens[m_descs.size() + 1] = epilogue_bl.length();
+  m_crypto->tx->reset_tx_handler(onwire_lens,
+                                 onwire_lens + m_descs.size() + 2);
+  m_crypto->tx->authenticated_encrypt_update(preamble_bl);
+  for (size_t i = 0; i < m_descs.size(); i++) {
+    if (segment_bls[i].length() > 0) {
+      m_crypto->tx->authenticated_encrypt_update(segment_bls[i]);
+    }
+  }
+  m_crypto->tx->authenticated_encrypt_update(epilogue_bl);
+  return m_crypto->tx->authenticated_encrypt_final();
+}
+
+bufferlist FrameAssembler::asm_crc_rev1(const preamble_block_t& preamble,
+                                        bufferlist segment_bls[]) const {
+  epilogue_crc_rev1_block_t epilogue;
+  // FIPS zeroization audit 20191115: this memset is not security related.
+  ::memset(&epilogue, 0, sizeof(epilogue));
+  epilogue.late_status |= FRAME_LATE_STATUS_COMPLETE;
+
+  bufferlist frame_bl(sizeof(preamble) + FRAME_CRC_SIZE + sizeof(epilogue));
+  frame_bl.append(reinterpret_cast<const char*>(&preamble), sizeof(preamble));
+
+  ceph_assert(segment_bls[0].length() == m_descs[0].logical_len);
+  if (segment_bls[0].length() > 0) {
+    uint32_t crc = segment_bls[0].crc32c(-1);
+    frame_bl.claim_append(segment_bls[0]);
+    encode(crc, frame_bl);
+  }
+  if (m_descs.size() == 1) {
+    return frame_bl;  // no epilogue if only one segment
+  }
+
+  for (size_t i = 1; i < m_descs.size(); i++) {
+    ceph_assert(segment_bls[i].length() == m_descs[i].logical_len);
+    epilogue.crc_values[i - 1] = segment_bls[i].crc32c(-1);
+    if (segment_bls[i].length() > 0) {
+      frame_bl.claim_append(segment_bls[i]);
+    }
+  }
+  frame_bl.append(reinterpret_cast<const char*>(&epilogue), sizeof(epilogue));
+  return frame_bl;
+}
+
+bufferlist FrameAssembler::asm_secure_rev1(const preamble_block_t& preamble,
+                                           bufferlist segment_bls[]) const {
+  bufferlist preamble_bl;
+  if (segment_bls[0].length() > FRAME_PREAMBLE_INLINE_SIZE) {
+    // first segment is partially inlined, inline buffer is full
+    preamble_bl.reserve(sizeof(preamble));
+    preamble_bl.append(reinterpret_cast<const char*>(&preamble),
+                       sizeof(preamble));
+    segment_bls[0].splice(0, FRAME_PREAMBLE_INLINE_SIZE, &preamble_bl);
+  } else {
+    // first segment is fully inlined, inline buffer may need padding
+    uint32_t pad_len = FRAME_PREAMBLE_INLINE_SIZE - segment_bls[0].length();
+    preamble_bl.reserve(sizeof(preamble) + pad_len);
+    preamble_bl.append(reinterpret_cast<const char*>(&preamble),
+                       sizeof(preamble));
+    preamble_bl.claim_append(segment_bls[0]);
+    if (pad_len > 0) {
+      preamble_bl.append_zero(pad_len);
+    }
+  }
+
+  m_crypto->tx->reset_tx_handler({preamble_bl.length()});
+  m_crypto->tx->authenticated_encrypt_update(preamble_bl);
+  auto frame_bl = m_crypto->tx->authenticated_encrypt_final();
+
+  if (segment_bls[0].length() > 0) {
+    m_crypto->tx->reset_tx_handler({segment_bls[0].length()});
+    m_crypto->tx->authenticated_encrypt_update(segment_bls[0]);
+    auto tmp = m_crypto->tx->authenticated_encrypt_final();
+    frame_bl.claim_append(tmp);
+  }
+  if (m_descs.size() == 1) {
+    return frame_bl;  // no epilogue if only one segment
+  }
+
+  epilogue_secure_rev1_block_t epilogue;
+  // FIPS zeroization audit 20191115: this memset is not security related.
+  ::memset(&epilogue, 0, sizeof(epilogue));
+  epilogue.late_status |= FRAME_LATE_STATUS_COMPLETE;
+  bufferlist epilogue_bl(sizeof(epilogue));
+  epilogue_bl.append(reinterpret_cast<const char*>(&epilogue),
+                     sizeof(epilogue));
+
+  // MAX_NUM_SEGMENTS - 1 + epilogue
+  uint32_t onwire_lens[MAX_NUM_SEGMENTS];
+  for (size_t i = 1; i < m_descs.size(); i++) {
+    onwire_lens[i - 1] = segment_bls[i].length();  // already padded
+  }
+  onwire_lens[m_descs.size() - 1] = epilogue_bl.length();
+  m_crypto->tx->reset_tx_handler(onwire_lens, onwire_lens + m_descs.size());
+  for (size_t i = 1; i < m_descs.size(); i++) {
+    if (segment_bls[i].length() > 0) {
+      m_crypto->tx->authenticated_encrypt_update(segment_bls[i]);
+    }
+  }
+  m_crypto->tx->authenticated_encrypt_update(epilogue_bl);
+  auto tmp = m_crypto->tx->authenticated_encrypt_final();
+  frame_bl.claim_append(tmp);
+  return frame_bl;
+}
+
+bufferlist FrameAssembler::assemble_frame(Tag tag, bufferlist segment_bls[],
+                                          const uint16_t segment_aligns[],
+                                          size_t segment_count) {
+  m_descs.resize(calc_num_segments(segment_bls, segment_count));
+  for (size_t i = 0; i < m_descs.size(); i++) {
+    m_descs[i].logical_len = segment_bls[i].length();
+    m_descs[i].align = segment_aligns[i];
+  }
+
+  preamble_block_t preamble;
+  fill_preamble(tag, preamble);
+
+  if (m_crypto->rx) {
+    for (size_t i = 0; i < m_descs.size(); i++) {
+      ceph_assert(segment_bls[i].length() == m_descs[i].logical_len);
+      // We're padding segments to biggest cipher's block size. Although
+      // AES-GCM can live without that as it's a stream cipher, we don't
+      // want to be fixed to stream ciphers only.
+      uint32_t padded_len = get_segment_padded_len(i);
+      if (padded_len > segment_bls[i].length()) {
+        uint32_t pad_len = padded_len - segment_bls[i].length();
+        segment_bls[i].reserve(pad_len);
+        segment_bls[i].append_zero(pad_len);
+      }
+    }
+    if (m_is_rev1) {
+      return asm_secure_rev1(preamble, segment_bls);
+    }
+    return asm_secure_rev0(preamble, segment_bls);
+  }
+  if (m_is_rev1) {
+    return asm_crc_rev1(preamble, segment_bls);
+  }
+  return asm_crc_rev0(preamble, segment_bls);
+}
+
+Tag FrameAssembler::disassemble_preamble(bufferlist& preamble_bl) {
+  if (m_crypto->rx) {
+    m_crypto->rx->reset_rx_handler();
+    if (m_is_rev1) {
+      ceph_assert(preamble_bl.length() == FRAME_PREAMBLE_WITH_INLINE_SIZE +
+                                          get_auth_tag_len());
+      m_crypto->rx->authenticated_decrypt_update_final(preamble_bl);
+    } else {
+      ceph_assert(preamble_bl.length() == sizeof(preamble_block_t));
+      m_crypto->rx->authenticated_decrypt_update(preamble_bl);
+    }
+  } else {
+    ceph_assert(preamble_bl.length() == sizeof(preamble_block_t));
+  }
+
+  // I expect ceph_le32 will make the endian conversion for me. Passing
+  // everything through ::Decode is unnecessary.
+  auto preamble = reinterpret_cast<const preamble_block_t*>(
+      preamble_bl.c_str());
+  // check preamble crc before any further processing
+  uint32_t crc = ceph_crc32c(
+      0, reinterpret_cast<const unsigned char*>(preamble),
+      sizeof(*preamble) - sizeof(preamble->crc));
+  if (crc != preamble->crc) {
+    throw FrameError(fmt::format(
+        "bad preamble crc calculated={} expected={}", crc, preamble->crc));
+  }
+
+  // see calc_num_segments()
+  if (preamble->num_segments < 1 ||
+      preamble->num_segments > MAX_NUM_SEGMENTS) {
+    throw FrameError(fmt::format(
+        "bad number of segments num_segments={}", preamble->num_segments));
+  }
+  if (preamble->num_segments > 1 &&
+      preamble->segments[preamble->num_segments - 1].length == 0) {
+    throw FrameError("last segment empty");
+  }
+
+  m_descs.resize(preamble->num_segments);
+  for (size_t i = 0; i < m_descs.size(); i++) {
+    m_descs[i].logical_len = preamble->segments[i].length;
+    m_descs[i].align = preamble->segments[i].alignment;
+  }
+  return static_cast<Tag>(preamble->tag);
+}
+
+bool FrameAssembler::disasm_all_crc_rev0(bufferlist segment_bls[],
+                                         bufferlist& epilogue_bl) const {
+  ceph_assert(epilogue_bl.length() == sizeof(epilogue_crc_rev0_block_t));
+  auto epilogue = reinterpret_cast<const epilogue_crc_rev0_block_t*>(
+      epilogue_bl.c_str());
+
+  for (size_t i = 0; i < m_descs.size(); i++) {
+    ceph_assert(segment_bls[i].length() == m_descs[i].logical_len);
+    check_segment_crc(segment_bls[i], epilogue->crc_values[i]);
+  }
+  return !(epilogue->late_flags & FRAME_LATE_FLAG_ABORTED);
+}
+
+bool FrameAssembler::disasm_all_secure_rev0(bufferlist segment_bls[],
+                                            bufferlist& epilogue_bl) const {
+  for (size_t i = 0; i < m_descs.size(); i++) {
+    ceph_assert(segment_bls[i].length() == get_segment_padded_len(i));
+    if (segment_bls[i].length() > 0) {
+      m_crypto->rx->authenticated_decrypt_update(segment_bls[i]);
+      unpad_zero(segment_bls[i], m_descs[i].logical_len);
+    }
+  }
+
+  ceph_assert(epilogue_bl.length() == sizeof(epilogue_secure_rev0_block_t) +
+                                      get_auth_tag_len());
+  m_crypto->rx->authenticated_decrypt_update_final(epilogue_bl);
+  auto epilogue = reinterpret_cast<const epilogue_secure_rev0_block_t*>(
+      epilogue_bl.c_str());
+  return !(epilogue->late_flags & FRAME_LATE_FLAG_ABORTED);
+}
+
+void FrameAssembler::disasm_first_crc_rev1(bufferlist& preamble_bl,
+                                           bufferlist& segment_bl) const {
+  ceph_assert(preamble_bl.length() == sizeof(preamble_block_t));
+  if (m_descs[0].logical_len > 0) {
+    ceph_assert(segment_bl.length() == m_descs[0].logical_len +
+                                       FRAME_CRC_SIZE);
+    bufferlist::const_iterator it(&segment_bl, m_descs[0].logical_len);
+    uint32_t expected_crc;
+    decode(expected_crc, it);
+    segment_bl.splice(m_descs[0].logical_len, FRAME_CRC_SIZE);
+    check_segment_crc(segment_bl, expected_crc);
+  } else {
+    ceph_assert(segment_bl.length() == 0);
+  }
+}
+
+bool FrameAssembler::disasm_remaining_crc_rev1(bufferlist segment_bls[],
+                                               bufferlist& epilogue_bl) const {
+  ceph_assert(epilogue_bl.length() == sizeof(epilogue_crc_rev1_block_t));
+  auto epilogue = reinterpret_cast<const epilogue_crc_rev1_block_t*>(
+      epilogue_bl.c_str());
+
+  for (size_t i = 1; i < m_descs.size(); i++) {
+    ceph_assert(segment_bls[i].length() == m_descs[i].logical_len);
+    check_segment_crc(segment_bls[i], epilogue->crc_values[i - 1]);
+  }
+  return check_epilogue_late_status(epilogue->late_status);
+}
+
+void FrameAssembler::disasm_first_secure_rev1(bufferlist& preamble_bl,
+                                              bufferlist& segment_bl) const {
+  ceph_assert(preamble_bl.length() == FRAME_PREAMBLE_WITH_INLINE_SIZE);
+  uint32_t padded_len = get_segment_padded_len(0);
+  if (padded_len > FRAME_PREAMBLE_INLINE_SIZE) {
+    ceph_assert(segment_bl.length() == padded_len + get_auth_tag_len() -
+                                       FRAME_PREAMBLE_INLINE_SIZE);
+    m_crypto->rx->reset_rx_handler();
+    m_crypto->rx->authenticated_decrypt_update_final(segment_bl);
+    // prepend the inline buffer (already decrypted) to segment_bl
+    bufferlist tmp;
+    segment_bl.swap(tmp);
+    preamble_bl.splice(sizeof(preamble_block_t), FRAME_PREAMBLE_INLINE_SIZE,
+                       &segment_bl);
+    segment_bl.claim_append(tmp);
+  } else {
+    ceph_assert(segment_bl.length() == 0);
+    preamble_bl.splice(sizeof(preamble_block_t), FRAME_PREAMBLE_INLINE_SIZE,
+                       &segment_bl);
+  }
+  unpad_zero(segment_bl, m_descs[0].logical_len);
+  ceph_assert(segment_bl.length() == m_descs[0].logical_len);
+}
+
+bool FrameAssembler::disasm_remaining_secure_rev1(
+    bufferlist segment_bls[], bufferlist& epilogue_bl) const {
+  m_crypto->rx->reset_rx_handler();
+  for (size_t i = 1; i < m_descs.size(); i++) {
+    ceph_assert(segment_bls[i].length() == get_segment_padded_len(i));
+    if (segment_bls[i].length() > 0) {
+      m_crypto->rx->authenticated_decrypt_update(segment_bls[i]);
+      unpad_zero(segment_bls[i], m_descs[i].logical_len);
+    }
+  }
+
+  ceph_assert(epilogue_bl.length() == sizeof(epilogue_secure_rev1_block_t) +
+                                      get_auth_tag_len());
+  m_crypto->rx->authenticated_decrypt_update_final(epilogue_bl);
+  auto epilogue = reinterpret_cast<const epilogue_secure_rev1_block_t*>(
+      epilogue_bl.c_str());
+  return check_epilogue_late_status(epilogue->late_status);
+}
+
+void FrameAssembler::disassemble_first_segment(bufferlist& preamble_bl,
+                                               bufferlist& segment_bl) const {
+  ceph_assert(!m_descs.empty());
+  if (m_is_rev1) {
+    if (m_crypto->rx) {
+      disasm_first_secure_rev1(preamble_bl, segment_bl);
+    } else {
+      disasm_first_crc_rev1(preamble_bl, segment_bl);
+    }
+  } else {
+    // noop, everything is handled in disassemble_remaining_segments()
+  }
+}
+
+bool FrameAssembler::disassemble_remaining_segments(
+    bufferlist segment_bls[], bufferlist& epilogue_bl) const {
+  ceph_assert(!m_descs.empty());
+  if (m_is_rev1) {
+    if (m_descs.size() == 1) {
+      // no epilogue if only one segment
+      ceph_assert(epilogue_bl.length() == 0);
+      return true;
+    }
+    if (m_crypto->rx) {
+      return disasm_remaining_secure_rev1(segment_bls, epilogue_bl);
+    }
+    return disasm_remaining_crc_rev1(segment_bls, epilogue_bl);
+  }
+  if (m_crypto->rx) {
+    return disasm_all_secure_rev0(segment_bls, epilogue_bl);
+  }
+  return disasm_all_crc_rev0(segment_bls, epilogue_bl);
+}
+
+std::ostream& operator<<(std::ostream& os, const FrameAssembler& frame_asm) {
+  if (!frame_asm.m_descs.empty()) {
+    os << frame_asm.get_preamble_onwire_len();
+    for (size_t i = 0; i < frame_asm.m_descs.size(); i++) {
+      os << " + " << frame_asm.get_segment_onwire_len(i)
+         << " (logical " << frame_asm.m_descs[i].logical_len
+         << "/" << frame_asm.m_descs[i].align << ")";
+    }
+    os << " + " << frame_asm.get_epilogue_onwire_len() << " ";
+  }
+  os << "rev1=" << frame_asm.m_is_rev1
+     << " rx=" << frame_asm.m_crypto->rx.get()
+     << " tx=" << frame_asm.m_crypto->tx.get();
+  return os;
+}
+
+}  // namespace ceph::msgr::v2
diff --git a/src/msg/async/frames_v2.h b/src/msg/async/frames_v2.h
new file mode 100644
index 00000000000..5ce4a4fcc55
--- /dev/null
+++ b/src/msg/async/frames_v2.h
@@ -0,0 +1,842 @@
+#ifndef _MSG_ASYNC_FRAMES_V2_
+#define _MSG_ASYNC_FRAMES_V2_
+
+#include "include/types.h"
+#include "common/Clock.h"
+#include "crypto_onwire.h"
+#include <array>
+#include <iosfwd>
+#include <utility>
+
+#include <boost/container/static_vector.hpp>
+
+/**
+ * Protocol V2 Frame Structures
+ * 
+ * Documentation in: doc/dev/msgr2.rst
+ **/
+
+namespace ceph::msgr::v2 {
+
+// We require these features from any peer, period, in order to encode
+// a entity_addrvec_t.
+const uint64_t msgr2_required = CEPH_FEATUREMASK_MSG_ADDR2;
+
+// We additionally assume the peer has the below features *purely for
+// the purpose of encoding the frames themselves*.  The only complex
+// types in the frames are entity_addr_t and entity_addrvec_t, and we
+// specifically want the peer to understand the (new in nautilus)
+// TYPE_ANY.  We treat narrow this assumption to frames because we
+// expect there may be future clients (the kernel) that understand
+// msgr v2 and understand this encoding but don't necessarily have
+// everything else that SERVER_NAUTILUS implies.  Yes, a fresh feature
+// bit would be a cleaner approach, but those are scarce these days.
+const uint64_t msgr2_frame_assumed =
+		   msgr2_required |
+		   CEPH_FEATUREMASK_SERVER_NAUTILUS;
+
+enum class Tag : __u8 {
+  HELLO = 1,
+  AUTH_REQUEST,
+  AUTH_BAD_METHOD,
+  AUTH_REPLY_MORE,
+  AUTH_REQUEST_MORE,
+  AUTH_DONE,
+  AUTH_SIGNATURE,
+  CLIENT_IDENT,
+  SERVER_IDENT,
+  IDENT_MISSING_FEATURES,
+  SESSION_RECONNECT,
+  SESSION_RESET,
+  SESSION_RETRY,
+  SESSION_RETRY_GLOBAL,
+  SESSION_RECONNECT_OK,
+  WAIT,
+  MESSAGE,
+  KEEPALIVE2,
+  KEEPALIVE2_ACK,
+  ACK
+};
+
+struct segment_t {
+  // TODO: this will be dropped with support for `allocation policies`.
+  // We need them because of the rx_buffers zero-copy optimization.
+  static constexpr __u16 PAGE_SIZE_ALIGNMENT = 4096;
+
+  static constexpr __u16 DEFAULT_ALIGNMENT = sizeof(void *);
+
+  ceph_le32 length;
+  ceph_le16 alignment;
+} __attribute__((packed));
+
+struct SegmentIndex {
+  struct Msg {
+    static constexpr std::size_t HEADER = 0;
+    static constexpr std::size_t FRONT = 1;
+    static constexpr std::size_t MIDDLE = 2;
+    static constexpr std::size_t DATA = 3;
+  };
+
+  struct Control {
+    static constexpr std::size_t PAYLOAD = 0;
+  };
+};
+
+static constexpr uint8_t CRYPTO_BLOCK_SIZE { 16 };
+
+static constexpr std::size_t MAX_NUM_SEGMENTS = 4;
+
+// V2 preamble consists of one or more preamble blocks depending on
+// the number of segments a particular frame needs. Each block holds
+// up to MAX_NUM_SEGMENTS segments and has its own CRC.
+//
+// XXX: currently the multi-segment facility is NOT implemented.
+struct preamble_block_t {  
+  // Tag. For multi-segmented frames the value is the same
+  // between subsequent preamble blocks.
+  __u8 tag;
+
+  // Number of segments to go in entire frame. First preable block has
+  // set this to just #segments, second #segments - MAX_NUM_SEGMENTS,
+  // third to #segments - MAX_NUM_SEGMENTS and so on.
+  __u8 num_segments;
+
+  segment_t segments[MAX_NUM_SEGMENTS];
+  __u8 _reserved[2];
+
+  // CRC32 for this single preamble block.
+  ceph_le32 crc;
+} __attribute__((packed));
+static_assert(sizeof(preamble_block_t) % CRYPTO_BLOCK_SIZE == 0);
+static_assert(std::is_standard_layout<preamble_block_t>::value);
+
+struct epilogue_crc_rev0_block_t {
+  __u8 late_flags;  // FRAME_LATE_FLAG_ABORTED
+  ceph_le32 crc_values[MAX_NUM_SEGMENTS];
+} __attribute__((packed));
+static_assert(std::is_standard_layout_v<epilogue_crc_rev0_block_t>);
+
+struct epilogue_crc_rev1_block_t {
+  __u8 late_status;  // FRAME_LATE_STATUS_*
+  ceph_le32 crc_values[MAX_NUM_SEGMENTS - 1];
+} __attribute__((packed));
+static_assert(std::is_standard_layout_v<epilogue_crc_rev1_block_t>);
+
+struct epilogue_secure_rev0_block_t {
+  __u8 late_flags;  // FRAME_LATE_FLAG_ABORTED
+  __u8 padding[CRYPTO_BLOCK_SIZE - sizeof(late_flags)];
+} __attribute__((packed));
+static_assert(sizeof(epilogue_secure_rev0_block_t) % CRYPTO_BLOCK_SIZE == 0);
+static_assert(std::is_standard_layout_v<epilogue_secure_rev0_block_t>);
+
+// epilogue_secure_rev0_block_t with late_flags changed to late_status
+struct epilogue_secure_rev1_block_t {
+  __u8 late_status;  // FRAME_LATE_STATUS_*
+  __u8 padding[CRYPTO_BLOCK_SIZE - sizeof(late_status)];
+} __attribute__((packed));
+static_assert(sizeof(epilogue_secure_rev1_block_t) % CRYPTO_BLOCK_SIZE == 0);
+static_assert(std::is_standard_layout_v<epilogue_secure_rev1_block_t>);
+
+static constexpr uint32_t FRAME_CRC_SIZE = 4;
+static constexpr uint32_t FRAME_PREAMBLE_INLINE_SIZE = 48;
+static_assert(FRAME_PREAMBLE_INLINE_SIZE % CRYPTO_BLOCK_SIZE == 0);
+// just for performance, nothing should break otherwise
+static_assert(sizeof(ceph_msg_header2) <= FRAME_PREAMBLE_INLINE_SIZE);
+static constexpr uint32_t FRAME_PREAMBLE_WITH_INLINE_SIZE =
+    sizeof(preamble_block_t) + FRAME_PREAMBLE_INLINE_SIZE;
+
+// A frame can be aborted by the sender after transmitting the
+// preamble and the first segment.  The remainder of the frame
+// is filled with zeros, up until the epilogue.
+//
+// This flag is for msgr2.0.  Note that in crc mode, late_flags
+// is not covered by any crc -- a single bit flip can result in
+// a completed frame being dropped or in an aborted frame with
+// garbage segment payloads being dispatched.
+#define FRAME_LATE_FLAG_ABORTED           (1<<0)
+
+// For msgr2.1, FRAME_LATE_STATUS_ABORTED has the same meaning
+// as FRAME_LATE_FLAG_ABORTED and late_status replaces late_flags.
+// Bit error detection in crc mode is achieved by using a 4-bit
+// nibble per flag with two code words that are far apart in terms
+// of Hamming Distance (HD=4, same as provided by CRC32-C for
+// input lengths over ~5K).
+#define FRAME_LATE_STATUS_ABORTED         0x1
+#define FRAME_LATE_STATUS_COMPLETE        0xe
+#define FRAME_LATE_STATUS_ABORTED_MASK    0xf
+
+#define FRAME_LATE_STATUS_RESERVED_TRUE   0x10
+#define FRAME_LATE_STATUS_RESERVED_FALSE  0xe0
+#define FRAME_LATE_STATUS_RESERVED_MASK   0xf0
+
+struct FrameError : std::runtime_error {
+  using runtime_error::runtime_error;
+};
+
+class FrameAssembler {
+public:
+  // crypto must be non-null
+  FrameAssembler(const ceph::crypto::onwire::rxtx_t* crypto, bool is_rev1)
+      : m_crypto(crypto), m_is_rev1(is_rev1) {}
+
+  void set_is_rev1(bool is_rev1) {
+    m_descs.clear();
+    m_is_rev1 = is_rev1;
+  }
+
+  bool get_is_rev1() {
+    return m_is_rev1;
+  }
+
+  size_t get_num_segments() const {
+    ceph_assert(!m_descs.empty());
+    return m_descs.size();
+  }
+
+  uint32_t get_segment_logical_len(size_t seg_idx) const {
+    ceph_assert(seg_idx < m_descs.size());
+    return m_descs[seg_idx].logical_len;
+  }
+
+  uint16_t get_segment_align(size_t seg_idx) const {
+    ceph_assert(seg_idx < m_descs.size());
+    return m_descs[seg_idx].align;
+  }
+
+  // Preamble:
+  //
+  //   preamble_block_t
+  //   [preamble inline buffer + auth tag -- only in msgr2.1 secure mode]
+  //
+  // The preamble is generated unconditionally.
+  //
+  // In msgr2.1 secure mode, the first segment is inlined into the
+  // preamble inline buffer, either fully or partially.
+  uint32_t get_preamble_onwire_len() const {
+    if (m_is_rev1 && m_crypto->rx) {
+      return FRAME_PREAMBLE_WITH_INLINE_SIZE + get_auth_tag_len();
+    }
+    return sizeof(preamble_block_t);
+  }
+
+  // Segment:
+  //
+  //   segment payload
+  //   [zero padding -- only in secure mode]
+  //   [crc or auth tag -- only in msgr2.1, only for the first segment]
+  //
+  // For an empty segment, nothing is generated.  In msgr2.1 secure
+  // mode, if the first segment gets fully inlined into the preamble
+  // inline buffer, it is considered empty.
+  uint32_t get_segment_onwire_len(size_t seg_idx) const {
+    ceph_assert(seg_idx < m_descs.size());
+    if (m_crypto->rx) {
+      uint32_t padded_len = get_segment_padded_len(seg_idx);
+      if (m_is_rev1 && seg_idx == 0) {
+        if (padded_len > FRAME_PREAMBLE_INLINE_SIZE) {
+          return padded_len + get_auth_tag_len() - FRAME_PREAMBLE_INLINE_SIZE;
+        }
+        return 0;
+      }
+      return padded_len;
+    }
+    if (m_is_rev1 && seg_idx == 0 && m_descs[0].logical_len > 0) {
+      return m_descs[0].logical_len + FRAME_CRC_SIZE;
+    }
+    return m_descs[seg_idx].logical_len;
+  }
+
+  // Epilogue:
+  //
+  //   epilogue_*_block_t
+  //   [auth tag -- only in secure mode]
+  //
+  // For msgr2.0, the epilogue is generated unconditionally.  In
+  // crc mode, it stores crcs for all segments; the preamble is
+  // covered by its own crc.  In secure mode, the epilogue auth tag
+  // covers the whole frame.
+  //
+  // For msgr2.1, the epilogue is generated only if the frame has
+  // more than one segment (i.e. at least one of second to fourth
+  // segments is not empty).  In crc mode, it stores crcs for
+  // second to fourh segments; the preamble and the first segment
+  // are covered by their own crcs.  In secure mode, the epilogue
+  // auth tag covers second to fourth segments; the preamble and the
+  // first segment (if not fully inlined into the preamble inline
+  // buffer) are covered by their own auth tags.
+  //
+  // Note that the auth tag format is an implementation detail of a
+  // particular cipher.  FrameAssembler is concerned only with where
+  // the auth tag is placed (at the end of the ciphertext) and how
+  // long it is (RxHandler::get_extra_size_at_final()).  This is to
+  // provide room for other encryption algorithms: currently we use
+  // AES-128-GCM with 16-byte tags, but it is possible to switch to
+  // e.g. AES-128-CBC + HMAC-SHA512 without affecting the protocol
+  // (except for the cipher negotiation, of course).
+  //
+  // Additionally, each variant of the epilogue contains either
+  // late_flags or late_status field that directs handling of frames
+  // with more than one segment.
+  uint32_t get_epilogue_onwire_len() const {
+    ceph_assert(!m_descs.empty());
+    if (m_is_rev1 && m_descs.size() == 1) {
+      return 0;
+    }
+    if (m_crypto->rx) {
+      return (m_is_rev1 ? sizeof(epilogue_secure_rev1_block_t) :
+                  sizeof(epilogue_secure_rev0_block_t)) + get_auth_tag_len();
+    }
+    return m_is_rev1 ? sizeof(epilogue_crc_rev1_block_t) :
+                       sizeof(epilogue_crc_rev0_block_t);
+  }
+
+  uint64_t get_frame_logical_len() const;
+  uint64_t get_frame_onwire_len() const;
+
+  bufferlist assemble_frame(Tag tag, bufferlist segment_bls[],
+                            const uint16_t segment_aligns[],
+                            size_t segment_count);
+
+  Tag disassemble_preamble(bufferlist& preamble_bl);
+
+  // Like msgr1, and unlike msgr2.0, msgr2.1 allows interpreting the
+  // first segment before reading in the rest of the frame.
+  //
+  // For msgr2.1 (set_is_rev1(true)), you may:
+  //
+  // - read in the first segment
+  // - call disassemble_first_segment()
+  // - use the contents of the first segment, for example to
+  //   look up user-provided buffers based on ceph_msg_header2::tid
+  // - read in the remaining segments, possibly directly into
+  //   user-provided buffers
+  // - read in epilogue
+  // - call disassemble_remaining_segments()
+  //
+  // For msgr2.0 (set_is_rev1(false)), disassemble_first_segment() is
+  // a noop.  To accomodate, disassemble_remaining_segments() always
+  // takes all segments and skips over the first segment in msgr2.1
+  // case.  You must:
+  //
+  // - read in all segments
+  // - read in epilogue
+  // - call disassemble_remaining_segments()
+  //
+  // disassemble_remaining_segments() returns true if the frame is
+  // ready for dispatching, or false if it was aborted by the sender
+  // and must be dropped.
+  void disassemble_first_segment(bufferlist& preamble_bl,
+                                 bufferlist& segment_bl) const;
+  bool disassemble_remaining_segments(bufferlist segment_bls[],
+                                      bufferlist& epilogue_bl) const;
+
+private:
+  struct segment_desc_t {
+    uint32_t logical_len;
+    uint16_t align;
+  };
+
+  uint32_t get_segment_padded_len(size_t seg_idx) const {
+    return p2roundup<uint32_t>(m_descs[seg_idx].logical_len,
+                               CRYPTO_BLOCK_SIZE);
+  }
+
+  uint32_t get_auth_tag_len() const {
+    return m_crypto->rx->get_extra_size_at_final();
+  }
+
+  bufferlist asm_crc_rev0(const preamble_block_t& preamble,
+                          bufferlist segment_bls[]) const;
+  bufferlist asm_secure_rev0(const preamble_block_t& preamble,
+                             bufferlist segment_bls[]) const;
+  bufferlist asm_crc_rev1(const preamble_block_t& preamble,
+                          bufferlist segment_bls[]) const;
+  bufferlist asm_secure_rev1(const preamble_block_t& preamble,
+                             bufferlist segment_bls[]) const;
+
+  bool disasm_all_crc_rev0(bufferlist segment_bls[],
+                           bufferlist& epilogue_bl) const;
+  bool disasm_all_secure_rev0(bufferlist segment_bls[],
+                              bufferlist& epilogue_bl) const;
+  void disasm_first_crc_rev1(bufferlist& preamble_bl,
+                             bufferlist& segment_bl) const;
+  bool disasm_remaining_crc_rev1(bufferlist segment_bls[],
+                                 bufferlist& epilogue_bl) const;
+  void disasm_first_secure_rev1(bufferlist& preamble_bl,
+                                bufferlist& segment_bl) const;
+  bool disasm_remaining_secure_rev1(bufferlist segment_bls[],
+                                    bufferlist& epilogue_bl) const;
+
+  void fill_preamble(Tag tag, preamble_block_t& preamble) const;
+  friend std::ostream& operator<<(std::ostream& os,
+                                  const FrameAssembler& frame_asm);
+
+  boost::container::static_vector<segment_desc_t, MAX_NUM_SEGMENTS> m_descs;
+  const ceph::crypto::onwire::rxtx_t* m_crypto;
+  bool m_is_rev1;  // msgr2.1?
+};
+
+template <class T, uint16_t... SegmentAlignmentVs>
+struct Frame {
+  static constexpr size_t SegmentsNumV = sizeof...(SegmentAlignmentVs);
+  static_assert(SegmentsNumV > 0 && SegmentsNumV <= MAX_NUM_SEGMENTS);
+protected:
+  std::array<ceph::bufferlist, SegmentsNumV> segments;
+
+private:
+  static constexpr std::array<uint16_t, SegmentsNumV> alignments {
+    SegmentAlignmentVs...
+  };
+
+public:
+  ceph::bufferlist get_buffer(FrameAssembler& tx_frame_asm) {
+    auto bl = tx_frame_asm.assemble_frame(T::tag, segments.data(),
+                                          alignments.data(), SegmentsNumV);
+    ceph_assert(bl.length() == tx_frame_asm.get_frame_onwire_len());
+    return bl;
+  }
+};
+
+// ControlFrames are used to manage transceiver state (like connections) and
+// orchestrate transfers of MessageFrames. They use only single segment with
+// marshalling facilities -- derived classes specify frame structure through
+// Args pack while ControlFrame provides common encode/decode machinery.
+template <class C, typename... Args>
+class ControlFrame : public Frame<C, segment_t::DEFAULT_ALIGNMENT /* single segment */> {
+protected:
+  ceph::bufferlist &get_payload_segment() {
+    return this->segments[SegmentIndex::Control::PAYLOAD];
+  }
+
+  // this tuple is only used when decoding values from a payload segment
+  std::tuple<Args...> _values;
+
+  // FIXME: for now, we assume specific features for the purpoess of encoding
+  // the frames themselves (*not* messages in message frames!).
+  uint64_t features = msgr2_frame_assumed;
+
+  template <typename T>
+  inline void _encode_payload_each(T &t) {
+    if constexpr (std::is_same<T, std::vector<uint32_t> const>()) {
+      encode((uint32_t)t.size(), this->get_payload_segment(), features);
+      for (const auto &elem : t) {
+        encode(elem, this->get_payload_segment(), features);
+      }
+    } else {
+      encode(t, this->get_payload_segment(), features);
+    }
+  }
+
+  template <typename T>
+  inline void _decode_payload_each(T &t, bufferlist::const_iterator &ti) const {
+    if constexpr (std::is_same<T, std::vector<uint32_t>>()) {
+      uint32_t size;
+      decode(size, ti);
+      t.resize(size);
+      for (uint32_t i = 0; i < size; ++i) {
+        decode(t[i], ti);
+      }
+    } else {
+      decode(t, ti);
+    }
+  }
+
+  template <std::size_t... Is>
+  inline void _decode_payload(bufferlist::const_iterator &ti,
+                              std::index_sequence<Is...>) const {
+    (_decode_payload_each((Args &)std::get<Is>(_values), ti), ...);
+  }
+
+  template <std::size_t N>
+  inline decltype(auto) get_val() {
+    return std::get<N>(_values);
+  }
+
+  ControlFrame()
+    : Frame<C, segment_t::DEFAULT_ALIGNMENT /* single segment */>() {
+  }
+
+  void _encode(const Args &... args) {
+    (_encode_payload_each(args), ...);
+  }
+
+  void _decode(const ceph::bufferlist &bl) {
+    auto ti = bl.cbegin();
+    _decode_payload(ti, std::index_sequence_for<Args...>());
+  }
+
+public:
+  static C Encode(const Args &... args) {
+    C c;
+    c._encode(args...);
+    return c;
+  }
+
+  static C Decode(const ceph::bufferlist &payload) {
+    C c;
+    c._decode(payload);
+    return c;
+  }
+};
+
+struct HelloFrame : public ControlFrame<HelloFrame,
+                                        uint8_t,          // entity type
+                                        entity_addr_t> {  // peer address
+  static const Tag tag = Tag::HELLO;
+  using ControlFrame::Encode;
+  using ControlFrame::Decode;
+
+  inline uint8_t &entity_type() { return get_val<0>(); }
+  inline entity_addr_t &peer_addr() { return get_val<1>(); }
+
+protected:
+  using ControlFrame::ControlFrame;
+};
+
+struct AuthRequestFrame : public ControlFrame<AuthRequestFrame,
+                                              uint32_t, // auth method
+                                              vector<uint32_t>, // preferred modes
+                                              bufferlist> { // auth payload
+  static const Tag tag = Tag::AUTH_REQUEST;
+  using ControlFrame::Encode;
+  using ControlFrame::Decode;
+
+  inline uint32_t &method() { return get_val<0>(); }
+  inline vector<uint32_t> &preferred_modes() { return get_val<1>(); }
+  inline bufferlist &auth_payload() { return get_val<2>(); }
+
+protected:
+  using ControlFrame::ControlFrame;
+};
+
+struct AuthBadMethodFrame : public ControlFrame<AuthBadMethodFrame,
+                                                uint32_t, // method
+                                                int32_t,  // result
+                                                std::vector<uint32_t>,   // allowed methods
+                                                std::vector<uint32_t>> { // allowed modes
+  static const Tag tag = Tag::AUTH_BAD_METHOD;
+  using ControlFrame::Encode;
+  using ControlFrame::Decode;
+
+  inline uint32_t &method() { return get_val<0>(); }
+  inline int32_t &result() { return get_val<1>(); }
+  inline std::vector<uint32_t> &allowed_methods() { return get_val<2>(); }
+  inline std::vector<uint32_t> &allowed_modes() { return get_val<3>(); }
+
+protected:
+  using ControlFrame::ControlFrame;
+};
+
+struct AuthReplyMoreFrame : public ControlFrame<AuthReplyMoreFrame,
+                                                bufferlist> { // auth payload
+  static const Tag tag = Tag::AUTH_REPLY_MORE;
+  using ControlFrame::Encode;
+  using ControlFrame::Decode;
+
+  inline bufferlist &auth_payload() { return get_val<0>(); }
+
+protected:
+  using ControlFrame::ControlFrame;
+};
+
+struct AuthRequestMoreFrame : public ControlFrame<AuthRequestMoreFrame,
+                                                  bufferlist> { // auth payload
+  static const Tag tag = Tag::AUTH_REQUEST_MORE;
+  using ControlFrame::Encode;
+  using ControlFrame::Decode;
+
+  inline bufferlist &auth_payload() { return get_val<0>(); }
+
+protected:
+  using ControlFrame::ControlFrame;
+};
+
+struct AuthDoneFrame : public ControlFrame<AuthDoneFrame,
+                                           uint64_t, // global id
+                                           uint32_t, // connection mode
+                                           bufferlist> { // auth method payload
+  static const Tag tag = Tag::AUTH_DONE;
+  using ControlFrame::Encode;
+  using ControlFrame::Decode;
+
+  inline uint64_t &global_id() { return get_val<0>(); }
+  inline uint32_t &con_mode() { return get_val<1>(); }
+  inline bufferlist &auth_payload() { return get_val<2>(); }
+
+protected:
+  using ControlFrame::ControlFrame;
+};
+
+struct AuthSignatureFrame
+    : public ControlFrame<AuthSignatureFrame,
+                          sha256_digest_t> {
+  static const Tag tag = Tag::AUTH_SIGNATURE;
+  using ControlFrame::Encode;
+  using ControlFrame::Decode;
+
+  inline sha256_digest_t &signature() { return get_val<0>(); }
+
+protected:
+  using ControlFrame::ControlFrame;
+};
+
+struct ClientIdentFrame
+    : public ControlFrame<ClientIdentFrame,
+                          entity_addrvec_t,  // my addresses
+                          entity_addr_t,  // target address
+                          int64_t,  // global_id
+                          uint64_t,  // global seq
+                          uint64_t,  // supported features
+                          uint64_t,  // required features
+                          uint64_t,  // flags
+                          uint64_t> {  // client cookie
+  static const Tag tag = Tag::CLIENT_IDENT;
+  using ControlFrame::Encode;
+  using ControlFrame::Decode;
+
+  inline entity_addrvec_t &addrs() { return get_val<0>(); }
+  inline entity_addr_t &target_addr() { return get_val<1>(); }
+  inline int64_t &gid() { return get_val<2>(); }
+  inline uint64_t &global_seq() { return get_val<3>(); }
+  inline uint64_t &supported_features() { return get_val<4>(); }
+  inline uint64_t &required_features() { return get_val<5>(); }
+  inline uint64_t &flags() { return get_val<6>(); }
+  inline uint64_t &cookie() { return get_val<7>(); }
+
+protected:
+  using ControlFrame::ControlFrame;
+};
+
+struct ServerIdentFrame
+    : public ControlFrame<ServerIdentFrame,
+                          entity_addrvec_t,  // my addresses
+                          int64_t,  // global_id
+                          uint64_t,  // global seq
+                          uint64_t,  // supported features
+                          uint64_t,  // required features
+                          uint64_t,  // flags
+                          uint64_t> {  // server cookie
+  static const Tag tag = Tag::SERVER_IDENT;
+  using ControlFrame::Encode;
+  using ControlFrame::Decode;
+
+  inline entity_addrvec_t &addrs() { return get_val<0>(); }
+  inline int64_t &gid() { return get_val<1>(); }
+  inline uint64_t &global_seq() { return get_val<2>(); }
+  inline uint64_t &supported_features() { return get_val<3>(); }
+  inline uint64_t &required_features() { return get_val<4>(); }
+  inline uint64_t &flags() { return get_val<5>(); }
+  inline uint64_t &cookie() { return get_val<6>(); }
+
+protected:
+  using ControlFrame::ControlFrame;
+};
+
+struct ReconnectFrame
+    : public ControlFrame<ReconnectFrame,
+                          entity_addrvec_t,  // my addresses
+                          uint64_t,  // client cookie
+                          uint64_t,  // server cookie
+                          uint64_t,  // global sequence
+                          uint64_t,  // connect sequence
+                          uint64_t> { // message sequence
+  static const Tag tag = Tag::SESSION_RECONNECT;
+  using ControlFrame::Encode;
+  using ControlFrame::Decode;
+
+  inline entity_addrvec_t &addrs() { return get_val<0>(); }
+  inline uint64_t &client_cookie() { return get_val<1>(); }
+  inline uint64_t &server_cookie() { return get_val<2>(); }
+  inline uint64_t &global_seq() { return get_val<3>(); }
+  inline uint64_t &connect_seq() { return get_val<4>(); }
+  inline uint64_t &msg_seq() { return get_val<5>(); }
+
+protected:
+  using ControlFrame::ControlFrame;
+};
+
+struct ResetFrame : public ControlFrame<ResetFrame,
+                                        bool> {  // full reset
+  static const Tag tag = Tag::SESSION_RESET;
+  using ControlFrame::Encode;
+  using ControlFrame::Decode;
+
+  inline bool &full() { return get_val<0>(); }
+
+protected:
+  using ControlFrame::ControlFrame;
+};
+
+struct RetryFrame : public ControlFrame<RetryFrame,
+                                        uint64_t> {  // connection seq
+  static const Tag tag = Tag::SESSION_RETRY;
+  using ControlFrame::Encode;
+  using ControlFrame::Decode;
+
+  inline uint64_t &connect_seq() { return get_val<0>(); }
+
+protected:
+  using ControlFrame::ControlFrame;
+};
+
+struct RetryGlobalFrame : public ControlFrame<RetryGlobalFrame,
+                                              uint64_t> { // global seq
+  static const Tag tag = Tag::SESSION_RETRY_GLOBAL;
+  using ControlFrame::Encode;
+  using ControlFrame::Decode;
+
+  inline uint64_t &global_seq() { return get_val<0>(); }
+
+protected:
+  using ControlFrame::ControlFrame;
+};
+
+struct WaitFrame : public ControlFrame<WaitFrame> {
+  static const Tag tag = Tag::WAIT;
+  using ControlFrame::Encode;
+  using ControlFrame::Decode;
+
+protected:
+  using ControlFrame::ControlFrame;
+};
+
+struct ReconnectOkFrame : public ControlFrame<ReconnectOkFrame,
+                                              uint64_t> { // message seq
+  static const Tag tag = Tag::SESSION_RECONNECT_OK;
+  using ControlFrame::Encode;
+  using ControlFrame::Decode;
+
+  inline uint64_t &msg_seq() { return get_val<0>(); }
+
+protected:
+  using ControlFrame::ControlFrame;
+};
+
+struct IdentMissingFeaturesFrame 
+    : public ControlFrame<IdentMissingFeaturesFrame,
+                          uint64_t> { // missing features mask
+  static const Tag tag = Tag::IDENT_MISSING_FEATURES;
+  using ControlFrame::Encode;
+  using ControlFrame::Decode;
+
+  inline uint64_t &features() { return get_val<0>(); }
+
+protected:
+  using ControlFrame::ControlFrame;
+};
+
+struct KeepAliveFrame : public ControlFrame<KeepAliveFrame,
+                                            utime_t> {  // timestamp
+  static const Tag tag = Tag::KEEPALIVE2;
+  using ControlFrame::Encode;
+  using ControlFrame::Decode;
+
+  static KeepAliveFrame Encode() {
+    return KeepAliveFrame::Encode(ceph_clock_now());
+  }
+
+  inline utime_t &timestamp() { return get_val<0>(); }
+
+protected:
+  using ControlFrame::ControlFrame;
+};
+
+struct KeepAliveFrameAck : public ControlFrame<KeepAliveFrameAck,
+                                               utime_t> { // ack timestamp
+  static const Tag tag = Tag::KEEPALIVE2_ACK;
+  using ControlFrame::Encode;
+  using ControlFrame::Decode;
+
+  inline utime_t &timestamp() { return get_val<0>(); }
+
+protected:
+  using ControlFrame::ControlFrame;
+};
+
+struct AckFrame : public ControlFrame<AckFrame,
+                                      uint64_t> { // message sequence
+  static const Tag tag = Tag::ACK;
+  using ControlFrame::Encode;
+  using ControlFrame::Decode;
+
+  inline uint64_t &seq() { return get_val<0>(); }
+
+protected:
+  using ControlFrame::ControlFrame;
+};
+
+using segment_bls_t =
+    boost::container::static_vector<bufferlist, MAX_NUM_SEGMENTS>;
+
+// This class is used for encoding/decoding header of the message frame.
+// Body is processed almost independently with the sole junction point
+// being the `extra_payload_len` passed to get_buffer().
+struct MessageFrame : public Frame<MessageFrame,
+                                   /* four segments */
+                                   segment_t::DEFAULT_ALIGNMENT,
+                                   segment_t::DEFAULT_ALIGNMENT,
+                                   segment_t::DEFAULT_ALIGNMENT,
+                                   segment_t::PAGE_SIZE_ALIGNMENT> {
+  static const Tag tag = Tag::MESSAGE;
+
+  static MessageFrame Encode(const ceph_msg_header2 &msg_header,
+                             const ceph::bufferlist &front,
+                             const ceph::bufferlist &middle,
+                             const ceph::bufferlist &data) {
+    MessageFrame f;
+    f.segments[SegmentIndex::Msg::HEADER].append(
+        reinterpret_cast<const char*>(&msg_header), sizeof(msg_header));
+
+    f.segments[SegmentIndex::Msg::FRONT] = front;
+    f.segments[SegmentIndex::Msg::MIDDLE] = middle;
+    f.segments[SegmentIndex::Msg::DATA] = data;
+
+    return f;
+  }
+
+  static MessageFrame Decode(segment_bls_t& recv_segments) {
+    MessageFrame f;
+    // transfer segments' bufferlists. If a MessageFrame contains less
+    // SegmentsNumV segments, the missing ones will be seen as zeroed.
+    for (__u8 idx = 0; idx < std::size(recv_segments); idx++) {
+      f.segments[idx] = std::move(recv_segments[idx]);
+    }
+    return f;
+  }
+
+  inline const ceph_msg_header2 &header() {
+    auto& hdrbl = segments[SegmentIndex::Msg::HEADER];
+    return reinterpret_cast<const ceph_msg_header2&>(*hdrbl.c_str());
+  }
+
+  ceph::bufferlist &front() {
+    return segments[SegmentIndex::Msg::FRONT];
+  }
+
+  ceph::bufferlist &middle() {
+    return segments[SegmentIndex::Msg::MIDDLE];
+  }
+
+  ceph::bufferlist &data() {
+    return segments[SegmentIndex::Msg::DATA];
+  }
+
+  uint32_t front_len() const {
+    return segments[SegmentIndex::Msg::FRONT].length();
+  }
+
+  uint32_t middle_len() const {
+    return segments[SegmentIndex::Msg::MIDDLE].length();
+  }
+
+  uint32_t data_len() const {
+    return segments[SegmentIndex::Msg::DATA].length();
+  }
+
+protected:
+  using Frame::Frame;
+};
+
+} // namespace ceph::msgr::v2
+
+#endif // _MSG_ASYNC_FRAMES_V2_
diff --git a/src/msg/async/net_handler.cc b/src/msg/async/net_handler.cc
index 19adb2c83f2..2b4e646d559 100644
--- a/src/msg/async/net_handler.cc
+++ b/src/msg/async/net_handler.cc
@@ -22,8 +22,10 @@
 #include <arpa/inet.h>
 
 #include "net_handler.h"
-#include "common/errno.h"
 #include "common/debug.h"
+#include "common/errno.h"
+#include "include/compat.h"
+#include "include/sock_compat.h"
 
 #define dout_subsys ceph_subsys_ms
 #undef dout_prefix
@@ -36,7 +38,7 @@ int NetHandler::create_socket(int domain, bool reuse_addr)
   int s;
   int r = 0;
 
-  if ((s = ::socket(domain, SOCK_STREAM, 0)) == -1) {
+  if ((s = socket_cloexec(domain, SOCK_STREAM, 0)) == -1) {
     r = errno;
     lderr(cct) << __func__ << " couldn't create socket " << cpp_strerror(r) << dendl;
     return -r;
@@ -82,22 +84,6 @@ int NetHandler::set_nonblock(int sd)
   return 0;
 }
 
-void NetHandler::set_close_on_exec(int sd)
-{
-  int flags = fcntl(sd, F_GETFD, 0);
-  if (flags < 0) {
-    int r = errno;
-    lderr(cct) << __func__ << " fcntl(F_GETFD): "
-	       << cpp_strerror(r) << dendl;
-    return;
-  }
-  if (fcntl(sd, F_SETFD, flags | FD_CLOEXEC)) {
-    int r = errno;
-    lderr(cct) << __func__ << " fcntl(F_SETFD): "
-	       << cpp_strerror(r) << dendl;
-  }
-}
-
 int NetHandler::set_socket_options(int sd, bool nodelay, int size)
 {
   int r = 0;
@@ -119,7 +105,7 @@ int NetHandler::set_socket_options(int sd, bool nodelay, int size)
   }
 
   // block ESIGPIPE
-#ifdef SO_NOSIGPIPE
+#ifdef CEPH_USE_SO_NOSIGPIPE
   int val = 1;
   r = ::setsockopt(sd, SOL_SOCKET, SO_NOSIGPIPE, (void*)&val, sizeof(val));
   if (r) {
@@ -136,9 +122,9 @@ void NetHandler::set_priority(int sd, int prio, int domain)
   if (prio < 0) {
     return;
   }
+  int r = -1;
 #ifdef IPTOS_CLASS_CS6
   int iptos = IPTOS_CLASS_CS6;
-  int r = -1;
   switch (domain) {
   case AF_INET:
     r = ::setsockopt(sd, IPPROTO_IP, IP_TOS, &iptos, sizeof(iptos));
diff --git a/src/msg/async/net_handler.h b/src/msg/async/net_handler.h
index c4fb73ee291..1904237724f 100644
--- a/src/msg/async/net_handler.h
+++ b/src/msg/async/net_handler.h
@@ -27,7 +27,6 @@ namespace ceph {
     int create_socket(int domain, bool reuse_addr=false);
     explicit NetHandler(CephContext *c): cct(c) {}
     int set_nonblock(int sd);
-    void set_close_on_exec(int sd);
     int set_socket_options(int sd, bool nodelay, int size);
     int connect(const entity_addr_t &addr, const entity_addr_t& bind_addr);
     
diff --git a/src/msg/async/rdma/Infiniband.cc b/src/msg/async/rdma/Infiniband.cc
index 37e1a530778..e2c06229f41 100644
--- a/src/msg/async/rdma/Infiniband.cc
+++ b/src/msg/async/rdma/Infiniband.cc
@@ -18,6 +18,8 @@
 #include "common/errno.h"
 #include "common/debug.h"
 #include "RDMAStack.h"
+#include <sys/time.h>
+#include <sys/resource.h>
 
 #define dout_subsys ceph_subsys_ms
 #undef dout_prefix
@@ -28,21 +30,24 @@ static const uint32_t MAX_INLINE_DATA = 0;
 static const uint32_t TCP_MSG_LEN = sizeof("0000:00000000:00000000:00000000:00000000000000000000000000000000");
 static const uint32_t CQ_DEPTH = 30000;
 
-Port::Port(CephContext *cct, struct ibv_context* ictxt, uint8_t ipn): ctxt(ictxt), port_num(ipn), port_attr(new ibv_port_attr)
+Port::Port(CephContext *cct, struct ibv_context* ictxt, uint8_t ipn): ctxt(ictxt), port_num(ipn),
+  gid_idx(cct->_conf.get_val<int64_t>("ms_async_rdma_gid_idx"))
 {
+  int r = ibv_query_port(ctxt, port_num, &port_attr);
+  if (r == -1) {
+    lderr(cct) << __func__  << " query port failed  " << cpp_strerror(errno) << dendl;
+    ceph_abort();
+  }
+
+  lid = port_attr.lid;
+  ceph_assert(gid_idx < port_attr.gid_tbl_len);
 #ifdef HAVE_IBV_EXP
   union ibv_gid cgid;
   struct ibv_exp_gid_attr gid_attr;
   bool malformed = false;
 
   ldout(cct,1) << __func__ << " using experimental verbs for gid" << dendl;
-  int r = ibv_query_port(ctxt, port_num, port_attr);
-  if (r == -1) {
-    lderr(cct) << __func__  << " query port failed  " << cpp_strerror(errno) << dendl;
-    ceph_abort();
-  }
 
-  lid = port_attr->lid;
 
   // search for requested GID in GIDs table
   ldout(cct, 1) << __func__ << " looking for local GID " << (cct->_conf->ms_async_rdma_local_gid)
@@ -66,7 +71,7 @@ Port::Port(CephContext *cct, struct ibv_context* ictxt, uint8_t ipn): ctxt(ictxt
 
   gid_attr.comp_mask = IBV_EXP_QUERY_GID_ATTR_TYPE;
 
-  for (gid_idx = 0; gid_idx < port_attr->gid_tbl_len; gid_idx++) {
+  for (gid_idx = 0; gid_idx < port_attr.gid_tbl_len; gid_idx++) {
     r = ibv_query_gid(ctxt, port_num, gid_idx, &gid);
     if (r) {
       lderr(cct) << __func__  << " query gid of port " << port_num << " index " << gid_idx << " failed  " << cpp_strerror(errno) << dendl;
@@ -86,19 +91,12 @@ Port::Port(CephContext *cct, struct ibv_context* ictxt, uint8_t ipn): ctxt(ictxt
     }
   }
 
-  if (gid_idx == port_attr->gid_tbl_len) {
+  if (gid_idx == port_attr.gid_tbl_len) {
     lderr(cct) << __func__ << " Requested local GID was not found in GID table" << dendl;
     ceph_abort();
   }
 #else
-  int r = ibv_query_port(ctxt, port_num, port_attr);
-  if (r == -1) {
-    lderr(cct) << __func__  << " query port failed  " << cpp_strerror(errno) << dendl;
-    ceph_abort();
-  }
-
-  lid = port_attr->lid;
-  r = ibv_query_gid(ctxt, port_num, 0, &gid);
+  r = ibv_query_gid(ctxt, port_num, gid_idx, &gid);
   if (r) {
     lderr(cct) << __func__  << " query gid failed  " << cpp_strerror(errno) << dendl;
     ceph_abort();
@@ -106,42 +104,54 @@ Port::Port(CephContext *cct, struct ibv_context* ictxt, uint8_t ipn): ctxt(ictxt
 #endif
 }
 
-
-Device::Device(CephContext *cct, ibv_device* d): device(d), device_attr(new ibv_device_attr), active_port(nullptr)
+Device::Device(CephContext *cct, ibv_device* ib_dev): device(ib_dev), active_port(nullptr)
 {
-  if (device == NULL) {
-    lderr(cct) << __func__ << " device == NULL" << cpp_strerror(errno) << dendl;
-    ceph_abort();
-  }
-  name = ibv_get_device_name(device);
+  ceph_assert(device);
   ctxt = ibv_open_device(device);
-  if (ctxt == NULL) {
-    lderr(cct) << __func__ << " open rdma device failed. " << cpp_strerror(errno) << dendl;
+  ceph_assert(ctxt);
+
+  name = ibv_get_device_name(device);
+
+  int r = ibv_query_device(ctxt, &device_attr);
+  if (r) {
+    lderr(cct) << __func__ << " failed to query rdma device. " << cpp_strerror(errno) << dendl;
     ceph_abort();
   }
-  int r = ibv_query_device(ctxt, device_attr);
-  if (r == -1) {
+}
+
+Device::Device(CephContext *cct, struct ibv_context *ib_ctx): device(ib_ctx->device),
+                                                              active_port(nullptr)
+{
+  ceph_assert(device);
+  ctxt = ib_ctx;
+  ceph_assert(ctxt);
+
+  name = ibv_get_device_name(device);
+
+  int r = ibv_query_device(ctxt, &device_attr);
+  if (r) {
     lderr(cct) << __func__ << " failed to query rdma device. " << cpp_strerror(errno) << dendl;
     ceph_abort();
   }
 }
 
 void Device::binding_port(CephContext *cct, int port_num) {
-  port_cnt = device_attr->phys_port_cnt;
-  for (uint8_t i = 0; i < port_cnt; ++i) {
-    Port *port = new Port(cct, ctxt, i+1);
-    if (i + 1 == port_num && port->get_port_attr()->state == IBV_PORT_ACTIVE) {
+  port_cnt = device_attr.phys_port_cnt;
+  for (uint8_t port_id = 1; port_id <= port_cnt; ++port_id) {
+    Port *port = new Port(cct, ctxt, port_id);
+    if (port_id == port_num && port->get_port_attr()->state == IBV_PORT_ACTIVE) {
       active_port = port;
-      ldout(cct, 1) << __func__ << " found active port " << i+1 << dendl;
+      ldout(cct, 1) << __func__ << " found active port " << static_cast<int>(port_id) << dendl;
       break;
     } else {
-      ldout(cct, 10) << __func__ << " port " << i+1 << " is not what we want. state: " << port->get_port_attr()->state << ")"<< dendl;
+      ldout(cct, 10) << __func__ << " port " << port_id << " is not what we want. state: "
+                     << ibv_port_state_str(port->get_port_attr()->state) << dendl;
+      delete port;
     }
-    delete port;
   }
   if (nullptr == active_port) {
     lderr(cct) << __func__ << "  port not found" << dendl;
-    assert(active_port);
+    ceph_assert(active_port);
   }
 }
 
@@ -150,7 +160,7 @@ Infiniband::QueuePair::QueuePair(
     CephContext *c, Infiniband& infiniband, ibv_qp_type type,
     int port, ibv_srq *srq,
     Infiniband::CompletionQueue* txcq, Infiniband::CompletionQueue* rxcq,
-    uint32_t max_send_wr, uint32_t max_recv_wr, uint32_t q_key)
+    uint32_t tx_queue_len, uint32_t rx_queue_len, struct rdma_cm_id *cid, uint32_t q_key)
 : cct(c), infiniband(infiniband),
   type(type),
   ctxt(infiniband.device->ctxt),
@@ -158,52 +168,113 @@ Infiniband::QueuePair::QueuePair(
   pd(infiniband.pd->pd),
   srq(srq),
   qp(NULL),
+  cm_id(cid), peer_cm_meta{0}, local_cm_meta{0},
   txcq(txcq),
   rxcq(rxcq),
-  initial_psn(0),
-  max_send_wr(max_send_wr),
-  max_recv_wr(max_recv_wr),
+  initial_psn(lrand48() & PSN_MSK),
+  // One extra WR for beacon
+  max_send_wr(tx_queue_len + 1),
+  max_recv_wr(rx_queue_len),
   q_key(q_key),
   dead(false)
 {
-  initial_psn = lrand48() & 0xffffff;
   if (type != IBV_QPT_RC && type != IBV_QPT_UD && type != IBV_QPT_RAW_PACKET) {
     lderr(cct) << __func__ << " invalid queue pair type" << cpp_strerror(errno) << dendl;
     ceph_abort();
   }
-  pd = infiniband.pd->pd;
 }
 
-int Infiniband::QueuePair::init()
+int Infiniband::QueuePair::modify_qp_to_error(void)
 {
-  ldout(cct, 20) << __func__ << " started." << dendl;
-  ibv_qp_init_attr qpia;
-  memset(&qpia, 0, sizeof(qpia));
-  qpia.send_cq = txcq->get_cq();
-  qpia.recv_cq = rxcq->get_cq();
-  qpia.srq = srq;                      // use the same shared receive queue
-  qpia.cap.max_send_wr  = max_send_wr; // max outstanding send requests
-  qpia.cap.max_send_sge = 1;           // max send scatter-gather elements
-  qpia.cap.max_inline_data = MAX_INLINE_DATA;          // max bytes of immediate data on send q
-  qpia.qp_type = type;                 // RC, UC, UD, or XRC
-  qpia.sq_sig_all = 0;                 // only generate CQEs on requested WQEs
-
-  qp = ibv_create_qp(pd, &qpia);
-  if (qp == NULL) {
-    lderr(cct) << __func__ << " failed to create queue pair" << cpp_strerror(errno) << dendl;
-    if (errno == ENOMEM) {
-      lderr(cct) << __func__ << " try reducing ms_async_rdma_receive_buffers, "
-				" ms_async_rdma_send_buffers or"
-				" ms_async_rdma_buffer_size" << dendl;
+    ibv_qp_attr qpa;
+    // FIPS zeroization audit 20191115: this memset is not security related.
+    memset(&qpa, 0, sizeof(qpa));
+    qpa.qp_state = IBV_QPS_ERR;
+    if (ibv_modify_qp(qp, &qpa, IBV_QP_STATE)) {
+      lderr(cct) << __func__ << " failed to transition to ERROR state: " << cpp_strerror(errno) << dendl;
+      return -1;
     }
+    ldout(cct, 20) << __func__ << " transition to ERROR state successfully." << dendl;
+    return 0;
+}
+
+int Infiniband::QueuePair::modify_qp_to_rts(void)
+{
+  // move from RTR state RTS
+  ibv_qp_attr qpa;
+  // FIPS zeroization audit 20191115: this memset is not security related.
+  memset(&qpa, 0, sizeof(qpa));
+  qpa.qp_state = IBV_QPS_RTS;
+  /*
+   * How long to wait before retrying if packet lost or server dead.
+   * Supposedly the timeout is 4.096us*2^timeout.  However, the actual
+   * timeout appears to be 4.096us*2^(timeout+1), so the setting
+   * below creates a 135ms timeout.
+   */
+  qpa.timeout = 0x12;
+  // How many times to retry after timeouts before giving up.
+  qpa.retry_cnt = 7;
+  /*
+   * How many times to retry after RNR (receiver not ready) condition
+   * before giving up. Occurs when the remote side has not yet posted
+   * a receive request.
+   */
+  qpa.rnr_retry = 7; // 7 is infinite retry.
+  qpa.sq_psn = local_cm_meta.psn;
+  qpa.max_rd_atomic = 1;
+
+  int attr_mask = IBV_QP_STATE | IBV_QP_TIMEOUT | IBV_QP_RETRY_CNT | IBV_QP_RNR_RETRY | IBV_QP_SQ_PSN | IBV_QP_MAX_QP_RD_ATOMIC;
+  int r = ibv_modify_qp(qp, &qpa, attr_mask);
+  if (r) {
+    lderr(cct) << __func__ << " failed to transition to RTS state: " << cpp_strerror(errno) << dendl;
     return -1;
   }
+  ldout(cct, 20) << __func__ << " transition to RTS state successfully." << dendl;
+  return 0;
+}
 
-  ldout(cct, 20) << __func__ << " successfully create queue pair: "
-                 << "qp=" << qp << dendl;
+int Infiniband::QueuePair::modify_qp_to_rtr(void)
+{
+  // move from INIT to RTR state
+  ibv_qp_attr qpa;
+  // FIPS zeroization audit 20191115: this memset is not security related.
+  memset(&qpa, 0, sizeof(qpa));
+  qpa.qp_state = IBV_QPS_RTR;
+  qpa.path_mtu = IBV_MTU_1024;
+  qpa.dest_qp_num = peer_cm_meta.local_qpn;
+  qpa.rq_psn = peer_cm_meta.psn;
+  qpa.max_dest_rd_atomic = 1;
+  qpa.min_rnr_timer = 0x12;
+  qpa.ah_attr.is_global = 1;
+  qpa.ah_attr.grh.hop_limit = 6;
+  qpa.ah_attr.grh.dgid = peer_cm_meta.gid;
+  qpa.ah_attr.grh.sgid_index = infiniband.get_device()->get_gid_idx();
+  qpa.ah_attr.grh.traffic_class = cct->_conf->ms_async_rdma_dscp;
+  //qpa.ah_attr.grh.flow_label = 0;
+
+  qpa.ah_attr.dlid = peer_cm_meta.lid;
+  qpa.ah_attr.sl = cct->_conf->ms_async_rdma_sl;
+  qpa.ah_attr.src_path_bits = 0;
+  qpa.ah_attr.port_num = (uint8_t)(ib_physical_port);
+
+  ldout(cct, 20) << __func__ << " Choosing gid_index " << (int)qpa.ah_attr.grh.sgid_index << ", sl " << (int)qpa.ah_attr.sl << dendl;
+
+  int attr_mask = IBV_QP_STATE | IBV_QP_AV | IBV_QP_PATH_MTU | IBV_QP_DEST_QPN | IBV_QP_RQ_PSN | IBV_QP_MIN_RNR_TIMER | IBV_QP_MAX_DEST_RD_ATOMIC;
+
+  int r = ibv_modify_qp(qp, &qpa, attr_mask);
+  if (r) {
+    lderr(cct) << __func__ << " failed to transition to RTR state: " << cpp_strerror(errno) << dendl;
+    return -1;
+  }
+  ldout(cct, 20) << __func__ << " transition to RTR state successfully." << dendl;
+  return 0;
+}
 
+int Infiniband::QueuePair::modify_qp_to_init(void)
+{
   // move from RESET to INIT state
   ibv_qp_attr qpa;
+  // FIPS zeroization audit 20191115: this memset is not security related.
   memset(&qpa, 0, sizeof(qpa));
   qpa.qp_state   = IBV_QPS_INIT;
   qpa.pkey_index = 0;
@@ -227,27 +298,175 @@ int Infiniband::QueuePair::init()
       ceph_abort();
   }
 
-  int ret = ibv_modify_qp(qp, &qpa, mask);
-  if (ret) {
-    ibv_destroy_qp(qp);
-    lderr(cct) << __func__ << " failed to transition to INIT state: "
-               << cpp_strerror(errno) << dendl;
+  if (ibv_modify_qp(qp, &qpa, mask)) {
+    lderr(cct) << __func__ << " failed to switch to INIT state Queue Pair, qp number: " << qp->qp_num
+               << " Error: " << cpp_strerror(errno) << dendl;
     return -1;
   }
-  ldout(cct, 20) << __func__ << " successfully change queue pair to INIT:"
-                 << " qp=" << qp << dendl;
+  ldout(cct, 20) << __func__ << " successfully switch to INIT state Queue Pair, qp number: " << qp->qp_num << dendl;
+  return 0;
+}
+
+int Infiniband::QueuePair::init()
+{
+  ldout(cct, 20) << __func__ << " started." << dendl;
+  ibv_qp_init_attr qpia;
+  // FIPS zeroization audit 20191115: this memset is not security related.
+  memset(&qpia, 0, sizeof(qpia));
+  qpia.send_cq = txcq->get_cq();
+  qpia.recv_cq = rxcq->get_cq();
+  if (srq) {
+    qpia.srq = srq;                      // use the same shared receive queue
+  } else {
+    qpia.cap.max_recv_wr = max_recv_wr;
+    qpia.cap.max_recv_sge = 1;
+  }
+  qpia.cap.max_send_wr  = max_send_wr; // max outstanding send requests
+  qpia.cap.max_send_sge = 1;           // max send scatter-gather elements
+  qpia.cap.max_inline_data = MAX_INLINE_DATA;          // max bytes of immediate data on send q
+  qpia.qp_type = type;                 // RC, UC, UD, or XRC
+  qpia.sq_sig_all = 0;                 // only generate CQEs on requested WQEs
+
+  if (!cct->_conf->ms_async_rdma_cm) {
+    qp = ibv_create_qp(pd, &qpia);
+    if (qp == NULL) {
+      lderr(cct) << __func__ << " failed to create queue pair" << cpp_strerror(errno) << dendl;
+      if (errno == ENOMEM) {
+        lderr(cct) << __func__ << " try reducing ms_async_rdma_receive_queue_length, "
+                                  " ms_async_rdma_send_buffers or"
+                                  " ms_async_rdma_buffer_size" << dendl;
+      }
+      return -1;
+    }
+    if (modify_qp_to_init() != 0) {
+      ibv_destroy_qp(qp);
+      return -1;
+    }
+  } else {
+    ceph_assert(cm_id->verbs == pd->context);
+    if (rdma_create_qp(cm_id, pd, &qpia)) {
+      lderr(cct) << __func__ << " failed to create queue pair with rdmacm library"
+                 << cpp_strerror(errno) << dendl;
+      return -1;
+    }
+    qp = cm_id->qp;
+  }
+  ldout(cct, 20) << __func__ << " successfully create queue pair: "
+                 << "qp=" << qp << dendl;
+  local_cm_meta.local_qpn = get_local_qp_number();
+  local_cm_meta.psn = get_initial_psn();
+  local_cm_meta.lid = infiniband.get_lid();
+  local_cm_meta.peer_qpn = 0;
+  local_cm_meta.gid = infiniband.get_gid();
+  if (!srq) {
+    int rq_wrs = infiniband.post_chunks_to_rq(max_recv_wr, this);
+    if (rq_wrs  == 0) {
+      lderr(cct) << __func__ << " intialize no SRQ Queue Pair, qp number: " << qp->qp_num
+                 << " fatal error: can't post SQ WR " << dendl;
+      return -1;
+    }
+    ldout(cct, 20) << __func__ << " initialize no SRQ Queue Pair, qp number: "
+                   << qp->qp_num << " post SQ WR " << rq_wrs << dendl;
+  }
+  return 0;
+}
+
+void Infiniband::QueuePair::wire_gid_to_gid(const char *wgid, ib_cm_meta_t* cm_meta_data)
+{
+  char tmp[9];
+  uint32_t v32;
+  int i;
+
+  for (tmp[8] = 0, i = 0; i < 4; ++i) {
+    memcpy(tmp, wgid + i * 8, 8);
+    sscanf(tmp, "%x", &v32);
+    *(uint32_t *)(&cm_meta_data->gid.raw[i * 4]) = ntohl(v32);
+  }
+}
+
+void Infiniband::QueuePair::gid_to_wire_gid(const ib_cm_meta_t& cm_meta_data, char wgid[])
+{
+  for (int i = 0; i < 4; ++i)
+    sprintf(&wgid[i * 8], "%08x", htonl(*(uint32_t *)(cm_meta_data.gid.raw + i * 4)));
+}
+
+/*
+ * return value
+ *   1: means no valid buffer read
+ *   0: means got enough buffer
+ * < 0: means error
+ */
+int Infiniband::QueuePair::recv_cm_meta(CephContext *cct, int socket_fd)
+{
+  char msg[TCP_MSG_LEN];
+  char gid[33];
+  ssize_t r = ::read(socket_fd, &msg, sizeof(msg));
+  // Drop incoming qpt
+  if (cct->_conf->ms_inject_socket_failures && socket_fd >= 0) {
+    if (rand() % cct->_conf->ms_inject_socket_failures == 0) {
+      ldout(cct, 0) << __func__ << " injecting socket failure" << dendl;
+      return -EINVAL;
+    }
+  }
+  if (r < 0) {
+    r = -errno;
+    lderr(cct) << __func__ << " got error " << r << ": "
+               << cpp_strerror(r) << dendl;
+  } else if (r == 0) { // valid disconnect message of length 0
+    ldout(cct, 10) << __func__ << " got disconnect message " << dendl;
+  } else if ((size_t)r != sizeof(msg)) { // invalid message
+    ldout(cct, 1) << __func__ << " got bad length (" << r << ") " << dendl;
+    r = -EINVAL;
+  } else { // valid message
+    sscanf(msg, "%hx:%x:%x:%x:%s", &(peer_cm_meta.lid), &(peer_cm_meta.local_qpn), &(peer_cm_meta.psn), &(peer_cm_meta.peer_qpn), gid);
+    wire_gid_to_gid(gid, &peer_cm_meta);
+    ldout(cct, 5) << __func__ << " recevd: " << peer_cm_meta.lid << ", " << peer_cm_meta.local_qpn
+                  << ", " << peer_cm_meta.psn << ", " << peer_cm_meta.peer_qpn << ", " << gid << dendl;
+  }
+  return r;
+}
+
+int Infiniband::QueuePair::send_cm_meta(CephContext *cct, int socket_fd)
+{
+  int retry = 0;
+  ssize_t r;
+
+  char msg[TCP_MSG_LEN];
+  char gid[33];
+retry:
+  gid_to_wire_gid(local_cm_meta, gid);
+  sprintf(msg, "%04x:%08x:%08x:%08x:%s", local_cm_meta.lid, local_cm_meta.local_qpn, local_cm_meta.psn, local_cm_meta.peer_qpn, gid);
+  ldout(cct, 10) << __func__ << " sending: " << local_cm_meta.lid << ", " << local_cm_meta.local_qpn
+                 << ", " << local_cm_meta.psn << ", " << local_cm_meta.peer_qpn << ", "  << gid  << dendl;
+  r = ::write(socket_fd, msg, sizeof(msg));
+  // Drop incoming qpt
+  if (cct->_conf->ms_inject_socket_failures && socket_fd >= 0) {
+    if (rand() % cct->_conf->ms_inject_socket_failures == 0) {
+      ldout(cct, 0) << __func__ << " injecting socket failure" << dendl;
+      return -EINVAL;
+    }
+  }
+
+  if ((size_t)r != sizeof(msg)) {
+    // FIXME need to handle EAGAIN instead of retry
+    if (r < 0 && (errno == EINTR || errno == EAGAIN) && retry < 3) {
+      retry++;
+      goto retry;
+    }
+    if (r < 0)
+      lderr(cct) << __func__ << " send returned error " << errno << ": "
+                 << cpp_strerror(errno) << dendl;
+    else
+      lderr(cct) << __func__ << " send got bad length (" << r << ") " << cpp_strerror(errno) << dendl;
+    return -errno;
+  }
   return 0;
 }
 
 /**
- * Change RC QueuePair into the ERROR state. This is necessary modify
- * the Queue Pair into the Error state and poll all of the relevant
- * Work Completions prior to destroying a Queue Pair.
- * Since destroying a Queue Pair does not guarantee that its Work
- * Completions are removed from the CQ upon destruction. Even if the
- * Work Completions are already in the CQ, it might not be possible to
- * retrieve them. If the Queue Pair is associated with an SRQ, it is
- * recommended wait for the affiliated event IBV_EVENT_QP_LAST_WQE_REACHED
+ * Switch QP to ERROR state and then post a beacon to be able to drain all
+ * WCEs and then safely destroy QP. See RDMADispatcher::handle_tx_event()
+ * for details.
  *
  * \return
  *      -errno if the QueuePair can't switch to ERROR
@@ -257,19 +476,27 @@ int Infiniband::QueuePair::to_dead()
 {
   if (dead)
     return 0;
-  ibv_qp_attr qpa;
-  memset(&qpa, 0, sizeof(qpa));
-  qpa.qp_state = IBV_QPS_ERR;
 
-  int mask = IBV_QP_STATE;
-  int ret = ibv_modify_qp(qp, &qpa, mask);
-  if (ret) {
-    lderr(cct) << __func__ << " failed to transition to ERROR state: "
-               << cpp_strerror(errno) << dendl;
+  if (modify_qp_to_error()) {
+    return -1;
+  }
+  ldout(cct, 20) << __func__ << " force trigger error state Queue Pair, qp number: " << local_cm_meta.local_qpn
+                 << " bound remote QueuePair, qp number: " << local_cm_meta.peer_qpn << dendl;
+
+  struct ibv_send_wr *bad_wr = nullptr, beacon;
+  // FIPS zeroization audit 20191115: this memset is not security related.
+  memset(&beacon, 0, sizeof(beacon));
+  beacon.wr_id = BEACON_WRID;
+  beacon.opcode = IBV_WR_SEND;
+  beacon.send_flags = IBV_SEND_SIGNALED;
+  if (ibv_post_send(qp, &beacon, &bad_wr)) {
+    lderr(cct) << __func__ << " failed to send a beacon: " << cpp_strerror(errno) << dendl;
     return -errno;
   }
+  ldout(cct, 20) << __func__ << " trigger error state Queue Pair, qp number: " << local_cm_meta.local_qpn << " Beacon sent " << dendl;
   dead = true;
-  return ret;
+
+  return 0;
 }
 
 int Infiniband::QueuePair::get_remote_qp_number(uint32_t *rqp) const
@@ -328,24 +555,6 @@ int Infiniband::QueuePair::get_state() const
   return qpa.qp_state;
 }
 
-/**
- * Return true if the queue pair is in an error state, false otherwise.
- */
-bool Infiniband::QueuePair::is_error() const
-{
-  ibv_qp_attr qpa;
-  ibv_qp_init_attr qpia;
-
-  int r = ibv_query_qp(qp, &qpa, -1, &qpia);
-  if (r) {
-    lderr(cct) << __func__ << " failed to get state: "
-      << cpp_strerror(errno) << dendl;
-    return true;
-  }
-  return qpa.cur_qp_state == IBV_QPS_ERR;
-}
-
-
 Infiniband::CompletionChannel::CompletionChannel(CephContext *c, Infiniband &ib)
   : cct(c), infiniband(ib), channel(NULL), cq(NULL), cq_events_that_need_ack(0)
 {
@@ -357,7 +566,7 @@ Infiniband::CompletionChannel::~CompletionChannel()
     int r = ibv_destroy_comp_channel(channel);
     if (r < 0)
       lderr(cct) << __func__ << " failed to destroy cc: " << cpp_strerror(errno) << dendl;
-    assert(r == 0);
+    ceph_assert(r == 0);
   }
 }
 
@@ -414,7 +623,7 @@ Infiniband::CompletionQueue::~CompletionQueue()
     int r = ibv_destroy_cq(cq);
     if (r < 0)
       lderr(cct) << __func__ << " failed to destroy cq: " << cpp_strerror(errno) << dendl;
-    assert(r == 0);
+    ceph_assert(r == 0);
   }
 }
 
@@ -474,8 +683,9 @@ Infiniband::ProtectionDomain::~ProtectionDomain()
 }
 
 
-Infiniband::MemoryManager::Chunk::Chunk(ibv_mr* m, uint32_t len, char* b)
-  : mr(m), bytes(len), offset(0), buffer(b)
+Infiniband::MemoryManager::Chunk::Chunk(ibv_mr* m, uint32_t bytes, char* buffer,
+    uint32_t offset, uint32_t bound, uint32_t lkey, QueuePair* qp)
+  : mr(m), qp(qp), lkey(lkey), bytes(bytes), offset(offset), bound(bound), buffer(buffer)
 {
 }
 
@@ -483,19 +693,14 @@ Infiniband::MemoryManager::Chunk::~Chunk()
 {
 }
 
-void Infiniband::MemoryManager::Chunk::set_offset(uint32_t o)
-{
-  offset = o;
-}
-
 uint32_t Infiniband::MemoryManager::Chunk::get_offset()
 {
   return offset;
 }
 
-void Infiniband::MemoryManager::Chunk::set_bound(uint32_t b)
+uint32_t Infiniband::MemoryManager::Chunk::get_size() const
 {
-  bound = b;
+  return bound - offset;
 }
 
 void Infiniband::MemoryManager::Chunk::prepare_read(uint32_t b)
@@ -511,31 +716,19 @@ uint32_t Infiniband::MemoryManager::Chunk::get_bound()
 
 uint32_t Infiniband::MemoryManager::Chunk::read(char* buf, uint32_t len)
 {
-  uint32_t left = bound - offset;
-  if (left >= len) {
-    memcpy(buf, buffer+offset, len);
-    offset += len;
-    return len;
-  } else {
-    memcpy(buf, buffer+offset, left);
-    offset = 0;
-    bound = 0;
-    return left;
-  }
+  uint32_t left = get_size();
+  uint32_t read_len = left <= len ? left : len;
+  memcpy(buf, buffer + offset, read_len);
+  offset += read_len;
+  return read_len;
 }
 
 uint32_t Infiniband::MemoryManager::Chunk::write(char* buf, uint32_t len)
 {
-  uint32_t left = bytes - offset;
-  if (left >= len) {
-    memcpy(buffer+offset, buf, len);
-    offset += len;
-    return len;
-  } else {
-    memcpy(buffer+offset, buf, left);
-    offset = bytes;
-    return left;
-  }
+  uint32_t write_len = (bytes - offset) <= len ? (bytes - offset) : len;
+  memcpy(buffer + offset, buf, write_len);
+  offset += write_len;
+  return write_len;
 }
 
 bool Infiniband::MemoryManager::Chunk::full()
@@ -543,63 +736,54 @@ bool Infiniband::MemoryManager::Chunk::full()
   return offset == bytes;
 }
 
-bool Infiniband::MemoryManager::Chunk::over()
-{
-  return Infiniband::MemoryManager::Chunk::offset == bound;
-}
-
-void Infiniband::MemoryManager::Chunk::clear()
+void Infiniband::MemoryManager::Chunk::reset_read_chunk()
 {
   offset = 0;
   bound = 0;
 }
 
-void Infiniband::MemoryManager::Chunk::post_srq(Infiniband *ib)
+void Infiniband::MemoryManager::Chunk::reset_write_chunk()
 {
-  ib->post_chunk(this);
+  offset = 0;
+  bound = bytes;
 }
 
 Infiniband::MemoryManager::Cluster::Cluster(MemoryManager& m, uint32_t s)
-  : manager(m), buffer_size(s), lock("cluster_lock")
+  : manager(m), buffer_size(s)
 {
 }
 
 Infiniband::MemoryManager::Cluster::~Cluster()
 {
   int r = ibv_dereg_mr(chunk_base->mr);
-  assert(r == 0);
+  ceph_assert(r == 0);
   const auto chunk_end = chunk_base + num_chunk;
   for (auto chunk = chunk_base; chunk != chunk_end; chunk++) {
     chunk->~Chunk();
   }
 
   ::free(chunk_base);
-  if (manager.enabled_huge_page)
-    manager.free_huge_pages(base);
-  else
-    ::free(base);
+  manager.free(base);
 }
 
 int Infiniband::MemoryManager::Cluster::fill(uint32_t num)
 {
-  assert(!base);
+  ceph_assert(!base);
   num_chunk = num;
   uint32_t bytes = buffer_size * num;
-  if (manager.enabled_huge_page) {
-    base = (char*)manager.malloc_huge_pages(bytes);
-  } else {
-    base = (char*)memalign(CEPH_PAGE_SIZE, bytes);
-  }
+
+  base = (char*)manager.malloc(bytes);
   end = base + bytes;
-  assert(base);
+  ceph_assert(base);
   chunk_base = static_cast<Chunk*>(::malloc(sizeof(Chunk) * num));
-  memset(chunk_base, 0, sizeof(Chunk) * num);
+  // FIPS zeroization audit 20191115: this memset is not security related.
+  memset(static_cast<void*>(chunk_base), 0, sizeof(Chunk) * num);
   free_chunks.reserve(num);
   ibv_mr* m = ibv_reg_mr(manager.pd->pd, base, bytes, IBV_ACCESS_REMOTE_WRITE | IBV_ACCESS_LOCAL_WRITE);
-  assert(m);
+  ceph_assert(m);
   Chunk* chunk = chunk_base;
   for (uint32_t offset = 0; offset < bytes; offset += buffer_size){
-    new(chunk) Chunk(m, buffer_size, base+offset);
+    new(chunk) Chunk(m, buffer_size, base + offset, 0, buffer_size, m->lkey);
     free_chunks.push_back(chunk);
     chunk++;
   }
@@ -608,86 +792,205 @@ int Infiniband::MemoryManager::Cluster::fill(uint32_t num)
 
 void Infiniband::MemoryManager::Cluster::take_back(std::vector<Chunk*> &ck)
 {
-  Mutex::Locker l(lock);
+  std::lock_guard l{lock};
   for (auto c : ck) {
-    c->clear();
+    c->reset_write_chunk();
     free_chunks.push_back(c);
   }
 }
 
-int Infiniband::MemoryManager::Cluster::get_buffers(std::vector<Chunk*> &chunks, size_t bytes)
+int Infiniband::MemoryManager::Cluster::get_buffers(std::vector<Chunk*> &chunks, size_t block_size)
 {
-  uint32_t num = bytes / buffer_size + 1;
-  if (bytes % buffer_size == 0)
-    --num;
-  int r = num;
-  Mutex::Locker l(lock);
-  if (free_chunks.empty())
-    return 0;
-  if (!bytes) {
-    r = free_chunks.size();
-    for (auto c : free_chunks)
-      chunks.push_back(c);
-    free_chunks.clear();
-    return r;
-  }
-  if (free_chunks.size() < num) {
-    num = free_chunks.size();
-    r = num;
-  }
-  for (uint32_t i = 0; i < num; ++i) {
+  std::lock_guard l{lock};
+  uint32_t chunk_buffer_number = (block_size + buffer_size - 1) / buffer_size;
+  chunk_buffer_number = free_chunks.size() < chunk_buffer_number ? free_chunks.size(): chunk_buffer_number;
+  uint32_t r = 0;
+
+  for (r = 0; r < chunk_buffer_number; ++r) {
     chunks.push_back(free_chunks.back());
     free_chunks.pop_back();
   }
   return r;
 }
 
+bool Infiniband::MemoryManager::MemPoolContext::can_alloc(unsigned nbufs)
+{
+  /* unlimited */
+  if (manager->cct->_conf->ms_async_rdma_receive_buffers <= 0)
+    return true;
+
+  if (n_bufs_allocated + nbufs > (unsigned)manager->cct->_conf->ms_async_rdma_receive_buffers) {
+    lderr(manager->cct) << __func__ << " WARNING: OUT OF RX BUFFERS: allocated: " <<
+        n_bufs_allocated << " requested: " << nbufs <<
+        " limit: " << manager->cct->_conf->ms_async_rdma_receive_buffers << dendl;
+    return false;
+  }
+
+  return true;
+}
+
+void Infiniband::MemoryManager::MemPoolContext::set_stat_logger(PerfCounters *logger) {
+  perf_logger = logger;
+  if (perf_logger != nullptr)
+    perf_logger->set(l_msgr_rdma_rx_bufs_total, n_bufs_allocated);
+}
+
+void Infiniband::MemoryManager::MemPoolContext::update_stats(int nbufs)
+{
+  n_bufs_allocated += nbufs;
 
-Infiniband::MemoryManager::MemoryManager(Device *d, ProtectionDomain *p, bool hugepage)
-  : device(d), pd(p)
+  if (!perf_logger)
+    return;
+
+  if (nbufs > 0) {
+    perf_logger->inc(l_msgr_rdma_rx_bufs_total, nbufs);
+  } else {
+    perf_logger->dec(l_msgr_rdma_rx_bufs_total, -nbufs);
+  }
+}
+
+void *Infiniband::MemoryManager::mem_pool::slow_malloc()
+{
+  // this will trigger pool expansion via PoolAllocator::malloc()
+  return PoolAllocator::with_context(ctx, [this] {
+    return boost::pool<PoolAllocator>::malloc();
+  });
+}
+
+Infiniband::MemoryManager::MemPoolContext*
+Infiniband::MemoryManager::PoolAllocator::g_ctx = nullptr;
+
+// lock is taken by mem_pool::slow_malloc()
+ceph::mutex& Infiniband::MemoryManager::PoolAllocator::get_lock()
+{
+  static ceph::mutex lock = ceph::make_mutex("pool-alloc-lock");
+  return lock;
+}
+
+char *Infiniband::MemoryManager::PoolAllocator::malloc(const size_type block_size)
+{
+  ceph_assert(g_ctx);
+  MemoryManager *manager = g_ctx->manager;
+  CephContext *cct = manager->cct;
+  size_t chunk_buffer_size = sizeof(Chunk) + cct->_conf->ms_async_rdma_buffer_size;
+  size_t chunk_buffer_number = block_size / chunk_buffer_size;
+
+  if (!g_ctx->can_alloc(chunk_buffer_number))
+    return NULL;
+
+  mem_info *minfo= static_cast<mem_info *>(manager->malloc(block_size + sizeof(mem_info)));
+  if (!minfo) {
+    lderr(cct) << __func__ << " failed to allocate " << chunk_buffer_number << " buffers "
+      " Its block size is : " << block_size + sizeof(mem_info) << dendl;
+    return NULL;
+  }
+
+  minfo->mr = ibv_reg_mr(manager->pd->pd, minfo->chunks, block_size, IBV_ACCESS_REMOTE_WRITE | IBV_ACCESS_LOCAL_WRITE);
+  if (minfo->mr == NULL) {
+    lderr(cct) << __func__ << " failed to do rdma memory registration " << block_size << " bytes. "
+      " relase allocated memory now." << dendl;
+    manager->free(minfo);
+    return NULL;
+  }
+
+  minfo->nbufs = chunk_buffer_number;
+  // save this chunk context
+  minfo->ctx   = g_ctx;
+
+  // note that the memory can be allocated before perf logger is set
+  g_ctx->update_stats(chunk_buffer_number);
+
+  /* initialize chunks */
+  Chunk *chunk = minfo->chunks;
+  for (unsigned i = 0; i < chunk_buffer_number; i++) {
+    new(chunk) Chunk(minfo->mr, cct->_conf->ms_async_rdma_buffer_size, chunk->data, 0, 0, minfo->mr->lkey);
+    chunk = reinterpret_cast<Chunk *>(reinterpret_cast<char *>(chunk) + chunk_buffer_size);
+  }
+
+  return reinterpret_cast<char *>(minfo->chunks);
+}
+
+
+void Infiniband::MemoryManager::PoolAllocator::free(char * const block)
+{
+  mem_info *m;
+  std::lock_guard l{get_lock()};
+    
+  Chunk *mem_info_chunk = reinterpret_cast<Chunk *>(block);
+  m = reinterpret_cast<mem_info *>(reinterpret_cast<char *>(mem_info_chunk) - offsetof(mem_info, chunks));
+  m->ctx->update_stats(-m->nbufs);
+  ibv_dereg_mr(m->mr);
+  m->ctx->manager->free(m);
+}
+
+Infiniband::MemoryManager::MemoryManager(CephContext *c, Device *d, ProtectionDomain *p)
+  : cct(c), device(d), pd(p),
+    rxbuf_pool_ctx(this),
+    rxbuf_pool(&rxbuf_pool_ctx, sizeof(Chunk) + c->_conf->ms_async_rdma_buffer_size,
+               c->_conf->ms_async_rdma_receive_buffers > 0 ?
+                  // if possible make initial pool size 2 * receive_queue_len
+                  // that way there will be no pool expansion upon receive of the
+                  // first packet.
+                  (c->_conf->ms_async_rdma_receive_buffers < 2 * c->_conf->ms_async_rdma_receive_queue_len ?
+                   c->_conf->ms_async_rdma_receive_buffers :  2 * c->_conf->ms_async_rdma_receive_queue_len) :
+                  // rx pool is infinite, we can set any initial size that we want
+                   2 * c->_conf->ms_async_rdma_receive_queue_len,
+                   device->device_attr.max_mr_size / (sizeof(Chunk) + cct->_conf->ms_async_rdma_buffer_size))
 {
-  enabled_huge_page = hugepage;
 }
 
 Infiniband::MemoryManager::~MemoryManager()
 {
-  if (channel)
-    delete channel;
   if (send)
     delete send;
 }
 
-void* Infiniband::MemoryManager::malloc_huge_pages(size_t size)
+void* Infiniband::MemoryManager::huge_pages_malloc(size_t size)
 {
-  size_t real_size = ALIGN_TO_PAGE_SIZE(size + HUGE_PAGE_SIZE);
-  char *ptr = (char *)mmap(NULL, real_size, PROT_READ | PROT_WRITE,MAP_PRIVATE | MAP_ANONYMOUS |MAP_POPULATE | MAP_HUGETLB,-1, 0);
+  size_t real_size = ALIGN_TO_PAGE_2MB(size) + HUGE_PAGE_SIZE_2MB;
+  char *ptr = (char *)mmap(NULL, real_size, PROT_READ | PROT_WRITE, MAP_PRIVATE | MAP_ANONYMOUS | MAP_POPULATE | MAP_HUGETLB, -1, 0);
   if (ptr == MAP_FAILED) {
-    ptr = (char *)malloc(real_size);
+    ptr = (char *)std::malloc(real_size);
     if (ptr == NULL) return NULL;
     real_size = 0;
   }
   *((size_t *)ptr) = real_size;
-  return ptr + HUGE_PAGE_SIZE;
+  return ptr + HUGE_PAGE_SIZE_2MB;
 }
 
-void Infiniband::MemoryManager::free_huge_pages(void *ptr)
+void Infiniband::MemoryManager::huge_pages_free(void *ptr)
 {
   if (ptr == NULL) return;
-  void *real_ptr = (char *)ptr -HUGE_PAGE_SIZE;
+  void *real_ptr = (char *)ptr - HUGE_PAGE_SIZE_2MB;
   size_t real_size = *((size_t *)real_ptr);
-  assert(real_size % HUGE_PAGE_SIZE == 0);
+  ceph_assert(real_size % HUGE_PAGE_SIZE_2MB == 0);
   if (real_size != 0)
     munmap(real_ptr, real_size);
   else
-    free(real_ptr);
+    std::free(real_ptr);
+}
+
+
+void* Infiniband::MemoryManager::malloc(size_t size)
+{
+  if (cct->_conf->ms_async_rdma_enable_hugepage)
+    return huge_pages_malloc(size);
+  else
+    return std::malloc(size);
 }
 
-void Infiniband::MemoryManager::register_rx_tx(uint32_t size, uint32_t rx_num, uint32_t tx_num)
+void Infiniband::MemoryManager::free(void *ptr)
 {
-  assert(device);
-  assert(pd);
-  channel = new Cluster(*this, size);
-  channel->fill(rx_num);
+  if (cct->_conf->ms_async_rdma_enable_hugepage)
+    huge_pages_free(ptr);
+  else
+    std::free(ptr);
+}
+
+void Infiniband::MemoryManager::create_tx_pool(uint32_t size, uint32_t tx_num)
+{
+  ceph_assert(device);
+  ceph_assert(pd);
 
   send = new Cluster(*this, size);
   send->fill(tx_num);
@@ -703,20 +1006,50 @@ int Infiniband::MemoryManager::get_send_buffers(std::vector<Chunk*> &c, size_t b
   return send->get_buffers(c, bytes);
 }
 
-int Infiniband::MemoryManager::get_channel_buffers(std::vector<Chunk*> &chunks, size_t bytes)
-{
-  return channel->get_buffers(chunks, bytes);
-}
-
-
-Infiniband::Infiniband(CephContext *cct, const std::string &device_name, uint8_t port_num)
-  : cct(cct), lock("IB lock"), device_name(device_name), port_num(port_num)
+static std::atomic<bool> init_prereq = {false};
+
+void Infiniband::verify_prereq(CephContext *cct) {
+   int rc = 0;
+   ldout(cct, 20) << __func__ << " ms_async_rdma_enable_hugepage value is: " << cct->_conf->ms_async_rdma_enable_hugepage <<  dendl;
+   if (cct->_conf->ms_async_rdma_enable_hugepage){
+     rc =  setenv("RDMAV_HUGEPAGES_SAFE","1",1);
+     ldout(cct, 0) << __func__ << " RDMAV_HUGEPAGES_SAFE is set as: " << getenv("RDMAV_HUGEPAGES_SAFE") <<  dendl;
+     if (rc) {
+       lderr(cct) << __func__ << " failed to export RDMA_HUGEPAGES_SAFE. On RDMA must be exported before using huge pages. Application aborts." << dendl;
+       ceph_abort();
+     }
+   }
+
+  //On RDMA MUST be called before fork
+   rc = ibv_fork_init();
+   if (rc) {
+      lderr(cct) << __func__ << " failed to call ibv_for_init(). On RDMA must be called before fork. Application aborts." << dendl;
+      ceph_abort();
+   }
+
+   //Check ulimit
+   struct rlimit limit;
+   getrlimit(RLIMIT_MEMLOCK, &limit);
+   if (limit.rlim_cur != RLIM_INFINITY || limit.rlim_max != RLIM_INFINITY) {
+      lderr(cct) << __func__ << "!!! WARNING !!! For RDMA to work properly user memlock (ulimit -l) must be big enough to allow large amount of registered memory."
+				  " We recommend setting this parameter to infinity" << dendl;
+   }
+   init_prereq = true;
+}
+
+Infiniband::Infiniband(CephContext *cct)
+  : cct(cct),
+    device_name(cct->_conf->ms_async_rdma_device_name),
+    port_num( cct->_conf->ms_async_rdma_port_num)
 {
+  if (!init_prereq)
+    verify_prereq(cct);
+  ldout(cct, 20) << __func__ << " constructing Infiniband..." << dendl;
 }
 
 void Infiniband::init()
 {
-  Mutex::Locker l(lock);
+  std::lock_guard l{lock};
 
   if (initialized)
     return;
@@ -725,60 +1058,71 @@ void Infiniband::init()
   initialized = true;
 
   device = device_list->get_device(device_name.c_str());
+  ceph_assert(device);
   device->binding_port(cct, port_num);
-  assert(device);
   ib_physical_port = device->active_port->get_port_num();
   pd = new ProtectionDomain(cct, device);
-  assert(NetHandler(cct).set_nonblock(device->ctxt->async_fd) == 0);
+  ceph_assert(NetHandler(cct).set_nonblock(device->ctxt->async_fd) == 0);
 
-  max_recv_wr = device->device_attr->max_srq_wr;
-  if (max_recv_wr > cct->_conf->ms_async_rdma_receive_buffers) {
-    max_recv_wr = cct->_conf->ms_async_rdma_receive_buffers;
-    ldout(cct, 1) << __func__ << " assigning: " << max_recv_wr << " receive buffers" << dendl;
+  support_srq = cct->_conf->ms_async_rdma_support_srq;
+  if (support_srq) {
+    ceph_assert(device->device_attr.max_srq);
+    rx_queue_len = device->device_attr.max_srq_wr;
+  }
+  else
+    rx_queue_len = device->device_attr.max_qp_wr;
+  if (rx_queue_len > cct->_conf->ms_async_rdma_receive_queue_len) {
+    rx_queue_len = cct->_conf->ms_async_rdma_receive_queue_len;
+    ldout(cct, 1) << __func__ << " assigning: " << rx_queue_len << " receive buffers" << dendl;
   } else {
-    ldout(cct, 1) << __func__ << " using the max allowed receive buffers: " << max_recv_wr << dendl;
+    ldout(cct, 0) << __func__ << " using the max allowed receive buffers: " << rx_queue_len << dendl;
+  }
+
+  // check for the misconfiguration
+  if (cct->_conf->ms_async_rdma_receive_buffers > 0 &&
+      rx_queue_len > (unsigned)cct->_conf->ms_async_rdma_receive_buffers) {
+    lderr(cct) << __func__ << " rdma_receive_queue_len (" <<
+                  rx_queue_len << ") > ms_async_rdma_receive_buffers(" <<
+                  cct->_conf->ms_async_rdma_receive_buffers << ")." << dendl;
+    ceph_abort();
   }
 
-  max_send_wr = device->device_attr->max_qp_wr;
-  if (max_send_wr > cct->_conf->ms_async_rdma_send_buffers) {
-    max_send_wr = cct->_conf->ms_async_rdma_send_buffers;
-    ldout(cct, 1) << __func__ << " assigning: " << max_send_wr << " send buffers"  << dendl;
+  // Keep extra one WR for a beacon to indicate all WCEs were consumed
+  tx_queue_len = device->device_attr.max_qp_wr - 1;
+  if (tx_queue_len > cct->_conf->ms_async_rdma_send_buffers) {
+    tx_queue_len = cct->_conf->ms_async_rdma_send_buffers;
+    ldout(cct, 1) << __func__ << " assigning: " << tx_queue_len << " send buffers"  << dendl;
   } else {
-    ldout(cct, 1) << __func__ << " using the max allowed send buffers: " << max_send_wr << dendl;
+    ldout(cct, 0) << __func__ << " using the max allowed send buffers: " << tx_queue_len << dendl;
   }
 
-  ldout(cct, 1) << __func__ << " device allow " << device->device_attr->max_cqe
-                << " completion entries" << dendl;
+  //check for the memory region size misconfiguration
+  if ((uint64_t)cct->_conf->ms_async_rdma_buffer_size * tx_queue_len > device->device_attr.max_mr_size) {
+    lderr(cct) << __func__ << " Out of max memory region size " << dendl;
+    ceph_abort();
+  }
 
-  memory_manager = new MemoryManager(device, pd,
-                                     cct->_conf->ms_async_rdma_enable_hugepage);
-  memory_manager->register_rx_tx(
-      cct->_conf->ms_async_rdma_buffer_size, max_recv_wr, max_send_wr);
+  ldout(cct, 1) << __func__ << " device allow " << device->device_attr.max_cqe
+                << " completion entries" << dendl;
 
-  srq = create_shared_receive_queue(max_recv_wr, MAX_SHARED_RX_SGE_COUNT);
-  post_channel_cluster();
+  memory_manager = new MemoryManager(cct, device, pd);
+  memory_manager->create_tx_pool(cct->_conf->ms_async_rdma_buffer_size, tx_queue_len);
 
-  dispatcher->polling_start();
+  if (support_srq) {
+    srq = create_shared_receive_queue(rx_queue_len, MAX_SHARED_RX_SGE_COUNT);
+    post_chunks_to_rq(rx_queue_len, NULL); //add to srq
+  }
 }
 
 Infiniband::~Infiniband()
 {
   if (!initialized)
     return;
-
-  if (dispatcher)
-    dispatcher->polling_stop();
-
-  ibv_destroy_srq(srq);
+  if (support_srq)
+    ibv_destroy_srq(srq);
   delete memory_manager;
   delete pd;
-}
-
-void Infiniband::set_dispatcher(RDMADispatcher *d)
-{
-  assert(!d ^ !dispatcher);
-
-  dispatcher = d;
+  delete device_list;
 }
 
 /**
@@ -794,6 +1138,7 @@ void Infiniband::set_dispatcher(RDMADispatcher *d)
 ibv_srq* Infiniband::create_shared_receive_queue(uint32_t max_wr, uint32_t max_sge)
 {
   ibv_srq_init_attr sia;
+  // FIPS zeroization audit 20191115: this memset is not security related.
   memset(&sia, 0, sizeof(sia));
   sia.srq_context = device->ctxt;
   sia.attr.max_wr = max_wr;
@@ -816,10 +1161,11 @@ int Infiniband::get_tx_buffers(std::vector<Chunk*> &c, size_t bytes)
  *      QueuePair on success or NULL if init fails
  * See QueuePair::QueuePair for parameter documentation.
  */
-Infiniband::QueuePair* Infiniband::create_queue_pair(CephContext *cct, CompletionQueue *tx, CompletionQueue* rx, ibv_qp_type type)
+Infiniband::QueuePair* Infiniband::create_queue_pair(CephContext *cct, CompletionQueue *tx,
+    CompletionQueue* rx, ibv_qp_type type, struct rdma_cm_id *cm_id)
 {
   Infiniband::QueuePair *qp = new QueuePair(
-      cct, *this, type, ib_physical_port, srq, tx, rx, max_send_wr, max_recv_wr);
+      cct, *this, type, ib_physical_port, srq, tx, rx, tx_queue_len, rx_queue_len, cm_id);
   if (qp->init()) {
     delete qp;
     return NULL;
@@ -827,37 +1173,61 @@ Infiniband::QueuePair* Infiniband::create_queue_pair(CephContext *cct, Completio
   return qp;
 }
 
-int Infiniband::post_chunk(Chunk* chunk)
+int Infiniband::post_chunks_to_rq(int rq_wr_num, QueuePair *qp)
 {
-  ibv_sge isge;
-  isge.addr = reinterpret_cast<uint64_t>(chunk->buffer);
-  isge.length = chunk->bytes;
-  isge.lkey = chunk->mr->lkey;
-  ibv_recv_wr rx_work_request;
+  int ret = 0;
+  Chunk *chunk = nullptr;
+
+  ibv_recv_wr *rx_work_request = static_cast<ibv_recv_wr*>(::calloc(rq_wr_num, sizeof(ibv_recv_wr)));
+  ibv_sge *isge = static_cast<ibv_sge*>(::calloc(rq_wr_num, sizeof(ibv_sge)));
+  ceph_assert(rx_work_request);
+  ceph_assert(isge);
+
+  int i = 0;
+  while (i < rq_wr_num) {
+    chunk = get_memory_manager()->get_rx_buffer();
+    if (chunk == nullptr) {
+      lderr(cct) << __func__ << " WARNING: out of memory. Request " << rq_wr_num <<
+                 " rx buffers. Only get " << i << " rx buffers." << dendl;
+      if (i == 0) {
+        ::free(rx_work_request);
+        ::free(isge);
+        return 0;
+      }
+      break; //get some buffers, so we need post them to recevie queue
+    }
 
-  memset(&rx_work_request, 0, sizeof(rx_work_request));
-  rx_work_request.wr_id = reinterpret_cast<uint64_t>(chunk);// stash descriptor ptr
-  rx_work_request.next = NULL;
-  rx_work_request.sg_list = &isge;
-  rx_work_request.num_sge = 1;
+    isge[i].addr = reinterpret_cast<uint64_t>(chunk->data);
+    isge[i].length = chunk->bytes;
+    isge[i].lkey = chunk->lkey;
 
-  ibv_recv_wr *badWorkRequest;
-  int ret = ibv_post_srq_recv(srq, &rx_work_request, &badWorkRequest);
-  if (ret)
-    return -errno;
-  return 0;
-}
+    rx_work_request[i].wr_id = reinterpret_cast<uint64_t>(chunk);// assign chunk address as work request id
 
-int Infiniband::post_channel_cluster()
-{
-  vector<Chunk*> free_chunks;
-  int r = memory_manager->get_channel_buffers(free_chunks, 0);
-  assert(r > 0);
-  for (vector<Chunk*>::iterator iter = free_chunks.begin(); iter != free_chunks.end(); ++iter) {
-    r = post_chunk(*iter);
-    assert(r == 0);
+    if (i != 0) {
+      rx_work_request[i - 1].next = &rx_work_request[i];
+    }
+    rx_work_request[i].sg_list = &isge[i];
+    rx_work_request[i].num_sge = 1;
+
+    if (qp && !qp->get_srq()) {
+       chunk->set_qp(qp);
+       qp->add_rq_wr(chunk);
+    }
+    i++;
   }
-  return 0;
+
+  ibv_recv_wr *badworkrequest = nullptr;
+  if (support_srq) {
+    ret = ibv_post_srq_recv(srq, rx_work_request, &badworkrequest);
+  } else {
+    ceph_assert(qp);
+    ret = ibv_post_recv(qp->get_qp(), rx_work_request, &badworkrequest);
+  }
+
+  ::free(rx_work_request);
+  ::free(isge);
+  ceph_assert(badworkrequest == nullptr && ret == 0);
+  return i;
 }
 
 Infiniband::CompletionChannel* Infiniband::create_comp_channel(CephContext *c)
@@ -882,99 +1252,18 @@ Infiniband::CompletionQueue* Infiniband::create_comp_queue(
   return cq;
 }
 
-// 1 means no valid buffer read, 0 means got enough buffer
-// else return < 0 means error
-int Infiniband::recv_msg(CephContext *cct, int sd, IBSYNMsg& im)
-{
-  char msg[TCP_MSG_LEN];
-  char gid[33];
-  ssize_t r = ::read(sd, &msg, sizeof(msg));
-  // Drop incoming qpt
-  if (cct->_conf->ms_inject_socket_failures && sd >= 0) {
-    if (rand() % cct->_conf->ms_inject_socket_failures == 0) {
-      ldout(cct, 0) << __func__ << " injecting socket failure" << dendl;
-      return -EINVAL;
-    }
-  }
-  if (r < 0) {
-    r = -errno;
-    lderr(cct) << __func__ << " got error " << r << ": "
-               << cpp_strerror(r) << dendl;
-  } else if (r == 0) { // valid disconnect message of length 0
-    ldout(cct, 10) << __func__ << " got disconnect message " << dendl;
-  } else if ((size_t)r != sizeof(msg)) { // invalid message
-    ldout(cct, 1) << __func__ << " got bad length (" << r << ") " << dendl;
-    r = -EINVAL;
-  } else { // valid message
-    sscanf(msg, "%hu:%x:%x:%x:%s", &(im.lid), &(im.qpn), &(im.psn), &(im.peer_qpn),gid);
-    wire_gid_to_gid(gid, &(im.gid));
-    ldout(cct, 5) << __func__ << " recevd: " << im.lid << ", " << im.qpn << ", " << im.psn << ", " << im.peer_qpn << ", " << gid  << dendl;
-  }
-  return r;
-}
-
-int Infiniband::send_msg(CephContext *cct, int sd, IBSYNMsg& im)
-{
-  int retry = 0;
-  ssize_t r;
-
-  char msg[TCP_MSG_LEN];
-  char gid[33];
-retry:
-  gid_to_wire_gid(&(im.gid), gid);
-  sprintf(msg, "%04x:%08x:%08x:%08x:%s", im.lid, im.qpn, im.psn, im.peer_qpn, gid);
-  ldout(cct, 10) << __func__ << " sending: " << im.lid << ", " << im.qpn << ", " << im.psn
-                 << ", " << im.peer_qpn << ", "  << gid  << dendl;
-  r = ::write(sd, msg, sizeof(msg));
-  // Drop incoming qpt
-  if (cct->_conf->ms_inject_socket_failures && sd >= 0) {
-    if (rand() % cct->_conf->ms_inject_socket_failures == 0) {
-      ldout(cct, 0) << __func__ << " injecting socket failure" << dendl;
-      return -EINVAL;
-    }
-  }
-
-  if ((size_t)r != sizeof(msg)) {
-    // FIXME need to handle EAGAIN instead of retry
-    if (r < 0 && (errno == EINTR || errno == EAGAIN) && retry < 3) {
-      retry++;
-      goto retry;
-    }
-    if (r < 0)
-      lderr(cct) << __func__ << " send returned error " << errno << ": "
-                 << cpp_strerror(errno) << dendl;
-    else
-      lderr(cct) << __func__ << " send got bad length (" << r << ") " << cpp_strerror(errno) << dendl;
-    return -errno;
-  }
-  return 0;
-}
-
-void Infiniband::wire_gid_to_gid(const char *wgid, union ibv_gid *gid)
-{
-  char tmp[9];
-  uint32_t v32;
-  int i;
-
-  for (tmp[8] = 0, i = 0; i < 4; ++i) {
-    memcpy(tmp, wgid + i * 8, 8);
-    sscanf(tmp, "%x", &v32);
-    *(uint32_t *)(&gid->raw[i * 4]) = ntohl(v32);
-  }
-}
-
-void Infiniband::gid_to_wire_gid(const union ibv_gid *gid, char wgid[])
-{
-  for (int i = 0; i < 4; ++i)
-    sprintf(&wgid[i * 8], "%08x", htonl(*(uint32_t *)(gid->raw + i * 4)));
-}
-
 Infiniband::QueuePair::~QueuePair()
 {
+  ldout(cct, 20) << __func__ << " destroy Queue Pair, qp number: " << qp->qp_num << " left SQ WR " << recv_queue.size() << dendl;
   if (qp) {
     ldout(cct, 20) << __func__ << " destroy qp=" << qp << dendl;
-    assert(!ibv_destroy_qp(qp));
+    ceph_assert(!ibv_destroy_qp(qp));
+  }
+
+  for (auto& chunk: recv_queue) {
+    infiniband.get_memory_manager()->release_rx_buffer(chunk);
   }
+  recv_queue.clear();
 }
 
 /**
diff --git a/src/msg/async/rdma/Infiniband.h b/src/msg/async/rdma/Infiniband.h
index 068896c982a..3af89f304fe 100644
--- a/src/msg/async/rdma/Infiniband.h
+++ b/src/msg/async/rdma/Infiniband.h
@@ -17,38 +17,52 @@
 #ifndef CEPH_INFINIBAND_H
 #define CEPH_INFINIBAND_H
 
-#include <string>
-#include <vector>
+#include <boost/pool/pool.hpp>
+// need this because boost messes with ceph log/assert definitions
+#include "include/ceph_assert.h"
 
 #include <infiniband/verbs.h>
+#include <rdma/rdma_cma.h>
 
+#include <atomic>
+#include <functional>
+#include <string>
+#include <vector>
+
+#include "include/common_fwd.h"
 #include "include/int_types.h"
 #include "include/page.h"
+#include "include/scope_guard.h"
 #include "common/debug.h"
 #include "common/errno.h"
-#include "common/Mutex.h"
+#include "common/ceph_mutex.h"
+#include "common/perf_counters.h"
 #include "msg/msg_types.h"
 #include "msg/async/net_handler.h"
 
-#define HUGE_PAGE_SIZE (2 * 1024 * 1024)
-#define ALIGN_TO_PAGE_SIZE(x) \
-  (((x) + HUGE_PAGE_SIZE -1) / HUGE_PAGE_SIZE * HUGE_PAGE_SIZE)
+#define HUGE_PAGE_SIZE_2MB (2 * 1024 * 1024)
+#define ALIGN_TO_PAGE_2MB(x) \
+    (((x) + (HUGE_PAGE_SIZE_2MB - 1)) & ~(HUGE_PAGE_SIZE_2MB - 1))
+
+#define PSN_LEN 24
+#define PSN_MSK ((1 << PSN_LEN) - 1)
+
+#define BEACON_WRID 0xDEADBEEF
 
-struct IBSYNMsg {
+struct ib_cm_meta_t {
   uint16_t lid;
-  uint32_t qpn;
+  uint32_t local_qpn;
   uint32_t psn;
   uint32_t peer_qpn;
   union ibv_gid gid;
 } __attribute__((packed));
 
 class RDMAStack;
-class CephContext;
 
 class Port {
   struct ibv_context* ctxt;
   int port_num;
-  struct ibv_port_attr* port_attr;
+  struct ibv_port_attr port_attr;
   uint16_t lid;
   int gid_idx;
   union ibv_gid gid;
@@ -58,7 +72,7 @@ class Port {
   uint16_t get_lid() { return lid; }
   ibv_gid  get_gid() { return gid; }
   int get_port_num() { return port_num; }
-  ibv_port_attr* get_port_attr() { return port_attr; }
+  ibv_port_attr* get_port_attr() { return &port_attr; }
   int get_gid_idx() { return gid_idx; }
 };
 
@@ -66,13 +80,14 @@ class Port {
 class Device {
   ibv_device *device;
   const char* name;
-  uint8_t  port_cnt;
+  uint8_t  port_cnt = 0;
  public:
-  explicit Device(CephContext *c, ibv_device* d);
+  explicit Device(CephContext *c, ibv_device* ib_dev);
+  explicit Device(CephContext *c, ibv_context *ib_ctx);
   ~Device() {
     if (active_port) {
       delete active_port;
-      assert(ibv_close_device(ctxt) == 0);
+      ceph_assert(ibv_close_device(ctxt) == 0);
     }
   }
   const char* get_name() { return name;}
@@ -81,25 +96,34 @@ class Device {
   int get_gid_idx() { return active_port->get_gid_idx(); }
   void binding_port(CephContext *c, int port_num);
   struct ibv_context *ctxt;
-  ibv_device_attr *device_attr;
+  ibv_device_attr device_attr;
   Port* active_port;
 };
 
 
 class DeviceList {
   struct ibv_device ** device_list;
+  struct ibv_context ** device_context_list;
   int num;
   Device** devices;
  public:
-  DeviceList(CephContext *cct): device_list(ibv_get_device_list(&num)) {
-    if (device_list == NULL || num == 0) {
-      lderr(cct) << __func__ << " failed to get rdma device list.  " << cpp_strerror(errno) << dendl;
-      ceph_abort();
+  explicit DeviceList(CephContext *cct): device_list(nullptr), device_context_list(nullptr),
+                                         num(0), devices(nullptr) {
+    device_list = ibv_get_device_list(&num);
+    ceph_assert(device_list);
+    ceph_assert(num);
+    if (cct->_conf->ms_async_rdma_cm) {
+        device_context_list = rdma_get_devices(NULL);
+        ceph_assert(device_context_list);
     }
     devices = new Device*[num];
 
     for (int i = 0;i < num; ++i) {
-      devices[i] = new Device(cct, device_list[i]);
+      if (cct->_conf->ms_async_rdma_cm) {
+         devices[i] = new Device(cct, device_context_list[i]);
+      } else {
+         devices[i] = new Device(cct, device_list[i]);
+      }
     }
   }
   ~DeviceList() {
@@ -108,10 +132,10 @@ class DeviceList {
     }
     delete []devices;
     ibv_free_device_list(device_list);
+    rdma_free_devices(device_context_list);
   }
 
   Device* get_device(const char* device_name) {
-    assert(devices);
     for (int i = 0; i < num; ++i) {
       if (!strlen(device_name) || !strcmp(device_name, devices[i]->get_name())) {
         return devices[i];
@@ -121,6 +145,50 @@ class DeviceList {
   }
 };
 
+// stat counters
+enum {
+  l_msgr_rdma_dispatcher_first = 94000,
+
+  l_msgr_rdma_polling,
+  l_msgr_rdma_inflight_tx_chunks,
+  l_msgr_rdma_rx_bufs_in_use,
+  l_msgr_rdma_rx_bufs_total,
+
+  l_msgr_rdma_tx_total_wc,
+  l_msgr_rdma_tx_total_wc_errors,
+  l_msgr_rdma_tx_wc_retry_errors,
+  l_msgr_rdma_tx_wc_wr_flush_errors,
+
+  l_msgr_rdma_rx_total_wc,
+  l_msgr_rdma_rx_total_wc_errors,
+  l_msgr_rdma_rx_fin,
+
+  l_msgr_rdma_handshake_errors,
+
+  l_msgr_rdma_total_async_events,
+  l_msgr_rdma_async_last_wqe_events,
+
+  l_msgr_rdma_created_queue_pair,
+  l_msgr_rdma_active_queue_pair,
+
+  l_msgr_rdma_dispatcher_last,
+};
+
+enum {
+  l_msgr_rdma_first = 95000,
+
+  l_msgr_rdma_tx_no_mem,
+  l_msgr_rdma_tx_parital_mem,
+  l_msgr_rdma_tx_failed,
+
+  l_msgr_rdma_tx_chunks,
+  l_msgr_rdma_tx_bytes,
+  l_msgr_rdma_rx_chunks,
+  l_msgr_rdma_rx_bytes,
+  l_msgr_rdma_pending_sent_conns,
+
+  l_msgr_rdma_last,
+};
 
 class RDMADispatcher;
 
@@ -134,32 +202,36 @@ class Infiniband {
     ibv_pd* const pd;
   };
 
-
+  class QueuePair;
   class MemoryManager {
    public:
     class Chunk {
      public:
-      Chunk(ibv_mr* m, uint32_t len, char* b);
+      Chunk(ibv_mr* m, uint32_t bytes, char* buffer, uint32_t offset = 0, uint32_t bound = 0, uint32_t lkey = 0, QueuePair* qp = nullptr);
       ~Chunk();
 
-      void set_offset(uint32_t o);
       uint32_t get_offset();
-      void set_bound(uint32_t b);
+      uint32_t get_size() const;
       void prepare_read(uint32_t b);
       uint32_t get_bound();
       uint32_t read(char* buf, uint32_t len);
       uint32_t write(char* buf, uint32_t len);
       bool full();
-      bool over();
-      void clear();
-      void post_srq(Infiniband *ib);
+      void reset_read_chunk();
+      void reset_write_chunk();
+      void set_qp(QueuePair *qp) { this->qp = qp; }
+      void clear_qp() { set_qp(nullptr); }
+      QueuePair* get_qp() { return qp; }
 
      public:
       ibv_mr* mr;
+      QueuePair *qp;
+      uint32_t lkey;
       uint32_t bytes;
-      uint32_t bound;
       uint32_t offset;
-      char* buffer;
+      uint32_t bound;
+      char* buffer; // TODO: remove buffer/refactor TX
+      char  data[0];
     };
 
     class Cluster {
@@ -181,25 +253,99 @@ class Infiniband {
 
       MemoryManager& manager;
       uint32_t buffer_size;
-      uint32_t num_chunk;
-      Mutex lock;
+      uint32_t num_chunk = 0;
+      ceph::mutex lock = ceph::make_mutex("cluster_lock");
       std::vector<Chunk*> free_chunks;
       char *base = nullptr;
       char *end = nullptr;
       Chunk* chunk_base = nullptr;
     };
 
-    MemoryManager(Device *d, ProtectionDomain *p, bool hugepage);
+    class MemPoolContext {
+      PerfCounters *perf_logger;
+
+     public:
+      MemoryManager *manager;
+      unsigned n_bufs_allocated;
+      // true if it is possible to alloc
+      // more memory for the pool
+      explicit MemPoolContext(MemoryManager *m) :
+        perf_logger(nullptr),
+        manager(m),
+        n_bufs_allocated(0) {}
+      bool can_alloc(unsigned nbufs);
+      void update_stats(int val);
+      void set_stat_logger(PerfCounters *logger);
+    };
+
+    class PoolAllocator {
+      struct mem_info {
+        ibv_mr   *mr;
+        MemPoolContext *ctx;
+        unsigned nbufs;
+        Chunk    chunks[0];
+      };
+     public:
+      typedef std::size_t size_type;
+      typedef std::ptrdiff_t difference_type;
+
+      static char * malloc(const size_type bytes);
+      static void free(char * const block);
+
+      template<typename Func>
+      static std::invoke_result_t<Func> with_context(MemPoolContext* ctx,
+						     Func&& func) {
+	std::lock_guard l{get_lock()};
+	g_ctx = ctx;
+	scope_guard reset_ctx{[] { g_ctx = nullptr; }};
+	return std::move(func)();
+      }
+    private:
+      static ceph::mutex& get_lock();
+      static MemPoolContext* g_ctx;
+    };
+
+    /**
+     * modify boost pool so that it is possible to
+     * have a thread safe 'context' when allocating/freeing
+     * the memory. It is needed to allow a different pool
+     * configurations and bookkeeping per CephContext and
+     * also to be able to use same allocator to deal with
+     * RX and TX pool.
+     * TODO: use boost pool to allocate TX chunks too
+     */
+    class mem_pool : public boost::pool<PoolAllocator> {
+     private:
+      MemPoolContext *ctx;
+      void *slow_malloc();
+
+     public:
+      ceph::mutex lock = ceph::make_mutex("mem_pool_lock");
+      explicit mem_pool(MemPoolContext *ctx, const size_type nrequested_size,
+          const size_type nnext_size = 32,
+          const size_type nmax_size = 0) :
+        pool(nrequested_size, nnext_size, nmax_size),
+        ctx(ctx) { }
+
+      void *malloc() {
+        if (!store().empty())
+          return (store().malloc)();
+        // need to alloc more memory...
+        // slow path code
+        return slow_malloc();
+      }
+    };
+
+    MemoryManager(CephContext *c, Device *d, ProtectionDomain *p);
     ~MemoryManager();
 
-    void* malloc_huge_pages(size_t size);
-    void free_huge_pages(void *ptr);
-    void register_rx_tx(uint32_t size, uint32_t rx_num, uint32_t tx_num);
+    void* malloc(size_t size);
+    void  free(void *ptr);
+
+    void create_tx_pool(uint32_t size, uint32_t tx_num);
     void return_tx(std::vector<Chunk*> &chunks);
     int get_send_buffers(std::vector<Chunk*> &c, size_t bytes);
-    int get_channel_buffers(std::vector<Chunk*> &chunks, size_t bytes);
     bool is_tx_buffer(const char* c) { return send->is_my_buffer(c); }
-    bool is_rx_buffer(const char* c) { return channel->is_my_buffer(c); }
     Chunk *get_tx_chunk_by_buffer(const char *c) {
       return send->get_chunk_by_buffer(c);
     }
@@ -207,18 +353,40 @@ class Infiniband {
       return send->buffer_size;
     }
 
-    bool enabled_huge_page;
+    Chunk *get_rx_buffer() {
+       std::lock_guard l{rxbuf_pool.lock};
+       return reinterpret_cast<Chunk *>(rxbuf_pool.malloc());
+    }
+
+    void release_rx_buffer(Chunk *chunk) {
+      std::lock_guard l{rxbuf_pool.lock};
+      chunk->clear_qp();
+      rxbuf_pool.free(chunk);
+    }
+
+    void set_rx_stat_logger(PerfCounters *logger) {
+      rxbuf_pool_ctx.set_stat_logger(logger);
+    }
 
+    CephContext  *cct;
    private:
-    Cluster* channel;//RECV
-    Cluster* send;// SEND
+    // TODO: Cluster -> TxPool txbuf_pool
+    // chunk layout fix
+    //  
+    Cluster* send = nullptr;// SEND
     Device *device;
     ProtectionDomain *pd;
+    MemPoolContext rxbuf_pool_ctx;
+    mem_pool     rxbuf_pool;
+
+
+    void* huge_pages_malloc(size_t size);
+    void  huge_pages_free(void *ptr);
   };
 
  private:
-  uint32_t max_send_wr = 0;
-  uint32_t max_recv_wr = 0;
+  uint32_t tx_queue_len = 0;
+  uint32_t rx_queue_len = 0;
   uint32_t max_sge = 0;
   uint8_t  ib_physical_port = 0;
   MemoryManager* memory_manager = nullptr;
@@ -226,21 +394,18 @@ class Infiniband {
   Device *device = NULL;
   ProtectionDomain *pd = NULL;
   DeviceList *device_list = nullptr;
-  RDMADispatcher *dispatcher = nullptr;
-  void wire_gid_to_gid(const char *wgid, union ibv_gid *gid);
-  void gid_to_wire_gid(const union ibv_gid *gid, char wgid[]);
   CephContext *cct;
-  Mutex lock;
+  ceph::mutex lock = ceph::make_mutex("IB lock");
   bool initialized = false;
   const std::string &device_name;
   uint8_t port_num;
+  bool support_srq = false;
 
  public:
-  explicit Infiniband(CephContext *c, const std::string &device_name, uint8_t p);
+  explicit Infiniband(CephContext *c);
   ~Infiniband();
   void init();
-
-  void set_dispatcher(RDMADispatcher *d);
+  static void verify_prereq(CephContext *cct);
 
   class CompletionChannel {
     static const uint32_t MAX_ACK_EVENT = 5000;
@@ -293,13 +458,18 @@ class Infiniband {
   // must call plumb() to bring the queue pair to the RTS state.
   class QueuePair {
    public:
+    typedef MemoryManager::Chunk Chunk;
     QueuePair(CephContext *c, Infiniband& infiniband, ibv_qp_type type,
               int ib_physical_port,  ibv_srq *srq,
               Infiniband::CompletionQueue* txcq,
               Infiniband::CompletionQueue* rxcq,
-              uint32_t max_send_wr, uint32_t max_recv_wr, uint32_t q_key = 0);
+              uint32_t tx_queue_len, uint32_t max_recv_wr, struct rdma_cm_id *cid, uint32_t q_key = 0);
     ~QueuePair();
 
+    int modify_qp_to_error();
+    int modify_qp_to_rts();
+    int modify_qp_to_rtr();
+    int modify_qp_to_init();
     int init();
 
     /**
@@ -328,15 +498,37 @@ class Infiniband {
      * Get the state of a QueuePair.
      */
     int get_state() const;
-    /**
-     * Return true if the queue pair is in an error state, false otherwise.
+    /*
+     * send/receive connection management meta data
      */
-    bool is_error() const;
+    int send_cm_meta(CephContext *cct, int socket_fd);
+    int recv_cm_meta(CephContext *cct, int socket_fd);
+    void wire_gid_to_gid(const char *wgid, ib_cm_meta_t* cm_meta_data);
+    void gid_to_wire_gid(const ib_cm_meta_t& cm_meta_data, char wgid[]);
     ibv_qp* get_qp() const { return qp; }
     Infiniband::CompletionQueue* get_tx_cq() const { return txcq; }
     Infiniband::CompletionQueue* get_rx_cq() const { return rxcq; }
     int to_dead();
     bool is_dead() const { return dead; }
+    ib_cm_meta_t& get_peer_cm_meta() { return peer_cm_meta; }
+    ib_cm_meta_t& get_local_cm_meta() { return local_cm_meta; }
+    void add_rq_wr(Chunk* chunk)
+    {
+      if (srq) return;
+
+      std::lock_guard l{lock};
+      recv_queue.push_back(chunk);
+    }
+
+    void remove_rq_wr(Chunk* chunk) {
+      if (srq) return;
+
+      std::lock_guard l{lock};
+      auto it = std::find(recv_queue.begin(), recv_queue.end(), chunk);
+      ceph_assert(it != recv_queue.end());
+      recv_queue.erase(it);
+    }
+    ibv_srq* get_srq() const { return srq; }
 
    private:
     CephContext  *cct;
@@ -347,6 +539,9 @@ class Infiniband {
     ibv_pd*      pd;             // protection domain
     ibv_srq*     srq;            // shared receive queue
     ibv_qp*      qp;             // infiniband verbs QP handle
+    struct rdma_cm_id *cm_id;
+    ib_cm_meta_t peer_cm_meta;
+    ib_cm_meta_t local_cm_meta;
     Infiniband::CompletionQueue* txcq;
     Infiniband::CompletionQueue* rxcq;
     uint32_t     initial_psn;    // initial packet sequence number
@@ -354,31 +549,39 @@ class Infiniband {
     uint32_t     max_recv_wr;
     uint32_t     q_key;
     bool dead;
+    vector<Chunk*> recv_queue;
+    ceph::mutex lock = ceph::make_mutex("queue_pair_lock");
   };
 
  public:
   typedef MemoryManager::Cluster Cluster;
   typedef MemoryManager::Chunk Chunk;
-  QueuePair* create_queue_pair(CephContext *c, CompletionQueue*, CompletionQueue*, ibv_qp_type type);
+  QueuePair* create_queue_pair(CephContext *c, CompletionQueue*, CompletionQueue*,
+      ibv_qp_type type, struct rdma_cm_id *cm_id);
   ibv_srq* create_shared_receive_queue(uint32_t max_wr, uint32_t max_sge);
-  int post_chunk(Chunk* chunk);
-  int post_channel_cluster();
+  // post rx buffers to srq, return number of buffers actually posted
+  int post_chunks_to_rq(int num, QueuePair *qp = nullptr);
+  void post_chunk_to_pool(Chunk* chunk) {
+    QueuePair *qp = chunk->get_qp();
+    if (qp != nullptr) {
+      qp->remove_rq_wr(chunk);
+    }
+    get_memory_manager()->release_rx_buffer(chunk);
+  }
   int get_tx_buffers(std::vector<Chunk*> &c, size_t bytes);
   CompletionChannel *create_comp_channel(CephContext *c);
   CompletionQueue *create_comp_queue(CephContext *c, CompletionChannel *cc=NULL);
   uint8_t get_ib_physical_port() { return ib_physical_port; }
-  int send_msg(CephContext *cct, int sd, IBSYNMsg& msg);
-  int recv_msg(CephContext *cct, int sd, IBSYNMsg& msg);
   uint16_t get_lid() { return device->get_lid(); }
   ibv_gid get_gid() { return device->get_gid(); }
   MemoryManager* get_memory_manager() { return memory_manager; }
   Device* get_device() { return device; }
   int get_async_fd() { return device->ctxt->async_fd; }
   bool is_tx_buffer(const char* c) { return memory_manager->is_tx_buffer(c);}
-  bool is_rx_buffer(const char* c) { return memory_manager->is_rx_buffer(c);}
   Chunk *get_tx_chunk_by_buffer(const char *c) { return memory_manager->get_tx_chunk_by_buffer(c); }
   static const char* wc_status_to_string(int status);
   static const char* qp_state_string(int status);
+  uint32_t get_rx_queue_len() const { return rx_queue_len; }
 };
 
 #endif
diff --git a/src/msg/async/rdma/RDMAConnectedSocketImpl.cc b/src/msg/async/rdma/RDMAConnectedSocketImpl.cc
index 66dc488cccf..c897f94f4d5 100644
--- a/src/msg/async/rdma/RDMAConnectedSocketImpl.cc
+++ b/src/msg/async/rdma/RDMAConnectedSocketImpl.cc
@@ -13,30 +13,57 @@
  * Foundation.  See file COPYING.
  *
  */
-
 #include "RDMAStack.h"
 
+class C_handle_connection_established : public EventCallback {
+  RDMAConnectedSocketImpl *csi;
+  bool active = true;
+ public:
+  C_handle_connection_established(RDMAConnectedSocketImpl *w) : csi(w) {}
+  void do_request(uint64_t fd) final {
+    if (active)
+      csi->handle_connection_established();
+  }
+  void close() {
+    active = false;
+  }
+};
+
+class C_handle_connection_read : public EventCallback {
+  RDMAConnectedSocketImpl *csi;
+  bool active = true;
+ public:
+  explicit C_handle_connection_read(RDMAConnectedSocketImpl *w): csi(w) {}
+  void do_request(uint64_t fd) final {
+    if (active)
+      csi->handle_connection();
+  }
+  void close() {
+    active = false;
+  }
+};
+
 #define dout_subsys ceph_subsys_ms
 #undef dout_prefix
 #define dout_prefix *_dout << " RDMAConnectedSocketImpl "
 
-RDMAConnectedSocketImpl::RDMAConnectedSocketImpl(CephContext *cct, Infiniband* ib, RDMADispatcher* s,
-						 RDMAWorker *w)
-  : cct(cct), connected(0), error(0), infiniband(ib),
-    dispatcher(s), worker(w), lock("RDMAConnectedSocketImpl::lock"),
-    is_server(false), con_handler(new C_handle_connection(this)),
+RDMAConnectedSocketImpl::RDMAConnectedSocketImpl(CephContext *cct, shared_ptr<Infiniband> &ib,
+                                                 shared_ptr<RDMADispatcher>& rdma_dispatcher,
+                                                 RDMAWorker *w)
+  : cct(cct), connected(0), error(0), ib(ib),
+    dispatcher(rdma_dispatcher), worker(w),
+    is_server(false), read_handler(new C_handle_connection_read(this)),
+    established_handler(new C_handle_connection_established(this)),
     active(false), pending(false)
 {
-  qp = infiniband->create_queue_pair(
-				     cct, s->get_tx_cq(), s->get_rx_cq(), IBV_QPT_RC);
-  my_msg.qpn = qp->get_local_qp_number();
-  my_msg.psn = qp->get_initial_psn();
-  my_msg.lid = infiniband->get_lid();
-  my_msg.peer_qpn = 0;
-  my_msg.gid = infiniband->get_gid();
-  notify_fd = dispatcher->register_qp(qp, this);
-  dispatcher->perf_logger->inc(l_msgr_rdma_created_queue_pair);
-  dispatcher->perf_logger->inc(l_msgr_rdma_active_queue_pair);
+  if (!cct->_conf->ms_async_rdma_cm) {
+    qp = ib->create_queue_pair(cct, dispatcher->get_tx_cq(), dispatcher->get_rx_cq(), IBV_QPT_RC, NULL);
+    local_qpn = qp->get_local_qp_number();
+    notify_fd = eventfd(0, EFD_CLOEXEC|EFD_NONBLOCK);
+    dispatcher->register_qp(qp, this);
+    dispatcher->perf_logger->inc(l_msgr_rdma_created_queue_pair);
+    dispatcher->perf_logger->inc(l_msgr_rdma_active_queue_pair);
+  }
 }
 
 RDMAConnectedSocketImpl::~RDMAConnectedSocketImpl()
@@ -44,29 +71,26 @@ RDMAConnectedSocketImpl::~RDMAConnectedSocketImpl()
   ldout(cct, 20) << __func__ << " destruct." << dendl;
   cleanup();
   worker->remove_pending_conn(this);
-  dispatcher->erase_qpn(my_msg.qpn);
-  Mutex::Locker l(lock);
+  dispatcher->schedule_qp_destroy(local_qpn);
+
+  for (unsigned i=0; i < wc.size(); ++i) {
+    dispatcher->post_chunk_to_pool(reinterpret_cast<Chunk*>(wc[i].wr_id));
+  }
+  for (unsigned i=0; i < buffers.size(); ++i) {
+    dispatcher->post_chunk_to_pool(buffers[i]);
+  }
+
+  std::lock_guard l{lock};
   if (notify_fd >= 0)
     ::close(notify_fd);
   if (tcp_fd >= 0)
     ::close(tcp_fd);
   error = ECONNRESET;
-  int ret = 0;
-  for (unsigned i=0; i < wc.size(); ++i) {
-    ret = infiniband->post_chunk(reinterpret_cast<Chunk*>(wc[i].wr_id));
-    assert(ret == 0);
-    dispatcher->perf_logger->dec(l_msgr_rdma_inqueue_rx_chunks);
-  }
-  for (unsigned i=0; i < buffers.size(); ++i) {
-    ret = infiniband->post_chunk(buffers[i]);
-    assert(ret == 0);
-    dispatcher->perf_logger->dec(l_msgr_rdma_inqueue_rx_chunks);
-  }
 }
 
 void RDMAConnectedSocketImpl::pass_wc(std::vector<ibv_wc> &&v)
 {
-  Mutex::Locker l(lock);
+  std::lock_guard l{lock};
   if (wc.empty())
     wc = std::move(v);
   else
@@ -76,7 +100,7 @@ void RDMAConnectedSocketImpl::pass_wc(std::vector<ibv_wc> &&v)
 
 void RDMAConnectedSocketImpl::get_wc(std::vector<ibv_wc> &w)
 {
-  Mutex::Locker l(lock);
+  std::lock_guard l{lock};
   if (wc.empty())
     return ;
   w.swap(wc);
@@ -84,89 +108,20 @@ void RDMAConnectedSocketImpl::get_wc(std::vector<ibv_wc> &w)
 
 int RDMAConnectedSocketImpl::activate()
 {
-  ibv_qp_attr qpa;
-  int r;
-
-  // now connect up the qps and switch to RTR
-  memset(&qpa, 0, sizeof(qpa));
-  qpa.qp_state = IBV_QPS_RTR;
-  qpa.path_mtu = IBV_MTU_1024;
-  qpa.dest_qp_num = peer_msg.qpn;
-  qpa.rq_psn = peer_msg.psn;
-  qpa.max_dest_rd_atomic = 1;
-  qpa.min_rnr_timer = 12;
-  //qpa.ah_attr.is_global = 0;
-  qpa.ah_attr.is_global = 1;
-  qpa.ah_attr.grh.hop_limit = 6;
-  qpa.ah_attr.grh.dgid = peer_msg.gid;
-
-  qpa.ah_attr.grh.sgid_index = infiniband->get_device()->get_gid_idx();
-
-  qpa.ah_attr.dlid = peer_msg.lid;
-  qpa.ah_attr.sl = cct->_conf->ms_async_rdma_sl;
-  qpa.ah_attr.grh.traffic_class = cct->_conf->ms_async_rdma_dscp;
-  qpa.ah_attr.src_path_bits = 0;
-  qpa.ah_attr.port_num = (uint8_t)(infiniband->get_ib_physical_port());
-
-  ldout(cct, 20) << __func__ << " Choosing gid_index " << (int)qpa.ah_attr.grh.sgid_index << ", sl " << (int)qpa.ah_attr.sl << dendl;
-
-  r = ibv_modify_qp(qp->get_qp(), &qpa, IBV_QP_STATE |
-      IBV_QP_AV |
-      IBV_QP_PATH_MTU |
-      IBV_QP_DEST_QPN |
-      IBV_QP_RQ_PSN |
-      IBV_QP_MIN_RNR_TIMER |
-      IBV_QP_MAX_DEST_RD_ATOMIC);
-  if (r) {
-    lderr(cct) << __func__ << " failed to transition to RTR state: "
-               << cpp_strerror(errno) << dendl;
+  qp->get_local_cm_meta().peer_qpn = qp->get_peer_cm_meta().local_qpn;
+  if (qp->modify_qp_to_rtr() != 0)
     return -1;
-  }
 
-  ldout(cct, 20) << __func__ << " transition to RTR state successfully." << dendl;
-
-  // now move to RTS
-  qpa.qp_state = IBV_QPS_RTS;
-
-  // How long to wait before retrying if packet lost or server dead.
-  // Supposedly the timeout is 4.096us*2^timeout.  However, the actual
-  // timeout appears to be 4.096us*2^(timeout+1), so the setting
-  // below creates a 135ms timeout.
-  qpa.timeout = 14;
-
-  // How many times to retry after timeouts before giving up.
-  qpa.retry_cnt = 7;
-
-  // How many times to retry after RNR (receiver not ready) condition
-  // before giving up. Occurs when the remote side has not yet posted
-  // a receive request.
-  qpa.rnr_retry = 7; // 7 is infinite retry.
-  qpa.sq_psn = my_msg.psn;
-  qpa.max_rd_atomic = 1;
-
-  r = ibv_modify_qp(qp->get_qp(), &qpa, IBV_QP_STATE |
-      IBV_QP_TIMEOUT |
-      IBV_QP_RETRY_CNT |
-      IBV_QP_RNR_RETRY |
-      IBV_QP_SQ_PSN |
-      IBV_QP_MAX_QP_RD_ATOMIC);
-  if (r) {
-    lderr(cct) << __func__ << " failed to transition to RTS state: "
-               << cpp_strerror(errno) << dendl;
+  if (qp->modify_qp_to_rts() != 0)
     return -1;
-  }
-
-  // the queue pair should be ready to use once the client has finished
-  // setting up their end.
-  ldout(cct, 20) << __func__ << " transition to RTS state successfully." << dendl;
-  ldout(cct, 20) << __func__ << " QueuePair: " << qp << " with qp:" << qp->get_qp() << dendl;
 
   if (!is_server) {
     connected = 1; //indicate successfully
-    ldout(cct, 20) << __func__ << " handle fake send, wake it up. QP: " << my_msg.qpn << dendl;
+    ldout(cct, 20) << __func__ << " handle fake send, wake it up. QP: " << local_qpn << dendl;
     submit(false);
   }
   active = true;
+  peer_qpn = qp->get_local_cm_meta().peer_qpn;
 
   return 0;
 }
@@ -175,12 +130,18 @@ int RDMAConnectedSocketImpl::try_connect(const entity_addr_t& peer_addr, const S
   ldout(cct, 20) << __func__ << " nonblock:" << opts.nonblock << ", nodelay:"
                  << opts.nodelay << ", rbuf_size: " << opts.rcbuf_size << dendl;
   NetHandler net(cct);
-  tcp_fd = net.connect(peer_addr, opts.connect_bind_addr);
+
+  // we construct a socket to transport ib sync message
+  // but we shouldn't block in tcp connecting
+  if (opts.nonblock) {
+    tcp_fd = net.nonblock_connect(peer_addr, opts.connect_bind_addr);
+  } else {
+    tcp_fd = net.connect(peer_addr, opts.connect_bind_addr);
+  }
 
   if (tcp_fd < 0) {
     return -errno;
   }
-  net.set_close_on_exec(tcp_fd);
 
   int r = net.set_socket_options(tcp_fd, opts.nodelay, opts.rcbuf_size);
   if (r < 0) {
@@ -191,19 +152,45 @@ int RDMAConnectedSocketImpl::try_connect(const entity_addr_t& peer_addr, const S
 
   ldout(cct, 20) << __func__ << " tcp_fd: " << tcp_fd << dendl;
   net.set_priority(tcp_fd, opts.priority, peer_addr.get_family());
-  my_msg.peer_qpn = 0;
-  r = infiniband->send_msg(cct, tcp_fd, my_msg);
-  if (r < 0)
-    return r;
+  r = 0;
+  if (opts.nonblock) {
+    worker->center.create_file_event(tcp_fd, EVENT_READABLE | EVENT_WRITABLE , established_handler);
+  } else {
+    r = handle_connection_established(false);
+  }
+  return r;
+}
 
-  worker->center.create_file_event(tcp_fd, EVENT_READABLE, con_handler);
+int RDMAConnectedSocketImpl::handle_connection_established(bool need_set_fault) {
+  ldout(cct, 20) << __func__ << " start " << dendl;
+  // delete read event
+  worker->center.delete_file_event(tcp_fd, EVENT_READABLE | EVENT_WRITABLE);
+  if (1 == connected) {
+    ldout(cct, 1) << __func__ << " warnning: logic failed " << dendl;
+    if (need_set_fault) {
+      fault();
+    }
+    return -1;
+  }
+  // send handshake msg to server
+  qp->get_local_cm_meta().peer_qpn = 0;
+  int r = qp->send_cm_meta(cct, tcp_fd);
+  if (r < 0) {
+    ldout(cct, 1) << __func__ << " send handshake msg failed." << r << dendl;
+    if (need_set_fault) {
+      fault();
+    }
+    return r;
+  }
+  worker->center.create_file_event(tcp_fd, EVENT_READABLE, read_handler);
+  ldout(cct, 20) << __func__ << " finish " << dendl;
   return 0;
 }
 
 void RDMAConnectedSocketImpl::handle_connection() {
-  ldout(cct, 20) << __func__ << " QP: " << my_msg.qpn << " tcp_fd: " << tcp_fd << " notify_fd: " << notify_fd << dendl;
-  int r = infiniband->recv_msg(cct, tcp_fd, peer_msg);
-  if (r < 0) {
+  ldout(cct, 20) << __func__ << " QP: " << local_qpn << " tcp_fd: " << tcp_fd << " notify_fd: " << notify_fd << dendl;
+  int r = qp->recv_cm_meta(cct, tcp_fd);
+  if (r <= 0) {
     if (r != -EAGAIN) {
       dispatcher->perf_logger->inc(l_msgr_rdma_handshake_errors);
       ldout(cct, 1) << __func__ << " recv handshake msg failed." << dendl;
@@ -212,39 +199,43 @@ void RDMAConnectedSocketImpl::handle_connection() {
     return;
   }
 
-  if (!is_server) {// syn + ack from server
-    my_msg.peer_qpn = peer_msg.qpn;
-    ldout(cct, 20) << __func__ << " peer msg :  < " << peer_msg.qpn << ", " << peer_msg.psn
-                   <<  ", " << peer_msg.lid << ", " << peer_msg.peer_qpn << "> " << dendl;
+  if (1 == connected) {
+    ldout(cct, 1) << __func__ << " warnning: logic failed: read len: " << r << dendl;
+    fault();
+    return;
+  }
+
+  if (!is_server) {// first time: cm meta sync + ack from server
     if (!connected) {
       r = activate();
-      assert(!r);
+      ceph_assert(!r);
     }
     notify();
-    r = infiniband->send_msg(cct, tcp_fd, my_msg);
+    r = qp->send_cm_meta(cct, tcp_fd);
     if (r < 0) {
       ldout(cct, 1) << __func__ << " send client ack failed." << dendl;
       dispatcher->perf_logger->inc(l_msgr_rdma_handshake_errors);
       fault();
     }
   } else {
-    if (peer_msg.peer_qpn == 0) {// syn from client
+    if (qp->get_peer_cm_meta().peer_qpn == 0) {// first time: cm meta sync from client
       if (active) {
         ldout(cct, 10) << __func__ << " server is already active." << dendl;
         return ;
       }
-      r = infiniband->send_msg(cct, tcp_fd, my_msg);
+      r = activate();
+      ceph_assert(!r);
+      r = qp->send_cm_meta(cct, tcp_fd);
       if (r < 0) {
         ldout(cct, 1) << __func__ << " server ack failed." << dendl;
         dispatcher->perf_logger->inc(l_msgr_rdma_handshake_errors);
         fault();
         return ;
       }
-      r = activate();
-      assert(!r);
-    } else { // ack from client
+    } else { // second time: cm meta ack from client
       connected = 1;
-      cleanup();
+      ldout(cct, 10) << __func__ << " handshake of rdma is done. server connected: " << connected << dendl;
+      //cleanup();
       submit(false);
       notify();
     }
@@ -253,64 +244,25 @@ void RDMAConnectedSocketImpl::handle_connection() {
 
 ssize_t RDMAConnectedSocketImpl::read(char* buf, size_t len)
 {
-  uint64_t i = 0;
-  int r = ::read(notify_fd, &i, sizeof(i));
-  ldout(cct, 20) << __func__ << " notify_fd : " << i << " in " << my_msg.qpn << " r = " << r << dendl;
-  ssize_t read = 0;
-  if (!buffers.empty())
-    read = read_buffers(buf,len);
+  eventfd_t event_val = 0;
+  int r = eventfd_read(notify_fd, &event_val);
+  ldout(cct, 20) << __func__ << " notify_fd : " << event_val << " in " << local_qpn
+                 << " r = " << r << dendl;
 
-  std::vector<ibv_wc> cqe;
-  get_wc(cqe);
-  if (cqe.empty()) {
-    if (!buffers.empty()) {
-      notify();
-    }
-    if (read > 0) {
-      return read;
-    }
-    if (error) {
-      return -error;
-    } else {
-      return -EAGAIN;
-    }
+  if (!active) {
+    ldout(cct, 1) << __func__ << " when ib not active. len: " << len << dendl;
+    return -EAGAIN;
   }
 
-  ldout(cct, 20) << __func__ << " poll queue got " << cqe.size() << " responses. QP: " << my_msg.qpn << dendl;
-  for (size_t i = 0; i < cqe.size(); ++i) {
-    ibv_wc* response = &cqe[i];
-    assert(response->status == IBV_WC_SUCCESS);
-    Chunk* chunk = reinterpret_cast<Chunk *>(response->wr_id);
-    ldout(cct, 25) << __func__ << " chunk length: " << response->byte_len << " bytes." << chunk << dendl;
-    chunk->prepare_read(response->byte_len);
-    worker->perf_logger->inc(l_msgr_rdma_rx_bytes, response->byte_len);
-    if (response->byte_len == 0) {
-      dispatcher->perf_logger->inc(l_msgr_rdma_rx_fin);
-      if (connected) {
-        error = ECONNRESET;
-        ldout(cct, 20) << __func__ << " got remote close msg..." << dendl;
-      }
-      assert(infiniband->post_chunk(chunk) == 0);
-      dispatcher->perf_logger->dec(l_msgr_rdma_inqueue_rx_chunks);
-    } else {
-      if (read == (ssize_t)len) {
-        buffers.push_back(chunk);
-        ldout(cct, 25) << __func__ << " buffers add a chunk: " << response->byte_len << dendl;
-      } else if (read + response->byte_len > (ssize_t)len) {
-        read += chunk->read(buf+read, (ssize_t)len-read);
-        buffers.push_back(chunk);
-        ldout(cct, 25) << __func__ << " buffers add a chunk: " << chunk->get_offset() << ":" << chunk->get_bound() << dendl;
-      } else {
-        read += chunk->read(buf+read, response->byte_len);
-        assert(infiniband->post_chunk(chunk) == 0);
-        dispatcher->perf_logger->dec(l_msgr_rdma_inqueue_rx_chunks);
-      }
-    }
+  if (0 == connected) {
+    ldout(cct, 1) << __func__ << " when ib not connected. len: " << len <<dendl;
+    return -EAGAIN;
   }
+  ssize_t read = 0;
+  read = read_buffers(buf,len);
 
-  worker->perf_logger->inc(l_msgr_rdma_rx_chunks, cqe.size());
   if (is_server && connected == 0) {
-    ldout(cct, 20) << __func__ << " we do not need last handshake, QP: " << my_msg.qpn << " peer QP: " << peer_msg.qpn << dendl;
+    ldout(cct, 20) << __func__ << " we do not need last handshake, QP: " << local_qpn << " peer QP: " << peer_qpn << dendl;
     connected = 1; //if so, we don't need the last handshake
     cleanup();
     submit(false);
@@ -325,76 +277,64 @@ ssize_t RDMAConnectedSocketImpl::read(char* buf, size_t len)
   return read == 0 ? -EAGAIN : read;
 }
 
-ssize_t RDMAConnectedSocketImpl::read_buffers(char* buf, size_t len)
+void RDMAConnectedSocketImpl::buffer_prefetch(void)
 {
-  size_t read = 0, tmp = 0;
-  auto c = buffers.begin();
-  for (; c != buffers.end() ; ++c) {
-    tmp = (*c)->read(buf+read, len-read);
-    read += tmp;
-    ldout(cct, 25) << __func__ << " this iter read: " << tmp << " bytes." << " offset: " << (*c)->get_offset() << " ,bound: " << (*c)->get_bound()  << ". Chunk:" << *c  << dendl;
-    if ((*c)->over()) {
-      assert(infiniband->post_chunk(*c) == 0);
-      dispatcher->perf_logger->dec(l_msgr_rdma_inqueue_rx_chunks);
-      ldout(cct, 25) << __func__ << " one chunk over." << dendl;
-    }
-    if (read == len) {
-      break;
+  std::vector<ibv_wc> cqe;
+  get_wc(cqe);
+  if(cqe.empty())
+    return;
+
+  for(size_t i = 0; i < cqe.size(); ++i) {
+    ibv_wc* response = &cqe[i];
+    ceph_assert(response->status == IBV_WC_SUCCESS);
+    Chunk* chunk = reinterpret_cast<Chunk *>(response->wr_id);
+    chunk->prepare_read(response->byte_len);
+
+    if (chunk->get_size() == 0) {
+      chunk->reset_read_chunk();
+      dispatcher->perf_logger->inc(l_msgr_rdma_rx_fin);
+      if (connected) {
+        error = ECONNRESET;
+        ldout(cct, 20) << __func__ << " got remote close msg..." << dendl;
+      }
+      dispatcher->post_chunk_to_pool(chunk);
+      continue;
+    } else {
+      buffers.push_back(chunk);
+      ldout(cct, 25) << __func__ << " buffers add a chunk: " << chunk->get_offset() << ":" << chunk->get_bound() << dendl;
     }
   }
-
-  if (c != buffers.end() && (*c)->over())
-    ++c;
-  buffers.erase(buffers.begin(), c);
-  ldout(cct, 25) << __func__ << " got " << read  << " bytes, buffers size: " << buffers.size() << dendl;
-  return read;
+  worker->perf_logger->inc(l_msgr_rdma_rx_chunks, cqe.size());
 }
 
-ssize_t RDMAConnectedSocketImpl::zero_copy_read(bufferptr &data)
+ssize_t RDMAConnectedSocketImpl::read_buffers(char* buf, size_t len)
 {
-  if (error)
-    return -error;
-  static const int MAX_COMPLETIONS = 16;
-  ibv_wc wc[MAX_COMPLETIONS];
-  ssize_t size = 0;
-
-  ibv_wc*  response;
-  Chunk* chunk;
-  bool loaded = false;
-  auto iter = buffers.begin();
-  if (iter != buffers.end()) {
-    chunk = *iter;
-    // FIXME need to handle release
-    // auto del = std::bind(&Chunk::post_srq, std::move(chunk), infiniband);
-    buffers.erase(iter);
-    loaded = true;
-    size = chunk->bound;
-  }
-
-  std::vector<ibv_wc> cqe;
-  get_wc(cqe);
-  if (cqe.empty())
-    return size == 0 ? -EAGAIN : size;
-
-  ldout(cct, 20) << __func__ << " pool completion queue got " << cqe.size() << " responses."<< dendl;
+  size_t read_size = 0, tmp = 0;
+  buffer_prefetch();
+  auto pchunk = buffers.begin();
+  while (pchunk != buffers.end()) {
+    tmp = (*pchunk)->read(buf + read_size, len - read_size);
+    read_size += tmp;
+    ldout(cct, 25) << __func__ << " read chunk " << *pchunk << " bytes length" << tmp << " offset: "
+                   << (*pchunk)->get_offset() << " ,bound: " << (*pchunk)->get_bound() << dendl;
+
+    if ((*pchunk)->get_size() == 0) {
+      (*pchunk)->reset_read_chunk();
+      dispatcher->post_chunk_to_pool(*pchunk);
+      update_post_backlog();
+      ldout(cct, 25) << __func__ << " read over one chunk " << dendl;
+      pchunk++;
+    }
 
-  for (size_t i = 0; i < cqe.size(); ++i) {
-    response = &wc[i];
-    chunk = reinterpret_cast<Chunk*>(response->wr_id);
-    chunk->prepare_read(response->byte_len);
-    if (!loaded && i == 0) {
-      // FIXME need to handle release
-      // auto del = std::bind(&Chunk::post_srq, std::move(chunk), infiniband);
-      size = chunk->bound;
-      continue;
+    if (read_size == len) {
+      break;
     }
-    buffers.push_back(chunk);
-    iter++;
   }
 
-  if (size == 0)
-    return -EAGAIN;
-  return size;
+  buffers.erase(buffers.begin(), pchunk);
+  ldout(cct, 25) << __func__ << " got " << read_size  << " bytes, buffers size: " << buffers.size() << dendl;
+  worker->perf_logger->inc(l_msgr_rdma_rx_bytes, read_size);
+  return read_size;
 }
 
 ssize_t RDMAConnectedSocketImpl::send(bufferlist &bl, bool more)
@@ -408,128 +348,133 @@ ssize_t RDMAConnectedSocketImpl::send(bufferlist &bl, bool more)
   if (!bytes)
     return 0;
   {
-    Mutex::Locker l(lock);
+    std::lock_guard l{lock};
     pending_bl.claim_append(bl);
     if (!connected) {
-      ldout(cct, 20) << __func__ << " fake send to upper, QP: " << my_msg.qpn << dendl;
+      ldout(cct, 20) << __func__ << " fake send to upper, QP: " << local_qpn << dendl;
       return bytes;
     }
   }
-  ldout(cct, 20) << __func__ << " QP: " << my_msg.qpn << dendl;
+  ldout(cct, 20) << __func__ << " QP: " << local_qpn << dendl;
   ssize_t r = submit(more);
   if (r < 0 && r != -EAGAIN)
     return r;
   return bytes;
 }
 
+size_t RDMAConnectedSocketImpl::tx_copy_chunk(std::vector<Chunk*> &tx_buffers,
+    size_t req_copy_len, decltype(std::cbegin(pending_bl.buffers()))& start,
+    const decltype(std::cbegin(pending_bl.buffers()))& end)
+{
+  ceph_assert(start != end);
+  auto chunk_idx = tx_buffers.size();
+  if (0 == worker->get_reged_mem(this, tx_buffers, req_copy_len)) {
+    ldout(cct, 1) << __func__ << " no enough buffers in worker " << worker << dendl;
+    worker->perf_logger->inc(l_msgr_rdma_tx_no_mem);
+    return 0;
+  }
+
+  Chunk *current_chunk = tx_buffers[chunk_idx];
+  size_t write_len = 0;
+  while (start != end) {
+    const uintptr_t addr = reinterpret_cast<uintptr_t>(start->c_str());
+
+    size_t slice_write_len = 0;
+    while (slice_write_len < start->length()) {
+      size_t real_len = current_chunk->write((char*)addr + slice_write_len, start->length() - slice_write_len);
+
+      slice_write_len += real_len;
+      write_len += real_len;
+      req_copy_len -= real_len;
+
+      if (current_chunk->full()) {
+        if (++chunk_idx == tx_buffers.size())
+          return write_len;
+        current_chunk = tx_buffers[chunk_idx];
+      }
+    }
+
+    ++start;
+  }
+  ceph_assert(req_copy_len == 0);
+  return write_len;
+}
+
 ssize_t RDMAConnectedSocketImpl::submit(bool more)
 {
   if (error)
     return -error;
-  Mutex::Locker l(lock);
+  std::lock_guard l{lock};
   size_t bytes = pending_bl.length();
   ldout(cct, 20) << __func__ << " we need " << bytes << " bytes. iov size: "
-                 << pending_bl.buffers().size() << dendl;
+                 << pending_bl.get_num_buffers() << dendl;
   if (!bytes)
     return 0;
 
-  auto fill_tx_via_copy = [this](std::vector<Chunk*> &tx_buffers, unsigned bytes,
-                                 std::list<bufferptr>::const_iterator &start,
-                                 std::list<bufferptr>::const_iterator &end) -> unsigned {
-    assert(start != end);
-    auto chunk_idx = tx_buffers.size();
-    int ret = worker->get_reged_mem(this, tx_buffers, bytes);
-    if (ret == 0) {
-      ldout(cct, 1) << __func__ << " no enough buffers in worker " << worker << dendl;
-      worker->perf_logger->inc(l_msgr_rdma_tx_no_mem);
-      return 0;
-    }
-
-    unsigned total_copied = 0;
-    Chunk *current_chunk = tx_buffers[chunk_idx];
-    while (start != end) {
-      const uintptr_t addr = reinterpret_cast<const uintptr_t>(start->c_str());
-      unsigned copied = 0;
-      while (copied < start->length()) {
-        uint32_t r = current_chunk->write((char*)addr+copied, start->length() - copied);
-        copied += r;
-        total_copied += r;
-        bytes -= r;
-        if (current_chunk->full()){
-          current_chunk = tx_buffers[++chunk_idx];
-          if (chunk_idx == tx_buffers.size())
-            return total_copied;
-        }
-      }
-      ++start;
-    }
-    assert(bytes == 0);
-    return total_copied;
-  };
-
   std::vector<Chunk*> tx_buffers;
-  std::list<bufferptr>::const_iterator it = pending_bl.buffers().begin();
-  std::list<bufferptr>::const_iterator copy_it = it;
-  unsigned total = 0;
-  unsigned need_reserve_bytes = 0;
+  auto it = std::cbegin(pending_bl.buffers());
+  auto copy_start = it;
+  size_t total_copied = 0, wait_copy_len = 0;
   while (it != pending_bl.buffers().end()) {
-    if (infiniband->is_tx_buffer(it->raw_c_str())) {
-      if (need_reserve_bytes) {
-        unsigned copied = fill_tx_via_copy(tx_buffers, need_reserve_bytes, copy_it, it);
-        total += copied;
-        if (copied < need_reserve_bytes)
+    if (ib->is_tx_buffer(it->raw_c_str())) {
+      if (wait_copy_len) {
+        size_t copied = tx_copy_chunk(tx_buffers, wait_copy_len, copy_start, it);
+        total_copied += copied;
+        if (copied < wait_copy_len)
           goto sending;
-        need_reserve_bytes = 0;
+        wait_copy_len = 0;
       }
-      assert(copy_it == it);
-      tx_buffers.push_back(infiniband->get_tx_chunk_by_buffer(it->raw_c_str()));
-      total += it->length();
-      ++copy_it;
+      ceph_assert(copy_start == it);
+      tx_buffers.push_back(ib->get_tx_chunk_by_buffer(it->raw_c_str()));
+      total_copied += it->length();
+      ++copy_start;
     } else {
-      need_reserve_bytes += it->length();
+      wait_copy_len += it->length();
     }
     ++it;
   }
-  if (need_reserve_bytes)
-    total += fill_tx_via_copy(tx_buffers, need_reserve_bytes, copy_it, it);
+  if (wait_copy_len)
+    total_copied += tx_copy_chunk(tx_buffers, wait_copy_len, copy_start, it);
 
  sending:
-  if (total == 0)
+  if (total_copied == 0)
     return -EAGAIN;
-  assert(total <= pending_bl.length());
+  ceph_assert(total_copied <= pending_bl.length());
   bufferlist swapped;
-  if (total < pending_bl.length()) {
+  if (total_copied < pending_bl.length()) {
     worker->perf_logger->inc(l_msgr_rdma_tx_parital_mem);
-    pending_bl.splice(total, pending_bl.length()-total, &swapped);
+    pending_bl.splice(total_copied, pending_bl.length() - total_copied, &swapped);
     pending_bl.swap(swapped);
   } else {
     pending_bl.clear();
   }
 
   ldout(cct, 20) << __func__ << " left bytes: " << pending_bl.length() << " in buffers "
-                 << pending_bl.buffers().size() << " tx chunks " << tx_buffers.size() << dendl;
+                 << pending_bl.get_num_buffers() << " tx chunks " << tx_buffers.size() << dendl;
 
   int r = post_work_request(tx_buffers);
   if (r < 0)
     return r;
 
-  ldout(cct, 20) << __func__ << " finished sending " << bytes << " bytes." << dendl;
+  ldout(cct, 20) << __func__ << " finished sending " << total_copied << " bytes." << dendl;
   return pending_bl.length() ? -EAGAIN : 0;
 }
 
 int RDMAConnectedSocketImpl::post_work_request(std::vector<Chunk*> &tx_buffers)
 {
-  ldout(cct, 20) << __func__ << " QP: " << my_msg.qpn << " " << tx_buffers[0] << dendl;
+  ldout(cct, 20) << __func__ << " QP: " << local_qpn << " " << tx_buffers[0] << dendl;
   vector<Chunk*>::iterator current_buffer = tx_buffers.begin();
   ibv_sge isge[tx_buffers.size()];
   uint32_t current_sge = 0;
   ibv_send_wr iswr[tx_buffers.size()];
   uint32_t current_swr = 0;
   ibv_send_wr* pre_wr = NULL;
+  uint32_t num = 0; 
 
+  // FIPS zeroization audit 20191115: these memsets are not security related.
   memset(iswr, 0, sizeof(iswr));
   memset(isge, 0, sizeof(isge));
-  current_buffer = tx_buffers.begin();
+ 
   while (current_buffer != tx_buffers.end()) {
     isge[current_sge].addr = reinterpret_cast<uint64_t>((*current_buffer)->buffer);
     isge[current_sge].length = (*current_buffer)->get_offset();
@@ -542,11 +487,8 @@ int RDMAConnectedSocketImpl::post_work_request(std::vector<Chunk*> &tx_buffers)
     iswr[current_swr].num_sge = 1;
     iswr[current_swr].opcode = IBV_WR_SEND;
     iswr[current_swr].send_flags = IBV_SEND_SIGNALED;
-    /*if (isge[current_sge].length < infiniband->max_inline_data) {
-      iswr[current_swr].send_flags = IBV_SEND_INLINE;
-      ldout(cct, 20) << __func__ << " send_inline." << dendl;
-      }*/
 
+    num++;
     worker->perf_logger->inc(l_msgr_rdma_tx_bytes, isge[current_sge].length);
     if (pre_wr)
       pre_wr->next = &iswr[current_swr];
@@ -556,7 +498,7 @@ int RDMAConnectedSocketImpl::post_work_request(std::vector<Chunk*> &tx_buffers)
     ++current_buffer;
   }
 
-  ibv_send_wr *bad_tx_work_request;
+  ibv_send_wr *bad_tx_work_request = nullptr;
   if (ibv_post_send(qp->get_qp(), iswr, &bad_tx_work_request)) {
     ldout(cct, 1) << __func__ << " failed to send data"
                   << " (most probably should be peer not ready): "
@@ -565,18 +507,20 @@ int RDMAConnectedSocketImpl::post_work_request(std::vector<Chunk*> &tx_buffers)
     return -errno;
   }
   worker->perf_logger->inc(l_msgr_rdma_tx_chunks, tx_buffers.size());
-  ldout(cct, 20) << __func__ << " qp state is " << Infiniband::qp_state_string(qp->get_state()) << dendl;
+  ldout(cct, 20) << __func__ << " qp state is " << get_qp_state() << dendl;
   return 0;
 }
 
 void RDMAConnectedSocketImpl::fin() {
   ibv_send_wr wr;
+  // FIPS zeroization audit 20191115: this memset is not security related.
   memset(&wr, 0, sizeof(wr));
+
   wr.wr_id = reinterpret_cast<uint64_t>(qp);
   wr.num_sge = 0;
   wr.opcode = IBV_WR_SEND;
   wr.send_flags = IBV_SEND_SIGNALED;
-  ibv_send_wr* bad_tx_work_request;
+  ibv_send_wr* bad_tx_work_request = nullptr;
   if (ibv_post_send(qp->get_qp(), &wr, &bad_tx_work_request)) {
     ldout(cct, 1) << __func__ << " failed to send message="
                   << " ibv_post_send failed(most probably should be peer not ready): "
@@ -587,23 +531,26 @@ void RDMAConnectedSocketImpl::fin() {
 }
 
 void RDMAConnectedSocketImpl::cleanup() {
-  if (con_handler && tcp_fd >= 0) {
-    (static_cast<C_handle_connection*>(con_handler))->close();
+  if (read_handler && tcp_fd >= 0) {
+    (static_cast<C_handle_connection_read*>(read_handler))->close();
     worker->center.submit_to(worker->center.get_id(), [this]() {
-      worker->center.delete_file_event(tcp_fd, EVENT_READABLE);
+      worker->center.delete_file_event(tcp_fd, EVENT_READABLE | EVENT_WRITABLE);
     }, false);
-    delete con_handler;
-    con_handler = nullptr;
+    delete read_handler;
+    read_handler = nullptr;
+  }
+  if (established_handler) {
+    (static_cast<C_handle_connection_established*>(established_handler))->close();
+    delete established_handler;
+    established_handler = nullptr;
   }
 }
 
 void RDMAConnectedSocketImpl::notify()
 {
-  uint64_t i = 1;
-  int ret;
-
-  ret = write(notify_fd, &i, sizeof(i));
-  assert(ret = sizeof(i));
+  eventfd_t event_val = 1;
+  int r = eventfd_write(notify_fd, event_val);
+  ceph_assert(r == 0);
 }
 
 void RDMAConnectedSocketImpl::shutdown()
@@ -625,10 +572,6 @@ void RDMAConnectedSocketImpl::close()
 void RDMAConnectedSocketImpl::fault()
 {
   ldout(cct, 1) << __func__ << " tcp fd " << tcp_fd << dendl;
-  /*if (qp) {
-    qp->to_dead();
-    qp = NULL;
-    }*/
   error = ECONNRESET;
   connected = 1;
   notify();
@@ -639,6 +582,17 @@ void RDMAConnectedSocketImpl::set_accept_fd(int sd)
   tcp_fd = sd;
   is_server = true;
   worker->center.submit_to(worker->center.get_id(), [this]() {
-			   worker->center.create_file_event(tcp_fd, EVENT_READABLE, con_handler);
+			   worker->center.create_file_event(tcp_fd, EVENT_READABLE, read_handler);
 			   }, true);
 }
+
+void RDMAConnectedSocketImpl::post_chunks_to_rq(int num)
+{
+  post_backlog += num - ib->post_chunks_to_rq(num, qp);
+}
+
+void RDMAConnectedSocketImpl::update_post_backlog()
+{
+  if (post_backlog)
+    post_backlog -= post_backlog - dispatcher->post_chunks_to_rq(post_backlog, qp);
+}
diff --git a/src/msg/async/rdma/RDMAIWARPConnectedSocketImpl.cc b/src/msg/async/rdma/RDMAIWARPConnectedSocketImpl.cc
new file mode 100644
index 00000000000..d55ced3c53f
--- /dev/null
+++ b/src/msg/async/rdma/RDMAIWARPConnectedSocketImpl.cc
@@ -0,0 +1,183 @@
+#include "RDMAStack.h"
+
+#define dout_subsys ceph_subsys_ms
+#undef dout_prefix
+#define dout_prefix *_dout << " RDMAIWARPConnectedSocketImpl "
+
+#define TIMEOUT_MS 3000
+#define RETRY_COUNT 7
+
+RDMAIWARPConnectedSocketImpl::RDMAIWARPConnectedSocketImpl(CephContext *cct, shared_ptr<Infiniband>& ib,
+                                                           shared_ptr<RDMADispatcher>& rdma_dispatcher,
+                                                           RDMAWorker *w, RDMACMInfo *info)
+  : RDMAConnectedSocketImpl(cct, ib, rdma_dispatcher, w), cm_con_handler(new C_handle_cm_connection(this))
+{
+  status = IDLE;
+  notify_fd = eventfd(0, EFD_CLOEXEC|EFD_NONBLOCK);
+  if (info) {
+    is_server = true;
+    cm_id = info->cm_id;
+    cm_channel = info->cm_channel;
+    status = RDMA_ID_CREATED;
+    peer_qpn = info->qp_num;
+    if (alloc_resource()) {
+      close_notify();
+      return;
+    }
+    worker->center.submit_to(worker->center.get_id(), [this]() {
+      worker->center.create_file_event(cm_channel->fd, EVENT_READABLE, cm_con_handler);
+      status = CHANNEL_FD_CREATED;
+    }, false);
+    status = RESOURCE_ALLOCATED;
+    qp->get_local_cm_meta().peer_qpn = peer_qpn;
+    qp->get_peer_cm_meta().local_qpn = peer_qpn;
+  } else {
+    is_server = false;
+    cm_channel = rdma_create_event_channel();
+    rdma_create_id(cm_channel, &cm_id, NULL, RDMA_PS_TCP);
+    status = RDMA_ID_CREATED;
+    ldout(cct, 20) << __func__ << " successfully created cm id: " << cm_id << dendl;
+  }
+}
+
+RDMAIWARPConnectedSocketImpl::~RDMAIWARPConnectedSocketImpl() {
+  ldout(cct, 20) << __func__ << " destruct." << dendl;
+  std::unique_lock l(close_mtx);
+  close_condition.wait(l, [&] { return closed; });
+  if (status >= RDMA_ID_CREATED) {
+    rdma_destroy_id(cm_id);
+    rdma_destroy_event_channel(cm_channel);
+  }
+}
+
+int RDMAIWARPConnectedSocketImpl::try_connect(const entity_addr_t& peer_addr, const SocketOptions &opts) {
+  worker->center.create_file_event(cm_channel->fd, EVENT_READABLE, cm_con_handler);
+  status = CHANNEL_FD_CREATED;
+  if (rdma_resolve_addr(cm_id, NULL, const_cast<struct sockaddr*>(peer_addr.get_sockaddr()), TIMEOUT_MS)) {
+    lderr(cct) << __func__ << " failed to resolve addr" << dendl;
+    return -1;
+  }
+  return 0;
+}
+
+void RDMAIWARPConnectedSocketImpl::close() {
+  error = ECONNRESET;
+  active = false;
+  if (status >= CONNECTED) {
+    rdma_disconnect(cm_id);
+  }
+  close_notify();
+}
+
+void RDMAIWARPConnectedSocketImpl::shutdown() {
+  error = ECONNRESET;
+  active = false;
+}
+
+void RDMAIWARPConnectedSocketImpl::handle_cm_connection() {
+  struct rdma_cm_event *event;
+  rdma_get_cm_event(cm_channel, &event);
+  ldout(cct, 20) << __func__ << " event name: " << rdma_event_str(event->event)
+                             << " (cm id: " << cm_id << ")" << dendl;
+  struct rdma_conn_param cm_params;
+  switch (event->event) {
+    case RDMA_CM_EVENT_ADDR_RESOLVED:
+      status = ADDR_RESOLVED;
+      if (rdma_resolve_route(cm_id, TIMEOUT_MS)) {
+        lderr(cct) << __func__ << " failed to resolve rdma addr" << dendl;
+        notify();
+      }
+      break;
+
+    case RDMA_CM_EVENT_ROUTE_RESOLVED:
+      status = ROUTE_RESOLVED;
+      if (alloc_resource()) {
+        lderr(cct) << __func__ << " failed to alloc resource while resolving the route" << dendl;
+        connected = -ECONNREFUSED;
+        notify();
+        break;
+      }
+
+      // FIPS zeroization audit 20191115: this memset is not security related.
+      memset(&cm_params, 0, sizeof(cm_params));
+      cm_params.retry_count = RETRY_COUNT;
+      cm_params.qp_num = local_qpn;
+      if (rdma_connect(cm_id, &cm_params)) {
+        lderr(cct) << __func__ << " failed to connect remote rdma port" << dendl;
+        connected = -ECONNREFUSED;
+        notify();
+      }
+      break;
+
+    case RDMA_CM_EVENT_ESTABLISHED:
+      ldout(cct, 20) << __func__ << " qp_num=" << cm_id->qp->qp_num << dendl;
+      status = CONNECTED;
+      if (!is_server) {
+        peer_qpn = event->param.conn.qp_num;
+        activate();
+        qp->get_local_cm_meta().peer_qpn = peer_qpn;
+        qp->get_peer_cm_meta().local_qpn = peer_qpn;
+        notify();
+      }
+      break;
+
+    case RDMA_CM_EVENT_ADDR_ERROR:
+    case RDMA_CM_EVENT_ROUTE_ERROR:
+    case RDMA_CM_EVENT_CONNECT_ERROR:
+    case RDMA_CM_EVENT_UNREACHABLE:
+    case RDMA_CM_EVENT_REJECTED:
+      lderr(cct) << __func__ << " rdma connection rejected" << dendl;
+      connected = -ECONNREFUSED;
+      notify();
+      break;
+
+    case RDMA_CM_EVENT_DISCONNECTED:
+      status = DISCONNECTED;
+      close_notify();
+      if (!error) {
+        error = ECONNRESET;
+        notify();
+      }
+      break;
+
+    case RDMA_CM_EVENT_DEVICE_REMOVAL:
+      break;
+
+    default:
+      ceph_abort_msg("unhandled event");
+      break;
+  }
+  rdma_ack_cm_event(event);
+}
+
+void RDMAIWARPConnectedSocketImpl::activate() {
+  ldout(cct, 30) << __func__ << dendl;
+  active = true;
+  connected = 1;
+}
+
+int RDMAIWARPConnectedSocketImpl::alloc_resource() {
+  ldout(cct, 30) << __func__ << dendl;
+  qp = ib->create_queue_pair(cct, dispatcher->get_tx_cq(),
+      dispatcher->get_rx_cq(), IBV_QPT_RC, cm_id);
+  if (!qp) {
+    return -1;
+  }
+  local_qpn = qp->get_local_qp_number();
+  dispatcher->register_qp(qp, this);
+  dispatcher->perf_logger->inc(l_msgr_rdma_created_queue_pair);
+  dispatcher->perf_logger->inc(l_msgr_rdma_active_queue_pair);
+  return 0;
+}
+
+void RDMAIWARPConnectedSocketImpl::close_notify() {
+  ldout(cct, 30) << __func__ << dendl;
+  if (status >= CHANNEL_FD_CREATED) {
+    worker->center.delete_file_event(cm_channel->fd, EVENT_READABLE);
+  }
+  std::unique_lock l(close_mtx);
+  if (!closed) {
+    closed = true;
+    close_condition.notify_all();
+  }
+}
diff --git a/src/msg/async/rdma/RDMAIWARPServerSocketImpl.cc b/src/msg/async/rdma/RDMAIWARPServerSocketImpl.cc
new file mode 100644
index 00000000000..e4a170ee8be
--- /dev/null
+++ b/src/msg/async/rdma/RDMAIWARPServerSocketImpl.cc
@@ -0,0 +1,119 @@
+#include <poll.h>
+
+#include "msg/async/net_handler.h"
+#include "RDMAStack.h"
+
+#define dout_subsys ceph_subsys_ms
+#undef dout_prefix
+#define dout_prefix *_dout << " RDMAIWARPServerSocketImpl "
+
+RDMAIWARPServerSocketImpl::RDMAIWARPServerSocketImpl(
+  CephContext *cct, shared_ptr<Infiniband>& ib,
+  shared_ptr<RDMADispatcher>& rdma_dispatcher, RDMAWorker *w,
+  entity_addr_t& a, unsigned addr_slot)
+  : RDMAServerSocketImpl(cct, ib, rdma_dispatcher, w, a, addr_slot)
+{
+}
+
+int RDMAIWARPServerSocketImpl::listen(entity_addr_t &sa,
+				      const SocketOptions &opt)
+{
+  ldout(cct, 20) << __func__ << " bind to rdma point" << dendl;
+  cm_channel = rdma_create_event_channel();
+  rdma_create_id(cm_channel, &cm_id, NULL, RDMA_PS_TCP);
+  ldout(cct, 20) << __func__ << " successfully created cm id: " << cm_id << dendl;
+  int rc = rdma_bind_addr(cm_id, const_cast<struct sockaddr*>(sa.get_sockaddr()));
+  if (rc < 0) {
+    rc = -errno;
+    ldout(cct, 10) << __func__ << " unable to bind to " << sa.get_sockaddr()
+                   << " on port " << sa.get_port() << ": " << cpp_strerror(errno) << dendl;
+    goto err;
+  }
+  rc = rdma_listen(cm_id, 128);
+  if (rc < 0) {
+    rc = -errno;
+    ldout(cct, 10) << __func__ << " unable to listen to " << sa.get_sockaddr()
+                   << " on port " << sa.get_port() << ": " << cpp_strerror(errno) << dendl;
+    goto err;
+  }
+  server_setup_socket = cm_channel->fd;
+  rc = net.set_nonblock(server_setup_socket);
+  if (rc < 0) {
+    goto err;
+  }
+  ldout(cct, 20) << __func__ << " fd of cm_channel is " << server_setup_socket << dendl;
+  return 0;
+
+err:
+  server_setup_socket = -1;
+  rdma_destroy_id(cm_id);
+  rdma_destroy_event_channel(cm_channel);
+  return rc;
+}
+
+int RDMAIWARPServerSocketImpl::accept(ConnectedSocket *sock, const SocketOptions &opt,
+    entity_addr_t *out, Worker *w)
+{
+  ldout(cct, 15) << __func__ << dendl;
+
+  ceph_assert(sock);
+  struct pollfd pfd = {
+    .fd = cm_channel->fd,
+    .events = POLLIN,
+    .revents = 0,
+  };
+  int ret = poll(&pfd, 1, 0);
+  ceph_assert(ret >= 0);
+  if (!ret)
+    return -EAGAIN;
+
+  struct rdma_cm_event *cm_event;
+  rdma_get_cm_event(cm_channel, &cm_event);
+  ldout(cct, 20) << __func__ << " event name: " << rdma_event_str(cm_event->event) << dendl;
+
+  struct rdma_cm_id *event_cm_id = cm_event->id;
+  struct rdma_event_channel *event_channel = rdma_create_event_channel();
+
+  if (net.set_nonblock(event_channel->fd) < 0) {
+      lderr(cct) << __func__ << " failed to switch event channel to non-block, close event channel " << dendl;
+      rdma_destroy_event_channel(event_channel);
+      rdma_ack_cm_event(cm_event);
+      return -errno;
+  }
+
+  rdma_migrate_id(event_cm_id, event_channel);
+
+  struct rdma_conn_param *remote_conn_param = &cm_event->param.conn;
+  struct rdma_conn_param local_conn_param;
+
+  RDMACMInfo info(event_cm_id, event_channel, remote_conn_param->qp_num);
+  RDMAIWARPConnectedSocketImpl* server =
+    new RDMAIWARPConnectedSocketImpl(cct, ib, dispatcher, dynamic_cast<RDMAWorker*>(w), &info);
+
+  // FIPS zeroization audit 20191115: this memset is not security related.
+  memset(&local_conn_param, 0, sizeof(local_conn_param));
+  local_conn_param.qp_num = server->get_local_qpn();
+
+  if (rdma_accept(event_cm_id, &local_conn_param)) {
+    return -EAGAIN;
+  }
+  server->activate();
+  ldout(cct, 20) << __func__ << " accepted a new QP" << dendl;
+
+  rdma_ack_cm_event(cm_event);
+
+  std::unique_ptr<RDMAConnectedSocketImpl> csi(server);
+  *sock = ConnectedSocket(std::move(csi));
+  struct sockaddr *addr = &event_cm_id->route.addr.dst_addr;
+  out->set_sockaddr(addr);
+
+  return 0;
+}
+
+void RDMAIWARPServerSocketImpl::abort_accept()
+{
+  if (server_setup_socket >= 0) {
+    rdma_destroy_id(cm_id);
+    rdma_destroy_event_channel(cm_channel);
+  }
+}
diff --git a/src/msg/async/rdma/RDMAServerSocketImpl.cc b/src/msg/async/rdma/RDMAServerSocketImpl.cc
index 6e473d12ea7..cc85832eddd 100644
--- a/src/msg/async/rdma/RDMAServerSocketImpl.cc
+++ b/src/msg/async/rdma/RDMAServerSocketImpl.cc
@@ -17,12 +17,20 @@
 #include "msg/async/net_handler.h"
 #include "RDMAStack.h"
 
+#include "include/compat.h"
+#include "include/sock_compat.h"
+
 #define dout_subsys ceph_subsys_ms
 #undef dout_prefix
 #define dout_prefix *_dout << " RDMAServerSocketImpl "
 
-RDMAServerSocketImpl::RDMAServerSocketImpl(CephContext *cct, Infiniband* i, RDMADispatcher *s, RDMAWorker *w, entity_addr_t& a)
-  : cct(cct), net(cct), server_setup_socket(-1), infiniband(i), dispatcher(s), worker(w), sa(a)
+RDMAServerSocketImpl::RDMAServerSocketImpl(
+  CephContext *cct, shared_ptr<Infiniband>& ib,
+  shared_ptr<RDMADispatcher>& rdma_dispatcher,
+  RDMAWorker *w, entity_addr_t& a, unsigned slot)
+  : ServerSocketImpl(a.get_type(), slot),
+    cct(cct), net(cct), server_setup_socket(-1), ib(ib),
+    dispatcher(rdma_dispatcher), worker(w), sa(a)
 {
 }
 
@@ -46,7 +54,6 @@ int RDMAServerSocketImpl::listen(entity_addr_t &sa, const SocketOptions &opt)
   if (rc < 0) {
     goto err;
   }
-  net.set_close_on_exec(server_setup_socket);
 
   rc = ::bind(server_setup_socket, sa.get_sockaddr(), sa.get_sockaddr_len());
   if (rc < 0) {
@@ -76,15 +83,15 @@ int RDMAServerSocketImpl::accept(ConnectedSocket *sock, const SocketOptions &opt
 {
   ldout(cct, 15) << __func__ << dendl;
 
-  assert(sock);
+  ceph_assert(sock);
+
   sockaddr_storage ss;
   socklen_t slen = sizeof(ss);
-  int sd = ::accept(server_setup_socket, (sockaddr*)&ss, &slen);
+  int sd = accept_cloexec(server_setup_socket, (sockaddr*)&ss, &slen);
   if (sd < 0) {
     return -errno;
   }
 
-  net.set_close_on_exec(sd);
   int r = net.set_nonblock(sd);
   if (r < 0) {
     ::close(sd);
@@ -97,14 +104,15 @@ int RDMAServerSocketImpl::accept(ConnectedSocket *sock, const SocketOptions &opt
     return -errno;
   }
 
-  assert(NULL != out); //out should not be NULL in accept connection
+  ceph_assert(NULL != out); //out should not be NULL in accept connection
 
+  out->set_type(addr_type);
   out->set_sockaddr((sockaddr*)&ss);
   net.set_priority(sd, opt.priority, out->get_family());
 
   RDMAConnectedSocketImpl* server;
   //Worker* w = dispatcher->get_stack()->get_worker();
-  server = new RDMAConnectedSocketImpl(cct, infiniband, dispatcher, dynamic_cast<RDMAWorker*>(w));
+  server = new RDMAConnectedSocketImpl(cct, ib, dispatcher, dynamic_cast<RDMAWorker*>(w));
   server->set_accept_fd(sd);
   ldout(cct, 20) << __func__ << " accepted a new QP, tcp_fd: " << sd << dendl;
   std::unique_ptr<RDMAConnectedSocketImpl> csi(server);
diff --git a/src/msg/async/rdma/RDMAStack.cc b/src/msg/async/rdma/RDMAStack.cc
index e16052f1dec..b68aeb1a8ef 100644
--- a/src/msg/async/rdma/RDMAStack.cc
+++ b/src/msg/async/rdma/RDMAStack.cc
@@ -15,10 +15,13 @@
  */
 
 #include <poll.h>
+#include <errno.h>
 #include <sys/time.h>
 #include <sys/resource.h>
 
 #include "include/str_list.h"
+#include "include/compat.h"
+#include "common/Cycles.h"
 #include "common/deleter.h"
 #include "common/Tub.h"
 #include "RDMAStack.h"
@@ -27,39 +30,25 @@
 #undef dout_prefix
 #define dout_prefix *_dout << "RDMAStack "
 
-static Tub<Infiniband> global_infiniband;
-
 RDMADispatcher::~RDMADispatcher()
 {
-  done = true;
-  polling_stop();
   ldout(cct, 20) << __func__ << " destructing rdma dispatcher" << dendl;
+  polling_stop();
 
-  assert(qp_conns.empty());
-  assert(num_qp_conn == 0);
-  assert(dead_queue_pairs.empty());
-  assert(num_dead_queue_pair == 0);
-
-  tx_cc->ack_events();
-  rx_cc->ack_events();
-  delete tx_cq;
-  delete rx_cq;
-  delete tx_cc;
-  delete rx_cc;
-  delete async_handler;
-
-  global_infiniband->set_dispatcher(nullptr);
+  ceph_assert(qp_conns.empty());
+  ceph_assert(num_qp_conn == 0);
+  ceph_assert(dead_queue_pairs.empty());
 }
 
-RDMADispatcher::RDMADispatcher(CephContext* c, RDMAStack* s)
-  : cct(c), async_handler(new C_handle_cq_async(this)), lock("RDMADispatcher::lock"),
-  w_lock("RDMADispatcher::for worker pending list"), stack(s)
+RDMADispatcher::RDMADispatcher(CephContext* c, shared_ptr<Infiniband>& ib)
+  : cct(c), ib(ib)
 {
   PerfCountersBuilder plb(cct, "AsyncMessenger::RDMADispatcher", l_msgr_rdma_dispatcher_first, l_msgr_rdma_dispatcher_last);
 
   plb.add_u64_counter(l_msgr_rdma_polling, "polling", "Whether dispatcher thread is polling");
   plb.add_u64_counter(l_msgr_rdma_inflight_tx_chunks, "inflight_tx_chunks", "The number of inflight tx chunks");
-  plb.add_u64_counter(l_msgr_rdma_inqueue_rx_chunks, "inqueue_rx_chunks", "The number of inqueue rx chunks");
+  plb.add_u64_counter(l_msgr_rdma_rx_bufs_in_use, "rx_bufs_in_use", "The number of rx buffers that are holding data and being processed");
+  plb.add_u64_counter(l_msgr_rdma_rx_bufs_total, "rx_bufs_total", "The total number of rx buffers");
 
   plb.add_u64_counter(l_msgr_rdma_tx_total_wc, "tx_total_wc", "The number of tx work comletions");
   plb.add_u64_counter(l_msgr_rdma_tx_total_wc_errors, "tx_total_wc_errors", "The number of tx errors");
@@ -81,26 +70,50 @@ RDMADispatcher::RDMADispatcher(CephContext* c, RDMAStack* s)
 
   perf_logger = plb.create_perf_counters();
   cct->get_perfcounters_collection()->add(perf_logger);
+  Cycles::init();
 }
 
 void RDMADispatcher::polling_start()
 {
-  tx_cc = global_infiniband->create_comp_channel(cct);
-  assert(tx_cc);
-  rx_cc = global_infiniband->create_comp_channel(cct);
-  assert(rx_cc);
-  tx_cq = global_infiniband->create_comp_queue(cct, tx_cc);
-  assert(tx_cq);
-  rx_cq = global_infiniband->create_comp_queue(cct, rx_cc);
-  assert(rx_cq);
+  // take lock because listen/connect can happen from different worker threads
+  std::lock_guard l{lock};
+
+  if (t.joinable()) 
+    return; // dispatcher thread already running 
+
+  ib->get_memory_manager()->set_rx_stat_logger(perf_logger);
+
+  tx_cc = ib->create_comp_channel(cct);
+  ceph_assert(tx_cc);
+  rx_cc = ib->create_comp_channel(cct);
+  ceph_assert(rx_cc);
+  tx_cq = ib->create_comp_queue(cct, tx_cc);
+  ceph_assert(tx_cq);
+  rx_cq = ib->create_comp_queue(cct, rx_cc);
+  ceph_assert(rx_cq);
 
   t = std::thread(&RDMADispatcher::polling, this);
+  ceph_pthread_setname(t.native_handle(), "rdma-polling");
 }
 
 void RDMADispatcher::polling_stop()
 {
-  if (t.joinable())
-    t.join();
+  {
+    std::lock_guard l{lock};
+    done = true;
+  }
+
+  if (!t.joinable())
+    return;
+
+  t.join();
+
+  tx_cc->ack_events();
+  rx_cc->ack_events();
+  delete tx_cq;
+  delete rx_cq;
+  delete tx_cc;
+  delete rx_cc;
 }
 
 void RDMADispatcher::handle_async_event()
@@ -108,38 +121,146 @@ void RDMADispatcher::handle_async_event()
   ldout(cct, 30) << __func__ << dendl;
   while (1) {
     ibv_async_event async_event;
-    if (ibv_get_async_event(global_infiniband->get_device()->ctxt, &async_event)) {
+    if (ibv_get_async_event(ib->get_device()->ctxt, &async_event)) {
       if (errno != EAGAIN)
        lderr(cct) << __func__ << " ibv_get_async_event failed. (errno=" << errno
                   << " " << cpp_strerror(errno) << ")" << dendl;
       return;
     }
     perf_logger->inc(l_msgr_rdma_total_async_events);
-    // FIXME: Currently we must ensure no other factor make QP in ERROR state,
-    // otherwise this qp can't be deleted in current cleanup flow.
-    if (async_event.event_type == IBV_EVENT_QP_LAST_WQE_REACHED) {
-      perf_logger->inc(l_msgr_rdma_async_last_wqe_events);
-      uint64_t qpn = async_event.element.qp->qp_num;
-      ldout(cct, 10) << __func__ << " event associated qp=" << async_event.element.qp
-                     << " evt: " << ibv_event_type_str(async_event.event_type) << dendl;
-      Mutex::Locker l(lock);
-      RDMAConnectedSocketImpl *conn = get_conn_lockless(qpn);
-      if (!conn) {
-        ldout(cct, 1) << __func__ << " missing qp_num=" << qpn << " discard event" << dendl;
-      } else {
-        ldout(cct, 1) << __func__ << " it's not forwardly stopped by us, reenable=" << conn << dendl;
-        conn->fault();
-        erase_qpn_lockless(qpn);
-      }
-    } else {
-      ldout(cct, 1) << __func__ << " ibv_get_async_event: dev=" << global_infiniband->get_device()->ctxt
-                    << " evt: " << ibv_event_type_str(async_event.event_type)
-                    << dendl;
+    ldout(cct, 1) << __func__ << "Event : " << ibv_event_type_str(async_event.event_type) << dendl;
+
+    switch (async_event.event_type) {
+      /***********************CQ events********************/
+      case IBV_EVENT_CQ_ERR:
+        lderr(cct) << __func__ << " Fatal Error, effect all QP bound with same CQ, "
+                   << " CQ Overflow, dev = " << ib->get_device()->ctxt
+                   << " Need destroy and recreate resource " << dendl;
+        break;
+      /***********************QP events********************/
+      case IBV_EVENT_QP_FATAL:
+        {
+          /* Error occurred on a QP and it transitioned to error state */
+          ibv_qp* ib_qp = async_event.element.qp;
+          uint32_t qpn = ib_qp->qp_num;
+          QueuePair* qp = get_qp(qpn);
+          lderr(cct) << __func__ << " Fatal Error, event associate qp number: " << qpn
+                     << " Queue Pair status: " << Infiniband::qp_state_string(qp->get_state())
+                     << " Event : " << ibv_event_type_str(async_event.event_type) << dendl;
+        }
+        break;
+      case IBV_EVENT_QP_LAST_WQE_REACHED:
+        {
+          /*
+           * 1. The QP bound with SRQ is in IBV_QPS_ERR state & no more WQE on the RQ of the QP
+           *    Reason: QP is force switched into Error before posting Beacon WR.
+           *            The QP's WRs will be flushed into CQ with IBV_WC_WR_FLUSH_ERR status
+           *            For SRQ, only WRs on the QP which is switched into Error status will be flushed.
+           *    Handle: Only confirm that qp enter into dead queue pairs
+           * 2. The CQE with error was generated for the last WQE
+           *    Handle: output error log
+           */
+          perf_logger->inc(l_msgr_rdma_async_last_wqe_events);
+          ibv_qp* ib_qp = async_event.element.qp;
+          uint32_t qpn = ib_qp->qp_num;
+          std::lock_guard l{lock};
+          RDMAConnectedSocketImpl *conn = get_conn_lockless(qpn);
+          QueuePair* qp = get_qp_lockless(qpn);
+
+          if (qp && !qp->is_dead()) {
+            lderr(cct) << __func__ << " QP not dead, event associate qp number: " << qpn
+                       << " Queue Pair status: " << Infiniband::qp_state_string(qp->get_state())
+                       << " Event : " << ibv_event_type_str(async_event.event_type) << dendl;
+          }
+          if (!conn) {
+            ldout(cct, 20) << __func__ << " Connection's QP maybe entered into dead status. "
+                           << " qp number: " << qpn << dendl;
+          } else {
+             conn->fault();
+             if (qp) {
+                if (!cct->_conf->ms_async_rdma_cm)
+                enqueue_dead_qp(qpn);
+             }
+          }
+        }
+        break;
+      case IBV_EVENT_QP_REQ_ERR:
+        /* Invalid Request Local Work Queue Error */
+        [[fallthrough]];
+      case IBV_EVENT_QP_ACCESS_ERR:
+        /* Local access violation error */
+        [[fallthrough]];
+      case IBV_EVENT_COMM_EST:
+        /* Communication was established on a QP */
+        [[fallthrough]];
+      case IBV_EVENT_SQ_DRAINED:
+        /* Send Queue was drained of outstanding messages in progress */
+        [[fallthrough]];
+      case IBV_EVENT_PATH_MIG:
+        /* A connection has migrated to the alternate path */
+        [[fallthrough]];
+      case IBV_EVENT_PATH_MIG_ERR:
+        /* A connection failed to migrate to the alternate path */
+        break;
+      /***********************SRQ events*******************/
+      case IBV_EVENT_SRQ_ERR:
+        /* Error occurred on an SRQ */
+        [[fallthrough]];
+      case IBV_EVENT_SRQ_LIMIT_REACHED:
+        /* SRQ limit was reached */
+        break;
+      /***********************Port events******************/
+      case IBV_EVENT_PORT_ACTIVE:
+        /* Link became active on a port */
+        [[fallthrough]];
+      case IBV_EVENT_PORT_ERR:
+        /* Link became unavailable on a port */
+        [[fallthrough]];
+      case IBV_EVENT_LID_CHANGE:
+        /* LID was changed on a port */
+        [[fallthrough]];
+      case IBV_EVENT_PKEY_CHANGE:
+        /* P_Key table was changed on a port */
+        [[fallthrough]];
+      case IBV_EVENT_SM_CHANGE:
+        /* SM was changed on a port */
+        [[fallthrough]];
+      case IBV_EVENT_CLIENT_REREGISTER:
+        /* SM sent a CLIENT_REREGISTER request to a port */
+        [[fallthrough]];
+      case IBV_EVENT_GID_CHANGE:
+        /* GID table was changed on a port */
+        break;
+
+      /***********************CA events******************/
+      //CA events:
+      case IBV_EVENT_DEVICE_FATAL:
+        /* CA is in FATAL state */
+        lderr(cct) << __func__ << " ibv_get_async_event: dev = " << ib->get_device()->ctxt
+                   << " evt: " << ibv_event_type_str(async_event.event_type) << dendl;
+        break;
+      default:
+        lderr(cct) << __func__ << " ibv_get_async_event: dev = " << ib->get_device()->ctxt
+                   << " unknown event: " << async_event.event_type << dendl;
+        break;
     }
     ibv_ack_async_event(&async_event);
   }
 }
 
+void RDMADispatcher::post_chunk_to_pool(Chunk* chunk)
+{
+  std::lock_guard l{lock};
+  ib->post_chunk_to_pool(chunk);
+  perf_logger->dec(l_msgr_rdma_rx_bufs_in_use);
+}
+
+int RDMADispatcher::post_chunks_to_rq(int num, QueuePair *qp)
+{
+  std::lock_guard l{lock};
+  return ib->post_chunks_to_rq(num, qp);
+}
+
 void RDMADispatcher::polling()
 {
   static int MAX_COMPLETIONS = 32;
@@ -148,8 +269,7 @@ void RDMADispatcher::polling()
   std::map<RDMAConnectedSocketImpl*, std::vector<ibv_wc> > polled;
   std::vector<ibv_wc> tx_cqe;
   ldout(cct, 20) << __func__ << " going to poll tx cq: " << tx_cq << " rx cq: " << rx_cq << dendl;
-  RDMAConnectedSocketImpl *conn = nullptr;
-  utime_t last_inactive = ceph_clock_now();
+  uint64_t last_inactive = Cycles::rdtsc();
   bool rearmed = false;
   int r = 0;
 
@@ -163,74 +283,42 @@ void RDMADispatcher::polling()
 
     int rx_ret = rx_cq->poll_cq(MAX_COMPLETIONS, wc);
     if (rx_ret > 0) {
-      ldout(cct, 20) << __func__ << " rt completion queue got " << rx_ret
+      ldout(cct, 20) << __func__ << " rx completion queue got " << rx_ret
                      << " responses."<< dendl;
-      perf_logger->inc(l_msgr_rdma_rx_total_wc, rx_ret);
-
-      Mutex::Locker l(lock);//make sure connected socket alive when pass wc
-      for (int i = 0; i < rx_ret; ++i) {
-        ibv_wc* response = &wc[i];
-        Chunk* chunk = reinterpret_cast<Chunk *>(response->wr_id);
-        ldout(cct, 25) << __func__ << " got chunk=" << chunk << " bytes:" << response->byte_len << " opcode:" << response->opcode << dendl;
-
-        assert(wc[i].opcode == IBV_WC_RECV);
-
-        if (response->status == IBV_WC_SUCCESS) {
-          conn = get_conn_lockless(response->qp_num);
-          if (!conn) {
-            assert(global_infiniband->is_rx_buffer(chunk->buffer));
-            r = global_infiniband->post_chunk(chunk);
-            ldout(cct, 1) << __func__ << " csi with qpn " << response->qp_num << " may be dead. chunk " << chunk << " will be back ? " << r << dendl;
-            assert(r == 0);
-          } else {
-            polled[conn].push_back(*response);
-          }
-        } else {
-          perf_logger->inc(l_msgr_rdma_rx_total_wc_errors);
-          ldout(cct, 1) << __func__ << " work request returned error for buffer(" << chunk
-              << ") status(" << response->status << ":"
-              << global_infiniband->wc_status_to_string(response->status) << ")" << dendl;
-          assert(global_infiniband->is_rx_buffer(chunk->buffer));
-          r = global_infiniband->post_chunk(chunk);
-          if (r) {
-            ldout(cct, 0) << __func__ << " post chunk failed, error: " << cpp_strerror(r) << dendl;
-            assert(r == 0);
-          }
-
-          conn = get_conn_lockless(response->qp_num);
-          if (conn && conn->is_connected())
-            conn->fault();
-        }
-      }
-
-      for (auto &&i : polled) {
-        perf_logger->inc(l_msgr_rdma_inqueue_rx_chunks, i.second.size());
-        i.first->pass_wc(std::move(i.second));
-      }
-      polled.clear();
+      handle_rx_event(wc, rx_ret);
     }
 
     if (!tx_ret && !rx_ret) {
-      // NOTE: Has TX just transitioned to idle? We should do it when idle!
-      // It's now safe to delete queue pairs (see comment by declaration
-      // for dead_queue_pairs).
-      // Additionally, don't delete qp while outstanding_buffers isn't empty,
-      // because we need to check qp's state before sending
       perf_logger->set(l_msgr_rdma_inflight_tx_chunks, inflight);
-      if (num_dead_queue_pair) {
-        Mutex::Locker l(lock); // FIXME reuse dead qp because creating one qp costs 1 ms
-        while (!dead_queue_pairs.empty()) {
-          ldout(cct, 10) << __func__ << " finally delete qp=" << dead_queue_pairs.back() << dendl;
-          delete dead_queue_pairs.back();
+      //
+      // Clean up dead QPs when rx/tx CQs are in idle. The thing is that
+      // we can destroy QPs even earlier, just when beacon has been received,
+      // but we have two CQs (rx & tx), thus beacon WC can be poped from tx
+      // CQ before other WCs are fully consumed from rx CQ. For safety, we
+      // wait for beacon and then "no-events" from CQs.
+      //
+      // Calling size() on vector without locks is totally fine, since we
+      // use it as a hint (accuracy is not important here)
+      //
+      if (!dead_queue_pairs.empty()) {
+        decltype(dead_queue_pairs) dead_qps;
+        {
+          std::lock_guard l{lock};
+          dead_queue_pairs.swap(dead_qps);
+        }
+
+        for (auto& qp: dead_qps) {
           perf_logger->dec(l_msgr_rdma_active_queue_pair);
-          dead_queue_pairs.pop_back();
-          --num_dead_queue_pair;
+          ldout(cct, 10) << __func__ << " finally delete qp = " << qp << dendl;
+          delete qp;
         }
       }
-      if (!num_qp_conn && done)
+
+      if (!num_qp_conn && done && dead_queue_pairs.empty())
         break;
 
-      if ((ceph_clock_now() - last_inactive).to_nsec() / 1000 > cct->_conf->ms_async_rdma_polling_us) {
+      uint64_t now = Cycles::rdtsc();
+      if (Cycles::to_microseconds(now - last_inactive) > cct->_conf->ms_async_rdma_polling_us) {
         handle_async_event();
         if (!rearmed) {
           // Clean up cq events after rearm notify ensure no new incoming event
@@ -243,15 +331,15 @@ void RDMADispatcher::polling()
 
         struct pollfd channel_poll[2];
         channel_poll[0].fd = tx_cc->get_fd();
-        channel_poll[0].events = POLLIN | POLLERR | POLLNVAL | POLLHUP;
+        channel_poll[0].events = POLLIN;
         channel_poll[0].revents = 0;
         channel_poll[1].fd = rx_cc->get_fd();
-        channel_poll[1].events = POLLIN | POLLERR | POLLNVAL | POLLHUP;
+        channel_poll[1].events = POLLIN;
         channel_poll[1].revents = 0;
         r = 0;
         perf_logger->set(l_msgr_rdma_polling, 0);
         while (!done && r == 0) {
-          r = poll(channel_poll, 2, 100);
+          r = TEMP_FAILURE_RETRY(poll(channel_poll, 2, 100));
           if (r < 0) {
             r = -errno;
             lderr(cct) << __func__ << " poll failed " << r << dendl;
@@ -262,7 +350,7 @@ void RDMADispatcher::polling()
           ldout(cct, 20) << __func__ << " got tx cq event." << dendl;
         if (r > 0 && rx_cc->get_cq_event())
           ldout(cct, 20) << __func__ << " got rx cq event." << dendl;
-        last_inactive = ceph_clock_now();
+        last_inactive = Cycles::rdtsc();
         perf_logger->set(l_msgr_rdma_polling, 1);
         rearmed = false;
       }
@@ -274,7 +362,7 @@ void RDMADispatcher::notify_pending_workers() {
   if (num_pending_workers) {
     RDMAWorker *w = nullptr;
     {
-      Mutex::Locker l(w_lock);
+      std::lock_guard l{w_lock};
       if (!pending_workers.empty()) {
         w = pending_workers.front();
         pending_workers.pop_front();
@@ -286,15 +374,12 @@ void RDMADispatcher::notify_pending_workers() {
   }
 }
 
-int RDMADispatcher::register_qp(QueuePair *qp, RDMAConnectedSocketImpl* csi)
+void RDMADispatcher::register_qp(QueuePair *qp, RDMAConnectedSocketImpl* csi)
 {
-  int fd = eventfd(0, EFD_CLOEXEC|EFD_NONBLOCK);
-  assert(fd >= 0);
-  Mutex::Locker l(lock);
-  assert(!qp_conns.count(qp->get_local_qp_number()));
+  std::lock_guard l{lock};
+  ceph_assert(!qp_conns.count(qp->get_local_qp_number()));
   qp_conns[qp->get_local_qp_number()] = std::make_pair(qp, csi);
   ++num_qp_conn;
-  return fd;
 }
 
 RDMAConnectedSocketImpl* RDMADispatcher::get_conn_lockless(uint32_t qp)
@@ -307,21 +392,65 @@ RDMAConnectedSocketImpl* RDMADispatcher::get_conn_lockless(uint32_t qp)
   return it->second.second;
 }
 
-void RDMADispatcher::erase_qpn_lockless(uint32_t qpn)
+Infiniband::QueuePair* RDMADispatcher::get_qp_lockless(uint32_t qp)
+{
+  // Try to find the QP in qp_conns firstly.
+  auto it = qp_conns.find(qp);
+  if (it != qp_conns.end())
+    return it->second.first;
+
+  // Try again in dead_queue_pairs.
+  for (auto &i: dead_queue_pairs)
+    if (i->get_local_qp_number() == qp)
+      return i;
+
+  return nullptr;
+}
+
+Infiniband::QueuePair* RDMADispatcher::get_qp(uint32_t qp)
+{
+  std::lock_guard l{lock};
+  return get_qp_lockless(qp);
+}
+
+void RDMADispatcher::enqueue_dead_qp(uint32_t qpn)
 {
+  std::lock_guard l{lock};
   auto it = qp_conns.find(qpn);
-  if (it == qp_conns.end())
+  if (it == qp_conns.end()) {
+    lderr(cct) << __func__ << " QP [" << qpn << "] is not registered." << dendl;
     return ;
-  ++num_dead_queue_pair;
-  dead_queue_pairs.push_back(it->second.first);
+  }
+  QueuePair *qp = it->second.first;
+  dead_queue_pairs.push_back(qp);
   qp_conns.erase(it);
   --num_qp_conn;
 }
 
-void RDMADispatcher::erase_qpn(uint32_t qpn)
+void RDMADispatcher::schedule_qp_destroy(uint32_t qpn)
 {
-  Mutex::Locker l(lock);
-  erase_qpn_lockless(qpn);
+  std::lock_guard l{lock};
+  auto it = qp_conns.find(qpn);
+  if (it == qp_conns.end()) {
+    lderr(cct) << __func__ << " QP [" << qpn << "] is not registered." << dendl;
+    return;
+  }
+  QueuePair *qp = it->second.first;
+  if (qp->to_dead()) {
+    //
+    // Failed to switch to dead. This is abnormal, but we can't
+    // do anything, so just destroy QP.
+    //
+    dead_queue_pairs.push_back(qp);
+    qp_conns.erase(it);
+    --num_qp_conn;
+  } else {
+    //
+    // Successfully switched to dead, thus keep entry in the map.
+    // But only zero out socked pointer in order to return null from
+    // get_conn_lockless();
+    it->second.second = nullptr;
+  }
 }
 
 void RDMADispatcher::handle_tx_event(ibv_wc *cqe, int n)
@@ -330,41 +459,87 @@ void RDMADispatcher::handle_tx_event(ibv_wc *cqe, int n)
 
   for (int i = 0; i < n; ++i) {
     ibv_wc* response = &cqe[i];
-    Chunk* chunk = reinterpret_cast<Chunk *>(response->wr_id);
-    ldout(cct, 25) << __func__ << " QP: " << response->qp_num
-                   << " len: " << response->byte_len << " , addr:" << chunk
-                   << " " << global_infiniband->wc_status_to_string(response->status) << dendl;
 
-    if (response->status != IBV_WC_SUCCESS) {
-      perf_logger->inc(l_msgr_rdma_tx_total_wc_errors);
-      if (response->status == IBV_WC_RETRY_EXC_ERR) {
-        ldout(cct, 1) << __func__ << " connection between server and client not working. Disconnect this now" << dendl;
-        perf_logger->inc(l_msgr_rdma_tx_wc_retry_errors);
-      } else if (response->status == IBV_WC_WR_FLUSH_ERR) {
-        ldout(cct, 1) << __func__ << " Work Request Flushed Error: this connection's qp="
-                      << response->qp_num << " should be down while this WR=" << response->wr_id
-                      << " still in flight." << dendl;
-        perf_logger->inc(l_msgr_rdma_tx_wc_wr_flush_errors);
-      } else {
-        ldout(cct, 1) << __func__ << " send work request returned error for buffer("
-                      << response->wr_id << ") status(" << response->status << "): "
-                      << global_infiniband->wc_status_to_string(response->status) << dendl;
-      }
+    // If it's beacon WR, enqueue the QP to be destroyed later
+    if (response->wr_id == BEACON_WRID) {
+      enqueue_dead_qp(response->qp_num);
+      continue;
+    }
 
-      Mutex::Locker l(lock);//make sure connected socket alive when pass wc
-      RDMAConnectedSocketImpl *conn = get_conn_lockless(response->qp_num);
+    ldout(cct, 20) << __func__ << " QP number: " << response->qp_num << " len: " << response->byte_len
+                   << " status: " << ib->wc_status_to_string(response->status) << dendl;
 
-      if (conn && conn->is_connected()) {
-        ldout(cct, 25) << __func__ << " qp state is : " << conn->get_qp_state() << dendl;//wangzhi
-        conn->fault();
-      } else {
-        ldout(cct, 1) << __func__ << " missing qp_num=" << response->qp_num << " discard event" << dendl;
+    if (response->status != IBV_WC_SUCCESS) {
+      switch(response->status) {
+        case IBV_WC_RETRY_EXC_ERR:
+          {
+            perf_logger->inc(l_msgr_rdma_tx_wc_retry_errors);
+
+            ldout(cct, 1) << __func__ << " Responder ACK timeout, possible disconnect, or Remote QP in bad state "
+                          << " WCE status(" << response->status << "): " << ib->wc_status_to_string(response->status)
+                          << " WCE QP number " << response->qp_num << " Opcode " << response->opcode
+                          << " wr_id: 0x" << std::hex << response->wr_id << std::dec << dendl;
+
+            std::lock_guard l{lock};
+            RDMAConnectedSocketImpl *conn = get_conn_lockless(response->qp_num);
+            if (conn) {
+              ldout(cct, 1) << __func__ << " SQ WR return error, remote Queue Pair, qp number: "
+                            << conn->get_peer_qpn() << dendl;
+            }
+          }
+          break;
+        case IBV_WC_WR_FLUSH_ERR:
+          {
+            perf_logger->inc(l_msgr_rdma_tx_wc_wr_flush_errors);
+
+            std::lock_guard l{lock};
+            QueuePair *qp = get_qp_lockless(response->qp_num);
+            if (qp) {
+              ldout(cct, 20) << __func__ << " qp state is " << Infiniband::qp_state_string(qp->get_state()) << dendl;
+            }
+            if (qp && qp->is_dead()) {
+              ldout(cct, 20) << __func__ << " outstanding SQ WR is flushed into CQ since QueuePair is dead " << dendl;
+            } else {
+              lderr(cct) << __func__ << " Invalid/Unsupported request to consume outstanding SQ WR,"
+                         << " WCE status(" << response->status << "): " << ib->wc_status_to_string(response->status)
+                         << " WCE QP number " << response->qp_num << " Opcode " << response->opcode
+                         << " wr_id: 0x" << std::hex << response->wr_id << std::dec << dendl;
+
+              RDMAConnectedSocketImpl *conn = get_conn_lockless(response->qp_num);
+              if (conn) {
+                ldout(cct, 1) << __func__ << " SQ WR return error, remote Queue Pair, qp number: "
+                              << conn->get_peer_qpn() << dendl;
+              }
+            }
+          }
+          break;
+
+        default:
+          {
+            lderr(cct) << __func__ << " SQ WR return error,"
+                       << " WCE status(" << response->status << "): " << ib->wc_status_to_string(response->status)
+                       << " WCE QP number " << response->qp_num << " Opcode " << response->opcode
+                       << " wr_id: 0x" << std::hex << response->wr_id << std::dec << dendl;
+
+            std::lock_guard l{lock};
+            RDMAConnectedSocketImpl *conn = get_conn_lockless(response->qp_num);
+            if (conn && conn->is_connected()) {
+              ldout(cct, 20) << __func__ << " SQ WR return error Queue Pair error state is : " << conn->get_qp_state()
+                             << " remote Queue Pair, qp number: " << conn->get_peer_qpn() << dendl;
+              conn->fault();
+            } else {
+              ldout(cct, 1) << __func__ << " Disconnected, qp_num = " << response->qp_num << " discard event" << dendl;
+            }
+          }
+          break;
       }
     }
 
-    //TX completion may come either from regular send message or from 'fin' message.
-    //In the case of 'fin' wr_id points to the QueuePair.
-    if (global_infiniband->get_memory_manager()->is_tx_buffer(chunk->buffer)) {
+    auto chunk = reinterpret_cast<Chunk *>(response->wr_id);
+    //TX completion may come either from
+    // 1) regular send message, WCE wr_id points to chunk
+    // 2) 'fin' message, wr_id points to the QP
+    if (ib->get_memory_manager()->is_tx_buffer(chunk->buffer)) {
       tx_chunks.push_back(chunk);
     } else if (reinterpret_cast<QueuePair*>(response->wr_id)->get_local_qp_number() == response->qp_num ) {
       ldout(cct, 1) << __func__ << " sending of the disconnect msg completed" << dendl;
@@ -392,16 +567,92 @@ void RDMADispatcher::post_tx_buffer(std::vector<Chunk*> &chunks)
     return ;
 
   inflight -= chunks.size();
-  global_infiniband->get_memory_manager()->return_tx(chunks);
+  ib->get_memory_manager()->return_tx(chunks);
   ldout(cct, 30) << __func__ << " release " << chunks.size()
                  << " chunks, inflight " << inflight << dendl;
   notify_pending_workers();
 }
 
+void RDMADispatcher::handle_rx_event(ibv_wc *cqe, int rx_number)
+{
+  perf_logger->inc(l_msgr_rdma_rx_total_wc, rx_number);
+  perf_logger->inc(l_msgr_rdma_rx_bufs_in_use, rx_number);
+
+  std::map<RDMAConnectedSocketImpl*, std::vector<ibv_wc> > polled;
+  std::lock_guard l{lock};//make sure connected socket alive when pass wc
+
+  for (int i = 0; i < rx_number; ++i) {
+    ibv_wc* response = &cqe[i];
+    Chunk* chunk = reinterpret_cast<Chunk *>(response->wr_id);
+    RDMAConnectedSocketImpl *conn = get_conn_lockless(response->qp_num);
+    QueuePair *qp = get_qp_lockless(response->qp_num);
+
+    switch (response->status) {
+      case IBV_WC_SUCCESS:
+        ceph_assert(response->opcode == IBV_WC_RECV);
+        if (!conn) {
+          ldout(cct, 1) << __func__ << " csi with qpn " << response->qp_num << " may be dead. chunk 0x"
+                        << std::hex << chunk << " will be back." << std::dec << dendl;
+          ib->post_chunk_to_pool(chunk);
+          perf_logger->dec(l_msgr_rdma_rx_bufs_in_use);
+        } else {
+          conn->post_chunks_to_rq(1);
+          polled[conn].push_back(*response);
+
+          if (qp != nullptr && !qp->get_srq()) {
+            qp->remove_rq_wr(chunk);
+            chunk->clear_qp();
+          }
+        }
+        break;
+
+      case IBV_WC_WR_FLUSH_ERR:
+        perf_logger->inc(l_msgr_rdma_rx_total_wc_errors);
+
+        if (qp) {
+          ldout(cct, 20) << __func__ << " qp state is " << Infiniband::qp_state_string(qp->get_state()) << dendl;
+        }
+        if (qp && qp->is_dead()) {
+          ldout(cct, 20) << __func__ << " outstanding RQ WR is flushed into CQ since QueuePair is dead " << dendl;
+        } else {
+          ldout(cct, 1) << __func__ << " RQ WR return error,"
+                     << " WCE status(" << response->status << "): " << ib->wc_status_to_string(response->status)
+                     << " WCE QP number " << response->qp_num << " Opcode " << response->opcode
+                     << " wr_id: 0x" << std::hex << response->wr_id << std::dec << dendl;
+          if (conn) {
+            ldout(cct, 1) << __func__ << " RQ WR return error, remote Queue Pair, qp number: "
+                       << conn->get_peer_qpn() << dendl;
+          }
+        }
+
+        ib->post_chunk_to_pool(chunk);
+        perf_logger->dec(l_msgr_rdma_rx_bufs_in_use);
+        break;
+
+      default:
+        perf_logger->inc(l_msgr_rdma_rx_total_wc_errors);
+
+        ldout(cct, 1) << __func__ << " RQ WR return error,"
+                      << " WCE status(" << response->status << "): " << ib->wc_status_to_string(response->status)
+                      << " WCE QP number " << response->qp_num << " Opcode " << response->opcode
+                      << " wr_id: 0x" << std::hex << response->wr_id << std::dec << dendl;
+        if (conn && conn->is_connected())
+          conn->fault();
+
+        ib->post_chunk_to_pool(chunk);
+        perf_logger->dec(l_msgr_rdma_rx_bufs_in_use);
+        break;
+    }
+  }
+
+  for (auto &i : polled)
+    i.first->pass_wc(std::move(i.second));
+  polled.clear();
+}
 
-RDMAWorker::RDMAWorker(CephContext *c, unsigned i)
-  : Worker(c, i), stack(nullptr),
-    tx_handler(new C_handle_cq_tx(this)), lock("RDMAWorker::lock")
+RDMAWorker::RDMAWorker(CephContext *c, unsigned worker_id)
+  : Worker(c, worker_id),
+    tx_handler(new C_handle_cq_tx(this))
 {
   // initialize perf_logger
   char name[128];
@@ -411,12 +662,11 @@ RDMAWorker::RDMAWorker(CephContext *c, unsigned i)
   plb.add_u64_counter(l_msgr_rdma_tx_no_mem, "tx_no_mem", "The count of no tx buffer");
   plb.add_u64_counter(l_msgr_rdma_tx_parital_mem, "tx_parital_mem", "The count of parital tx buffer");
   plb.add_u64_counter(l_msgr_rdma_tx_failed, "tx_failed_post", "The number of tx failed posted");
-  plb.add_u64_counter(l_msgr_rdma_rx_no_registered_mem, "rx_no_registered_mem", "The count of no registered buffer when receiving");
 
   plb.add_u64_counter(l_msgr_rdma_tx_chunks, "tx_chunks", "The number of tx chunks transmitted");
-  plb.add_u64_counter(l_msgr_rdma_tx_bytes, "tx_bytes", "The bytes of tx chunks transmitted");
+  plb.add_u64_counter(l_msgr_rdma_tx_bytes, "tx_bytes", "The bytes of tx chunks transmitted", NULL, 0, unit_t(UNIT_BYTES));
   plb.add_u64_counter(l_msgr_rdma_rx_chunks, "rx_chunks", "The number of rx chunks transmitted");
-  plb.add_u64_counter(l_msgr_rdma_rx_bytes, "rx_bytes", "The bytes of rx chunks transmitted");
+  plb.add_u64_counter(l_msgr_rdma_rx_bytes, "rx_bytes", "The bytes of rx chunks transmitted", NULL, 0, unit_t(UNIT_BYTES));
   plb.add_u64_counter(l_msgr_rdma_pending_sent_conns, "pending_sent_conns", "The count of pending sent conns");
 
   perf_logger = plb.create_perf_counters();
@@ -430,16 +680,21 @@ RDMAWorker::~RDMAWorker()
 
 void RDMAWorker::initialize()
 {
-  if (!dispatcher) {
-    dispatcher = stack->get_dispatcher();
-  }
+  ceph_assert(dispatcher);
 }
 
-int RDMAWorker::listen(entity_addr_t &sa, const SocketOptions &opt,ServerSocket *sock)
+int RDMAWorker::listen(entity_addr_t &sa, unsigned addr_slot,
+		       const SocketOptions &opt,ServerSocket *sock)
 {
-  global_infiniband->init();
-
-  auto p = new RDMAServerSocketImpl(cct, global_infiniband.get(), get_stack()->get_dispatcher(), this, sa);
+  ib->init();
+  dispatcher->polling_start();
+
+  RDMAServerSocketImpl *p;
+  if (cct->_conf->ms_async_rdma_type == "iwarp") {
+    p = new RDMAIWARPServerSocketImpl(cct, ib, dispatcher, this, sa, addr_slot);
+  } else {
+    p = new RDMAServerSocketImpl(cct, ib, dispatcher, this, sa, addr_slot);
+  }
   int r = p->listen(sa, opt);
   if (r < 0) {
     delete p;
@@ -452,9 +707,15 @@ int RDMAWorker::listen(entity_addr_t &sa, const SocketOptions &opt,ServerSocket
 
 int RDMAWorker::connect(const entity_addr_t &addr, const SocketOptions &opts, ConnectedSocket *socket)
 {
-  global_infiniband->init();
-
-  RDMAConnectedSocketImpl* p = new RDMAConnectedSocketImpl(cct, global_infiniband.get(), get_stack()->get_dispatcher(), this);
+  ib->init();
+  dispatcher->polling_start();
+
+  RDMAConnectedSocketImpl* p;
+  if (cct->_conf->ms_async_rdma_type == "iwarp") {
+    p = new RDMAIWARPConnectedSocketImpl(cct, ib, dispatcher, this);
+  } else {
+    p = new RDMAConnectedSocketImpl(cct, ib, dispatcher, this);
+  }
   int r = p->try_connect(addr, opts);
 
   if (r < 0) {
@@ -469,12 +730,11 @@ int RDMAWorker::connect(const entity_addr_t &addr, const SocketOptions &opts, Co
 
 int RDMAWorker::get_reged_mem(RDMAConnectedSocketImpl *o, std::vector<Chunk*> &c, size_t bytes)
 {
-  assert(center.in_thread());
-  int r = global_infiniband->get_tx_buffers(c, bytes);
-  assert(r >= 0);
-  size_t got = global_infiniband->get_memory_manager()->get_tx_buffer_size() * r;
+  ceph_assert(center.in_thread());
+  int r = ib->get_tx_buffers(c, bytes);
+  size_t got = ib->get_memory_manager()->get_tx_buffer_size() * r;
   ldout(cct, 30) << __func__ << " need " << bytes << " bytes, reserve " << got << " registered  bytes, inflight " << dispatcher->inflight << dendl;
-  stack->get_dispatcher()->inflight += r;
+  dispatcher->inflight += r;
   if (got >= bytes)
     return r;
 
@@ -512,50 +772,19 @@ void RDMAWorker::handle_pending_message()
   dispatcher->notify_pending_workers();
 }
 
-RDMAStack::RDMAStack(CephContext *cct, const string &t): NetworkStack(cct, t)
+RDMAStack::RDMAStack(CephContext *cct, const string &t)
+  : NetworkStack(cct, t), ib(make_shared<Infiniband>(cct)),
+    rdma_dispatcher(make_shared<RDMADispatcher>(cct, ib))
 {
-  //
-  //On RDMA MUST be called before fork
-  //
-
-  int rc = ibv_fork_init();
-  if (rc) {
-     lderr(cct) << __func__ << " failed to call ibv_for_init(). On RDMA must be called before fork. Application aborts." << dendl;
-     ceph_abort();
-  }
-
-  ldout(cct, 1) << __func__ << " ms_async_rdma_enable_hugepage value is: " << cct->_conf->ms_async_rdma_enable_hugepage <<  dendl;
-  if (cct->_conf->ms_async_rdma_enable_hugepage) {
-    rc =  setenv("RDMAV_HUGEPAGES_SAFE","1",1);
-    ldout(cct, 1) << __func__ << " RDMAV_HUGEPAGES_SAFE is set as: " << getenv("RDMAV_HUGEPAGES_SAFE") <<  dendl;
-    if (rc) {
-      lderr(cct) << __func__ << " failed to export RDMA_HUGEPAGES_SAFE. On RDMA must be exported before using huge pages. Application aborts." << dendl;
-      ceph_abort();
-    }
-  }
-
-  //Check ulimit
-  struct rlimit limit;
-  getrlimit(RLIMIT_MEMLOCK, &limit);
-  if (limit.rlim_cur != RLIM_INFINITY || limit.rlim_max != RLIM_INFINITY) {
-     lderr(cct) << __func__ << "!!! WARNING !!! For RDMA to work properly user memlock (ulimit -l) must be big enough to allow large amount of registered memory."
-				  " We recommend setting this parameter to infinity" << dendl;
-  }
-
-  if (!global_infiniband)
-    global_infiniband.construct(
-      cct, cct->_conf->ms_async_rdma_device_name, cct->_conf->ms_async_rdma_port_num);
   ldout(cct, 20) << __func__ << " constructing RDMAStack..." << dendl;
-  dispatcher = new RDMADispatcher(cct, this);
-  global_infiniband->set_dispatcher(dispatcher);
 
   unsigned num = get_num_worker();
   for (unsigned i = 0; i < num; ++i) {
     RDMAWorker* w = dynamic_cast<RDMAWorker*>(get_worker(i));
-    w->set_stack(this);
+    w->set_dispatcher(rdma_dispatcher);
+    w->set_ib(ib);
   }
-
-  ldout(cct, 20) << " creating RDMAStack:" << this << " with dispatcher:" << dispatcher << dendl;
+  ldout(cct, 20) << " creating RDMAStack:" << this << " with dispatcher:" << rdma_dispatcher.get() << dendl;
 }
 
 RDMAStack::~RDMAStack()
@@ -563,8 +792,6 @@ RDMAStack::~RDMAStack()
   if (cct->_conf->ms_async_rdma_enable_hugepage) {
     unsetenv("RDMAV_HUGEPAGES_SAFE");	//remove env variable on destruction
   }
-
-  delete dispatcher;
 }
 
 void RDMAStack::spawn_worker(unsigned i, std::function<void ()> &&func)
@@ -575,6 +802,6 @@ void RDMAStack::spawn_worker(unsigned i, std::function<void ()> &&func)
 
 void RDMAStack::join_worker(unsigned i)
 {
-  assert(threads.size() > i && threads[i].joinable());
+  ceph_assert(threads.size() > i && threads[i].joinable());
   threads[i].join();
 }
diff --git a/src/msg/async/rdma/RDMAStack.h b/src/msg/async/rdma/RDMAStack.h
index 3c2d90094d1..45a043d2e23 100644
--- a/src/msg/async/rdma/RDMAStack.h
+++ b/src/msg/async/rdma/RDMAStack.h
@@ -34,48 +34,20 @@ class RDMAServerSocketImpl;
 class RDMAStack;
 class RDMAWorker;
 
-enum {
-  l_msgr_rdma_dispatcher_first = 94000,
-
-  l_msgr_rdma_polling,
-  l_msgr_rdma_inflight_tx_chunks,
-  l_msgr_rdma_inqueue_rx_chunks,
-
-  l_msgr_rdma_tx_total_wc,
-  l_msgr_rdma_tx_total_wc_errors,
-  l_msgr_rdma_tx_wc_retry_errors,
-  l_msgr_rdma_tx_wc_wr_flush_errors,
-
-  l_msgr_rdma_rx_total_wc,
-  l_msgr_rdma_rx_total_wc_errors,
-  l_msgr_rdma_rx_fin,
-
-  l_msgr_rdma_handshake_errors,
-
-  l_msgr_rdma_total_async_events,
-  l_msgr_rdma_async_last_wqe_events,
-
-  l_msgr_rdma_created_queue_pair,
-  l_msgr_rdma_active_queue_pair,
-
-  l_msgr_rdma_dispatcher_last,
-};
-
-
 class RDMADispatcher {
   typedef Infiniband::MemoryManager::Chunk Chunk;
   typedef Infiniband::QueuePair QueuePair;
 
   std::thread t;
   CephContext *cct;
-  Infiniband::CompletionQueue* tx_cq;
-  Infiniband::CompletionQueue* rx_cq;
-  Infiniband::CompletionChannel *tx_cc, *rx_cc;
-  EventCallbackRef async_handler;
+  shared_ptr<Infiniband> ib;
+  Infiniband::CompletionQueue* tx_cq = nullptr;
+  Infiniband::CompletionQueue* rx_cq = nullptr;
+  Infiniband::CompletionChannel *tx_cc = nullptr, *rx_cc = nullptr;
   bool done = false;
-  std::atomic<uint64_t> num_dead_queue_pair = {0};
   std::atomic<uint64_t> num_qp_conn = {0};
-  Mutex lock; // protect `qp_conns`, `dead_queue_pairs`
+  // protect `qp_conns`, `dead_queue_pairs`
+  ceph::mutex lock = ceph::make_mutex("RDMADispatcher::lock");
   // qp_num -> InfRcConnection
   // The main usage of `qp_conns` is looking up connection by qp_num,
   // so the lifecycle of element in `qp_conns` is the lifecycle of qp.
@@ -83,9 +55,9 @@ class RDMADispatcher {
   /**
    * 1. Connection call mark_down
    * 2. Move the Queue Pair into the Error state(QueuePair::to_dead)
-   * 3. Wait for the affiliated event IBV_EVENT_QP_LAST_WQE_REACHED(handle_async_event)
-   * 4. Wait for CQ to be empty(handle_tx_event)
-   * 5. Destroy the QP by calling ibv_destroy_qp()(handle_tx_event)
+   * 3. Post a beacon
+   * 4. Wait for beacon which indicates queues are drained
+   * 5. Destroy the QP by calling ibv_destroy_qp()
    *
    * @param qp The qp needed to dead
    */
@@ -99,69 +71,47 @@ class RDMADispatcher {
   std::vector<QueuePair*> dead_queue_pairs;
 
   std::atomic<uint64_t> num_pending_workers = {0};
-  Mutex w_lock; // protect pending workers
+  // protect pending workers
+  ceph::mutex w_lock =
+    ceph::make_mutex("RDMADispatcher::for worker pending list");
   // fixme: lockfree
   std::list<RDMAWorker*> pending_workers;
-  RDMAStack* stack;
-
-  class C_handle_cq_async : public EventCallback {
-    RDMADispatcher *dispatcher;
-   public:
-    C_handle_cq_async(RDMADispatcher *w): dispatcher(w) {}
-    void do_request(int fd) {
-      // worker->handle_tx_event();
-      dispatcher->handle_async_event();
-    }
-  };
+  void enqueue_dead_qp(uint32_t qp);
 
  public:
   PerfCounters *perf_logger;
 
-  explicit RDMADispatcher(CephContext* c, RDMAStack* s);
+  explicit RDMADispatcher(CephContext* c, shared_ptr<Infiniband>& ib);
   virtual ~RDMADispatcher();
   void handle_async_event();
 
   void polling_start();
   void polling_stop();
   void polling();
-  int register_qp(QueuePair *qp, RDMAConnectedSocketImpl* csi);
+  void register_qp(QueuePair *qp, RDMAConnectedSocketImpl* csi);
   void make_pending_worker(RDMAWorker* w) {
-    Mutex::Locker l(w_lock);
+    std::lock_guard l{w_lock};
     auto it = std::find(pending_workers.begin(), pending_workers.end(), w);
     if (it != pending_workers.end())
       return;
     pending_workers.push_back(w);
     ++num_pending_workers;
   }
-  RDMAStack* get_stack() { return stack; }
   RDMAConnectedSocketImpl* get_conn_lockless(uint32_t qp);
-  void erase_qpn_lockless(uint32_t qpn);
-  void erase_qpn(uint32_t qpn);
+  QueuePair* get_qp_lockless(uint32_t qp);
+  QueuePair* get_qp(uint32_t qp);
+  void schedule_qp_destroy(uint32_t qp);
   Infiniband::CompletionQueue* get_tx_cq() const { return tx_cq; }
   Infiniband::CompletionQueue* get_rx_cq() const { return rx_cq; }
   void notify_pending_workers();
   void handle_tx_event(ibv_wc *cqe, int n);
   void post_tx_buffer(std::vector<Chunk*> &chunks);
+  void handle_rx_event(ibv_wc *cqe, int rx_number);
 
   std::atomic<uint64_t> inflight = {0};
-};
-
-
-enum {
-  l_msgr_rdma_first = 95000,
-
-  l_msgr_rdma_tx_no_mem,
-  l_msgr_rdma_tx_parital_mem,
-  l_msgr_rdma_tx_failed,
-  l_msgr_rdma_rx_no_registered_mem,
 
-  l_msgr_rdma_tx_chunks,
-  l_msgr_rdma_tx_bytes,
-  l_msgr_rdma_rx_chunks,
-  l_msgr_rdma_rx_bytes,
-  l_msgr_rdma_pending_sent_conns,
-
-  l_msgr_rdma_last,
+  void post_chunk_to_pool(Chunk* chunk);
+  int post_chunks_to_rq(int num, QueuePair *qp = nullptr);
 };
 
 class RDMAWorker : public Worker {
@@ -170,17 +120,17 @@ class RDMAWorker : public Worker {
   typedef Infiniband::MemoryManager::Chunk Chunk;
   typedef Infiniband::MemoryManager MemoryManager;
   typedef std::vector<Chunk*>::iterator ChunkIter;
-  RDMAStack *stack;
+  shared_ptr<Infiniband> ib;
   EventCallbackRef tx_handler;
   std::list<RDMAConnectedSocketImpl*> pending_sent_conns;
-  RDMADispatcher* dispatcher = nullptr;
-  Mutex lock;
+  shared_ptr<RDMADispatcher> dispatcher;
+  ceph::mutex lock = ceph::make_mutex("RDMAWorker::lock");
 
   class C_handle_cq_tx : public EventCallback {
     RDMAWorker *worker;
     public:
-    C_handle_cq_tx(RDMAWorker *w): worker(w) {}
-    void do_request(int fd) {
+    explicit C_handle_cq_tx(RDMAWorker *w): worker(w) {}
+    void do_request(uint64_t fd) {
       worker->handle_pending_message();
     }
   };
@@ -189,57 +139,73 @@ class RDMAWorker : public Worker {
   PerfCounters *perf_logger;
   explicit RDMAWorker(CephContext *c, unsigned i);
   virtual ~RDMAWorker();
-  virtual int listen(entity_addr_t &addr, const SocketOptions &opts, ServerSocket *) override;
+  virtual int listen(entity_addr_t &addr,
+		     unsigned addr_slot,
+		     const SocketOptions &opts, ServerSocket *) override;
   virtual int connect(const entity_addr_t &addr, const SocketOptions &opts, ConnectedSocket *socket) override;
   virtual void initialize() override;
-  RDMAStack *get_stack() { return stack; }
   int get_reged_mem(RDMAConnectedSocketImpl *o, std::vector<Chunk*> &c, size_t bytes);
   void remove_pending_conn(RDMAConnectedSocketImpl *o) {
-    assert(center.in_thread());
+    ceph_assert(center.in_thread());
     pending_sent_conns.remove(o);
   }
   void handle_pending_message();
-  void set_stack(RDMAStack *s) { stack = s; }
+  void set_dispatcher(shared_ptr<RDMADispatcher>& dispatcher) { this->dispatcher = dispatcher; }
+  void set_ib(shared_ptr<Infiniband> &ib) {this->ib = ib;}
   void notify_worker() {
     center.dispatch_event_external(tx_handler);
   }
 };
 
+struct RDMACMInfo {
+  RDMACMInfo(rdma_cm_id *cid, rdma_event_channel *cm_channel_, uint32_t qp_num_)
+    : cm_id(cid), cm_channel(cm_channel_), qp_num(qp_num_) {}
+  rdma_cm_id *cm_id;
+  rdma_event_channel *cm_channel;
+  uint32_t qp_num;
+};
+
 class RDMAConnectedSocketImpl : public ConnectedSocketImpl {
  public:
   typedef Infiniband::MemoryManager::Chunk Chunk;
   typedef Infiniband::CompletionChannel CompletionChannel;
   typedef Infiniband::CompletionQueue CompletionQueue;
 
- private:
+ protected:
   CephContext *cct;
   Infiniband::QueuePair *qp;
-  IBSYNMsg peer_msg;
-  IBSYNMsg my_msg;
+  uint32_t peer_qpn = 0;
+  uint32_t local_qpn = 0;
   int connected;
   int error;
-  Infiniband* infiniband;
-  RDMADispatcher* dispatcher;
+  shared_ptr<Infiniband> ib;
+  shared_ptr<RDMADispatcher> dispatcher;
   RDMAWorker* worker;
   std::vector<Chunk*> buffers;
   int notify_fd = -1;
   bufferlist pending_bl;
 
-  Mutex lock;
+  ceph::mutex lock = ceph::make_mutex("RDMAConnectedSocketImpl::lock");
   std::vector<ibv_wc> wc;
   bool is_server;
-  EventCallbackRef con_handler;
+  EventCallbackRef read_handler;
+  EventCallbackRef established_handler;
   int tcp_fd = -1;
   bool active;// qp is active ?
   bool pending;
+  int post_backlog = 0;
 
   void notify();
+  void buffer_prefetch(void);
   ssize_t read_buffers(char* buf, size_t len);
   int post_work_request(std::vector<Chunk*>&);
+  size_t tx_copy_chunk(std::vector<Chunk*> &tx_buffers, size_t req_copy_len,
+      decltype(std::cbegin(pending_bl.buffers()))& start,
+      const decltype(std::cbegin(pending_bl.buffers()))& end);
 
  public:
-  RDMAConnectedSocketImpl(CephContext *cct, Infiniband* ib, RDMADispatcher* s,
-                          RDMAWorker *w);
+  RDMAConnectedSocketImpl(CephContext *cct, shared_ptr<Infiniband>& ib,
+      shared_ptr<RDMADispatcher>& rdma_dispatcher, RDMAWorker *w);
   virtual ~RDMAConnectedSocketImpl();
 
   void pass_wc(std::vector<ibv_wc> &&v);
@@ -247,74 +213,126 @@ class RDMAConnectedSocketImpl : public ConnectedSocketImpl {
   virtual int is_connected() override { return connected; }
 
   virtual ssize_t read(char* buf, size_t len) override;
-  virtual ssize_t zero_copy_read(bufferptr &data) override;
   virtual ssize_t send(bufferlist &bl, bool more) override;
   virtual void shutdown() override;
   virtual void close() override;
   virtual int fd() const override { return notify_fd; }
   void fault();
   const char* get_qp_state() { return Infiniband::qp_state_string(qp->get_state()); }
+  uint32_t get_peer_qpn () const { return peer_qpn; }
+  uint32_t get_local_qpn () const { return local_qpn; }
   ssize_t submit(bool more);
   int activate();
   void fin();
   void handle_connection();
+  int handle_connection_established(bool need_set_fault = true);
   void cleanup();
   void set_accept_fd(int sd);
-  int try_connect(const entity_addr_t&, const SocketOptions &opt);
+  virtual int try_connect(const entity_addr_t&, const SocketOptions &opt);
   bool is_pending() {return pending;}
   void set_pending(bool val) {pending = val;}
-  class C_handle_connection : public EventCallback {
-    RDMAConnectedSocketImpl *csi;
-    bool active;
-   public:
-    C_handle_connection(RDMAConnectedSocketImpl *w): csi(w), active(true) {}
-    void do_request(int fd) {
-      if (active)
-        csi->handle_connection();
-    }
-    void close() {
-      active = false;
-    }
+  void post_chunks_to_rq(int num);
+  void update_post_backlog();
+};
+
+enum RDMA_CM_STATUS {
+  IDLE = 1,
+  RDMA_ID_CREATED,
+  CHANNEL_FD_CREATED,
+  RESOURCE_ALLOCATED,
+  ADDR_RESOLVED,
+  ROUTE_RESOLVED,
+  CONNECTED,
+  DISCONNECTED,
+  ERROR
+};
+
+class RDMAIWARPConnectedSocketImpl : public RDMAConnectedSocketImpl {
+  public:
+    RDMAIWARPConnectedSocketImpl(CephContext *cct, shared_ptr<Infiniband>& ib,
+        shared_ptr<RDMADispatcher>& rdma_dispatcher, RDMAWorker *w, RDMACMInfo *info = nullptr);
+    ~RDMAIWARPConnectedSocketImpl();
+    virtual int try_connect(const entity_addr_t&, const SocketOptions &opt) override;
+    virtual void close() override;
+    virtual void shutdown() override;
+    virtual void handle_cm_connection();
+    void activate();
+    int alloc_resource();
+    void close_notify();
+
+  private:
+    rdma_cm_id *cm_id = nullptr;
+    rdma_event_channel *cm_channel = nullptr;
+    EventCallbackRef cm_con_handler;
+    std::mutex close_mtx;
+    std::condition_variable close_condition;
+    bool closed = false;
+    RDMA_CM_STATUS status = IDLE;
+
+
+  class C_handle_cm_connection : public EventCallback {
+    RDMAIWARPConnectedSocketImpl *csi;
+    public:
+      C_handle_cm_connection(RDMAIWARPConnectedSocketImpl *w): csi(w) {}
+      void do_request(uint64_t fd) {
+        csi->handle_cm_connection();
+      }
   };
 };
 
 class RDMAServerSocketImpl : public ServerSocketImpl {
-  CephContext *cct;
-  NetHandler net;
-  int server_setup_socket;
-  Infiniband* infiniband;
-  RDMADispatcher *dispatcher;
-  RDMAWorker *worker;
-  entity_addr_t sa;
+  protected:
+    CephContext *cct;
+    NetHandler net;
+    int server_setup_socket;
+    shared_ptr<Infiniband> ib;
+    shared_ptr<RDMADispatcher> dispatcher;
+    RDMAWorker *worker;
+    entity_addr_t sa;
 
  public:
-  RDMAServerSocketImpl(CephContext *cct, Infiniband* i, RDMADispatcher *s, RDMAWorker *w, entity_addr_t& a);
+  RDMAServerSocketImpl(CephContext *cct, shared_ptr<Infiniband>& ib,
+                       shared_ptr<RDMADispatcher>& rdma_dispatcher,
+		       RDMAWorker *w, entity_addr_t& a, unsigned slot);
 
-  int listen(entity_addr_t &sa, const SocketOptions &opt);
+  virtual int listen(entity_addr_t &sa, const SocketOptions &opt);
   virtual int accept(ConnectedSocket *s, const SocketOptions &opts, entity_addr_t *out, Worker *w) override;
   virtual void abort_accept() override;
   virtual int fd() const override { return server_setup_socket; }
-  int get_fd() { return server_setup_socket; }
+};
+
+class RDMAIWARPServerSocketImpl : public RDMAServerSocketImpl {
+  public:
+    RDMAIWARPServerSocketImpl(
+      CephContext *cct, shared_ptr<Infiniband>& ib,
+      shared_ptr<RDMADispatcher>& rdma_dispatcher,
+      RDMAWorker* w, entity_addr_t& addr, unsigned addr_slot);
+    virtual int listen(entity_addr_t &sa, const SocketOptions &opt) override;
+    virtual int accept(ConnectedSocket *s, const SocketOptions &opts, entity_addr_t *out, Worker *w) override;
+    virtual void abort_accept() override;
+  private:
+    rdma_cm_id *cm_id = nullptr;
+    rdma_event_channel *cm_channel = nullptr;
 };
 
 class RDMAStack : public NetworkStack {
   vector<std::thread> threads;
-  RDMADispatcher *dispatcher;
+  PerfCounters *perf_counter;
+  shared_ptr<Infiniband> ib;
+  shared_ptr<RDMADispatcher> rdma_dispatcher;
 
   std::atomic<bool> fork_finished = {false};
 
  public:
   explicit RDMAStack(CephContext *cct, const string &t);
   virtual ~RDMAStack();
-  virtual bool support_zero_copy_read() const override { return false; }
-  virtual bool nonblock_connect_need_writable_event() const { return false; }
+  virtual bool nonblock_connect_need_writable_event() const override { return false; }
 
   virtual void spawn_worker(unsigned i, std::function<void ()> &&func) override;
   virtual void join_worker(unsigned i) override;
-  RDMADispatcher *get_dispatcher() { return dispatcher; }
-
   virtual bool is_ready() override { return fork_finished.load(); };
   virtual void ready() override { fork_finished = true; };
 };
 
+
 #endif
diff --git a/src/msg/msg_types.cc b/src/msg/msg_types.cc
index 67a699c9c4d..068d45a4643 100644
--- a/src/msg/msg_types.cc
+++ b/src/msg/msg_types.cc
@@ -16,8 +16,9 @@ void entity_name_t::dump(Formatter *f) const
 
 void entity_addr_t::dump(Formatter *f) const
 {
-  f->dump_unsigned("nonce", nonce);
+  f->dump_string("type", get_type_name(type));
   f->dump_stream("addr") << get_sockaddr();
+  f->dump_unsigned("nonce", nonce);
 }
 
 void entity_inst_t::dump(Formatter *f) const
@@ -61,25 +62,41 @@ void entity_inst_t::generate_test_instances(list<entity_inst_t*>& o)
   o.push_back(a);
 }
 
-bool entity_addr_t::parse(const char *s, const char **end)
+bool entity_addr_t::parse(const std::string_view s)
+{
+  const char* start = s.data();
+  const char* end = nullptr;
+  bool got = parse(start, &end);
+  return got && end == start + s.size();
+}
+
+bool entity_addr_t::parse(const char *s, const char **end, int default_type)
 {
-  memset(this, 0, sizeof(*this));
+  *this = entity_addr_t();
 
   const char *start = s;
+  if (end) {
+    *end = s;
+  }
 
-  int newtype = TYPE_DEFAULT;
-  if (strncmp("legacy:", s, 7) == 0) {
-    start += 7;
+  int newtype;
+  if (strncmp("v1:", s, 3) == 0) {
+    start += 3;
     newtype = TYPE_LEGACY;
-  } else if (strncmp("msgr2:", s, 6) == 0) {
-    start += 6;
+  } else if (strncmp("v2:", s, 3) == 0) {
+    start += 3;
     newtype = TYPE_MSGR2;
+  } else if (strncmp("any:", s, 4) == 0) {
+    start += 4;
+    newtype = TYPE_ANY;
   } else if (*s == '-') {
-    *this = entity_addr_t();
+    newtype = TYPE_NONE;
     if (end) {
       *end = s + 1;
     }
     return true;
+  } else {
+    newtype = default_type ? default_type : TYPE_DEFAULT;
   }
 
   bool brackets = false;
@@ -140,6 +157,9 @@ bool entity_addr_t::parse(const char *s, const char **end)
     // parse a port, too!
     p++;
     int port = atoi(p);
+    if (port > MAX_PORT_NUMBER) {
+      return false;
+    }
     set_port(port);
     while (*p && *p >= '0' && *p <= '9')
       p++;
@@ -168,81 +188,117 @@ ostream& operator<<(ostream& out, const entity_addr_t &addr)
   if (addr.type == entity_addr_t::TYPE_NONE) {
     return out << "-";
   }
-  if (addr.type != entity_addr_t::TYPE_DEFAULT) {
+  if (addr.type != entity_addr_t::TYPE_ANY) {
     out << entity_addr_t::get_type_name(addr.type) << ":";
   }
   out << addr.get_sockaddr() << '/' << addr.nonce;
   return out;
 }
 
-ostream& operator<<(ostream& out, const sockaddr_storage &ss)
+ostream& operator<<(ostream& out, const sockaddr *psa)
 {
   char buf[NI_MAXHOST] = { 0 };
-  char serv[NI_MAXSERV] = { 0 };
-  size_t hostlen;
-
-  if (ss.ss_family == AF_INET)
-    hostlen = sizeof(struct sockaddr_in);
-  else if (ss.ss_family == AF_INET6)
-    hostlen = sizeof(struct sockaddr_in6);
-  else
-    hostlen = sizeof(struct sockaddr_storage);
-  getnameinfo((struct sockaddr *)&ss, hostlen, buf, sizeof(buf),
-	      serv, sizeof(serv),
-	      NI_NUMERICHOST | NI_NUMERICSERV);
-  if (ss.ss_family == AF_INET6)
-    return out << '[' << buf << "]:" << serv;
-  return out << buf << ':' << serv;
+
+  switch (psa->sa_family) {
+  case AF_INET:
+    {
+      const sockaddr_in *sa = (const sockaddr_in*)psa;
+      inet_ntop(AF_INET, &sa->sin_addr, buf, NI_MAXHOST);
+      return out << buf << ':'
+		 << ntohs(sa->sin_port);
+    }
+  case AF_INET6:
+    {
+      const sockaddr_in6 *sa = (const sockaddr_in6*)psa;
+      inet_ntop(AF_INET6, &sa->sin6_addr, buf, NI_MAXHOST);
+      return out << '[' << buf << "]:"
+		 << ntohs(sa->sin6_port);
+    }
+  default:
+    return out << "(unrecognized address family " << psa->sa_family << ")";
+  }
 }
 
-ostream& operator<<(ostream& out, const sockaddr *sa)
+ostream& operator<<(ostream& out, const sockaddr_storage &ss)
 {
-  char buf[NI_MAXHOST] = { 0 };
-  char serv[NI_MAXSERV] = { 0 };
-  size_t hostlen;
-
-  if (sa->sa_family == AF_INET)
-    hostlen = sizeof(struct sockaddr_in);
-  else if (sa->sa_family == AF_INET6)
-    hostlen = sizeof(struct sockaddr_in6);
-  else
-    hostlen = sizeof(struct sockaddr_storage);
-  getnameinfo(sa, hostlen, buf, sizeof(buf),
-	      serv, sizeof(serv),
-	      NI_NUMERICHOST | NI_NUMERICSERV);
-  if (sa->sa_family == AF_INET6)
-    return out << '[' << buf << "]:" << serv;
-  return out << buf << ':' << serv;
+  return out << (const sockaddr*)&ss;
 }
 
+
 // entity_addrvec_t
 
-void entity_addrvec_t::encode(bufferlist& bl, uint64_t features) const
+bool entity_addrvec_t::parse(const char *s, const char **end)
 {
-  if ((features & CEPH_FEATURE_MSG_ADDR2) == 0) {
-    // encode a single legacy entity_addr_t for unfeatured peers
-    if (v.size() > 0) {
-      for (vector<entity_addr_t>::const_iterator p = v.begin();
-           p != v.end(); ++p) {
-        if ((*p).type == entity_addr_t::TYPE_LEGACY) {
-	  ::encode(*p, bl, 0);
-	  return;
-	}
+  const char *orig_s = s;
+  const char *static_end;
+  if (!end) {
+    end = &static_end;
+  } else {
+    *end = s;
+  }
+  v.clear();
+  bool brackets = false;
+  if (*s == '[') {
+    // weirdness: make sure this isn't an IPV6 addr!
+    entity_addr_t a;
+    const char *p;
+    if (!a.parse(s, &p) || !a.is_ipv6()) {
+      // it's not
+      brackets = true;
+      ++s;
+    }
+  }
+  while (*s) {
+    entity_addr_t a;
+    bool r = a.parse(s, end);
+    if (!r) {
+      if (brackets) {
+	v.clear();
+	*end = orig_s;
+	return false;
       }
-      ::encode(v[0], bl, 0);
+      break;
+    }
+    v.push_back(a);
+    s = *end;
+    if (!brackets) {
+      break;
+    }
+    if (*s != ',') {
+      break;
+    }
+    ++s;
+  }
+  if (brackets) {
+    if (*s == ']') {
+      ++s;
+      *end = s;
     } else {
-      ::encode(entity_addr_t(), bl, 0);
+      *end = orig_s;
+      v.clear();
+      return false;
     }
+  }
+  return !v.empty();
+}
+
+void entity_addrvec_t::encode(bufferlist& bl, uint64_t features) const
+{
+  using ceph::encode;
+  if ((features & CEPH_FEATURE_MSG_ADDR2) == 0) {
+    // encode a single legacy entity_addr_t for unfeatured peers
+    encode(legacy_addr(), bl, 0);
     return;
   }
-  ::encode((__u8)2, bl);
-  ::encode(v, bl, features);
+  encode((__u8)2, bl);
+  encode(v, bl, features);
 }
 
-void entity_addrvec_t::decode(bufferlist::iterator& bl)
+void entity_addrvec_t::decode(bufferlist::const_iterator& bl)
 {
+  using ceph::decode;
   __u8 marker;
-  ::decode(marker, bl);
+  decode(marker, bl);
   if (marker == 0) {
     // legacy!
     entity_addr_t addr;
@@ -254,12 +310,26 @@ void entity_addrvec_t::decode(bufferlist::iterator& bl)
   if (marker == 1) {
     entity_addr_t addr;
     DECODE_START(1, bl);
-    ::decode(addr.type, bl);
-    ::decode(addr.nonce, bl);
+    decode(addr.type, bl);
+    decode(addr.nonce, bl);
     __u32 elen;
-    ::decode(elen, bl);
+    decode(elen, bl);
     if (elen) {
-      bl.copy(elen, (char*)addr.get_sockaddr());
+      struct sockaddr *sa = (struct sockaddr *)addr.get_sockaddr();
+#if defined(__FreeBSD__) || defined(__APPLE__)
+      sa->sa_len = 0;
+#endif
+      uint16_t ss_family;
+      if (elen < sizeof(ss_family)) {
+        throw ceph::buffer::malformed_input("elen smaller than family len");
+      }
+      decode(ss_family, bl);
+      sa->sa_family = ss_family;
+      elen -= sizeof(ss_family);
+      if (elen > addr.get_sockaddr_len() - sizeof(sa->sa_family)) {
+        throw ceph::buffer::malformed_input("elen exceeds sockaddr len");
+      }
+      bl.copy(elen, sa->sa_data);
     }
     DECODE_FINISH(bl);
     v.clear();
@@ -268,7 +338,7 @@ void entity_addrvec_t::decode(bufferlist::iterator& bl)
   }
   if (marker > 2)
     throw buffer::malformed_input("entity_addrvec_marker > 2");
-  ::decode(v, bl);
+  decode(v, bl);
 }
 
 void entity_addrvec_t::dump(Formatter *f) const
@@ -290,3 +360,22 @@ void entity_addrvec_t::generate_test_instances(list<entity_addrvec_t*>& ls)
   ls.back()->v.push_back(entity_addr_t());
   ls.back()->v.push_back(entity_addr_t());
 }
+
+std::string entity_addr_t::ip_only_to_str() const 
+{
+  const char *host_ip = NULL;
+  char addr_buf[INET6_ADDRSTRLEN];
+  switch (get_family()) {
+  case AF_INET:
+    host_ip = inet_ntop(AF_INET, &in4_addr().sin_addr, 
+                        addr_buf, INET_ADDRSTRLEN);
+    break;
+  case AF_INET6:
+    host_ip = inet_ntop(AF_INET6, &in6_addr().sin6_addr, 
+                        addr_buf, INET6_ADDRSTRLEN);
+    break;
+  default:
+    break;
+  }
+  return host_ip ? host_ip : "";
+}
diff --git a/src/msg/msg_types.h b/src/msg/msg_types.h
index 5632950f309..db1f20720a9 100644
--- a/src/msg/msg_types.h
+++ b/src/msg/msg_types.h
@@ -1,4 +1,4 @@
-// -*- mode:C++; tab-width:8; c-basic-offset:2; indent-tabs-mode:t -*- 
+// -*- mode:C++; tab-width:8; c-basic-offset:2; indent-tabs-mode:t -*-
 // vim: ts=8 sw=2 smarttab
 /*
  * Ceph - scalable distributed file system
@@ -7,14 +7,16 @@
  *
  * This is free software; you can redistribute it and/or
  * modify it under the terms of the GNU Lesser General Public
- * License version 2.1, as published by the Free Software 
+ * License version 2.1, as published by the Free Software
  * Foundation.  See file COPYING.
- * 
+ *
  */
 
 #ifndef CEPH_MSG_TYPES_H
 #define CEPH_MSG_TYPES_H
 
+#include <sstream>
+
 #include <netinet/in.h>
 
 #include "include/ceph_features.h"
@@ -22,12 +24,14 @@
 #include "include/blobhash.h"
 #include "include/encoding.h"
 
+#define MAX_PORT_NUMBER 65535
+
 namespace ceph {
   class Formatter;
 }
 
-extern ostream& operator<<(ostream& out, const sockaddr_storage &ss);
-extern ostream& operator<<(ostream& out, const sockaddr *sa);
+std::ostream& operator<<(std::ostream& out, const sockaddr_storage &ss);
+std::ostream& operator<<(std::ostream& out, const sockaddr *sa);
 
 typedef uint8_t entity_type_t;
 
@@ -48,7 +52,7 @@ public:
   // cons
   entity_name_t() : _type(0), _num(0) { }
   entity_name_t(int t, int64_t n) : _type(t), _num(n) { }
-  explicit entity_name_t(const ceph_entity_name &n) : 
+  explicit entity_name_t(const ceph_entity_name &n) :
     _type(n.type), _num(n.num) { }
 
   // static cons
@@ -57,7 +61,7 @@ public:
   static entity_name_t OSD(int64_t i=NEW) { return entity_name_t(TYPE_OSD, i); }
   static entity_name_t CLIENT(int64_t i=NEW) { return entity_name_t(TYPE_CLIENT, i); }
   static entity_name_t MGR(int64_t i=NEW) { return entity_name_t(TYPE_MGR, i); }
-  
+
   int64_t num() const { return _num; }
   int type() const { return _type; }
   const char *type_str() const {
@@ -77,7 +81,7 @@ public:
     return n;
   }
 
-  bool parse(const string& s) {
+  bool parse(const std::string& s) {
     const char *start = s.c_str();
     char *end;
     bool got = parse(start, &end);
@@ -114,17 +118,17 @@ public:
     denc(v._type, p);
     denc(v._num, p);
   }
-  void dump(Formatter *f) const;
+  void dump(ceph::Formatter *f) const;
 
-  static void generate_test_instances(list<entity_name_t*>& o);
+  static void generate_test_instances(std::list<entity_name_t*>& o);
 };
 WRITE_CLASS_DENC(entity_name_t)
 
-inline bool operator== (const entity_name_t& l, const entity_name_t& r) { 
+inline bool operator== (const entity_name_t& l, const entity_name_t& r) {
   return (l.type() == r.type()) && (l.num() == r.num()); }
-inline bool operator!= (const entity_name_t& l, const entity_name_t& r) { 
+inline bool operator!= (const entity_name_t& l, const entity_name_t& r) {
   return (l.type() != r.type()) || (l.num() != r.num()); }
-inline bool operator< (const entity_name_t& l, const entity_name_t& r) { 
+inline bool operator< (const entity_name_t& l, const entity_name_t& r) {
   return (l.type() < r.type()) || (l.type() == r.type() && l.num() < r.num()); }
 
 inline std::ostream& operator<<(std::ostream& out, const entity_name_t& addr) {
@@ -135,7 +139,7 @@ inline std::ostream& operator<<(std::ostream& out, const entity_name_t& addr) {
     return out << addr.type_str() << '.' << addr.num();
 }
 inline std::ostream& operator<<(std::ostream& out, const ceph_entity_name& addr) {
-  return out << *(const entity_name_t*)&addr;
+  return out << entity_name_t{addr.type, static_cast<int64_t>(addr.num)};
 }
 
 namespace std {
@@ -148,71 +152,98 @@ namespace std {
   };
 } // namespace std
 
-/*
- * an entity's network address.
- * includes a random value that prevents it from being reused.
- * thus identifies a particular process instance.
- * ipv4 for now.
- */
-
-#if defined(__linux__) || defined(DARWIN) || defined(__FreeBSD__)
-/*
- * encode sockaddr.ss_family as network byte order 
- */
-static inline void encode(const sockaddr_storage& a, bufferlist& bl) {
-  struct sockaddr_storage ss = a;
-#if defined(DARWIN) || defined(__FreeBSD__)
-  unsigned short *ss_family = reinterpret_cast<unsigned short*>(&ss);
-  *ss_family = htons(a.ss_family);
-#else
-  ss.ss_family = htons(ss.ss_family);
-#endif
-  ::encode_raw(ss, bl);
-}
-static inline void decode(sockaddr_storage& a, bufferlist::iterator& bl) {
-  ::decode_raw(a, bl);
-#if defined(DARWIN) || defined(__FreeBSD__)
-  unsigned short *ss_family = reinterpret_cast<unsigned short *>(&a);
-  a.ss_family = ntohs(*ss_family);
-  a.ss_len = 0;
-#else
-  a.ss_family = ntohs(a.ss_family);
-#endif
-}
-#endif
-
 // define a wire format for sockaddr that matches Linux's.
 struct ceph_sockaddr_storage {
-  __le16 ss_family;
-  __u8 __ss_padding[128 - sizeof(__le16)];
+  ceph_le16 ss_family;
+  __u8 __ss_padding[128 - sizeof(ceph_le16)];
 
-  void encode(bufferlist& bl) const {
+  void encode(ceph::buffer::list& bl) const {
     struct ceph_sockaddr_storage ss = *this;
     ss.ss_family = htons(ss.ss_family);
-    ::encode_raw(ss, bl);
+    ceph::encode_raw(ss, bl);
   }
 
-  void decode(bufferlist::iterator& bl) {
+  void decode(ceph::buffer::list::const_iterator& bl) {
     struct ceph_sockaddr_storage ss;
-    ::decode_raw(ss, bl);
+    ceph::decode_raw(ss, bl);
     ss.ss_family = ntohs(ss.ss_family);
     *this = ss;
   }
 } __attribute__ ((__packed__));
 WRITE_CLASS_ENCODER(ceph_sockaddr_storage)
 
+/*
+ * encode sockaddr.ss_family as network byte order
+ */
+static inline void encode(const sockaddr_storage& a, ceph::buffer::list& bl) {
+#if defined(__linux__)
+  struct sockaddr_storage ss = a;
+  ss.ss_family = htons(ss.ss_family);
+  ceph::encode_raw(ss, bl);
+#elif defined(__FreeBSD__) || defined(__APPLE__)
+  ceph_sockaddr_storage ss{};
+  auto src = (unsigned char const *)&a;
+  auto dst = (unsigned char *)&ss;
+  src += sizeof(a.ss_len);
+  ss.ss_family = a.ss_family;
+  src += sizeof(a.ss_family);
+  dst += sizeof(ss.ss_family);
+  const auto copy_size = std::min((unsigned char*)(&a + 1) - src,
+				  (unsigned char*)(&ss + 1) - dst);
+  ::memcpy(dst, src, copy_size);
+  encode(ss, bl);
+#else
+  ceph_sockaddr_storage ss{};
+  ::memset(&ss, '\0', sizeof(ss));
+  ::memcpy(&wireaddr, &ss, std::min(sizeof(ss), sizeof(a)));
+  encode(ss, bl);
+#endif
+}
+static inline void decode(sockaddr_storage& a,
+			  ceph::buffer::list::const_iterator& bl) {
+#if defined(__linux__)
+  ceph::decode_raw(a, bl);
+  a.ss_family = ntohs(a.ss_family);
+#elif defined(__FreeBSD__) || defined(__APPLE__)
+  ceph_sockaddr_storage ss{};
+  decode(ss, bl);
+  auto src = (unsigned char const *)&ss;
+  auto dst = (unsigned char *)&a;
+  a.ss_len = 0;
+  dst += sizeof(a.ss_len);
+  a.ss_family = ss.ss_family;
+  src += sizeof(ss.ss_family);
+  dst += sizeof(a.ss_family);
+  auto const copy_size = std::min((unsigned char*)(&ss + 1) - src,
+				  (unsigned char*)(&a + 1) - dst);
+  ::memcpy(dst, src, copy_size);
+#else
+  ceph_sockaddr_storage ss{};
+  decode(ss, bl);
+  ::memcpy(&a, &ss, std::min(sizeof(ss), sizeof(a)));
+#endif
+}
+
+/*
+ * an entity's network address.
+ * includes a random value that prevents it from being reused.
+ * thus identifies a particular process instance.
+ * ipv4 for now.
+ */
 struct entity_addr_t {
   typedef enum {
     TYPE_NONE = 0,
     TYPE_LEGACY = 1,  ///< legacy msgr1 protocol (ceph jewel and older)
     TYPE_MSGR2 = 2,   ///< msgr2 protocol (new in ceph kraken)
+    TYPE_ANY = 3,  ///< ambiguous
   } type_t;
-  static const type_t TYPE_DEFAULT = TYPE_LEGACY;
-  static const char *get_type_name(int t) {
+  static const type_t TYPE_DEFAULT = TYPE_MSGR2;
+  static std::string_view get_type_name(int t) {
     switch (t) {
     case TYPE_NONE: return "none";
-    case TYPE_LEGACY: return "legacy";
-    case TYPE_MSGR2: return "msgr2";
+    case TYPE_LEGACY: return "v1";
+    case TYPE_MSGR2: return "v2";
+    case TYPE_ANY: return "any";
     default: return "???";
     }
   };
@@ -225,7 +256,7 @@ struct entity_addr_t {
     sockaddr_in6 sin6;
   } u;
 
-  entity_addr_t() : type(0), nonce(0) { 
+  entity_addr_t() : type(0), nonce(0) {
     memset(&u, 0, sizeof(u));
   }
   entity_addr_t(__u32 _type, __u32 _nonce) : type(_type), nonce(_nonce) {
@@ -242,6 +273,9 @@ struct entity_addr_t {
 
   uint32_t get_type() const { return type; }
   void set_type(uint32_t t) { type = t; }
+  bool is_legacy() const { return type == TYPE_LEGACY; }
+  bool is_msgr2() const { return type == TYPE_MSGR2; }
+  bool is_any() const { return type == TYPE_ANY; }
 
   __u32 get_nonce() const { return nonce; }
   void set_nonce(__u32 n) { nonce = n; }
@@ -253,6 +287,13 @@ struct entity_addr_t {
     u.sa.sa_family = f;
   }
 
+  bool is_ipv4() const {
+    return u.sa.sa_family == AF_INET;
+  }
+  bool is_ipv6() const {
+    return u.sa.sa_family == AF_INET6;
+  }
+
   sockaddr_in &in4_addr() {
     return u.sin;
   }
@@ -281,11 +322,18 @@ struct entity_addr_t {
   {
     switch (sa->sa_family) {
     case AF_INET:
+      // pre-zero, since we're only copying a portion of the source
+      memset(&u, 0, sizeof(u));
       memcpy(&u.sin, sa, sizeof(u.sin));
       break;
     case AF_INET6:
+      // pre-zero, since we're only copying a portion of the source
+      memset(&u, 0, sizeof(u));
       memcpy(&u.sin6, sa, sizeof(u.sin6));
       break;
+    case AF_UNSPEC:
+      memset(&u, 0, sizeof(u));
+      break;
     default:
       return false;
     }
@@ -350,7 +398,7 @@ struct entity_addr_t {
       return true;
     return false;
   }
-  
+
   bool is_same_host(const entity_addr_t &o) const {
     if (u.sa.sa_family != o.u.sa.sa_family)
       return false;
@@ -384,88 +432,118 @@ struct entity_addr_t {
     }
   }
 
-  bool parse(const char *s, const char **end = 0);
+  std::string ip_only_to_str() const;
+
+  std::string get_legacy_str() const {
+    std::ostringstream ss;
+    ss << get_sockaddr() << "/" << get_nonce();
+    return ss.str();
+  }
 
-  void decode_legacy_addr_after_marker(bufferlist::iterator& bl)
+  bool parse(const std::string_view s);
+  bool parse(const char *s, const char **end = 0, int type=0);
+
+  void decode_legacy_addr_after_marker(ceph::buffer::list::const_iterator& bl)
   {
+    using ceph::decode;
     __u8 marker;
     __u16 rest;
-    ::decode(marker, bl);
-    ::decode(rest, bl);
-    type = TYPE_LEGACY;
-    ::decode(nonce, bl);
+    decode(marker, bl);
+    decode(rest, bl);
+    decode(nonce, bl);
     sockaddr_storage ss;
-#if defined(__linux__) || defined(DARWIN) || defined(__FreeBSD__)
-    ::decode(ss, bl);
-#else
-    ceph_sockaddr_storage wireaddr;
-    ::memset(&wireaddr, '\0', sizeof(wireaddr));
-    ::decode(wireaddr, bl);
-    unsigned copysize = MIN(sizeof(wireaddr), sizeof(ss));
-    ::memcpy(&ss, &wireaddr, copysize);
-#endif
+    decode(ss, bl);
     set_sockaddr((sockaddr*)&ss);
+    if (get_family() == AF_UNSPEC) {
+      type = TYPE_NONE;
+    } else {
+      type = TYPE_LEGACY;
+    }
   }
 
   // Right now, these only deal with sockaddr_storage that have only family and content.
   // Apparently on BSD there is also an ss_len that we need to handle; this requires
   // broader study
 
-  void encode(bufferlist& bl, uint64_t features) const {
+  void encode(ceph::buffer::list& bl, uint64_t features) const {
+    using ceph::encode;
     if ((features & CEPH_FEATURE_MSG_ADDR2) == 0) {
-      ::encode((__u32)0, bl);
-      ::encode(nonce, bl);
+      encode((__u32)0, bl);
+      encode(nonce, bl);
       sockaddr_storage ss = get_sockaddr_storage();
-#if defined(__linux__) || defined(DARWIN) || defined(__FreeBSD__)
-      ::encode(ss, bl);
-#else
-      ceph_sockaddr_storage wireaddr;
-      ::memset(&wireaddr, '\0', sizeof(wireaddr));
-      unsigned copysize = MIN(sizeof(wireaddr), sizeof(ss));
-      // ceph_sockaddr_storage is in host byte order
-      ::memcpy(&wireaddr, &ss, copysize);
-      ::encode(wireaddr, bl);
-#endif
+      encode(ss, bl);
       return;
     }
-    ::encode((__u8)1, bl);
+    encode((__u8)1, bl);
     ENCODE_START(1, 1, bl);
-    ::encode(type, bl);
-    ::encode(nonce, bl);
+    if (HAVE_FEATURE(features, SERVER_NAUTILUS)) {
+      encode(type, bl);
+    } else {
+      // map any -> legacy for old clients.  this is primary for the benefit
+      // of OSDMap's blacklist, but is reasonable in general since any: is
+      // meaningless for pre-nautilus clients or daemons.
+      auto t = type;
+      if (t == TYPE_ANY) {
+	t = TYPE_LEGACY;
+      }
+      encode(t, bl);
+    }
+    encode(nonce, bl);
     __u32 elen = get_sockaddr_len();
-    ::encode(elen, bl);
+#if (__FreeBSD__) || defined(__APPLE__)
+      elen -= sizeof(u.sa.sa_len);
+#endif
+    encode(elen, bl);
     if (elen) {
-      bl.append((char*)get_sockaddr(), elen);
+      uint16_t ss_family = u.sa.sa_family;
+
+      encode(ss_family, bl);
+      elen -= sizeof(u.sa.sa_family);
+      bl.append(u.sa.sa_data, elen);
     }
     ENCODE_FINISH(bl);
   }
-  void decode(bufferlist::iterator& bl) {
+  void decode(ceph::buffer::list::const_iterator& bl) {
+    using ceph::decode;
     __u8 marker;
-    ::decode(marker, bl);
+    decode(marker, bl);
     if (marker == 0) {
       decode_legacy_addr_after_marker(bl);
       return;
     }
     if (marker != 1)
-      throw buffer::malformed_input("entity_addr_t marker != 1");
+      throw ceph::buffer::malformed_input("entity_addr_t marker != 1");
     DECODE_START(1, bl);
-    ::decode(type, bl);
-    ::decode(nonce, bl);
+    decode(type, bl);
+    decode(nonce, bl);
     __u32 elen;
-    ::decode(elen, bl);
+    decode(elen, bl);
     if (elen) {
-      bl.copy(elen, (char*)get_sockaddr());
+#if defined(__FreeBSD__) || defined(__APPLE__)
+      u.sa.sa_len = 0;
+#endif
+      uint16_t ss_family;
+      if (elen < sizeof(ss_family)) {
+	throw buffer::malformed_input("elen smaller than family len");
+      }
+      decode(ss_family, bl);
+      u.sa.sa_family = ss_family;
+      elen -= sizeof(ss_family);
+      if (elen > get_sockaddr_len() - sizeof(u.sa.sa_family)) {
+	throw buffer::malformed_input("elen exceeds sockaddr len");
+      }
+      bl.copy(elen, u.sa.sa_data);
     }
     DECODE_FINISH(bl);
   }
 
-  void dump(Formatter *f) const;
+  void dump(ceph::Formatter *f) const;
 
-  static void generate_test_instances(list<entity_addr_t*>& o);
+  static void generate_test_instances(std::list<entity_addr_t*>& o);
 };
 WRITE_CLASS_ENCODER_FEATURES(entity_addr_t)
 
-ostream& operator<<(ostream& out, const entity_addr_t &addr);
+std::ostream& operator<<(std::ostream& out, const entity_addr_t &addr);
 
 inline bool operator==(const entity_addr_t& a, const entity_addr_t& b) { return memcmp(&a, &b, sizeof(a)) == 0; }
 inline bool operator!=(const entity_addr_t& a, const entity_addr_t& b) { return memcmp(&a, &b, sizeof(a)) != 0; }
@@ -475,29 +553,180 @@ inline bool operator>(const entity_addr_t& a, const entity_addr_t& b) { return m
 inline bool operator>=(const entity_addr_t& a, const entity_addr_t& b) { return memcmp(&a, &b, sizeof(a)) >= 0; }
 
 namespace std {
-  template<> struct hash< entity_addr_t >
-  {
-    size_t operator()( const entity_addr_t& x ) const
-    {
-      static blobhash H;
-      return H((const char*)&x, sizeof(x));
-    }
-  };
+template<> struct hash<entity_addr_t> {
+  size_t operator()( const entity_addr_t& x ) const {
+    static blobhash H;
+    return H(&x, sizeof(x));
+  }
+};
 } // namespace std
 
 struct entity_addrvec_t {
-  vector<entity_addr_t> v;
+  std::vector<entity_addr_t> v;
+
+  entity_addrvec_t() {}
+  explicit entity_addrvec_t(const entity_addr_t& a) : v({ a }) {}
 
   unsigned size() const { return v.size(); }
   bool empty() const { return v.empty(); }
 
-  void encode(bufferlist& bl, uint64_t features) const;
-  void decode(bufferlist::iterator& bl);
-  void dump(Formatter *f) const;
-  static void generate_test_instances(list<entity_addrvec_t*>& ls);
+  entity_addr_t legacy_addr() const {
+    for (auto& a : v) {
+      if (a.type == entity_addr_t::TYPE_LEGACY) {
+	return a;
+      }
+    }
+    return entity_addr_t();
+  }
+  entity_addr_t as_legacy_addr() const {
+    for (auto& a : v) {
+      if (a.is_legacy()) {
+	return a;
+      }
+      if (a.is_any()) {
+	auto b = a;
+	b.set_type(entity_addr_t::TYPE_LEGACY);
+	return b;
+      }
+    }
+    // hrm... lie!
+    auto a = front();
+    a.set_type(entity_addr_t::TYPE_LEGACY);
+    return a;
+  }
+  entity_addr_t front() const {
+    if (!v.empty()) {
+      return v.front();
+    }
+    return entity_addr_t();
+  }
+  entity_addr_t legacy_or_front_addr() const {
+    for (auto& a : v) {
+      if (a.type == entity_addr_t::TYPE_LEGACY) {
+	return a;
+      }
+    }
+    if (!v.empty()) {
+      return v.front();
+    }
+    return entity_addr_t();
+  }
+  std::string get_legacy_str() const {
+    return legacy_or_front_addr().get_legacy_str();
+  }
+
+  entity_addr_t msgr2_addr() const {
+    for (auto &a : v) {
+      if (a.type == entity_addr_t::TYPE_MSGR2) {
+        return a;
+      }
+    }
+    return entity_addr_t();
+  }
+  bool has_msgr2() const {
+    for (auto& a : v) {
+      if (a.is_msgr2()) {
+	return true;
+      }
+    }
+    return false;
+  }
+
+  bool parse(const char *s, const char **end = 0);
+
+  void get_ports(std::set<int> *ports) const {
+    for (auto& a : v) {
+      ports->insert(a.get_port());
+    }
+  }
+  std::set<int> get_ports() const {
+    std::set<int> r;
+    get_ports(&r);
+    return r;
+  }
+
+  void encode(ceph::buffer::list& bl, uint64_t features) const;
+  void decode(ceph::buffer::list::const_iterator& bl);
+  void dump(ceph::Formatter *f) const;
+  static void generate_test_instances(std::list<entity_addrvec_t*>& ls);
+
+  bool legacy_equals(const entity_addrvec_t& o) const {
+    if (v == o.v) {
+      return true;
+    }
+    if (v.size() == 1 &&
+	front().is_legacy() &&
+	front() == o.legacy_addr()) {
+      return true;
+    }
+    if (o.v.size() == 1 &&
+	o.front().is_legacy() &&
+	o.front() == legacy_addr()) {
+      return true;
+    }
+    return false;
+  }
+
+  bool probably_equals(const entity_addrvec_t& o) const {
+    for (unsigned i = 0; i < v.size(); ++i) {
+      if (!v[i].probably_equals(o.v[i])) {
+	return false;
+      }
+    }
+    return true;
+  }
+  bool contains(const entity_addr_t& a) const {
+    for (auto& i : v) {
+      if (a == i) {
+	return true;
+      }
+    }
+    return false;
+  }
+  bool is_same_host(const entity_addr_t& a) const {
+    for (auto& i : v) {
+      if (i.is_same_host(a)) {
+	return true;
+      }
+    }
+    return false;
+  }
+
+  friend std::ostream& operator<<(std::ostream& out, const entity_addrvec_t& av) {
+    if (av.v.empty()) {
+      return out;
+    } else if (av.v.size() == 1) {
+      return out << av.v[0];
+    } else {
+      return out << av.v;
+    }
+  }
+
+  friend bool operator==(const entity_addrvec_t& l, const entity_addrvec_t& r) {
+    return l.v == r.v;
+  }
+  friend bool operator!=(const entity_addrvec_t& l, const entity_addrvec_t& r) {
+    return l.v != r.v;
+  }
+  friend bool operator<(const entity_addrvec_t& l, const entity_addrvec_t& r) {
+    return l.v < r.v;  // see lexicographical_compare()
+  }
 };
 WRITE_CLASS_ENCODER_FEATURES(entity_addrvec_t);
 
+namespace std {
+template<> struct hash<entity_addrvec_t> {
+  size_t operator()( const entity_addrvec_t& x) const {
+    static blobhash H;
+    size_t r = 0;
+    for (auto& i : x.v) {
+      r += H((const char*)&i, sizeof(i));
+    }
+    return r;
+  }
+};
+} // namespace std
+
 /*
  * a particular entity instance
  */
@@ -514,28 +743,30 @@ struct entity_inst_t {
     return i;
   }
 
-  void encode(bufferlist& bl, uint64_t features) const {
-    ::encode(name, bl);
-    ::encode(addr, bl, features);
+  void encode(ceph::buffer::list& bl, uint64_t features) const {
+    using ceph::encode;
+    encode(name, bl);
+    encode(addr, bl, features);
   }
-  void decode(bufferlist::iterator& bl) {
-    ::decode(name, bl);
-    ::decode(addr, bl);
+  void decode(ceph::buffer::list::const_iterator& bl) {
+    using ceph::decode;
+    decode(name, bl);
+    decode(addr, bl);
   }
 
-  void dump(Formatter *f) const;
-  static void generate_test_instances(list<entity_inst_t*>& o);
+  void dump(ceph::Formatter *f) const;
+  static void generate_test_instances(std::list<entity_inst_t*>& o);
 };
 WRITE_CLASS_ENCODER_FEATURES(entity_inst_t)
 
 
-inline bool operator==(const entity_inst_t& a, const entity_inst_t& b) { 
+inline bool operator==(const entity_inst_t& a, const entity_inst_t& b) {
   return a.name == b.name && a.addr == b.addr;
 }
-inline bool operator!=(const entity_inst_t& a, const entity_inst_t& b) { 
+inline bool operator!=(const entity_inst_t& a, const entity_inst_t& b) {
   return a.name != b.name || a.addr != b.addr;
 }
-inline bool operator<(const entity_inst_t& a, const entity_inst_t& b) { 
+inline bool operator<(const entity_inst_t& a, const entity_inst_t& b) {
   return a.name < b.name || (a.name == b.name && a.addr < b.addr);
 }
 inline bool operator<=(const entity_inst_t& a, const entity_inst_t& b) {
@@ -557,11 +788,11 @@ namespace std {
 } // namespace std
 
 
-inline ostream& operator<<(ostream& out, const entity_inst_t &i)
+inline std::ostream& operator<<(std::ostream& out, const entity_inst_t &i)
 {
   return out << i.name << " " << i.addr;
 }
-inline ostream& operator<<(ostream& out, const ceph_entity_inst &i)
+inline std::ostream& operator<<(std::ostream& out, const ceph_entity_inst &i)
 {
   entity_inst_t n = i;
   return out << n;
diff --git a/src/msg/simple/Accepter.cc b/src/msg/simple/Accepter.cc
deleted file mode 100644
index 5779cc7abec..00000000000
--- a/src/msg/simple/Accepter.cc
+++ /dev/null
@@ -1,409 +0,0 @@
-// -*- mode:C++; tab-width:8; c-basic-offset:2; indent-tabs-mode:t -*- 
-// vim: ts=8 sw=2 smarttab
-/*
- * Ceph - scalable distributed file system
- *
- * Copyright (C) 2004-2006 Sage Weil <sage@newdream.net>
- *
- * This is free software; you can redistribute it and/or
- * modify it under the terms of the GNU Lesser General Public
- * License version 2.1, as published by the Free Software 
- * Foundation.  See file COPYING.
- * 
- */
-
-#include "include/compat.h"
-#include <sys/socket.h>
-#include <netinet/tcp.h>
-#include <sys/uio.h>
-#include <limits.h>
-#include <poll.h>
-
-#include "msg/msg_types.h"
-#include "msg/Message.h"
-
-#include "Accepter.h"
-#include "Pipe.h"
-#include "SimpleMessenger.h"
-
-#include "common/debug.h"
-#include "common/errno.h"
-#include "common/safe_io.h"
-
-#define dout_subsys ceph_subsys_ms
-
-#undef dout_prefix
-#define dout_prefix *_dout << "accepter."
-
-
-/********************************************
- * Accepter
- */
-
-static int set_close_on_exec(int fd)
-{
-  int flags = fcntl(fd, F_GETFD, 0);
-  if (flags < 0) {
-    return errno;
-  }
-  if (fcntl(fd, F_SETFD, flags | FD_CLOEXEC)) {
-    return errno;
-  }
-  return 0;
-}
-
-int Accepter::create_selfpipe(int *pipe_rd, int *pipe_wr) {
-  int selfpipe[2];
-  int ret = ::pipe2(selfpipe, (O_CLOEXEC|O_NONBLOCK));
-  if (ret < 0 ) {
-    lderr(msgr->cct) << __func__ << " unable to create the selfpipe: "
-                    << cpp_strerror(errno) << dendl;
-    return -errno;
-  }
-  *pipe_rd = selfpipe[0];
-  *pipe_wr = selfpipe[1];
-  return 0;
-}
-
-int Accepter::bind(const entity_addr_t &bind_addr, const set<int>& avoid_ports)
-{
-  const md_config_t *conf = msgr->cct->_conf;
-  // bind to a socket
-  ldout(msgr->cct,10) <<  __func__ << dendl;
-  
-  int family;
-  switch (bind_addr.get_family()) {
-  case AF_INET:
-  case AF_INET6:
-    family = bind_addr.get_family();
-    break;
-
-  default:
-    // bind_addr is empty
-    family = conf->ms_bind_ipv6 ? AF_INET6 : AF_INET;
-  }
-
-  /* socket creation */
-  listen_sd = ::socket(family, SOCK_STREAM, 0);
-  ldout(msgr->cct,10) <<  __func__ << " socket sd: " << listen_sd << dendl;
-  if (listen_sd < 0) {
-    lderr(msgr->cct) << __func__ << " unable to create socket: "
-		     << cpp_strerror(errno) << dendl;
-    return -errno;
-  }
-
-  if (set_close_on_exec(listen_sd)) {
-    lderr(msgr->cct) << __func__ << " unable to set_close_exec(): "
-		     << cpp_strerror(errno) << dendl;
-  }
-  
-
-  // use whatever user specified (if anything)
-  entity_addr_t listen_addr = bind_addr;
-  if (listen_addr.get_type() == entity_addr_t::TYPE_NONE) {
-    listen_addr.set_type(entity_addr_t::TYPE_LEGACY);
-  }
-  listen_addr.set_family(family);
-
-  /* bind to port */
-  int rc = -1;
-  int r = -1;
-
-  for (int i = 0; i < conf->ms_bind_retry_count; i++) {
-
-    if (i > 0) {
-        lderr(msgr->cct) << __func__ << " was unable to bind. Trying again in " 
-			 << conf->ms_bind_retry_delay << " seconds " << dendl;
-        sleep(conf->ms_bind_retry_delay);
-    }
-
-    if (listen_addr.get_port()) {
-        // specific port
-
-        // reuse addr+port when possible
-        int on = 1;
-        rc = ::setsockopt(listen_sd, SOL_SOCKET, SO_REUSEADDR, &on, sizeof(on));
-        if (rc < 0) {
-            lderr(msgr->cct) << __func__ << " unable to setsockopt: "
-                             << cpp_strerror(errno) << dendl;
-            r = -errno;
-            continue;
-        }
-
-        rc = ::bind(listen_sd, listen_addr.get_sockaddr(),
-		    listen_addr.get_sockaddr_len());
-        if (rc < 0) {
-            lderr(msgr->cct) << __func__ << " unable to bind to " << listen_addr
-			     << ": " << cpp_strerror(errno) << dendl;
-            r = -errno;
-            continue;
-        }
-    } else {
-        // try a range of ports
-        for (int port = msgr->cct->_conf->ms_bind_port_min; 
-		port <= msgr->cct->_conf->ms_bind_port_max; port++) {
-            if (avoid_ports.count(port))
-                continue;
-
-            listen_addr.set_port(port);
-            rc = ::bind(listen_sd, listen_addr.get_sockaddr(),
-			listen_addr.get_sockaddr_len());
-            if (rc == 0)
-                break;
-        }
-        if (rc < 0) {
-            lderr(msgr->cct) <<  __func__ << "  unable to bind to " << listen_addr
-                             << " on any port in range " << msgr->cct->_conf->ms_bind_port_min
-                             << "-" << msgr->cct->_conf->ms_bind_port_max
-                             << ": " << cpp_strerror(errno)
-                             << dendl;
-            r = -errno;
-            // Clear port before retry, otherwise we shall fail again.
-            listen_addr.set_port(0); 
-            continue;
-        }
-        ldout(msgr->cct,10) << __func__ << " bound on random port " 
-			    << listen_addr << dendl;
-    }
-
-    if (rc == 0)
-        break;
-  }
-
-  // It seems that binding completely failed, return with that exit status
-  if (rc < 0) {
-      lderr(msgr->cct) << __func__ << " was unable to bind after " 
-		       << conf->ms_bind_retry_count << " attempts: " 
-		       << cpp_strerror(errno) << dendl;
-      ::close(listen_sd);
-      listen_sd = -1;
-      return r;
-  }
-
-  // what port did we get?
-  sockaddr_storage ss;
-  socklen_t llen = sizeof(ss);
-  rc = getsockname(listen_sd, (sockaddr*)&ss, &llen);
-  if (rc < 0) {
-    rc = -errno;
-    lderr(msgr->cct) << __func__ << " failed getsockname: " 
-		     << cpp_strerror(rc) << dendl;
-    ::close(listen_sd);
-    listen_sd = -1;
-    return rc;
-  }
-  listen_addr.set_sockaddr((sockaddr*)&ss);
-  
-  if (msgr->cct->_conf->ms_tcp_rcvbuf) {
-    int size = msgr->cct->_conf->ms_tcp_rcvbuf;
-    rc = ::setsockopt(listen_sd, SOL_SOCKET, SO_RCVBUF, 
-			(void*)&size, sizeof(size));
-    if (rc < 0)  {
-      rc = -errno;
-      lderr(msgr->cct) <<  __func__ << "  failed to set SO_RCVBUF to " 
-		       << size << ": " << cpp_strerror(rc) << dendl;
-      ::close(listen_sd);
-      listen_sd = -1;
-      return rc;
-    }
-  }
-
-  ldout(msgr->cct,10) <<  __func__ << " bound to " << listen_addr << dendl;
-
-  // listen!
-  rc = ::listen(listen_sd, msgr->cct->_conf->ms_tcp_listen_backlog);
-  if (rc < 0) {
-    rc = -errno;
-    lderr(msgr->cct) <<  __func__ << " unable to listen on " << listen_addr
-		     << ": " << cpp_strerror(rc) << dendl;
-    ::close(listen_sd);
-    listen_sd = -1;
-    return rc;
-  }
-  
-  msgr->set_myaddr(bind_addr);
-  if (bind_addr != entity_addr_t())
-    msgr->learned_addr(bind_addr);
-  else
-    assert(msgr->get_need_addr());  // should still be true.
-
-  if (msgr->get_myaddr().get_port() == 0) {
-    msgr->set_myaddr(listen_addr);
-  }
-  entity_addr_t addr = msgr->get_myaddr();
-  addr.nonce = nonce;
-  msgr->set_myaddr(addr);
-
-  msgr->init_local_connection();
-
-  rc = create_selfpipe(&shutdown_rd_fd, &shutdown_wr_fd);
-  if (rc < 0) {
-    lderr(msgr->cct) <<  __func__ << " unable to create signalling pipe " << listen_addr
-		     << ": " << cpp_strerror(rc) << dendl;
-    return rc;
-  }
-
-  ldout(msgr->cct,1) <<  __func__ << " my_inst.addr is " << msgr->get_myaddr()
-		     << " need_addr=" << msgr->get_need_addr() << dendl;
-  return 0;
-}
-
-int Accepter::rebind(const set<int>& avoid_ports)
-{
-  ldout(msgr->cct,1) << __func__ << " avoid " << avoid_ports << dendl;
-  
-  entity_addr_t addr = msgr->get_myaddr();
-  set<int> new_avoid = avoid_ports;
-  new_avoid.insert(addr.get_port());
-  addr.set_port(0);
-
-  // adjust the nonce; we want our entity_addr_t to be truly unique.
-  nonce += 1000000;
-  msgr->my_inst.addr.nonce = nonce;
-  ldout(msgr->cct,10) << __func__ << " new nonce " << nonce << " and inst " 
-			<< msgr->my_inst << dendl;
-
-  ldout(msgr->cct,10) << " will try " << addr << " and avoid ports " << new_avoid << dendl;
-  int r = bind(addr, new_avoid);
-  if (r == 0)
-    start();
-  return r;
-}
-
-int Accepter::start()
-{
-  ldout(msgr->cct,1) << __func__ << dendl;
-
-  // start thread
-  create("ms_accepter");
-
-  return 0;
-}
-
-void *Accepter::entry()
-{
-  ldout(msgr->cct,1) << __func__ << " start" << dendl;
-  
-  int errors = 0;
-  int ch;
-
-  struct pollfd pfd[2];
-  memset(pfd, 0, sizeof(pfd));
-
-  pfd[0].fd = listen_sd;
-  pfd[0].events = POLLIN | POLLERR | POLLNVAL | POLLHUP;
-  pfd[1].fd = shutdown_rd_fd;
-  pfd[1].events = POLLIN | POLLERR | POLLNVAL | POLLHUP;
-  while (!done) {
-    ldout(msgr->cct,20) << __func__ << " calling poll for sd:" << listen_sd << dendl;
-    int r = poll(pfd, 2, -1);
-    if (r < 0) {
-      if (errno == EINTR) {
-        continue;
-      }
-      ldout(msgr->cct,1) << __func__ << " poll got error"  
- 			  << " errno " << errno << " " << cpp_strerror(errno) << dendl;
-      break;
-    }
-    ldout(msgr->cct,10) << __func__ << " poll returned oke: " << r << dendl;
-    ldout(msgr->cct,20) << __func__ <<  " pfd.revents[0]=" << pfd[0].revents << dendl;
-    ldout(msgr->cct,20) << __func__ <<  " pfd.revents[1]=" << pfd[1].revents << dendl;
-
-    if (pfd[0].revents & (POLLERR | POLLNVAL | POLLHUP)) {
-      ldout(msgr->cct,1) << __func__ << " poll got errors in revents "  
- 			 <<  pfd[0].revents << dendl;
-      break;
-    }
-    if (pfd[1].revents & (POLLIN | POLLERR | POLLNVAL | POLLHUP)) {
-      // We got "signaled" to exit the poll
-      // clean the selfpipe
-      if (::read(shutdown_rd_fd, &ch, 1) == -1) {
-        if (errno != EAGAIN)
-          ldout(msgr->cct,1) << __func__ << " Cannot read selfpipe: "
- 			      << " errno " << errno << " " << cpp_strerror(errno) << dendl;
-        }
-      break;
-    }
-    if (done) break;
-
-    // accept
-    sockaddr_storage ss;
-    socklen_t slen = sizeof(ss);
-    int sd = ::accept(listen_sd, (sockaddr*)&ss, &slen);
-    if (sd >= 0) {
-      int r = set_close_on_exec(sd);
-      if (r) {
-	ldout(msgr->cct,1) << __func__ << " set_close_on_exec() failed "
-	      << cpp_strerror(r) << dendl;
-      }
-      errors = 0;
-      ldout(msgr->cct,10) << __func__ << " incoming on sd " << sd << dendl;
-      
-      msgr->add_accept_pipe(sd);
-    } else {
-      ldout(msgr->cct,0) << __func__ << " no incoming connection?  sd = " << sd
-	      << " errno " << errno << " " << cpp_strerror(errno) << dendl;
-      if (++errors > 4)
-	break;
-    }
-  }
-
-  ldout(msgr->cct,20) << __func__ << " closing" << dendl;
-  // socket is closed right after the thread has joined.
-  // closing it here might race
-  if (shutdown_rd_fd >= 0) {
-    ::close(shutdown_rd_fd);
-    shutdown_rd_fd = -1;
-  }
-
-  ldout(msgr->cct,10) << __func__ << " stopping" << dendl;
-  return 0;
-}
-
-void Accepter::stop()
-{
-  done = true;
-  ldout(msgr->cct,10) << __func__ << " accept listening on: " << listen_sd << dendl;
-
-  if (shutdown_wr_fd < 0)
-    return;
-
-  // Send a byte to the shutdown pipe that the thread is listening to
-  char buf[1] = { 0x0 };
-  int ret = safe_write(shutdown_wr_fd, buf, 1);
-  if (ret < 0) {
-    ldout(msgr->cct,1) << __func__ << "close failed: "
-             << " errno " << errno << " " << cpp_strerror(errno) << dendl;
-  } else {
-    ldout(msgr->cct,15) << __func__ << " signaled poll" << dendl;
-  }
-  VOID_TEMP_FAILURE_RETRY(close(shutdown_wr_fd));
-  shutdown_wr_fd = -1;
-
-  // wait for thread to stop before closing the socket, to avoid
-  // racing against fd re-use.
-  if (is_started()) {
-    ldout(msgr->cct,5) << __func__ << " wait for thread to join." << dendl;
-    join();
-  }
-
-  if (listen_sd >= 0) {
-    if (::close(listen_sd) < 0) {
-      ldout(msgr->cct,1) << __func__ << "close listen_sd failed: "
-	      << " errno " << errno << " " << cpp_strerror(errno) << dendl;
-    }
-    listen_sd = -1;
-  }
-  if (shutdown_rd_fd >= 0) {
-    if (::close(shutdown_rd_fd) < 0) {
-      ldout(msgr->cct,1) << __func__ << "close shutdown_rd_fd failed: "
-	      << " errno " << errno << " " << cpp_strerror(errno) << dendl;
-    }
-    shutdown_rd_fd = -1;
-  }
-  done = false;
-}
-
-
-
-
diff --git a/src/msg/simple/Accepter.h b/src/msg/simple/Accepter.h
deleted file mode 100644
index 7824c3a16f0..00000000000
--- a/src/msg/simple/Accepter.h
+++ /dev/null
@@ -1,50 +0,0 @@
-// -*- mode:C++; tab-width:8; c-basic-offset:2; indent-tabs-mode:t -*- 
-// vim: ts=8 sw=2 smarttab
-/*
- * Ceph - scalable distributed file system
- *
- * Copyright (C) 2004-2006 Sage Weil <sage@newdream.net>
- *
- * This is free software; you can redistribute it and/or
- * modify it under the terms of the GNU Lesser General Public
- * License version 2.1, as published by the Free Software 
- * Foundation.  See file COPYING.
- * 
- */
-
-#ifndef CEPH_MSG_ACCEPTER_H
-#define CEPH_MSG_ACCEPTER_H
-
-#include "common/Thread.h"
-
-class SimpleMessenger;
-struct entity_addr_t;
-
-/**
- * If the SimpleMessenger binds to a specific address, the Accepter runs
- * and listens for incoming connections.
- */
-class Accepter : public Thread {
-  SimpleMessenger *msgr;
-  bool done;
-  int listen_sd;
-  uint64_t nonce;
-  int shutdown_rd_fd;
-  int shutdown_wr_fd;
-  int create_selfpipe(int *pipe_rd, int *pipe_wr);
-
-public:
-  Accepter(SimpleMessenger *r, uint64_t n) 
-    : msgr(r), done(false), listen_sd(-1), nonce(n),
-      shutdown_rd_fd(-1), shutdown_wr_fd(-1)
-    {}
-    
-  void *entry() override;
-  void stop();
-  int bind(const entity_addr_t &bind_addr, const set<int>& avoid_ports);
-  int rebind(const set<int>& avoid_port);
-  int start();
-};
-
-
-#endif
diff --git a/src/msg/simple/Pipe.cc b/src/msg/simple/Pipe.cc
deleted file mode 100644
index 4a7ab9acab7..00000000000
--- a/src/msg/simple/Pipe.cc
+++ /dev/null
@@ -1,2757 +0,0 @@
-// -*- mode:C++; tab-width:8; c-basic-offset:2; indent-tabs-mode:t -*- 
-// vim: ts=8 sw=2 smarttab
-/*
- * Ceph - scalable distributed file system
- *
- * Copyright (C) 2004-2006 Sage Weil <sage@newdream.net>
- *
- * This is free software; you can redistribute it and/or
- * modify it under the terms of the GNU Lesser General Public
- * License version 2.1, as published by the Free Software 
- * Foundation.  See file COPYING.
- * 
- */
-
-#include <sys/types.h>
-#include <sys/socket.h>
-#include <netinet/in.h>
-#include <netinet/ip.h>
-#include <netinet/tcp.h>
-#include <sys/uio.h>
-#include <limits.h>
-#include <poll.h>
-
-#include "msg/Message.h"
-#include "Pipe.h"
-#include "SimpleMessenger.h"
-
-#include "common/debug.h"
-#include "common/errno.h"
-#include "common/valgrind.h"
-
-// Below included to get encode_encrypt(); That probably should be in Crypto.h, instead
-
-#include "auth/Crypto.h"
-#include "auth/cephx/CephxProtocol.h"
-#include "auth/AuthSessionHandler.h"
-
-#include "include/sock_compat.h"
-
-// Constant to limit starting sequence number to 2^31.  Nothing special about it, just a big number.  PLR
-#define SEQ_MASK  0x7fffffff 
-#define dout_subsys ceph_subsys_ms
-
-#undef dout_prefix
-#define dout_prefix *_dout << *this
-ostream& Pipe::_pipe_prefix(std::ostream &out) const {
-  return out << "-- " << msgr->get_myinst().addr << " >> " << peer_addr << " pipe(" << this
-	     << " sd=" << sd << " :" << port
-             << " s=" << state
-             << " pgs=" << peer_global_seq
-             << " cs=" << connect_seq
-             << " l=" << policy.lossy
-             << " c=" << connection_state
-             << ").";
-}
-
-ostream& operator<<(ostream &out, const Pipe &pipe) {
-  return pipe._pipe_prefix(out);
-}
-
-/**
- * The DelayedDelivery is for injecting delays into Message delivery off
- * the socket. It is only enabled if delays are requested, and if they
- * are then it pulls Messages off the DelayQueue and puts them into the
- * in_q (SimpleMessenger::dispatch_queue).
- * Please note that this probably has issues with Pipe shutdown and
- * replacement semantics. I've tried, but no guarantees.
- */
-class Pipe::DelayedDelivery: public Thread {
-  Pipe *pipe;
-  std::deque< pair<utime_t,Message*> > delay_queue;
-  Mutex delay_lock;
-  Cond delay_cond;
-  int flush_count;
-  bool active_flush;
-  bool stop_delayed_delivery;
-  bool delay_dispatching; // we are in fast dispatch now
-  bool stop_fast_dispatching_flag; // we need to stop fast dispatching
-
-public:
-  explicit DelayedDelivery(Pipe *p)
-    : pipe(p),
-      delay_lock("Pipe::DelayedDelivery::delay_lock"), flush_count(0),
-      active_flush(false),
-      stop_delayed_delivery(false),
-      delay_dispatching(false),
-      stop_fast_dispatching_flag(false) { }
-  ~DelayedDelivery() override {
-    discard();
-  }
-  void *entry() override;
-  void queue(utime_t release, Message *m) {
-    Mutex::Locker l(delay_lock);
-    delay_queue.push_back(make_pair(release, m));
-    delay_cond.Signal();
-  }
-  void discard();
-  void flush();
-  bool is_flushing() {
-    Mutex::Locker l(delay_lock);
-    return flush_count > 0 || active_flush;
-  }
-  void wait_for_flush() {
-    Mutex::Locker l(delay_lock);
-    while (flush_count > 0 || active_flush)
-      delay_cond.Wait(delay_lock);
-  }
-  void stop() {
-    delay_lock.Lock();
-    stop_delayed_delivery = true;
-    delay_cond.Signal();
-    delay_lock.Unlock();
-  }
-  void steal_for_pipe(Pipe *new_owner) {
-    Mutex::Locker l(delay_lock);
-    pipe = new_owner;
-  }
-  /**
-   * We need to stop fast dispatching before we need to stop putting
-   * normal messages into the DispatchQueue.
-   */
-  void stop_fast_dispatching();
-};
-
-/**************************************
- * Pipe
- */
-
-Pipe::Pipe(SimpleMessenger *r, int st, PipeConnection *con)
-  : RefCountedObject(r->cct),
-    reader_thread(this),
-    writer_thread(this),
-    delay_thread(NULL),
-    msgr(r),
-    conn_id(r->dispatch_queue.get_id()),
-    recv_ofs(0),
-    recv_len(0),
-    sd(-1), port(0),
-    peer_type(-1),
-    pipe_lock("SimpleMessenger::Pipe::pipe_lock"),
-    state(st),
-    connection_state(NULL),
-    reader_running(false), reader_needs_join(false),
-    reader_dispatching(false), notify_on_dispatch_done(false),
-    writer_running(false),
-    in_q(&(r->dispatch_queue)),
-    send_keepalive(false),
-    send_keepalive_ack(false),
-    connect_seq(0), peer_global_seq(0),
-    out_seq(0), in_seq(0), in_seq_acked(0) {
-  ANNOTATE_BENIGN_RACE_SIZED(&sd, sizeof(sd), "Pipe socket");
-  ANNOTATE_BENIGN_RACE_SIZED(&state, sizeof(state), "Pipe state");
-  ANNOTATE_BENIGN_RACE_SIZED(&recv_len, sizeof(recv_len), "Pipe recv_len");
-  ANNOTATE_BENIGN_RACE_SIZED(&recv_ofs, sizeof(recv_ofs), "Pipe recv_ofs");
-  if (con) {
-    connection_state = con;
-    connection_state->reset_pipe(this);
-  } else {
-    connection_state = new PipeConnection(msgr->cct, msgr);
-    connection_state->pipe = get();
-  }
-
-  if (randomize_out_seq()) {
-    lsubdout(msgr->cct,ms,15) << "Pipe(): Could not get random bytes to set seq number for session reset; set seq number to " << out_seq << dendl;
-  }
-    
-
-  msgr->timeout = msgr->cct->_conf->ms_tcp_read_timeout * 1000; //convert to ms
-  if (msgr->timeout == 0)
-    msgr->timeout = -1;
-
-  recv_max_prefetch = msgr->cct->_conf->ms_tcp_prefetch_max_size;
-  recv_buf = new char[recv_max_prefetch];
-}
-
-Pipe::~Pipe()
-{
-  assert(out_q.empty());
-  assert(sent.empty());
-  delete delay_thread;
-  delete[] recv_buf;
-}
-
-void Pipe::handle_ack(uint64_t seq)
-{
-  lsubdout(msgr->cct, ms, 15) << "reader got ack seq " << seq << dendl;
-  // trim sent list
-  while (!sent.empty() &&
-	 sent.front()->get_seq() <= seq) {
-    Message *m = sent.front();
-    sent.pop_front();
-    lsubdout(msgr->cct, ms, 10) << "reader got ack seq "
-				<< seq << " >= " << m->get_seq() << " on " << m << " " << *m << dendl;
-    m->put();
-  }
-}
-
-void Pipe::start_reader()
-{
-  assert(pipe_lock.is_locked());
-  assert(!reader_running);
-  if (reader_needs_join) {
-    reader_thread.join();
-    reader_needs_join = false;
-  }
-  reader_running = true;
-  reader_thread.create("ms_pipe_read", msgr->cct->_conf->ms_rwthread_stack_bytes);
-}
-
-void Pipe::maybe_start_delay_thread()
-{
-  if (!delay_thread) {
-    auto pos = msgr->cct->_conf->get_val<std::string>("ms_inject_delay_type").find(ceph_entity_type_name(connection_state->peer_type));
-    if (pos != string::npos) {
-      lsubdout(msgr->cct, ms, 1) << "setting up a delay queue on Pipe " << this << dendl;
-      delay_thread = new DelayedDelivery(this);
-      delay_thread->create("ms_pipe_delay");
-    }
-  }
-}
-
-void Pipe::start_writer()
-{
-  assert(pipe_lock.is_locked());
-  assert(!writer_running);
-  writer_running = true;
-  writer_thread.create("ms_pipe_write", msgr->cct->_conf->ms_rwthread_stack_bytes);
-}
-
-void Pipe::join_reader()
-{
-  if (!reader_running)
-    return;
-  cond.Signal();
-  pipe_lock.Unlock();
-  reader_thread.join();
-  pipe_lock.Lock();
-  reader_needs_join = false;
-}
-
-void Pipe::DelayedDelivery::discard()
-{
-  lgeneric_subdout(pipe->msgr->cct, ms, 20) << *pipe << "DelayedDelivery::discard" << dendl;
-  Mutex::Locker l(delay_lock);
-  while (!delay_queue.empty()) {
-    Message *m = delay_queue.front().second;
-    pipe->in_q->dispatch_throttle_release(m->get_dispatch_throttle_size());
-    m->put();
-    delay_queue.pop_front();
-  }
-}
-
-void Pipe::DelayedDelivery::flush()
-{
-  lgeneric_subdout(pipe->msgr->cct, ms, 20) << *pipe << "DelayedDelivery::flush" << dendl;
-  Mutex::Locker l(delay_lock);
-  flush_count = delay_queue.size();
-  delay_cond.Signal();
-}
-
-void *Pipe::DelayedDelivery::entry()
-{
-  Mutex::Locker locker(delay_lock);
-  lgeneric_subdout(pipe->msgr->cct, ms, 20) << *pipe << "DelayedDelivery::entry start" << dendl;
-
-  while (!stop_delayed_delivery) {
-    if (delay_queue.empty()) {
-      lgeneric_subdout(pipe->msgr->cct, ms, 30) << *pipe << "DelayedDelivery::entry sleeping on delay_cond because delay queue is empty" << dendl;
-      delay_cond.Wait(delay_lock);
-      continue;
-    }
-    utime_t release = delay_queue.front().first;
-    Message *m = delay_queue.front().second;
-    string delay_msg_type = pipe->msgr->cct->_conf->ms_inject_delay_msg_type;
-    if (!flush_count &&
-        (release > ceph_clock_now() &&
-         (delay_msg_type.empty() || m->get_type_name() == delay_msg_type))) {
-      lgeneric_subdout(pipe->msgr->cct, ms, 10) << *pipe << "DelayedDelivery::entry sleeping on delay_cond until " << release << dendl;
-      delay_cond.WaitUntil(delay_lock, release);
-      continue;
-    }
-    lgeneric_subdout(pipe->msgr->cct, ms, 10) << *pipe << "DelayedDelivery::entry dequeuing message " << m << " for delivery, past " << release << dendl;
-    delay_queue.pop_front();
-    if (flush_count > 0) {
-      --flush_count;
-      active_flush = true;
-    }
-    if (pipe->in_q->can_fast_dispatch(m)) {
-      if (!stop_fast_dispatching_flag) {
-        delay_dispatching = true;
-        delay_lock.Unlock();
-        pipe->in_q->fast_dispatch(m);
-        delay_lock.Lock();
-        delay_dispatching = false;
-        if (stop_fast_dispatching_flag) {
-          // we need to let the stopping thread proceed
-          delay_cond.Signal();
-          delay_lock.Unlock();
-          delay_lock.Lock();
-        }
-      }
-    } else {
-      pipe->in_q->enqueue(m, m->get_priority(), pipe->conn_id);
-    }
-    active_flush = false;
-  }
-  lgeneric_subdout(pipe->msgr->cct, ms, 20) << *pipe << "DelayedDelivery::entry stop" << dendl;
-  return NULL;
-}
-
-void Pipe::DelayedDelivery::stop_fast_dispatching() {
-  Mutex::Locker l(delay_lock);
-  stop_fast_dispatching_flag = true;
-  while (delay_dispatching)
-    delay_cond.Wait(delay_lock);
-}
-
-
-int Pipe::accept()
-{
-  ldout(msgr->cct,10) << "accept" << dendl;
-  assert(pipe_lock.is_locked());
-  assert(state == STATE_ACCEPTING);
-
-  pipe_lock.Unlock();
-
-  // vars
-  bufferlist addrs;
-  entity_addr_t socket_addr;
-  socklen_t len;
-  int r;
-  char banner[strlen(CEPH_BANNER)+1];
-  bufferlist addrbl;
-  ceph_msg_connect connect;
-  ceph_msg_connect_reply reply;
-  Pipe *existing = 0;
-  bufferptr bp;
-  bufferlist authorizer, authorizer_reply;
-  bool authorizer_valid;
-  uint64_t feat_missing;
-  bool replaced = false;
-  // this variable denotes if the connection attempt from peer is a hard 
-  // reset or not, it is true if there is an existing connection and the
-  // connection sequence from peer is equal to zero
-  bool is_reset_from_peer = false;
-  CryptoKey session_key;
-  int removed; // single-use down below
-
-  // this should roughly mirror pseudocode at
-  //  http://ceph.com/wiki/Messaging_protocol
-  int reply_tag = 0;
-  uint64_t existing_seq = -1;
-
-  // used for reading in the remote acked seq on connect
-  uint64_t newly_acked_seq = 0;
-
-  recv_reset();
-
-  set_socket_options();
-
-  // announce myself.
-  r = tcp_write(CEPH_BANNER, strlen(CEPH_BANNER));
-  if (r < 0) {
-    ldout(msgr->cct,10) << "accept couldn't write banner" << dendl;
-    goto fail_unlocked;
-  }
-
-  // and my addr
-  ::encode(msgr->my_inst.addr, addrs, 0);  // legacy
-
-  port = msgr->my_inst.addr.get_port();
-
-  // and peer's socket addr (they might not know their ip)
-  sockaddr_storage ss;
-  len = sizeof(ss);
-  r = ::getpeername(sd, (sockaddr*)&ss, &len);
-  if (r < 0) {
-    ldout(msgr->cct,0) << "accept failed to getpeername " << cpp_strerror(errno) << dendl;
-    goto fail_unlocked;
-  }
-  socket_addr.set_sockaddr((sockaddr*)&ss);
-  ::encode(socket_addr, addrs, 0);  // legacy
-
-  r = tcp_write(addrs.c_str(), addrs.length());
-  if (r < 0) {
-    ldout(msgr->cct,10) << "accept couldn't write my+peer addr" << dendl;
-    goto fail_unlocked;
-  }
-
-  ldout(msgr->cct,1) << "accept sd=" << sd << " " << socket_addr << dendl;
-  
-  // identify peer
-  if (tcp_read(banner, strlen(CEPH_BANNER)) < 0) {
-    ldout(msgr->cct,10) << "accept couldn't read banner" << dendl;
-    goto fail_unlocked;
-  }
-  if (memcmp(banner, CEPH_BANNER, strlen(CEPH_BANNER))) {
-    banner[strlen(CEPH_BANNER)] = 0;
-    ldout(msgr->cct,1) << "accept peer sent bad banner '" << banner << "' (should be '" << CEPH_BANNER << "')" << dendl;
-    goto fail_unlocked;
-  }
-  {
-    bufferptr tp(sizeof(ceph_entity_addr));
-    addrbl.push_back(std::move(tp));
-  }
-  if (tcp_read(addrbl.c_str(), addrbl.length()) < 0) {
-    ldout(msgr->cct,10) << "accept couldn't read peer_addr" << dendl;
-    goto fail_unlocked;
-  }
-  {
-    bufferlist::iterator ti = addrbl.begin();
-    ::decode(peer_addr, ti);
-  }
-
-  ldout(msgr->cct,10) << "accept peer addr is " << peer_addr << dendl;
-  if (peer_addr.is_blank_ip()) {
-    // peer apparently doesn't know what ip they have; figure it out for them.
-    int port = peer_addr.get_port();
-    peer_addr.u = socket_addr.u;
-    peer_addr.set_port(port);
-    ldout(msgr->cct,0) << "accept peer addr is really " << peer_addr
-	    << " (socket is " << socket_addr << ")" << dendl;
-  }
-  set_peer_addr(peer_addr);  // so that connection_state gets set up
-  
-  while (1) {
-    if (tcp_read((char*)&connect, sizeof(connect)) < 0) {
-      ldout(msgr->cct,10) << "accept couldn't read connect" << dendl;
-      goto fail_unlocked;
-    }
-
-    authorizer.clear();
-    if (connect.authorizer_len) {
-      bp = buffer::create(connect.authorizer_len);
-      if (tcp_read(bp.c_str(), connect.authorizer_len) < 0) {
-        ldout(msgr->cct,10) << "accept couldn't read connect authorizer" << dendl;
-        goto fail_unlocked;
-      }
-      authorizer.push_back(std::move(bp));
-      authorizer_reply.clear();
-    }
-
-    ldout(msgr->cct,20) << "accept got peer connect_seq " << connect.connect_seq
-	     << " global_seq " << connect.global_seq
-	     << dendl;
-    
-    msgr->lock.Lock();   // FIXME
-    pipe_lock.Lock();
-    if (msgr->dispatch_queue.stop)
-      goto shutting_down;
-    if (state != STATE_ACCEPTING) {
-      goto shutting_down;
-    }
-
-    // note peer's type, flags
-    set_peer_type(connect.host_type);
-    policy = msgr->get_policy(connect.host_type);
-    ldout(msgr->cct,10) << "accept of host_type " << connect.host_type
-			<< ", policy.lossy=" << policy.lossy
-			<< " policy.server=" << policy.server
-			<< " policy.standby=" << policy.standby
-			<< " policy.resetcheck=" << policy.resetcheck
-			<< dendl;
-
-    memset(&reply, 0, sizeof(reply));
-    reply.protocol_version = msgr->get_proto_version(peer_type, false);
-    msgr->lock.Unlock();
-
-    // mismatch?
-    ldout(msgr->cct,10) << "accept my proto " << reply.protocol_version
-	     << ", their proto " << connect.protocol_version << dendl;
-    if (connect.protocol_version != reply.protocol_version) {
-      reply.tag = CEPH_MSGR_TAG_BADPROTOVER;
-      goto reply;
-    }
-
-    // require signatures for cephx?
-    if (connect.authorizer_protocol == CEPH_AUTH_CEPHX) {
-      if (peer_type == CEPH_ENTITY_TYPE_OSD ||
-	  peer_type == CEPH_ENTITY_TYPE_MDS) {
-	if (msgr->cct->_conf->cephx_require_signatures ||
-	    msgr->cct->_conf->cephx_cluster_require_signatures) {
-	  ldout(msgr->cct,10) << "using cephx, requiring MSG_AUTH feature bit for cluster" << dendl;
-	  policy.features_required |= CEPH_FEATURE_MSG_AUTH;
-	}
-      } else {
-	if (msgr->cct->_conf->cephx_require_signatures ||
-	    msgr->cct->_conf->cephx_service_require_signatures) {
-	  ldout(msgr->cct,10) << "using cephx, requiring MSG_AUTH feature bit for service" << dendl;
-	  policy.features_required |= CEPH_FEATURE_MSG_AUTH;
-	}
-      }
-    }
-
-    feat_missing = policy.features_required & ~(uint64_t)connect.features;
-    if (feat_missing) {
-      ldout(msgr->cct,1) << "peer missing required features " << std::hex << feat_missing << std::dec << dendl;
-      reply.tag = CEPH_MSGR_TAG_FEATURES;
-      goto reply;
-    }
-    
-    // Check the authorizer.  If not good, bail out.
-
-    pipe_lock.Unlock();
-
-    if (!msgr->verify_authorizer(connection_state.get(), peer_type, connect.authorizer_protocol, authorizer,
-				 authorizer_reply, authorizer_valid, session_key) ||
-	!authorizer_valid) {
-      ldout(msgr->cct,0) << "accept: got bad authorizer" << dendl;
-      pipe_lock.Lock();
-      if (state != STATE_ACCEPTING)
-	goto shutting_down_msgr_unlocked;
-      reply.tag = CEPH_MSGR_TAG_BADAUTHORIZER;
-      session_security.reset();
-      goto reply;
-    } 
-
-    // We've verified the authorizer for this pipe, so set up the session security structure.  PLR
-
-    ldout(msgr->cct,10) << "accept:  setting up session_security." << dendl;
-
-  retry_existing_lookup:
-    msgr->lock.Lock();
-    pipe_lock.Lock();
-    if (msgr->dispatch_queue.stop)
-      goto shutting_down;
-    if (state != STATE_ACCEPTING)
-      goto shutting_down;
-    
-    // existing?
-    existing = msgr->_lookup_pipe(peer_addr);
-    if (existing) {
-      existing->pipe_lock.Lock(true);  // skip lockdep check (we are locking a second Pipe here)
-      if (existing->reader_dispatching) {
-	/** we need to wait, or we can deadlock if downstream
-	 *  fast_dispatchers are (naughtily!) waiting on resources
-	 *  held by somebody trying to make use of the SimpleMessenger lock.
-	 *  So drop locks, wait, and retry. It just looks like a slow network
-	 *  to everybody else.
-	 *
-	 *  We take a ref to existing here since it might get reaped before we
-	 *  wake up (see bug #15870).  We can be confident that it lived until
-	 *  locked it since we held the msgr lock from _lookup_pipe through to
-	 *  locking existing->lock and checking reader_dispatching.
-	 */
-	existing->get();
-	pipe_lock.Unlock();
-	msgr->lock.Unlock();
-	existing->notify_on_dispatch_done = true;
-	while (existing->reader_dispatching)
-	  existing->cond.Wait(existing->pipe_lock);
-	existing->pipe_lock.Unlock();
-	existing->put();
-	existing = nullptr;
-	goto retry_existing_lookup;
-      }
-
-      if (connect.global_seq < existing->peer_global_seq) {
-	ldout(msgr->cct,10) << "accept existing " << existing << ".gseq " << existing->peer_global_seq
-		 << " > " << connect.global_seq << ", RETRY_GLOBAL" << dendl;
-	reply.tag = CEPH_MSGR_TAG_RETRY_GLOBAL;
-	reply.global_seq = existing->peer_global_seq;  // so we can send it below..
-	existing->pipe_lock.Unlock();
-	msgr->lock.Unlock();
-	goto reply;
-      } else {
-	ldout(msgr->cct,10) << "accept existing " << existing << ".gseq " << existing->peer_global_seq
-		 << " <= " << connect.global_seq << ", looks ok" << dendl;
-      }
-      
-      if (existing->policy.lossy) {
-	ldout(msgr->cct,0) << "accept replacing existing (lossy) channel (new one lossy="
-	        << policy.lossy << ")" << dendl;
-	existing->was_session_reset();
-	goto replace;
-      }
-
-      ldout(msgr->cct,0) << "accept connect_seq " << connect.connect_seq
-			 << " vs existing " << existing->connect_seq
-			 << " state " << existing->get_state_name() << dendl;
-
-      if (connect.connect_seq == 0 && existing->connect_seq > 0) {
-	ldout(msgr->cct,0) << "accept peer reset, then tried to connect to us, replacing" << dendl;
-        // this is a hard reset from peer
-        is_reset_from_peer = true;
-	if (policy.resetcheck)
-	  existing->was_session_reset(); // this resets out_queue, msg_ and connect_seq #'s
-	goto replace;
-      }
-
-      if (connect.connect_seq < existing->connect_seq) {
-	// old attempt, or we sent READY but they didn't get it.
-	ldout(msgr->cct,10) << "accept existing " << existing << ".cseq " << existing->connect_seq
-			    << " > " << connect.connect_seq << ", RETRY_SESSION" << dendl;
-	goto retry_session;
-      }
-
-      if (connect.connect_seq == existing->connect_seq) {
-	// if the existing connection successfully opened, and/or
-	// subsequently went to standby, then the peer should bump
-	// their connect_seq and retry: this is not a connection race
-	// we need to resolve here.
-	if (existing->state == STATE_OPEN ||
-	    existing->state == STATE_STANDBY) {
-	  ldout(msgr->cct,10) << "accept connection race, existing " << existing
-			      << ".cseq " << existing->connect_seq
-			      << " == " << connect.connect_seq
-			      << ", OPEN|STANDBY, RETRY_SESSION" << dendl;
-	  goto retry_session;
-	}
-
-	// connection race?
-	if (peer_addr < msgr->my_inst.addr ||
-	    existing->policy.server) {
-	  // incoming wins
-	  ldout(msgr->cct,10) << "accept connection race, existing " << existing << ".cseq " << existing->connect_seq
-		   << " == " << connect.connect_seq << ", or we are server, replacing my attempt" << dendl;
-	  if (!(existing->state == STATE_CONNECTING ||
-		existing->state == STATE_WAIT))
-	    lderr(msgr->cct) << "accept race bad state, would replace, existing="
-			     << existing->get_state_name()
-			     << " " << existing << ".cseq=" << existing->connect_seq
-			     << " == " << connect.connect_seq
-			     << dendl;
-	  assert(existing->state == STATE_CONNECTING ||
-		 existing->state == STATE_WAIT);
-	  goto replace;
-	} else {
-	  // our existing outgoing wins
-	  ldout(msgr->cct,10) << "accept connection race, existing " << existing << ".cseq " << existing->connect_seq
-		   << " == " << connect.connect_seq << ", sending WAIT" << dendl;
-	  assert(peer_addr > msgr->my_inst.addr);
-	  if (!(existing->state == STATE_CONNECTING))
-	    lderr(msgr->cct) << "accept race bad state, would send wait, existing="
-			     << existing->get_state_name()
-			     << " " << existing << ".cseq=" << existing->connect_seq
-			     << " == " << connect.connect_seq
-			     << dendl;
-	  assert(existing->state == STATE_CONNECTING);
-	  // make sure our outgoing connection will follow through
-	  existing->_send_keepalive();
-	  reply.tag = CEPH_MSGR_TAG_WAIT;
-	  existing->pipe_lock.Unlock();
-	  msgr->lock.Unlock();
-	  goto reply;
-	}
-      }
-
-      assert(connect.connect_seq > existing->connect_seq);
-      assert(connect.global_seq >= existing->peer_global_seq);
-      if (policy.resetcheck &&   // RESETSESSION only used by servers; peers do not reset each other
-	  existing->connect_seq == 0) {
-	ldout(msgr->cct,0) << "accept we reset (peer sent cseq " << connect.connect_seq 
-		 << ", " << existing << ".cseq = " << existing->connect_seq
-		 << "), sending RESETSESSION" << dendl;
-	reply.tag = CEPH_MSGR_TAG_RESETSESSION;
-	msgr->lock.Unlock();
-	existing->pipe_lock.Unlock();
-	goto reply;
-      }
-
-      // reconnect
-      ldout(msgr->cct,10) << "accept peer sent cseq " << connect.connect_seq
-	       << " > " << existing->connect_seq << dendl;
-      goto replace;
-    } // existing
-    else if (connect.connect_seq > 0) {
-      // we reset, and they are opening a new session
-      ldout(msgr->cct,0) << "accept we reset (peer sent cseq " << connect.connect_seq << "), sending RESETSESSION" << dendl;
-      msgr->lock.Unlock();
-      reply.tag = CEPH_MSGR_TAG_RESETSESSION;
-      goto reply;
-    } else {
-      // new session
-      ldout(msgr->cct,10) << "accept new session" << dendl;
-      existing = NULL;
-      goto open;
-    }
-    ceph_abort();
-
-  retry_session:
-    assert(existing->pipe_lock.is_locked());
-    assert(pipe_lock.is_locked());
-    reply.tag = CEPH_MSGR_TAG_RETRY_SESSION;
-    reply.connect_seq = existing->connect_seq + 1;
-    existing->pipe_lock.Unlock();
-    msgr->lock.Unlock();
-    goto reply;    
-
-  reply:
-    assert(pipe_lock.is_locked());
-    reply.features = ((uint64_t)connect.features & policy.features_supported) | policy.features_required;
-    reply.authorizer_len = authorizer_reply.length();
-    pipe_lock.Unlock();
-    r = tcp_write((char*)&reply, sizeof(reply));
-    if (r < 0)
-      goto fail_unlocked;
-    if (reply.authorizer_len) {
-      r = tcp_write(authorizer_reply.c_str(), authorizer_reply.length());
-      if (r < 0)
-	goto fail_unlocked;
-    }
-  }
-  
- replace:
-  assert(existing->pipe_lock.is_locked());
-  assert(pipe_lock.is_locked());
-  // if it is a hard reset from peer, we don't need a round-trip to negotiate in/out sequence
-  if ((connect.features & CEPH_FEATURE_RECONNECT_SEQ) && !is_reset_from_peer) {
-    reply_tag = CEPH_MSGR_TAG_SEQ;
-    existing_seq = existing->in_seq;
-  }
-  ldout(msgr->cct,10) << "accept replacing " << existing << dendl;
-  existing->stop();
-  existing->unregister_pipe();
-  replaced = true;
-
-  if (existing->policy.lossy) {
-    // disconnect from the Connection
-    assert(existing->connection_state);
-    if (existing->connection_state->clear_pipe(existing))
-      msgr->dispatch_queue.queue_reset(existing->connection_state.get());
-  } else {
-    // queue a reset on the new connection, which we're dumping for the old
-    msgr->dispatch_queue.queue_reset(connection_state.get());
-
-    // drop my Connection, and take a ref to the existing one. do not
-    // clear existing->connection_state, since read_message and
-    // write_message both dereference it without pipe_lock.
-    connection_state = existing->connection_state;
-
-    // make existing Connection reference us
-    connection_state->reset_pipe(this);
-
-    if (existing->delay_thread) {
-      existing->delay_thread->steal_for_pipe(this);
-      delay_thread = existing->delay_thread;
-      existing->delay_thread = NULL;
-      delay_thread->flush();
-    }
-
-    // steal incoming queue
-    uint64_t replaced_conn_id = conn_id;
-    conn_id = existing->conn_id;
-    existing->conn_id = replaced_conn_id;
-
-    // reset the in_seq if this is a hard reset from peer,
-    // otherwise we respect our original connection's value
-    in_seq = is_reset_from_peer ? 0 : existing->in_seq;
-    in_seq_acked = in_seq;
-
-    // steal outgoing queue and out_seq
-    existing->requeue_sent();
-    out_seq = existing->out_seq;
-    ldout(msgr->cct,10) << "accept re-queuing on out_seq " << out_seq << " in_seq " << in_seq << dendl;
-    for (map<int, list<Message*> >::iterator p = existing->out_q.begin();
-         p != existing->out_q.end();
-         ++p)
-      out_q[p->first].splice(out_q[p->first].begin(), p->second);
-  }
-  existing->stop_and_wait();
-  existing->pipe_lock.Unlock();
-
- open:
-  // open
-  assert(pipe_lock.is_locked());
-  connect_seq = connect.connect_seq + 1;
-  peer_global_seq = connect.global_seq;
-  assert(state == STATE_ACCEPTING);
-  state = STATE_OPEN;
-  ldout(msgr->cct,10) << "accept success, connect_seq = " << connect_seq << ", sending READY" << dendl;
-
-  // send READY reply
-  reply.tag = (reply_tag ? reply_tag : CEPH_MSGR_TAG_READY);
-  reply.features = policy.features_supported;
-  reply.global_seq = msgr->get_global_seq();
-  reply.connect_seq = connect_seq;
-  reply.flags = 0;
-  reply.authorizer_len = authorizer_reply.length();
-  if (policy.lossy)
-    reply.flags = reply.flags | CEPH_MSG_CONNECT_LOSSY;
-
-  connection_state->set_features((uint64_t)reply.features & (uint64_t)connect.features);
-  ldout(msgr->cct,10) << "accept features " << connection_state->get_features() << dendl;
-
-  session_security.reset(
-      get_auth_session_handler(msgr->cct,
-			       connect.authorizer_protocol,
-			       session_key,
-			       connection_state->get_features()));
-
-  // notify
-  msgr->dispatch_queue.queue_accept(connection_state.get());
-  msgr->ms_deliver_handle_fast_accept(connection_state.get());
-
-  // ok!
-  if (msgr->dispatch_queue.stop)
-    goto shutting_down;
-  removed = msgr->accepting_pipes.erase(this);
-  assert(removed == 1);
-  register_pipe();
-  msgr->lock.Unlock();
-  pipe_lock.Unlock();
-
-  r = tcp_write((char*)&reply, sizeof(reply));
-  if (r < 0) {
-    goto fail_registered;
-  }
-
-  if (reply.authorizer_len) {
-    r = tcp_write(authorizer_reply.c_str(), authorizer_reply.length());
-    if (r < 0) {
-      goto fail_registered;
-    }
-  }
-
-  if (reply_tag == CEPH_MSGR_TAG_SEQ) {
-    if (tcp_write((char*)&existing_seq, sizeof(existing_seq)) < 0) {
-      ldout(msgr->cct,2) << "accept write error on in_seq" << dendl;
-      goto fail_registered;
-    }
-    if (tcp_read((char*)&newly_acked_seq, sizeof(newly_acked_seq)) < 0) {
-      ldout(msgr->cct,2) << "accept read error on newly_acked_seq" << dendl;
-      goto fail_registered;
-    }
-  }
-
-  pipe_lock.Lock();
-  discard_requeued_up_to(newly_acked_seq);
-  if (state != STATE_CLOSED) {
-    ldout(msgr->cct,10) << "accept starting writer, state " << get_state_name() << dendl;
-    start_writer();
-  }
-  ldout(msgr->cct,20) << "accept done" << dendl;
-
-  maybe_start_delay_thread();
-
-  return 0;   // success.
-
- fail_registered:
-  ldout(msgr->cct, 10) << "accept fault after register" << dendl;
-
-  if (msgr->cct->_conf->ms_inject_internal_delays) {
-    ldout(msgr->cct, 10) << " sleep for " << msgr->cct->_conf->ms_inject_internal_delays << dendl;
-    utime_t t;
-    t.set_from_double(msgr->cct->_conf->ms_inject_internal_delays);
-    t.sleep();
-  }
-
- fail_unlocked:
-  pipe_lock.Lock();
-  if (state != STATE_CLOSED) {
-    bool queued = is_queued();
-    ldout(msgr->cct, 10) << "  queued = " << (int)queued << dendl;
-    if (queued) {
-      state = policy.server ? STATE_STANDBY : STATE_CONNECTING;
-    } else if (replaced) {
-      state = STATE_STANDBY;
-    } else {
-      state = STATE_CLOSED;
-      state_closed = true;
-    }
-    fault();
-    if (queued || replaced)
-      start_writer();
-  }
-  return -1;
-
- shutting_down:
-  msgr->lock.Unlock();
- shutting_down_msgr_unlocked:
-  assert(pipe_lock.is_locked());
-
-  if (msgr->cct->_conf->ms_inject_internal_delays) {
-    ldout(msgr->cct, 10) << " sleep for " << msgr->cct->_conf->ms_inject_internal_delays << dendl;
-    utime_t t;
-    t.set_from_double(msgr->cct->_conf->ms_inject_internal_delays);
-    t.sleep();
-  }
-
-  state = STATE_CLOSED;
-  state_closed = true;
-  fault();
-  return -1;
-}
-
-void Pipe::set_socket_options()
-{
-  // disable Nagle algorithm?
-  if (msgr->cct->_conf->ms_tcp_nodelay) {
-    int flag = 1;
-    int r = ::setsockopt(sd, IPPROTO_TCP, TCP_NODELAY, (char*)&flag, sizeof(flag));
-    if (r < 0) {
-      r = -errno;
-      ldout(msgr->cct,0) << "couldn't set TCP_NODELAY: "
-                         << cpp_strerror(r) << dendl;
-    }
-  }
-  if (msgr->cct->_conf->ms_tcp_rcvbuf) {
-    int size = msgr->cct->_conf->ms_tcp_rcvbuf;
-    int r = ::setsockopt(sd, SOL_SOCKET, SO_RCVBUF, (void*)&size, sizeof(size));
-    if (r < 0)  {
-      r = -errno;
-      ldout(msgr->cct,0) << "couldn't set SO_RCVBUF to " << size
-                         << ": " << cpp_strerror(r) << dendl;
-    }
-  }
-
-  // block ESIGPIPE
-#if defined(SO_NOSIGPIPE)
-  int val = 1;
-  int r = ::setsockopt(sd, SOL_SOCKET, SO_NOSIGPIPE, (void*)&val, sizeof(val));
-  if (r) {
-    r = -errno;
-    ldout(msgr->cct,0) << "couldn't set SO_NOSIGPIPE: "
-                       << cpp_strerror(r) << dendl;
-  }
-#endif
-
-#ifdef SO_PRIORITY
-  int prio = msgr->get_socket_priority();
-  if (prio >= 0) {
-    int r = -1;
-#ifdef IPTOS_CLASS_CS6
-    int iptos = IPTOS_CLASS_CS6;
-    int addr_family = 0;
-    if (!peer_addr.is_blank_ip()) {
-      addr_family = peer_addr.get_family();
-    } else {
-      addr_family = msgr->get_myaddr().get_family();
-    }
-    switch (addr_family) {
-    case AF_INET:
-      r = ::setsockopt(sd, IPPROTO_IP, IP_TOS, &iptos, sizeof(iptos));
-      break;
-    case AF_INET6:
-      r = ::setsockopt(sd, IPPROTO_IPV6, IPV6_TCLASS, &iptos, sizeof(iptos));
-      break;
-    default:
-      lderr(msgr->cct) << "couldn't set ToS of unknown family ("
-		       << addr_family << ")"
-		       << " to " << iptos << dendl;
-      return;
-    }
-    if (r < 0) {
-      r = -errno;
-      ldout(msgr->cct,0) << "couldn't set TOS to " << iptos
-			 << ": " << cpp_strerror(r) << dendl;
-    }
-#endif
-    // setsockopt(IPTOS_CLASS_CS6) sets the priority of the socket as 0.
-    // See http://goo.gl/QWhvsD and http://goo.gl/laTbjT
-    // We need to call setsockopt(SO_PRIORITY) after it.
-    r = ::setsockopt(sd, SOL_SOCKET, SO_PRIORITY, &prio, sizeof(prio));
-    if (r < 0) {
-      r = -errno;
-      ldout(msgr->cct,0) << "couldn't set SO_PRIORITY to " << prio
-                         << ": " << cpp_strerror(r) << dendl;
-    }
-  }
-#endif
-}
-
-int Pipe::connect()
-{
-  bool got_bad_auth = false;
-
-  ldout(msgr->cct,10) << "connect " << connect_seq << dendl;
-  assert(pipe_lock.is_locked());
-
-  __u32 cseq = connect_seq;
-  __u32 gseq = msgr->get_global_seq();
-
-  // stop reader thread
-  join_reader();
-
-  pipe_lock.Unlock();
-  
-  char tag = -1;
-  int rc = -1;
-  struct msghdr msg;
-  struct iovec msgvec[2];
-  int msglen;
-  char banner[strlen(CEPH_BANNER) + 1];  // extra byte makes coverity happy
-  entity_addr_t paddr;
-  entity_addr_t peer_addr_for_me, socket_addr;
-  AuthAuthorizer *authorizer = NULL;
-  bufferlist addrbl, myaddrbl;
-  const md_config_t *conf = msgr->cct->_conf;
-
-  // close old socket.  this is safe because we stopped the reader thread above.
-  if (sd >= 0)
-    ::close(sd);
-
-  // create socket?
-  sd = ::socket(peer_addr.get_family(), SOCK_STREAM, 0);
-  if (sd < 0) {
-    rc = -errno;
-    lderr(msgr->cct) << "connect couldn't create socket " << cpp_strerror(rc) << dendl;
-    goto fail;
-  }
-
-  recv_reset();
-
-  set_socket_options();
-
-  {
-    entity_addr_t addr2bind = msgr->get_myaddr();
-    if (msgr->cct->_conf->ms_bind_before_connect && (!addr2bind.is_blank_ip())) {
-      addr2bind.set_port(0);
-      int r = ::bind(sd , addr2bind.get_sockaddr(), addr2bind.get_sockaddr_len());
-      if (r < 0) {
-        ldout(msgr->cct,2) << "client bind error " << ", " << cpp_strerror(errno) << dendl;
-        goto fail;
-      }
-    }
-  }
-
-  // connect!
-  ldout(msgr->cct,10) << "connecting to " << peer_addr << dendl;
-  rc = ::connect(sd, peer_addr.get_sockaddr(), peer_addr.get_sockaddr_len());
-  if (rc < 0) {
-    int stored_errno = errno;
-    ldout(msgr->cct,2) << "connect error " << peer_addr
-	     << ", " << cpp_strerror(stored_errno) << dendl;
-    if (stored_errno == ECONNREFUSED) {
-      ldout(msgr->cct, 2) << "connection refused!" << dendl;
-      msgr->dispatch_queue.queue_refused(connection_state.get());
-    }
-    goto fail;
-  }
-
-  // verify banner
-  // FIXME: this should be non-blocking, or in some other way verify the banner as we get it.
-  rc = tcp_read((char*)&banner, strlen(CEPH_BANNER));
-  if (rc < 0) {
-    ldout(msgr->cct,2) << "connect couldn't read banner, " << cpp_strerror(rc) << dendl;
-    goto fail;
-  }
-  if (memcmp(banner, CEPH_BANNER, strlen(CEPH_BANNER))) {
-    ldout(msgr->cct,0) << "connect protocol error (bad banner) on peer " << peer_addr << dendl;
-    goto fail;
-  }
-
-  memset(&msg, 0, sizeof(msg));
-  msgvec[0].iov_base = banner;
-  msgvec[0].iov_len = strlen(CEPH_BANNER);
-  msg.msg_iov = msgvec;
-  msg.msg_iovlen = 1;
-  msglen = msgvec[0].iov_len;
-  rc = do_sendmsg(&msg, msglen);
-  if (rc < 0) {
-    ldout(msgr->cct,2) << "connect couldn't write my banner, " << cpp_strerror(rc) << dendl;
-    goto fail;
-  }
-
-  // identify peer
-  {
-#if defined(__linux__) || defined(DARWIN) || defined(__FreeBSD__)
-    bufferptr p(sizeof(ceph_entity_addr) * 2);
-#else
-    int wirelen = sizeof(__u32) * 2 + sizeof(ceph_sockaddr_storage);
-    bufferptr p(wirelen * 2);
-#endif
-    addrbl.push_back(std::move(p));
-  }
-  rc = tcp_read(addrbl.c_str(), addrbl.length());
-  if (rc < 0) {
-    ldout(msgr->cct,2) << "connect couldn't read peer addrs, " << cpp_strerror(rc) << dendl;
-    goto fail;
-  }
-  try {
-    bufferlist::iterator p = addrbl.begin();
-    ::decode(paddr, p);
-    ::decode(peer_addr_for_me, p);
-  }
-  catch (buffer::error& e) {
-    ldout(msgr->cct,2) << "connect couldn't decode peer addrs: " << e.what()
-		       << dendl;
-    goto fail;
-  }
-  port = peer_addr_for_me.get_port();
-
-  ldout(msgr->cct,20) << "connect read peer addr " << paddr << " on socket " << sd << dendl;
-  if (peer_addr != paddr) {
-    if (paddr.is_blank_ip() &&
-	peer_addr.get_port() == paddr.get_port() &&
-	peer_addr.get_nonce() == paddr.get_nonce()) {
-      ldout(msgr->cct,0) << "connect claims to be " 
-	      << paddr << " not " << peer_addr << " - presumably this is the same node!" << dendl;
-    } else {
-      ldout(msgr->cct,10) << "connect claims to be "
-			  << paddr << " not " << peer_addr << dendl;
-      goto fail;
-    }
-  }
-
-  ldout(msgr->cct,20) << "connect peer addr for me is " << peer_addr_for_me << dendl;
-
-  msgr->learned_addr(peer_addr_for_me);
-
-  ::encode(msgr->my_inst.addr, myaddrbl, 0);  // legacy
-
-  memset(&msg, 0, sizeof(msg));
-  msgvec[0].iov_base = myaddrbl.c_str();
-  msgvec[0].iov_len = myaddrbl.length();
-  msg.msg_iov = msgvec;
-  msg.msg_iovlen = 1;
-  msglen = msgvec[0].iov_len;
-  rc = do_sendmsg(&msg, msglen);
-  if (rc < 0) {
-    ldout(msgr->cct,2) << "connect couldn't write my addr, " << cpp_strerror(rc) << dendl;
-    goto fail;
-  }
-  ldout(msgr->cct,10) << "connect sent my addr " << msgr->my_inst.addr << dendl;
-
-
-  while (1) {
-    delete authorizer;
-    authorizer = msgr->get_authorizer(peer_type, false);
-    bufferlist authorizer_reply;
-
-    ceph_msg_connect connect;
-    connect.features = policy.features_supported;
-    connect.host_type = msgr->get_myinst().name.type();
-    connect.global_seq = gseq;
-    connect.connect_seq = cseq;
-    connect.protocol_version = msgr->get_proto_version(peer_type, true);
-    connect.authorizer_protocol = authorizer ? authorizer->protocol : 0;
-    connect.authorizer_len = authorizer ? authorizer->bl.length() : 0;
-    if (authorizer) 
-      ldout(msgr->cct,10) << "connect.authorizer_len=" << connect.authorizer_len
-	       << " protocol=" << connect.authorizer_protocol << dendl;
-    connect.flags = 0;
-    if (policy.lossy)
-      connect.flags |= CEPH_MSG_CONNECT_LOSSY;  // this is fyi, actually, server decides!
-    memset(&msg, 0, sizeof(msg));
-    msgvec[0].iov_base = (char*)&connect;
-    msgvec[0].iov_len = sizeof(connect);
-    msg.msg_iov = msgvec;
-    msg.msg_iovlen = 1;
-    msglen = msgvec[0].iov_len;
-    if (authorizer) {
-      msgvec[1].iov_base = authorizer->bl.c_str();
-      msgvec[1].iov_len = authorizer->bl.length();
-      msg.msg_iovlen++;
-      msglen += msgvec[1].iov_len;
-    }
-
-    ldout(msgr->cct,10) << "connect sending gseq=" << gseq << " cseq=" << cseq
-	     << " proto=" << connect.protocol_version << dendl;
-    rc = do_sendmsg(&msg, msglen);
-    if (rc < 0) {
-      ldout(msgr->cct,2) << "connect couldn't write gseq, cseq, " << cpp_strerror(rc) << dendl;
-      goto fail;
-    }
-
-    ldout(msgr->cct,20) << "connect wrote (self +) cseq, waiting for reply" << dendl;
-    ceph_msg_connect_reply reply;
-    rc = tcp_read((char*)&reply, sizeof(reply));
-    if (rc < 0) {
-      ldout(msgr->cct,2) << "connect read reply " << cpp_strerror(rc) << dendl;
-      goto fail;
-    }
-
-    ldout(msgr->cct,20) << "connect got reply tag " << (int)reply.tag
-			<< " connect_seq " << reply.connect_seq
-			<< " global_seq " << reply.global_seq
-			<< " proto " << reply.protocol_version
-			<< " flags " << (int)reply.flags
-			<< " features " << reply.features
-			<< dendl;
-
-    authorizer_reply.clear();
-
-    if (reply.authorizer_len) {
-      ldout(msgr->cct,10) << "reply.authorizer_len=" << reply.authorizer_len << dendl;
-      bufferptr bp = buffer::create(reply.authorizer_len);
-      rc = tcp_read(bp.c_str(), reply.authorizer_len);
-      if (rc < 0) {
-        ldout(msgr->cct,10) << "connect couldn't read connect authorizer_reply" << cpp_strerror(rc) << dendl;
-	goto fail;
-      }
-      authorizer_reply.push_back(bp);
-    }
-
-    if (authorizer) {
-      bufferlist::iterator iter = authorizer_reply.begin();
-      if (!authorizer->verify_reply(iter)) {
-        ldout(msgr->cct,0) << "failed verifying authorize reply" << dendl;
-	goto fail;
-      }
-    }
-
-    if (conf->ms_inject_internal_delays) {
-      ldout(msgr->cct, 10) << " sleep for " << msgr->cct->_conf->ms_inject_internal_delays << dendl;
-      utime_t t;
-      t.set_from_double(msgr->cct->_conf->ms_inject_internal_delays);
-      t.sleep();
-    }
-
-    pipe_lock.Lock();
-    if (state != STATE_CONNECTING) {
-      ldout(msgr->cct,0) << "connect got RESETSESSION but no longer connecting" << dendl;
-      goto stop_locked;
-    }
-
-    if (reply.tag == CEPH_MSGR_TAG_FEATURES) {
-      ldout(msgr->cct,0) << "connect protocol feature mismatch, my " << std::hex
-	      << connect.features << " < peer " << reply.features
-	      << " missing " << (reply.features & ~policy.features_supported)
-	      << std::dec << dendl;
-      goto fail_locked;
-    }
-
-    if (reply.tag == CEPH_MSGR_TAG_BADPROTOVER) {
-      ldout(msgr->cct,0) << "connect protocol version mismatch, my " << connect.protocol_version
-	      << " != " << reply.protocol_version << dendl;
-      goto fail_locked;
-    }
-
-    if (reply.tag == CEPH_MSGR_TAG_BADAUTHORIZER) {
-      ldout(msgr->cct,0) << "connect got BADAUTHORIZER" << dendl;
-      if (got_bad_auth)
-        goto stop_locked;
-      got_bad_auth = true;
-      pipe_lock.Unlock();
-      delete authorizer;
-      authorizer = msgr->get_authorizer(peer_type, true);  // try harder
-      continue;
-    }
-    if (reply.tag == CEPH_MSGR_TAG_RESETSESSION) {
-      ldout(msgr->cct,0) << "connect got RESETSESSION" << dendl;
-      was_session_reset();
-      cseq = 0;
-      pipe_lock.Unlock();
-      continue;
-    }
-    if (reply.tag == CEPH_MSGR_TAG_RETRY_GLOBAL) {
-      gseq = msgr->get_global_seq(reply.global_seq);
-      ldout(msgr->cct,10) << "connect got RETRY_GLOBAL " << reply.global_seq
-	       << " chose new " << gseq << dendl;
-      pipe_lock.Unlock();
-      continue;
-    }
-    if (reply.tag == CEPH_MSGR_TAG_RETRY_SESSION) {
-      assert(reply.connect_seq > connect_seq);
-      ldout(msgr->cct,10) << "connect got RETRY_SESSION " << connect_seq
-	       << " -> " << reply.connect_seq << dendl;
-      cseq = connect_seq = reply.connect_seq;
-      pipe_lock.Unlock();
-      continue;
-    }
-
-    if (reply.tag == CEPH_MSGR_TAG_WAIT) {
-      ldout(msgr->cct,3) << "connect got WAIT (connection race)" << dendl;
-      state = STATE_WAIT;
-      goto stop_locked;
-    }
-
-    if (reply.tag == CEPH_MSGR_TAG_READY ||
-        reply.tag == CEPH_MSGR_TAG_SEQ) {
-      uint64_t feat_missing = policy.features_required & ~(uint64_t)reply.features;
-      if (feat_missing) {
-	ldout(msgr->cct,1) << "missing required features " << std::hex << feat_missing << std::dec << dendl;
-	goto fail_locked;
-      }
-
-      if (reply.tag == CEPH_MSGR_TAG_SEQ) {
-        ldout(msgr->cct,10) << "got CEPH_MSGR_TAG_SEQ, reading acked_seq and writing in_seq" << dendl;
-        uint64_t newly_acked_seq = 0;
-        rc = tcp_read((char*)&newly_acked_seq, sizeof(newly_acked_seq));
-        if (rc < 0) {
-          ldout(msgr->cct,2) << "connect read error on newly_acked_seq" << cpp_strerror(rc) << dendl;
-          goto fail_locked;
-        }
-	ldout(msgr->cct,2) << " got newly_acked_seq " << newly_acked_seq
-			   << " vs out_seq " << out_seq << dendl;
-	while (newly_acked_seq > out_seq) {
-	  Message *m = _get_next_outgoing();
-	  assert(m);
-	  ldout(msgr->cct,2) << " discarding previously sent " << m->get_seq()
-			     << " " << *m << dendl;
-	  assert(m->get_seq() <= newly_acked_seq);
-	  m->put();
-	  ++out_seq;
-	}
-        if (tcp_write((char*)&in_seq, sizeof(in_seq)) < 0) {
-          ldout(msgr->cct,2) << "connect write error on in_seq" << dendl;
-          goto fail_locked;
-        }
-      }
-
-      // hooray!
-      peer_global_seq = reply.global_seq;
-      policy.lossy = reply.flags & CEPH_MSG_CONNECT_LOSSY;
-      state = STATE_OPEN;
-      connect_seq = cseq + 1;
-      assert(connect_seq == reply.connect_seq);
-      backoff = utime_t();
-      connection_state->set_features((uint64_t)reply.features & (uint64_t)connect.features);
-      ldout(msgr->cct,10) << "connect success " << connect_seq << ", lossy = " << policy.lossy
-	       << ", features " << connection_state->get_features() << dendl;
-      
-
-      // If we have an authorizer, get a new AuthSessionHandler to deal with ongoing security of the
-      // connection.  PLR
-
-      if (authorizer != NULL) {
-	session_security.reset(
-            get_auth_session_handler(msgr->cct,
-				     authorizer->protocol,
-				     authorizer->session_key,
-				     connection_state->get_features()));
-      }  else {
-        // We have no authorizer, so we shouldn't be applying security to messages in this pipe.  PLR
-	session_security.reset();
-      }
-
-      msgr->dispatch_queue.queue_connect(connection_state.get());
-      msgr->ms_deliver_handle_fast_connect(connection_state.get());
-      
-      if (!reader_running) {
-	ldout(msgr->cct,20) << "connect starting reader" << dendl;
-	start_reader();
-      }
-      maybe_start_delay_thread();
-      delete authorizer;
-      return 0;
-    }
-    
-    // protocol error
-    ldout(msgr->cct,0) << "connect got bad tag " << (int)tag << dendl;
-    goto fail_locked;
-  }
-
- fail:
-  if (conf->ms_inject_internal_delays) {
-    ldout(msgr->cct, 10) << " sleep for " << msgr->cct->_conf->ms_inject_internal_delays << dendl;
-    utime_t t;
-    t.set_from_double(msgr->cct->_conf->ms_inject_internal_delays);
-    t.sleep();
-  }
-
-  pipe_lock.Lock();
- fail_locked:
-  if (state == STATE_CONNECTING)
-    fault();
-  else
-    ldout(msgr->cct,3) << "connect fault, but state = " << get_state_name()
-		       << " != connecting, stopping" << dendl;
-
- stop_locked:
-  delete authorizer;
-  return rc;
-}
-
-void Pipe::register_pipe()
-{
-  ldout(msgr->cct,10) << "register_pipe" << dendl;
-  assert(msgr->lock.is_locked());
-  Pipe *existing = msgr->_lookup_pipe(peer_addr);
-  assert(existing == NULL);
-  msgr->rank_pipe[peer_addr] = this;
-}
-
-void Pipe::unregister_pipe()
-{
-  assert(msgr->lock.is_locked());
-  ceph::unordered_map<entity_addr_t,Pipe*>::iterator p = msgr->rank_pipe.find(peer_addr);
-  if (p != msgr->rank_pipe.end() && p->second == this) {
-    ldout(msgr->cct,10) << "unregister_pipe" << dendl;
-    msgr->rank_pipe.erase(p);
-  } else {
-    ldout(msgr->cct,10) << "unregister_pipe - not registered" << dendl;
-    msgr->accepting_pipes.erase(this);  // somewhat overkill, but safe.
-  }
-}
-
-void Pipe::join()
-{
-  ldout(msgr->cct, 20) << "join" << dendl;
-  if (writer_thread.is_started())
-    writer_thread.join();
-  if (reader_thread.is_started())
-    reader_thread.join();
-  if (delay_thread) {
-    ldout(msgr->cct, 20) << "joining delay_thread" << dendl;
-    delay_thread->stop();
-    delay_thread->join();
-  }
-}
-
-void Pipe::requeue_sent()
-{
-  if (sent.empty())
-    return;
-
-  list<Message*>& rq = out_q[CEPH_MSG_PRIO_HIGHEST];
-  while (!sent.empty()) {
-    Message *m = sent.back();
-    sent.pop_back();
-    ldout(msgr->cct,10) << "requeue_sent " << *m << " for resend seq " << out_seq
-			<< " (" << m->get_seq() << ")" << dendl;
-    rq.push_front(m);
-    out_seq--;
-  }
-}
-
-void Pipe::discard_requeued_up_to(uint64_t seq)
-{
-  ldout(msgr->cct, 10) << "discard_requeued_up_to " << seq << dendl;
-  if (out_q.count(CEPH_MSG_PRIO_HIGHEST) == 0)
-    return;
-  list<Message*>& rq = out_q[CEPH_MSG_PRIO_HIGHEST];
-  while (!rq.empty()) {
-    Message *m = rq.front();
-    if (m->get_seq() == 0 || m->get_seq() > seq)
-      break;
-    ldout(msgr->cct,10) << "discard_requeued_up_to " << *m << " for resend seq " << out_seq
-			<< " <= " << seq << ", discarding" << dendl;
-    m->put();
-    rq.pop_front();
-    out_seq++;
-  }
-  if (rq.empty())
-    out_q.erase(CEPH_MSG_PRIO_HIGHEST);
-}
-
-/*
- * Tears down the Pipe's message queues, and removes them from the DispatchQueue
- * Must hold pipe_lock prior to calling.
- */
-void Pipe::discard_out_queue()
-{
-  ldout(msgr->cct,10) << "discard_queue" << dendl;
-
-  for (list<Message*>::iterator p = sent.begin(); p != sent.end(); ++p) {
-    ldout(msgr->cct,20) << "  discard " << *p << dendl;
-    (*p)->put();
-  }
-  sent.clear();
-  for (map<int,list<Message*> >::iterator p = out_q.begin(); p != out_q.end(); ++p)
-    for (list<Message*>::iterator r = p->second.begin(); r != p->second.end(); ++r) {
-      ldout(msgr->cct,20) << "  discard " << *r << dendl;
-      (*r)->put();
-    }
-  out_q.clear();
-}
-
-void Pipe::fault(bool onread)
-{
-  const md_config_t *conf = msgr->cct->_conf;
-  assert(pipe_lock.is_locked());
-  cond.Signal();
-
-  if (onread && state == STATE_CONNECTING) {
-    ldout(msgr->cct,10) << "fault already connecting, reader shutting down" << dendl;
-    return;
-  }
-  
-  ldout(msgr->cct,2) << "fault " << cpp_strerror(errno) << dendl;
-
-  if (state == STATE_CLOSED ||
-      state == STATE_CLOSING) {
-    ldout(msgr->cct,10) << "fault already closed|closing" << dendl;
-    if (connection_state->clear_pipe(this))
-      msgr->dispatch_queue.queue_reset(connection_state.get());
-    return;
-  }
-
-  shutdown_socket();
-
-  // lossy channel?
-  if (policy.lossy && state != STATE_CONNECTING) {
-    ldout(msgr->cct,10) << "fault on lossy channel, failing" << dendl;
-
-    // disconnect from Connection, and mark it failed.  future messages
-    // will be dropped.
-    assert(connection_state);
-    stop();
-    bool cleared = connection_state->clear_pipe(this);
-
-    // crib locks, blech.  note that Pipe is now STATE_CLOSED and the
-    // rank_pipe entry is ignored by others.
-    pipe_lock.Unlock();
-
-    if (conf->ms_inject_internal_delays) {
-      ldout(msgr->cct, 10) << " sleep for " << msgr->cct->_conf->ms_inject_internal_delays << dendl;
-      utime_t t;
-      t.set_from_double(msgr->cct->_conf->ms_inject_internal_delays);
-      t.sleep();
-    }
-
-    msgr->lock.Lock();
-    pipe_lock.Lock();
-    unregister_pipe();
-    msgr->lock.Unlock();
-
-    if (delay_thread)
-      delay_thread->discard();
-    in_q->discard_queue(conn_id);
-    discard_out_queue();
-    if (cleared)
-      msgr->dispatch_queue.queue_reset(connection_state.get());
-    return;
-  }
-
-  // queue delayed items immediately
-  if (delay_thread)
-    delay_thread->flush();
-
-  // requeue sent items
-  requeue_sent();
-
-  if (policy.standby && !is_queued()) {
-    ldout(msgr->cct,0) << "fault with nothing to send, going to standby" << dendl;
-    state = STATE_STANDBY;
-    return;
-  }
-
-  if (state != STATE_CONNECTING) {
-    if (policy.server) {
-      ldout(msgr->cct,0) << "fault, server, going to standby" << dendl;
-      state = STATE_STANDBY;
-    } else {
-      ldout(msgr->cct,0) << "fault, initiating reconnect" << dendl;
-      connect_seq++;
-      state = STATE_CONNECTING;
-    }
-    backoff = utime_t();
-  } else if (backoff == utime_t()) {
-    ldout(msgr->cct,0) << "fault" << dendl;
-    backoff.set_from_double(conf->ms_initial_backoff);
-  } else {
-    ldout(msgr->cct,10) << "fault waiting " << backoff << dendl;
-    cond.WaitInterval(pipe_lock, backoff);
-    backoff += backoff;
-    if (backoff > conf->ms_max_backoff)
-      backoff.set_from_double(conf->ms_max_backoff);
-    ldout(msgr->cct,10) << "fault done waiting or woke up" << dendl;
-  }
-}
-
-int Pipe::randomize_out_seq()
-{
-  if (connection_state->get_features() & CEPH_FEATURE_MSG_AUTH) {
-    // Set out_seq to a random value, so CRC won't be predictable.   Don't bother checking seq_error
-    // here.  We'll check it on the call.  PLR
-    int seq_error = get_random_bytes((char *)&out_seq, sizeof(out_seq));
-    out_seq &= SEQ_MASK;
-    lsubdout(msgr->cct, ms, 10) << "randomize_out_seq " << out_seq << dendl;
-    return seq_error;
-  } else {
-    // previously, seq #'s always started at 0.
-    out_seq = 0;
-    return 0;
-  }
-}
-
-void Pipe::was_session_reset()
-{
-  assert(pipe_lock.is_locked());
-
-  ldout(msgr->cct,10) << "was_session_reset" << dendl;
-  in_q->discard_queue(conn_id);
-  if (delay_thread)
-    delay_thread->discard();
-  discard_out_queue();
-
-  msgr->dispatch_queue.queue_remote_reset(connection_state.get());
-
-  if (randomize_out_seq()) {
-    lsubdout(msgr->cct,ms,15) << "was_session_reset(): Could not get random bytes to set seq number for session reset; set seq number to " << out_seq << dendl;
-  }
-
-  in_seq = 0;
-  connect_seq = 0;
-}
-
-void Pipe::stop()
-{
-  ldout(msgr->cct,10) << "stop" << dendl;
-  assert(pipe_lock.is_locked());
-  state = STATE_CLOSED;
-  state_closed = true;
-  cond.Signal();
-  shutdown_socket();
-}
-
-void Pipe::stop_and_wait()
-{
-  assert(pipe_lock.is_locked_by_me());
-  if (state != STATE_CLOSED)
-    stop();
-
-  if (msgr->cct->_conf->ms_inject_internal_delays) {
-    ldout(msgr->cct, 10) << __func__ << " sleep for "
-			 << msgr->cct->_conf->ms_inject_internal_delays
-			 << dendl;
-    utime_t t;
-    t.set_from_double(msgr->cct->_conf->ms_inject_internal_delays);
-    t.sleep();
-  }
-  
-  if (delay_thread) {
-    pipe_lock.Unlock();
-    delay_thread->stop_fast_dispatching();
-    pipe_lock.Lock();
-  }
-  while (reader_running &&
-	 reader_dispatching)
-    cond.Wait(pipe_lock);
-}
-
-/* read msgs from socket.
- * also, server.
- */
-void Pipe::reader()
-{
-  pipe_lock.Lock();
-
-  if (state == STATE_ACCEPTING) {
-    accept();
-    assert(pipe_lock.is_locked());
-  }
-
-  // loop.
-  while (state != STATE_CLOSED &&
-	 state != STATE_CONNECTING) {
-    assert(pipe_lock.is_locked());
-
-    // sleep if (re)connecting
-    if (state == STATE_STANDBY) {
-      ldout(msgr->cct,20) << "reader sleeping during reconnect|standby" << dendl;
-      cond.Wait(pipe_lock);
-      continue;
-    }
-
-    // get a reference to the AuthSessionHandler while we have the pipe_lock
-    ceph::shared_ptr<AuthSessionHandler> auth_handler = session_security;
-
-    pipe_lock.Unlock();
-
-    char tag = -1;
-    ldout(msgr->cct,20) << "reader reading tag..." << dendl;
-    if (tcp_read((char*)&tag, 1) < 0) {
-      pipe_lock.Lock();
-      ldout(msgr->cct,2) << "reader couldn't read tag, " << cpp_strerror(errno) << dendl;
-      fault(true);
-      continue;
-    }
-
-    if (tag == CEPH_MSGR_TAG_KEEPALIVE) {
-      ldout(msgr->cct,2) << "reader got KEEPALIVE" << dendl;
-      pipe_lock.Lock();
-      connection_state->set_last_keepalive(ceph_clock_now());
-      continue;
-    }
-    if (tag == CEPH_MSGR_TAG_KEEPALIVE2) {
-      ldout(msgr->cct,30) << "reader got KEEPALIVE2 tag ..." << dendl;
-      ceph_timespec t;
-      int rc = tcp_read((char*)&t, sizeof(t));
-      pipe_lock.Lock();
-      if (rc < 0) {
-	ldout(msgr->cct,2) << "reader couldn't read KEEPALIVE2 stamp "
-			   << cpp_strerror(errno) << dendl;
-	fault(true);
-      } else {
-	send_keepalive_ack = true;
-	keepalive_ack_stamp = utime_t(t);
-	ldout(msgr->cct,2) << "reader got KEEPALIVE2 " << keepalive_ack_stamp
-			   << dendl;
-	connection_state->set_last_keepalive(ceph_clock_now());
-	cond.Signal();
-      }
-      continue;
-    }
-    if (tag == CEPH_MSGR_TAG_KEEPALIVE2_ACK) {
-      ldout(msgr->cct,2) << "reader got KEEPALIVE_ACK" << dendl;
-      struct ceph_timespec t;
-      int rc = tcp_read((char*)&t, sizeof(t));
-      pipe_lock.Lock();
-      if (rc < 0) {
-	ldout(msgr->cct,2) << "reader couldn't read KEEPALIVE2 stamp " << cpp_strerror(errno) << dendl;
-	fault(true);
-      } else {
-	connection_state->set_last_keepalive_ack(utime_t(t));
-      }
-      continue;
-    }
-
-    // open ...
-    if (tag == CEPH_MSGR_TAG_ACK) {
-      ldout(msgr->cct,20) << "reader got ACK" << dendl;
-      ceph_le64 seq;
-      int rc = tcp_read((char*)&seq, sizeof(seq));
-      pipe_lock.Lock();
-      if (rc < 0) {
-	ldout(msgr->cct,2) << "reader couldn't read ack seq, " << cpp_strerror(errno) << dendl;
-	fault(true);
-      } else if (state != STATE_CLOSED) {
-        handle_ack(seq);
-      }
-      continue;
-    }
-
-    else if (tag == CEPH_MSGR_TAG_MSG) {
-      ldout(msgr->cct,20) << "reader got MSG" << dendl;
-      Message *m = 0;
-      int r = read_message(&m, auth_handler.get());
-
-      pipe_lock.Lock();
-      
-      if (!m) {
-	if (r < 0)
-	  fault(true);
-	continue;
-      }
-
-      m->trace.event("pipe read message");
-
-      if (state == STATE_CLOSED ||
-	  state == STATE_CONNECTING) {
-	in_q->dispatch_throttle_release(m->get_dispatch_throttle_size());
-	m->put();
-	continue;
-      }
-
-      // check received seq#.  if it is old, drop the message.  
-      // note that incoming messages may skip ahead.  this is convenient for the client
-      // side queueing because messages can't be renumbered, but the (kernel) client will
-      // occasionally pull a message out of the sent queue to send elsewhere.  in that case
-      // it doesn't matter if we "got" it or not.
-      if (m->get_seq() <= in_seq) {
-	ldout(msgr->cct,0) << "reader got old message "
-		<< m->get_seq() << " <= " << in_seq << " " << m << " " << *m
-		<< ", discarding" << dendl;
-	in_q->dispatch_throttle_release(m->get_dispatch_throttle_size());
-	m->put();
-	if (connection_state->has_feature(CEPH_FEATURE_RECONNECT_SEQ) &&
-	    msgr->cct->_conf->ms_die_on_old_message)
-	  assert(0 == "old msgs despite reconnect_seq feature");
-	continue;
-      }
-      if (m->get_seq() > in_seq + 1) {
-	ldout(msgr->cct,0) << "reader missed message?  skipped from seq "
-			   << in_seq << " to " << m->get_seq() << dendl;
-	if (msgr->cct->_conf->ms_die_on_skipped_message)
-	  assert(0 == "skipped incoming seq");
-      }
-
-      m->set_connection(connection_state.get());
-
-      // note last received message.
-      in_seq = m->get_seq();
-
-      cond.Signal();  // wake up writer, to ack this
-      
-      ldout(msgr->cct,10) << "reader got message "
-	       << m->get_seq() << " " << m << " " << *m
-	       << dendl;
-      in_q->fast_preprocess(m);
-
-      if (delay_thread) {
-        utime_t release;
-        if (rand() % 10000 < msgr->cct->_conf->ms_inject_delay_probability * 10000.0) {
-          release = m->get_recv_stamp();
-          release += msgr->cct->_conf->ms_inject_delay_max * (double)(rand() % 10000) / 10000.0;
-          lsubdout(msgr->cct, ms, 1) << "queue_received will delay until " << release << " on " << m << " " << *m << dendl;
-        }
-        delay_thread->queue(release, m);
-      } else {
-        if (in_q->can_fast_dispatch(m)) {
-	  reader_dispatching = true;
-          pipe_lock.Unlock();
-          in_q->fast_dispatch(m);
-          pipe_lock.Lock();
-	  reader_dispatching = false;
-	  if (state == STATE_CLOSED ||
-	      notify_on_dispatch_done) { // there might be somebody waiting
-	    notify_on_dispatch_done = false;
-	    cond.Signal();
-	  }
-        } else {
-          in_q->enqueue(m, m->get_priority(), conn_id);
-        }
-      }
-    }
-    
-    else if (tag == CEPH_MSGR_TAG_CLOSE) {
-      ldout(msgr->cct,20) << "reader got CLOSE" << dendl;
-      pipe_lock.Lock();
-      if (state == STATE_CLOSING) {
-	state = STATE_CLOSED;
-	state_closed = true;
-      } else {
-	state = STATE_CLOSING;
-      }
-      cond.Signal();
-      break;
-    }
-    else {
-      ldout(msgr->cct,0) << "reader bad tag " << (int)tag << dendl;
-      pipe_lock.Lock();
-      fault(true);
-    }
-  }
-
- 
-  // reap?
-  reader_running = false;
-  reader_needs_join = true;
-  unlock_maybe_reap();
-  ldout(msgr->cct,10) << "reader done" << dendl;
-}
-
-/* write msgs to socket.
- * also, client.
- */
-void Pipe::writer()
-{
-  pipe_lock.Lock();
-  while (state != STATE_CLOSED) {// && state != STATE_WAIT) {
-    ldout(msgr->cct,10) << "writer: state = " << get_state_name()
-			<< " policy.server=" << policy.server << dendl;
-
-    // standby?
-    if (is_queued() && state == STATE_STANDBY && !policy.server)
-      state = STATE_CONNECTING;
-
-    // connect?
-    if (state == STATE_CONNECTING) {
-      assert(!policy.server);
-      connect();
-      continue;
-    }
-    
-    if (state == STATE_CLOSING) {
-      // write close tag
-      ldout(msgr->cct,20) << "writer writing CLOSE tag" << dendl;
-      char tag = CEPH_MSGR_TAG_CLOSE;
-      state = STATE_CLOSED;
-      state_closed = true;
-      pipe_lock.Unlock();
-      if (sd >= 0) {
-	// we can ignore return value, actually; we don't care if this succeeds.
-	int r = ::write(sd, &tag, 1);
-	(void)r;
-      }
-      pipe_lock.Lock();
-      continue;
-    }
-
-    if (state != STATE_CONNECTING && state != STATE_WAIT && state != STATE_STANDBY &&
-	(is_queued() || in_seq > in_seq_acked)) {
-
-      // keepalive?
-      if (send_keepalive) {
-	int rc;
-	if (connection_state->has_feature(CEPH_FEATURE_MSGR_KEEPALIVE2)) {
-	  pipe_lock.Unlock();
-	  rc = write_keepalive2(CEPH_MSGR_TAG_KEEPALIVE2,
-				ceph_clock_now());
-	} else {
-	  pipe_lock.Unlock();
-	  rc = write_keepalive();
-	}
-	pipe_lock.Lock();
-	if (rc < 0) {
-	  ldout(msgr->cct,2) << "writer couldn't write keepalive[2], "
-			     << cpp_strerror(errno) << dendl;
-	  fault();
- 	  continue;
-	}
-	send_keepalive = false;
-      }
-      if (send_keepalive_ack) {
-	utime_t t = keepalive_ack_stamp;
-	pipe_lock.Unlock();
-	int rc = write_keepalive2(CEPH_MSGR_TAG_KEEPALIVE2_ACK, t);
-	pipe_lock.Lock();
-	if (rc < 0) {
-	  ldout(msgr->cct,2) << "writer couldn't write keepalive_ack, " << cpp_strerror(errno) << dendl;
-	  fault();
-	  continue;
-	}
-	send_keepalive_ack = false;
-      }
-
-      // send ack?
-      if (in_seq > in_seq_acked) {
-	uint64_t send_seq = in_seq;
-	pipe_lock.Unlock();
-	int rc = write_ack(send_seq);
-	pipe_lock.Lock();
-	if (rc < 0) {
-	  ldout(msgr->cct,2) << "writer couldn't write ack, " << cpp_strerror(errno) << dendl;
-	  fault();
- 	  continue;
-	}
-	in_seq_acked = send_seq;
-      }
-
-      // grab outgoing message
-      Message *m = _get_next_outgoing();
-      if (m) {
-	m->set_seq(++out_seq);
-	if (!policy.lossy) {
-	  // put on sent list
-	  sent.push_back(m); 
-	  m->get();
-	}
-
-	// associate message with Connection (for benefit of encode_payload)
-	m->set_connection(connection_state.get());
-
-	uint64_t features = connection_state->get_features();
-
-	if (m->empty_payload())
-	  ldout(msgr->cct,20) << "writer encoding " << m->get_seq() << " features " << features
-			      << " " << m << " " << *m << dendl;
-	else
-	  ldout(msgr->cct,20) << "writer half-reencoding " << m->get_seq() << " features " << features
-			      << " " << m << " " << *m << dendl;
-
-	// encode and copy out of *m
-	m->encode(features, msgr->crcflags);
-
-	// prepare everything
-	const ceph_msg_header& header = m->get_header();
-	const ceph_msg_footer& footer = m->get_footer();
-
-	// Now that we have all the crcs calculated, handle the
-	// digital signature for the message, if the pipe has session
-	// security set up.  Some session security options do not
-	// actually calculate and check the signature, but they should
-	// handle the calls to sign_message and check_signature.  PLR
-	if (session_security.get() == NULL) {
-	  ldout(msgr->cct, 20) << "writer no session security" << dendl;
-	} else {
-	  if (session_security->sign_message(m)) {
-	    ldout(msgr->cct, 20) << "writer failed to sign seq # " << header.seq
-				 << "): sig = " << footer.sig << dendl;
-	  } else {
-	    ldout(msgr->cct, 20) << "writer signed seq # " << header.seq
-				 << "): sig = " << footer.sig << dendl;
-	  }
-	}
-
-	bufferlist blist = m->get_payload();
-	blist.append(m->get_middle());
-	blist.append(m->get_data());
-
-        pipe_lock.Unlock();
-
-        m->trace.event("pipe writing message");
-
-        ldout(msgr->cct,20) << "writer sending " << m->get_seq() << " " << m << dendl;
-	int rc = write_message(header, footer, blist);
-
-	pipe_lock.Lock();
-	if (rc < 0) {
-          ldout(msgr->cct,1) << "writer error sending " << m << ", "
-		  << cpp_strerror(errno) << dendl;
-	  fault();
-        }
-	m->put();
-      }
-      continue;
-    }
-    
-    // wait
-    ldout(msgr->cct,20) << "writer sleeping" << dendl;
-    cond.Wait(pipe_lock);
-  }
-  
-  ldout(msgr->cct,20) << "writer finishing" << dendl;
-
-  // reap?
-  writer_running = false;
-  unlock_maybe_reap();
-  ldout(msgr->cct,10) << "writer done" << dendl;
-}
-
-void Pipe::unlock_maybe_reap()
-{
-  if (!reader_running && !writer_running) {
-    shutdown_socket();
-    pipe_lock.Unlock();
-    if (delay_thread && delay_thread->is_flushing()) {
-      delay_thread->wait_for_flush();
-    }
-    msgr->queue_reap(this);
-  } else {
-    pipe_lock.Unlock();
-  }
-}
-
-static void alloc_aligned_buffer(bufferlist& data, unsigned len, unsigned off)
-{
-  // create a buffer to read into that matches the data alignment
-  unsigned left = len;
-  if (off & ~CEPH_PAGE_MASK) {
-    // head
-    unsigned head = 0;
-    head = MIN(CEPH_PAGE_SIZE - (off & ~CEPH_PAGE_MASK), left);
-    data.push_back(buffer::create(head));
-    left -= head;
-  }
-  unsigned middle = left & CEPH_PAGE_MASK;
-  if (middle > 0) {
-    data.push_back(buffer::create_page_aligned(middle));
-    left -= middle;
-  }
-  if (left) {
-    data.push_back(buffer::create(left));
-  }
-}
-
-int Pipe::read_message(Message **pm, AuthSessionHandler* auth_handler)
-{
-  int ret = -1;
-  // envelope
-  //ldout(msgr->cct,10) << "receiver.read_message from sd " << sd  << dendl;
-  
-  ceph_msg_header header; 
-  ceph_msg_footer footer;
-  __u32 header_crc = 0;
-
-  if (connection_state->has_feature(CEPH_FEATURE_NOSRCADDR)) {
-    if (tcp_read((char*)&header, sizeof(header)) < 0)
-      return -1;
-    if (msgr->crcflags & MSG_CRC_HEADER) {
-      header_crc = ceph_crc32c(0, (unsigned char *)&header, sizeof(header) - sizeof(header.crc));
-    }
-  } else {
-    ceph_msg_header_old oldheader;
-    if (tcp_read((char*)&oldheader, sizeof(oldheader)) < 0)
-      return -1;
-    // this is fugly
-    memcpy(&header, &oldheader, sizeof(header));
-    header.src = oldheader.src.name;
-    header.reserved = oldheader.reserved;
-    if (msgr->crcflags & MSG_CRC_HEADER) {
-      header.crc = oldheader.crc;
-      header_crc = ceph_crc32c(0, (unsigned char *)&oldheader, sizeof(oldheader) - sizeof(oldheader.crc));
-    }
-  }
-
-  ldout(msgr->cct,20) << "reader got envelope type=" << header.type
-           << " src " << entity_name_t(header.src)
-           << " front=" << header.front_len
-	   << " data=" << header.data_len
-	   << " off " << header.data_off
-           << dendl;
-
-  // verify header crc
-  if ((msgr->crcflags & MSG_CRC_HEADER) && header_crc != header.crc) {
-    ldout(msgr->cct,0) << "reader got bad header crc " << header_crc << " != " << header.crc << dendl;
-    return -1;
-  }
-
-  bufferlist front, middle, data;
-  int front_len, middle_len;
-  unsigned data_len, data_off;
-  int aborted;
-  Message *message;
-  utime_t recv_stamp = ceph_clock_now();
-
-  if (policy.throttler_messages) {
-    ldout(msgr->cct,10) << "reader wants " << 1 << " message from policy throttler "
-			<< policy.throttler_messages->get_current() << "/"
-			<< policy.throttler_messages->get_max() << dendl;
-    policy.throttler_messages->get();
-  }
-
-  uint64_t message_size = header.front_len + header.middle_len + header.data_len;
-  if (message_size) {
-    if (policy.throttler_bytes) {
-      ldout(msgr->cct,10) << "reader wants " << message_size << " bytes from policy throttler "
-	       << policy.throttler_bytes->get_current() << "/"
-	       << policy.throttler_bytes->get_max() << dendl;
-      policy.throttler_bytes->get(message_size);
-    }
-
-    // throttle total bytes waiting for dispatch.  do this _after_ the
-    // policy throttle, as this one does not deadlock (unless dispatch
-    // blocks indefinitely, which it shouldn't).  in contrast, the
-    // policy throttle carries for the lifetime of the message.
-    ldout(msgr->cct,10) << "reader wants " << message_size << " from dispatch throttler "
-	     << in_q->dispatch_throttler.get_current() << "/"
-	     << in_q->dispatch_throttler.get_max() << dendl;
-    in_q->dispatch_throttler.get(message_size);
-  }
-
-  utime_t throttle_stamp = ceph_clock_now();
-
-  // read front
-  front_len = header.front_len;
-  if (front_len) {
-    bufferptr bp = buffer::create(front_len);
-    if (tcp_read(bp.c_str(), front_len) < 0)
-      goto out_dethrottle;
-    front.push_back(std::move(bp));
-    ldout(msgr->cct,20) << "reader got front " << front.length() << dendl;
-  }
-
-  // read middle
-  middle_len = header.middle_len;
-  if (middle_len) {
-    bufferptr bp = buffer::create(middle_len);
-    if (tcp_read(bp.c_str(), middle_len) < 0)
-      goto out_dethrottle;
-    middle.push_back(std::move(bp));
-    ldout(msgr->cct,20) << "reader got middle " << middle.length() << dendl;
-  }
-
-
-  // read data
-  data_len = le32_to_cpu(header.data_len);
-  data_off = le32_to_cpu(header.data_off);
-  if (data_len) {
-    unsigned offset = 0;
-    unsigned left = data_len;
-
-    bufferlist newbuf, rxbuf;
-    bufferlist::iterator blp;
-    int rxbuf_version = 0;
-	
-    while (left > 0) {
-      // wait for data
-      if (tcp_read_wait() < 0)
-	goto out_dethrottle;
-
-      // get a buffer
-      connection_state->lock.Lock();
-      map<ceph_tid_t,pair<bufferlist,int> >::iterator p = connection_state->rx_buffers.find(header.tid);
-      if (p != connection_state->rx_buffers.end()) {
-	if (rxbuf.length() == 0 || p->second.second != rxbuf_version) {
-	  ldout(msgr->cct,10) << "reader seleting rx buffer v " << p->second.second
-		   << " at offset " << offset
-		   << " len " << p->second.first.length() << dendl;
-	  rxbuf = p->second.first;
-	  rxbuf_version = p->second.second;
-	  // make sure it's big enough
-	  if (rxbuf.length() < data_len)
-	    rxbuf.push_back(buffer::create(data_len - rxbuf.length()));
-	  blp = p->second.first.begin();
-	  blp.advance(offset);
-	}
-      } else {
-	if (!newbuf.length()) {
-	  ldout(msgr->cct,20) << "reader allocating new rx buffer at offset " << offset << dendl;
-	  alloc_aligned_buffer(newbuf, data_len, data_off);
-	  blp = newbuf.begin();
-	  blp.advance(offset);
-	}
-      }
-      bufferptr bp = blp.get_current_ptr();
-      int read = MIN(bp.length(), left);
-      ldout(msgr->cct,20) << "reader reading nonblocking into " << (void*)bp.c_str() << " len " << bp.length() << dendl;
-      ssize_t got = tcp_read_nonblocking(bp.c_str(), read);
-      ldout(msgr->cct,30) << "reader read " << got << " of " << read << dendl;
-      connection_state->lock.Unlock();
-      if (got < 0)
-	goto out_dethrottle;
-      if (got > 0) {
-	blp.advance(got);
-	data.append(bp, 0, got);
-	offset += got;
-	left -= got;
-      } // else we got a signal or something; just loop.
-    }
-  }
-
-  // footer
-  if (connection_state->has_feature(CEPH_FEATURE_MSG_AUTH)) {
-    if (tcp_read((char*)&footer, sizeof(footer)) < 0)
-      goto out_dethrottle;
-  } else {
-    ceph_msg_footer_old old_footer;
-    if (tcp_read((char*)&old_footer, sizeof(old_footer)) < 0)
-      goto out_dethrottle;
-    footer.front_crc = old_footer.front_crc;
-    footer.middle_crc = old_footer.middle_crc;
-    footer.data_crc = old_footer.data_crc;
-    footer.sig = 0;
-    footer.flags = old_footer.flags;
-  }
-  
-  aborted = (footer.flags & CEPH_MSG_FOOTER_COMPLETE) == 0;
-  ldout(msgr->cct,10) << "aborted = " << aborted << dendl;
-  if (aborted) {
-    ldout(msgr->cct,0) << "reader got " << front.length() << " + " << middle.length() << " + " << data.length()
-	    << " byte message.. ABORTED" << dendl;
-    ret = 0;
-    goto out_dethrottle;
-  }
-
-  ldout(msgr->cct,20) << "reader got " << front.length() << " + " << middle.length() << " + " << data.length()
-	   << " byte message" << dendl;
-  message = decode_message(msgr->cct, msgr->crcflags, header, footer,
-                           front, middle, data, connection_state.get());
-  if (!message) {
-    ret = -EINVAL;
-    goto out_dethrottle;
-  }
-
-  //
-  //  Check the signature if one should be present.  A zero return indicates success. PLR
-  //
-
-  if (auth_handler == NULL) {
-    ldout(msgr->cct, 10) << "No session security set" << dendl;
-  } else {
-    if (auth_handler->check_message_signature(message)) {
-      ldout(msgr->cct, 0) << "Signature check failed" << dendl;
-      message->put();
-      ret = -EINVAL;
-      goto out_dethrottle;
-    } 
-  }
-
-  message->set_byte_throttler(policy.throttler_bytes);
-  message->set_message_throttler(policy.throttler_messages);
-
-  // store reservation size in message, so we don't get confused
-  // by messages entering the dispatch queue through other paths.
-  message->set_dispatch_throttle_size(message_size);
-
-  message->set_recv_stamp(recv_stamp);
-  message->set_throttle_stamp(throttle_stamp);
-  message->set_recv_complete_stamp(ceph_clock_now());
-
-  *pm = message;
-  return 0;
-
- out_dethrottle:
-  // release bytes reserved from the throttlers on failure
-  if (policy.throttler_messages) {
-    ldout(msgr->cct,10) << "reader releasing " << 1 << " message to policy throttler "
-			<< policy.throttler_messages->get_current() << "/"
-			<< policy.throttler_messages->get_max() << dendl;
-    policy.throttler_messages->put();
-  }
-  if (message_size) {
-    if (policy.throttler_bytes) {
-      ldout(msgr->cct,10) << "reader releasing " << message_size << " bytes to policy throttler "
-			  << policy.throttler_bytes->get_current() << "/"
-			  << policy.throttler_bytes->get_max() << dendl;
-      policy.throttler_bytes->put(message_size);
-    }
-
-    in_q->dispatch_throttle_release(message_size);
-  }
-  return ret;
-}
-
-/* 
- SIGPIPE suppression - for platforms without SO_NOSIGPIPE or MSG_NOSIGNAL
-  http://krokisplace.blogspot.in/2010/02/suppressing-sigpipe-in-library.html 
-  http://www.microhowto.info/howto/ignore_sigpipe_without_affecting_other_threads_in_a_process.html 
-*/
-void Pipe::suppress_sigpipe()
-{
-#if !defined(MSG_NOSIGNAL) && !defined(SO_NOSIGPIPE)
-  /*
-    We want to ignore possible SIGPIPE that we can generate on write.
-    SIGPIPE is delivered *synchronously* and *only* to the thread
-    doing the write.  So if it is reported as already pending (which
-    means the thread blocks it), then we do nothing: if we generate
-    SIGPIPE, it will be merged with the pending one (there's no
-    queuing), and that suits us well.  If it is not pending, we block
-    it in this thread (and we avoid changing signal action, because it
-    is per-process).
-  */
-  sigset_t pending;
-  sigemptyset(&pending);
-  sigpending(&pending);
-  sigpipe_pending = sigismember(&pending, SIGPIPE);
-  if (!sigpipe_pending) {
-    sigset_t blocked;
-    sigemptyset(&blocked);
-    pthread_sigmask(SIG_BLOCK, &sigpipe_mask, &blocked);
-
-    /* Maybe is was blocked already?  */
-    sigpipe_unblock = ! sigismember(&blocked, SIGPIPE);
-  }
-#endif  /* !defined(MSG_NOSIGNAL) && !defined(SO_NOSIGPIPE) */
-}
-
-
-void Pipe::restore_sigpipe()
-{
-#if !defined(MSG_NOSIGNAL) && !defined(SO_NOSIGPIPE)
-  /*
-    If SIGPIPE was pending already we do nothing.  Otherwise, if it
-    become pending (i.e., we generated it), then we sigwait() it (thus
-    clearing pending status).  Then we unblock SIGPIPE, but only if it
-    were us who blocked it.
-  */
-  if (!sigpipe_pending) {
-    sigset_t pending;
-    sigemptyset(&pending);
-    sigpending(&pending);
-    if (sigismember(&pending, SIGPIPE)) {
-      /*
-        Protect ourselves from a situation when SIGPIPE was sent
-        by the user to the whole process, and was delivered to
-        other thread before we had a chance to wait for it.
-      */
-      static const struct timespec nowait = { 0, 0 };
-      TEMP_FAILURE_RETRY(sigtimedwait(&sigpipe_mask, NULL, &nowait));
-    }
-
-    if (sigpipe_unblock)
-      pthread_sigmask(SIG_UNBLOCK, &sigpipe_mask, NULL);
-  }
-#endif  /* !defined(MSG_NOSIGNAL) && !defined(SO_NOSIGPIPE) */
-}
-
-
-int Pipe::do_sendmsg(struct msghdr *msg, unsigned len, bool more)
-{
-  suppress_sigpipe();
-  while (len > 0) {
-    int r;
-#if defined(MSG_NOSIGNAL)
-    r = ::sendmsg(sd, msg, MSG_NOSIGNAL | (more ? MSG_MORE : 0));
-#else
-    r = ::sendmsg(sd, msg, (more ? MSG_MORE : 0));
-#endif
-    if (r == 0) 
-      ldout(msgr->cct,10) << "do_sendmsg hmm do_sendmsg got r==0!" << dendl;
-    if (r < 0) {
-      r = -errno; 
-      ldout(msgr->cct,1) << "do_sendmsg error " << cpp_strerror(r) << dendl;
-      restore_sigpipe();
-      return r;
-    }
-    if (state == STATE_CLOSED) {
-      ldout(msgr->cct,10) << "do_sendmsg oh look, state == CLOSED, giving up" << dendl;
-      restore_sigpipe();
-      return -EINTR; // close enough
-    }
-
-    len -= r;
-    if (len == 0) break;
-    
-    // hrmph.  trim r bytes off the front of our message.
-    ldout(msgr->cct,20) << "do_sendmsg short write did " << r << ", still have " << len << dendl;
-    while (r > 0) {
-      if (msg->msg_iov[0].iov_len <= (size_t)r) {
-	// lose this whole item
-	//ldout(msgr->cct,30) << "skipping " << msg->msg_iov[0].iov_len << ", " << (msg->msg_iovlen-1) << " v, " << r << " left" << dendl;
-	r -= msg->msg_iov[0].iov_len;
-	msg->msg_iov++;
-	msg->msg_iovlen--;
-      } else {
-	// partial!
-	//ldout(msgr->cct,30) << "adjusting " << msg->msg_iov[0].iov_len << ", " << msg->msg_iovlen << " v, " << r << " left" << dendl;
-	msg->msg_iov[0].iov_base = (char *)msg->msg_iov[0].iov_base + r;
-	msg->msg_iov[0].iov_len -= r;
-	break;
-      }
-    }
-  }
-  restore_sigpipe();
-  return 0;
-}
-
-
-int Pipe::write_ack(uint64_t seq)
-{
-  ldout(msgr->cct,10) << "write_ack " << seq << dendl;
-
-  char c = CEPH_MSGR_TAG_ACK;
-  ceph_le64 s;
-  s = seq;
-
-  struct msghdr msg;
-  memset(&msg, 0, sizeof(msg));
-  struct iovec msgvec[2];
-  msgvec[0].iov_base = &c;
-  msgvec[0].iov_len = 1;
-  msgvec[1].iov_base = &s;
-  msgvec[1].iov_len = sizeof(s);
-  msg.msg_iov = msgvec;
-  msg.msg_iovlen = 2;
-  
-  if (do_sendmsg(&msg, 1 + sizeof(s), true) < 0)
-    return -1;	
-  return 0;
-}
-
-int Pipe::write_keepalive()
-{
-  ldout(msgr->cct,10) << "write_keepalive" << dendl;
-
-  char c = CEPH_MSGR_TAG_KEEPALIVE;
-
-  struct msghdr msg;
-  memset(&msg, 0, sizeof(msg));
-  struct iovec msgvec[2];
-  msgvec[0].iov_base = &c;
-  msgvec[0].iov_len = 1;
-  msg.msg_iov = msgvec;
-  msg.msg_iovlen = 1;
-  
-  if (do_sendmsg(&msg, 1) < 0)
-    return -1;	
-  return 0;
-}
-
-int Pipe::write_keepalive2(char tag, const utime_t& t)
-{
-  ldout(msgr->cct,10) << "write_keepalive2 " << (int)tag << " " << t << dendl;
-  struct ceph_timespec ts;
-  t.encode_timeval(&ts);
-  struct msghdr msg;
-  memset(&msg, 0, sizeof(msg));
-  struct iovec msgvec[2];
-  msgvec[0].iov_base = &tag;
-  msgvec[0].iov_len = 1;
-  msgvec[1].iov_base = &ts;
-  msgvec[1].iov_len = sizeof(ts);
-  msg.msg_iov = msgvec;
-  msg.msg_iovlen = 2;
-
-  if (do_sendmsg(&msg, 1 + sizeof(ts)) < 0)
-    return -1;
-  return 0;
-}
-
-
-int Pipe::write_message(const ceph_msg_header& header, const ceph_msg_footer& footer, bufferlist& blist)
-{
-  int ret;
-
-  // set up msghdr and iovecs
-  struct msghdr msg;
-  memset(&msg, 0, sizeof(msg));
-  msg.msg_iov = msgvec;
-  int msglen = 0;
-  
-  // send tag
-  char tag = CEPH_MSGR_TAG_MSG;
-  msgvec[msg.msg_iovlen].iov_base = &tag;
-  msgvec[msg.msg_iovlen].iov_len = 1;
-  msglen++;
-  msg.msg_iovlen++;
-
-  // send envelope
-  ceph_msg_header_old oldheader;
-  if (connection_state->has_feature(CEPH_FEATURE_NOSRCADDR)) {
-    msgvec[msg.msg_iovlen].iov_base = (char*)&header;
-    msgvec[msg.msg_iovlen].iov_len = sizeof(header);
-    msglen += sizeof(header);
-    msg.msg_iovlen++;
-  } else {
-    memcpy(&oldheader, &header, sizeof(header));
-    oldheader.src.name = header.src;
-    oldheader.src.addr = connection_state->get_peer_addr();
-    oldheader.orig_src = oldheader.src;
-    oldheader.reserved = header.reserved;
-    if (msgr->crcflags & MSG_CRC_HEADER) {
-	oldheader.crc = ceph_crc32c(0, (unsigned char*)&oldheader,
-				    sizeof(oldheader) - sizeof(oldheader.crc));
-    } else {
-	oldheader.crc = 0;
-    }
-    msgvec[msg.msg_iovlen].iov_base = (char*)&oldheader;
-    msgvec[msg.msg_iovlen].iov_len = sizeof(oldheader);
-    msglen += sizeof(oldheader);
-    msg.msg_iovlen++;
-  }
-
-  // payload (front+data)
-  list<bufferptr>::const_iterator pb = blist.buffers().begin();
-  unsigned b_off = 0;  // carry-over buffer offset, if any
-  unsigned bl_pos = 0; // blist pos
-  unsigned left = blist.length();
-
-  while (left > 0) {
-    unsigned donow = MIN(left, pb->length()-b_off);
-    if (donow == 0) {
-      ldout(msgr->cct,0) << "donow = " << donow << " left " << left << " pb->length " << pb->length()
-                         << " b_off " << b_off << dendl;
-    }
-    assert(donow > 0);
-    ldout(msgr->cct,30) << " bl_pos " << bl_pos << " b_off " << b_off
-	     << " leftinchunk " << left
-	     << " buffer len " << pb->length()
-	     << " writing " << donow 
-	     << dendl;
-    
-    if (msg.msg_iovlen >= SM_IOV_MAX-2) {
-      if (do_sendmsg(&msg, msglen, true))
-	goto fail;
-      
-      // and restart the iov
-      msg.msg_iov = msgvec;
-      msg.msg_iovlen = 0;
-      msglen = 0;
-    }
-    
-    msgvec[msg.msg_iovlen].iov_base = (void*)(pb->c_str()+b_off);
-    msgvec[msg.msg_iovlen].iov_len = donow;
-    msglen += donow;
-    msg.msg_iovlen++;
-    
-    assert(left >= donow);
-    left -= donow;
-    b_off += donow;
-    bl_pos += donow;
-    if (left == 0)
-      break;
-    while (b_off == pb->length()) {
-      ++pb;
-      b_off = 0;
-    }
-  }
-  assert(left == 0);
-
-  // send footer; if receiver doesn't support signatures, use the old footer format
-
-  ceph_msg_footer_old old_footer;
-  if (connection_state->has_feature(CEPH_FEATURE_MSG_AUTH)) {
-    msgvec[msg.msg_iovlen].iov_base = (void*)&footer;
-    msgvec[msg.msg_iovlen].iov_len = sizeof(footer);
-    msglen += sizeof(footer);
-    msg.msg_iovlen++;
-  } else {
-    if (msgr->crcflags & MSG_CRC_HEADER) {
-      old_footer.front_crc = footer.front_crc;
-      old_footer.middle_crc = footer.middle_crc;
-    } else {
-	old_footer.front_crc = old_footer.middle_crc = 0;
-    }
-    old_footer.data_crc = msgr->crcflags & MSG_CRC_DATA ? footer.data_crc : 0;
-    old_footer.flags = footer.flags;   
-    msgvec[msg.msg_iovlen].iov_base = (char*)&old_footer;
-    msgvec[msg.msg_iovlen].iov_len = sizeof(old_footer);
-    msglen += sizeof(old_footer);
-    msg.msg_iovlen++;
-  }
-
-  // send
-  if (do_sendmsg(&msg, msglen))
-    goto fail;
-
-  ret = 0;
-
- out:
-  return ret;
-
- fail:
-  ret = -1;
-  goto out;
-}
-
-
-int Pipe::tcp_read(char *buf, unsigned len)
-{
-  if (sd < 0)
-    return -EINVAL;
-
-  while (len > 0) {
-
-    if (msgr->cct->_conf->ms_inject_socket_failures && sd >= 0) {
-      if (rand() % msgr->cct->_conf->ms_inject_socket_failures == 0) {
-	ldout(msgr->cct, 0) << "injecting socket failure" << dendl;
-	::shutdown(sd, SHUT_RDWR);
-      }
-    }
-
-    if (tcp_read_wait() < 0)
-      return -1;
-
-    ssize_t got = tcp_read_nonblocking(buf, len);
-
-    if (got < 0)
-      return -1;
-
-    len -= got;
-    buf += got;
-    //lgeneric_dout(cct, DBL) << "tcp_read got " << got << ", " << len << " left" << dendl;
-  }
-  return 0;
-}
-
-int Pipe::tcp_read_wait()
-{
-  if (sd < 0)
-    return -EINVAL;
-  struct pollfd pfd;
-  short evmask;
-  pfd.fd = sd;
-  pfd.events = POLLIN;
-#if defined(__linux__)
-  pfd.events |= POLLRDHUP;
-#endif
-
-  if (has_pending_data())
-    return 0;
-
-  int r = poll(&pfd, 1, msgr->timeout);
-  if (r < 0)
-    return -errno;
-  if (r == 0)
-    return -EAGAIN;
-
-  evmask = POLLERR | POLLHUP | POLLNVAL;
-#if defined(__linux__)
-  evmask |= POLLRDHUP;
-#endif
-  if (pfd.revents & evmask)
-    return -1;
-
-  if (!(pfd.revents & POLLIN))
-    return -1;
-
-  return 0;
-}
-
-ssize_t Pipe::do_recv(char *buf, size_t len, int flags)
-{
-again:
-  ssize_t got = ::recv( sd, buf, len, flags );
-  if (got < 0) {
-    if (errno == EINTR) {
-      goto again;
-    }
-    ldout(msgr->cct, 10) << __func__ << " socket " << sd << " returned "
-		     << got << " " << cpp_strerror(errno) << dendl;
-    return -1;
-  }
-  if (got == 0) {
-    return -1;
-  }
-  return got;
-}
-
-ssize_t Pipe::buffered_recv(char *buf, size_t len, int flags)
-{
-  size_t left = len;
-  ssize_t total_recv = 0;
-  if (recv_len > recv_ofs) {
-    int to_read = MIN(recv_len - recv_ofs, left);
-    memcpy(buf, &recv_buf[recv_ofs], to_read);
-    recv_ofs += to_read;
-    left -= to_read;
-    if (left == 0) {
-      return to_read;
-    }
-    buf += to_read;
-    total_recv += to_read;
-  }
-
-  /* nothing left in the prefetch buffer */
-
-  if (left > recv_max_prefetch) {
-    /* this was a large read, we don't prefetch for these */
-    ssize_t ret = do_recv(buf, left, flags );
-    if (ret < 0) {
-      if (total_recv > 0)
-        return total_recv;
-      return ret;
-    }
-    total_recv += ret;
-    return total_recv;
-  }
-
-
-  ssize_t got = do_recv(recv_buf, recv_max_prefetch, flags);
-  if (got < 0) {
-    if (total_recv > 0)
-      return total_recv;
-
-    return got;
-  }
-
-  recv_len = (size_t)got;
-  got = MIN(left, (size_t)got);
-  memcpy(buf, recv_buf, got);
-  recv_ofs = got;
-  total_recv += got;
-  return total_recv;
-}
-
-ssize_t Pipe::tcp_read_nonblocking(char *buf, unsigned len)
-{
-  ssize_t got = buffered_recv(buf, len, MSG_DONTWAIT );
-  if (got < 0) {
-    ldout(msgr->cct, 10) << __func__ << " socket " << sd << " returned "
-		         << got << " " << cpp_strerror(errno) << dendl;
-    return -1;
-  }
-  if (got == 0) {
-    /* poll() said there was data, but we didn't read any - peer
-     * sent a FIN.  Maybe POLLRDHUP signals this, but this is
-     * standard socket behavior as documented by Stevens.
-     */
-    return -1;
-  }
-  return got;
-}
-
-int Pipe::tcp_write(const char *buf, unsigned len)
-{
-  if (sd < 0)
-    return -1;
-  struct pollfd pfd;
-  pfd.fd = sd;
-  pfd.events = POLLOUT | POLLHUP | POLLNVAL | POLLERR;
-#if defined(__linux__)
-  pfd.events |= POLLRDHUP;
-#endif
-
-  if (msgr->cct->_conf->ms_inject_socket_failures && sd >= 0) {
-    if (rand() % msgr->cct->_conf->ms_inject_socket_failures == 0) {
-      ldout(msgr->cct, 0) << "injecting socket failure" << dendl;
-      ::shutdown(sd, SHUT_RDWR);
-    }
-  }
-
-  if (poll(&pfd, 1, -1) < 0)
-    return -1;
-
-  if (!(pfd.revents & POLLOUT))
-    return -1;
-
-  //lgeneric_dout(cct, DBL) << "tcp_write writing " << len << dendl;
-  assert(len > 0);
-  suppress_sigpipe();
-
-  while (len > 0) {
-    int did;
-#if defined(MSG_NOSIGNAL)
-    did = ::send( sd, buf, len, MSG_NOSIGNAL );
-#else
-    did = ::send( sd, buf, len, 0);
-#endif
-    if (did < 0) {
-      //lgeneric_dout(cct, 1) << "tcp_write error did = " << did << " " << cpp_strerror(errno) << dendl;
-      //lgeneric_derr(cct, 1) << "tcp_write error did = " << did << " " << cpp_strerror(errno) << dendl;
-      return did;
-    }
-    len -= did;
-    buf += did;
-    //lgeneric_dout(cct, DBL) << "tcp_write did " << did << ", " << len << " left" << dendl;
-  }
-  restore_sigpipe();
-
-  return 0;
-}
diff --git a/src/msg/simple/Pipe.h b/src/msg/simple/Pipe.h
deleted file mode 100644
index 9dd00d1b48f..00000000000
--- a/src/msg/simple/Pipe.h
+++ /dev/null
@@ -1,325 +0,0 @@
-// -*- mode:C++; tab-width:8; c-basic-offset:2; indent-tabs-mode:t -*- 
-// vim: ts=8 sw=2 smarttab
-/*
- * Ceph - scalable distributed file system
- *
- * Copyright (C) 2004-2006 Sage Weil <sage@newdream.net>
- *
- * This is free software; you can redistribute it and/or
- * modify it under the terms of the GNU Lesser General Public
- * License version 2.1, as published by the Free Software 
- * Foundation.  See file COPYING.
- * 
- */
-
-#ifndef CEPH_MSGR_PIPE_H
-#define CEPH_MSGR_PIPE_H
-
-#include "include/memory.h"
-#include "auth/AuthSessionHandler.h"
-
-#include "msg/msg_types.h"
-#include "msg/Messenger.h"
-#include "PipeConnection.h"
-
-
-class SimpleMessenger;
-class DispatchQueue;
-
-static const int SM_IOV_MAX = (IOV_MAX >= 1024 ? IOV_MAX / 4 : IOV_MAX);
-
-  /**
-   * The Pipe is the most complex SimpleMessenger component. It gets
-   * two threads, one each for reading and writing on a socket it's handed
-   * at creation time, and is responsible for everything that happens on
-   * that socket. Besides message transmission, it's responsible for
-   * propagating socket errors to the SimpleMessenger and then sticking
-   * around in a state where it can provide enough data for the SimpleMessenger
-   * to provide reliable Message delivery when it manages to reconnect.
-   */
-  class Pipe : public RefCountedObject {
-    /**
-     * The Reader thread handles all reads off the socket -- not just
-     * Messages, but also acks and other protocol bits (excepting startup,
-     * when the Writer does a couple of reads).
-     * All the work is implemented in Pipe itself, of course.
-     */
-    class Reader : public Thread {
-      Pipe *pipe;
-    public:
-      explicit Reader(Pipe *p) : pipe(p) {}
-      void *entry() override { pipe->reader(); return 0; }
-    } reader_thread;
-
-    /**
-     * The Writer thread handles all writes to the socket (after startup).
-     * All the work is implemented in Pipe itself, of course.
-     */
-    class Writer : public Thread {
-      Pipe *pipe;
-    public:
-      explicit Writer(Pipe *p) : pipe(p) {}
-      void *entry() override { pipe->writer(); return 0; }
-    } writer_thread;
-
-    class DelayedDelivery;
-    DelayedDelivery *delay_thread;
-  public:
-    Pipe(SimpleMessenger *r, int st, PipeConnection *con);
-    ~Pipe() override;
-
-    SimpleMessenger *msgr;
-    uint64_t conn_id;
-    ostream& _pipe_prefix(std::ostream &out) const;
-
-    Pipe* get() {
-      return static_cast<Pipe*>(RefCountedObject::get());
-    }
-
-    bool is_connected() {
-      Mutex::Locker l(pipe_lock);
-      return state == STATE_OPEN;
-    }
-
-    char *recv_buf;
-    size_t recv_max_prefetch;
-    size_t recv_ofs;
-    size_t recv_len;
-
-    enum {
-      STATE_ACCEPTING,
-      STATE_CONNECTING,
-      STATE_OPEN,
-      STATE_STANDBY,
-      STATE_CLOSED,
-      STATE_CLOSING,
-      STATE_WAIT       // just wait for racing connection
-    };
-
-    static const char *get_state_name(int s) {
-      switch (s) {
-      case STATE_ACCEPTING: return "accepting";
-      case STATE_CONNECTING: return "connecting";
-      case STATE_OPEN: return "open";
-      case STATE_STANDBY: return "standby";
-      case STATE_CLOSED: return "closed";
-      case STATE_CLOSING: return "closing";
-      case STATE_WAIT: return "wait";
-      default: return "UNKNOWN";
-      }
-    }
-    const char *get_state_name() {
-      return get_state_name(state);
-    }
-
-  private:
-    int sd;
-    struct iovec msgvec[SM_IOV_MAX];
-#if !defined(MSG_NOSIGNAL) && !defined(SO_NOSIGPIPE)
-    sigset_t sigpipe_mask;
-    bool sigpipe_pending;
-    bool sigpipe_unblock;
-#endif
-
-  public:
-    int port;
-    int peer_type;
-    entity_addr_t peer_addr;
-    Messenger::Policy policy;
-    
-    Mutex pipe_lock;
-    int state;
-    std::atomic<bool> state_closed = { false }; // true iff state = STATE_CLOSED
-
-    // session_security handles any signatures or encryptions required for this pipe's msgs. PLR
-
-    ceph::shared_ptr<AuthSessionHandler> session_security;
-
-  protected:
-    friend class SimpleMessenger;
-    PipeConnectionRef connection_state;
-
-    utime_t backoff;         // backoff time
-
-    bool reader_running, reader_needs_join;
-    bool reader_dispatching; /// reader thread is dispatching without pipe_lock
-    bool notify_on_dispatch_done; /// something wants a signal when dispatch done
-    bool writer_running;
-
-    map<int, list<Message*> > out_q;  // priority queue for outbound msgs
-    DispatchQueue *in_q;
-    list<Message*> sent;
-    Cond cond;
-    bool send_keepalive;
-    bool send_keepalive_ack;
-    utime_t keepalive_ack_stamp;
-    bool halt_delivery; //if a pipe's queue is destroyed, stop adding to it
-    
-    __u32 connect_seq, peer_global_seq;
-    uint64_t out_seq;
-    uint64_t in_seq, in_seq_acked;
-    
-    void set_socket_options();
-
-    int accept();   // server handshake
-    int connect();  // client handshake
-    void reader();
-    void writer();
-    void unlock_maybe_reap();
-
-    int randomize_out_seq();
-
-    int read_message(Message **pm,
-		     AuthSessionHandler *session_security_copy);
-    int write_message(const ceph_msg_header& h, const ceph_msg_footer& f, bufferlist& body);
-    /**
-     * Write the given data (of length len) to the Pipe's socket. This function
-     * will loop until all passed data has been written out.
-     * If more is set, the function will optimize socket writes
-     * for additional data (by passing the MSG_MORE flag, aka TCP_CORK).
-     *
-     * @param msg The msghdr to write out
-     * @param len The length of the data in msg
-     * @param more Should be set true if this is one part of a larger message
-     * @return 0, or -1 on failure (unrecoverable -- close the socket).
-     */
-    int do_sendmsg(struct msghdr *msg, unsigned len, bool more=false);
-    int write_ack(uint64_t s);
-    int write_keepalive();
-    int write_keepalive2(char tag, const utime_t &t);
-
-    void suppress_sigpipe();
-    void restore_sigpipe();
-
-
-    void fault(bool reader=false);
-
-    void was_session_reset();
-
-    /* Clean up sent list */
-    void handle_ack(uint64_t seq);
-
-    public:
-    Pipe(const Pipe& other);
-    const Pipe& operator=(const Pipe& other);
-
-    void start_reader();
-    void start_writer();
-    void maybe_start_delay_thread();
-    void join_reader();
-
-    // public constructors
-    static const Pipe& Server(int s);
-    static const Pipe& Client(const entity_addr_t& pi);
-
-    uint64_t get_out_seq() { return out_seq; }
-
-    bool is_queued() { return !out_q.empty() || send_keepalive || send_keepalive_ack; }
-
-    entity_addr_t& get_peer_addr() { return peer_addr; }
-
-    void set_peer_addr(const entity_addr_t& a) {
-      if (&peer_addr != &a)  // shut up valgrind
-        peer_addr = a;
-      connection_state->set_peer_addr(a);
-    }
-    void set_peer_type(int t) {
-      peer_type = t;
-      connection_state->set_peer_type(t);
-    }
-
-    void register_pipe();
-    void unregister_pipe();
-    void join();
-    /// stop a Pipe by closing its socket and setting it to STATE_CLOSED
-    void stop();
-    /// stop() a Pipe if not already done, and wait for it to finish any
-    /// fast_dispatch in progress.
-    void stop_and_wait();
-
-    void _send(Message *m) {
-      assert(pipe_lock.is_locked());
-      out_q[m->get_priority()].push_back(m);
-      cond.Signal();
-    }
-    void _send_keepalive() {
-      assert(pipe_lock.is_locked());
-      send_keepalive = true;
-      cond.Signal();
-    }
-    Message *_get_next_outgoing() {
-      assert(pipe_lock.is_locked());
-      Message *m = 0;
-      while (!m && !out_q.empty()) {
-        map<int, list<Message*> >::reverse_iterator p = out_q.rbegin();
-        if (!p->second.empty()) {
-          m = p->second.front();
-          p->second.pop_front();
-        }
-        if (p->second.empty())
-          out_q.erase(p->first);
-      }
-      return m;
-    }
-
-    /// move all messages in the sent list back into the queue at the highest priority.
-    void requeue_sent();
-    /// discard messages requeued by requeued_sent() up to a given seq
-    void discard_requeued_up_to(uint64_t seq);
-    void discard_out_queue();
-
-    void shutdown_socket() {
-      recv_reset();
-      if (sd >= 0)
-        ::shutdown(sd, SHUT_RDWR);
-    }
-
-    void recv_reset() {
-      recv_len = 0;
-      recv_ofs = 0;
-    }
-    ssize_t do_recv(char *buf, size_t len, int flags);
-    ssize_t buffered_recv(char *buf, size_t len, int flags);
-    bool has_pending_data() { return recv_len > recv_ofs; }
-
-    /**
-     * do a blocking read of len bytes from socket
-     *
-     * @param buf buffer to read into
-     * @param len exact number of bytes to read
-     * @return 0 for success, or -1 on error
-     */
-    int tcp_read(char *buf, unsigned len);
-
-    /**
-     * wait for bytes to become available on the socket
-     *
-     * @return 0 for success, or -1 on error
-     */
-    int tcp_read_wait();
-
-    /**
-     * non-blocking read of available bytes on socket
-     *
-     * This is expected to be used after tcp_read_wait(), and will return
-     * an error if there is no data on the socket to consume.
-     *
-     * @param buf buffer to read into
-     * @param len maximum number of bytes to read
-     * @return bytes read, or -1 on error or when there is no data
-     */
-    ssize_t tcp_read_nonblocking(char *buf, unsigned len);
-
-    /**
-     * blocking write of bytes to socket
-     *
-     * @param buf buffer
-     * @param len number of bytes to write
-     * @return 0 for success, or -1 on error
-     */
-    int tcp_write(const char *buf, unsigned len);
-
-  };
-
-
-#endif
diff --git a/src/msg/simple/PipeConnection.cc b/src/msg/simple/PipeConnection.cc
deleted file mode 100644
index 96e27a4fd30..00000000000
--- a/src/msg/simple/PipeConnection.cc
+++ /dev/null
@@ -1,96 +0,0 @@
-// -*- mode:C++; tab-width:8; c-basic-offset:2; indent-tabs-mode:t -*-
-// vim: ts=8 sw=2 smarttab
-/*
- * Ceph - scalable distributed file system
- *
- * Copyright (C) 2004-2006 Sage Weil <sage@newdream.net>
- *
- * This is free software; you can redistribute it and/or
- * modify it under the terms of the GNU Lesser General Public
- * License version 2.1, as published by the Free Software
- * Foundation.  See file COPYING.
- *
- */
-
-#include "msg/Message.h"
-#include "Pipe.h"
-#include "SimpleMessenger.h"
-#include "PipeConnection.h"
-
-PipeConnection::~PipeConnection()
-{
-  if (pipe) {
-    pipe->put();
-    pipe = NULL;
-  }
-}
-
-Pipe* PipeConnection::get_pipe()
-{
-  Mutex::Locker l(lock);
-  if (pipe)
-    return pipe->get();
-  return NULL;
-}
-
-bool PipeConnection::try_get_pipe(Pipe **p)
-{
-  Mutex::Locker l(lock);
-  if (failed) {
-    *p = NULL;
-  } else {
-    if (pipe)
-      *p = pipe->get();
-    else
-      *p = NULL;
-  }
-  return !failed;
-}
-
-bool PipeConnection::clear_pipe(Pipe *old_p)
-{
-  Mutex::Locker l(lock);
-  if (old_p == pipe) {
-    pipe->put();
-    pipe = NULL;
-    failed = true;
-    return true;
-  }
-  return false;
-}
-
-void PipeConnection::reset_pipe(Pipe *p)
-{
-  Mutex::Locker l(lock);
-  if (pipe)
-    pipe->put();
-  pipe = p->get();
-}
-
-bool PipeConnection::is_connected()
-{
-  return static_cast<SimpleMessenger*>(msgr)->is_connected(this);
-}
-
-int PipeConnection::send_message(Message *m)
-{
-  assert(msgr);
-  return static_cast<SimpleMessenger*>(msgr)->send_message(m, this);
-}
-
-void PipeConnection::send_keepalive()
-{
-  static_cast<SimpleMessenger*>(msgr)->send_keepalive(this);
-}
-
-void PipeConnection::mark_down()
-{
-  if (msgr)
-    static_cast<SimpleMessenger*>(msgr)->mark_down(this);
-}
-
-void PipeConnection::mark_disposable()
-{
-  if (msgr)
-    static_cast<SimpleMessenger*>(msgr)->mark_disposable(this);
-}
diff --git a/src/msg/simple/PipeConnection.h b/src/msg/simple/PipeConnection.h
deleted file mode 100644
index 04139193854..00000000000
--- a/src/msg/simple/PipeConnection.h
+++ /dev/null
@@ -1,55 +0,0 @@
-// -*- mode:C++; tab-width:8; c-basic-offset:2; indent-tabs-mode:t -*-
-// vim: ts=8 sw=2 smarttab
-/*
- * Ceph - scalable distributed file system
- *
- * Copyright (C) 2014 Red Hat
- *
- * This is free software; you can redistribute it and/or
- * modify it under the terms of the GNU Lesser General Public
- * License version 2.1, as published by the Free Software
- * Foundation.  See file COPYING.
- *
- */
-
-#ifndef CEPH_MSG_PIPECONNECTION_H
-#define CEPH_MSG_PIPECONNECTION_H
-
-#include "msg/Connection.h"
-
-class Pipe;
-
-class PipeConnection : public Connection {
-  Pipe* pipe;
-
-  friend class boost::intrusive_ptr<PipeConnection>;
-  friend class Pipe;
-
-public:
-
-  PipeConnection(CephContext *cct, Messenger *m)
-    : Connection(cct, m),
-      pipe(NULL) { }
-
-  ~PipeConnection() override;
-
-  Pipe* get_pipe();
-
-  bool try_get_pipe(Pipe** p);
-
-  bool clear_pipe(Pipe* old_p);
-
-  void reset_pipe(Pipe* p);
-
-  bool is_connected() override;
-
-  int send_message(Message *m) override;
-  void send_keepalive() override;
-  void mark_down() override;
-  void mark_disposable() override;
-
-}; /* PipeConnection */
-
-typedef boost::intrusive_ptr<PipeConnection> PipeConnectionRef;
-
-#endif
diff --git a/src/msg/simple/SimpleMessenger.cc b/src/msg/simple/SimpleMessenger.cc
deleted file mode 100644
index 78e190d027e..00000000000
--- a/src/msg/simple/SimpleMessenger.cc
+++ /dev/null
@@ -1,757 +0,0 @@
-// -*- mode:C++; tab-width:8; c-basic-offset:2; indent-tabs-mode:t -*- 
-// vim: ts=8 sw=2 smarttab
-/*
- * Ceph - scalable distributed file system
- *
- * Copyright (C) 2004-2006 Sage Weil <sage@newdream.net>
- *
- * This is free software; you can redistribute it and/or
- * modify it under the terms of the GNU Lesser General Public
- * License version 2.1, as published by the Free Software 
- * Foundation.  See file COPYING.
- * 
- */
-
-#include <errno.h>
-#include <iostream>
-#include <fstream>
-
-
-#include "SimpleMessenger.h"
-
-#include "common/config.h"
-#include "common/Timer.h"
-#include "common/errno.h"
-#include "common/valgrind.h"
-#include "auth/Crypto.h"
-#include "include/Spinlock.h"
-
-#define dout_subsys ceph_subsys_ms
-#undef dout_prefix
-#define dout_prefix _prefix(_dout, this)
-static ostream& _prefix(std::ostream *_dout, SimpleMessenger *msgr) {
-  return *_dout << "-- " << msgr->get_myaddr() << " ";
-}
-
-
-/*******************
- * SimpleMessenger
- */
-
-SimpleMessenger::SimpleMessenger(CephContext *cct, entity_name_t name,
-				 string mname, uint64_t _nonce)
-  : SimplePolicyMessenger(cct, name,mname, _nonce),
-    accepter(this, _nonce),
-    dispatch_queue(cct, this, mname),
-    reaper_thread(this),
-    nonce(_nonce),
-    lock("SimpleMessenger::lock"), need_addr(true), did_bind(false),
-    global_seq(0),
-    cluster_protocol(0),
-    reaper_started(false), reaper_stop(false),
-    timeout(0),
-    local_connection(new PipeConnection(cct, this))
-{
-  ANNOTATE_BENIGN_RACE_SIZED(&timeout, sizeof(timeout),
-                             "SimpleMessenger read timeout");
-  ceph_spin_init(&global_seq_lock);
-  init_local_connection();
-}
-
-/**
- * Destroy the SimpleMessenger. Pretty simple since all the work is done
- * elsewhere.
- */
-SimpleMessenger::~SimpleMessenger()
-{
-  assert(!did_bind); // either we didn't bind or we shut down the Accepter
-  assert(rank_pipe.empty()); // we don't have any running Pipes.
-  assert(!reaper_started); // the reaper thread is stopped
-  ceph_spin_destroy(&global_seq_lock);
-}
-
-void SimpleMessenger::ready()
-{
-  ldout(cct,10) << "ready " << get_myaddr() << dendl;
-  dispatch_queue.start();
-
-  lock.Lock();
-  if (did_bind)
-    accepter.start();
-  lock.Unlock();
-}
-
-
-int SimpleMessenger::shutdown()
-{
-  ldout(cct,10) << "shutdown " << get_myaddr() << dendl;
-  mark_down_all();
-
-  // break ref cycles on the loopback connection
-  local_connection->set_priv(NULL);
-
-  lock.Lock();
-  stop_cond.Signal();
-  stopped = true;
-  lock.Unlock();
-
-  return 0;
-}
-
-int SimpleMessenger::_send_message(Message *m, const entity_inst_t& dest)
-{
-  // set envelope
-  m->get_header().src = get_myname();
-  m->set_cct(cct);
-
-  if (!m->get_priority()) m->set_priority(get_default_send_priority());
- 
-  ldout(cct,1) <<"--> " << dest.name << " "
-          << dest.addr << " -- " << *m
-    	  << " -- ?+" << m->get_data().length()
-	  << " " << m 
-	  << dendl;
-
-  if (dest.addr == entity_addr_t()) {
-    ldout(cct,0) << "send_message message " << *m
-                 << " with empty dest " << dest.addr << dendl;
-    m->put();
-    return -EINVAL;
-  }
-
-  lock.Lock();
-  Pipe *pipe = _lookup_pipe(dest.addr);
-  submit_message(m, (pipe ? pipe->connection_state.get() : NULL),
-                 dest.addr, dest.name.type(), true);
-  lock.Unlock();
-  return 0;
-}
-
-int SimpleMessenger::_send_message(Message *m, Connection *con)
-{
-  //set envelope
-  m->get_header().src = get_myname();
-
-  if (!m->get_priority()) m->set_priority(get_default_send_priority());
-
-  ldout(cct,1) << "--> " << con->get_peer_addr()
-      << " -- " << *m
-      << " -- ?+" << m->get_data().length()
-      << " " << m << " con " << con
-      << dendl;
-
-  submit_message(m, static_cast<PipeConnection*>(con),
-		 con->get_peer_addr(), con->get_peer_type(), false);
-  return 0;
-}
-
-/**
- * If my_inst.addr doesn't have an IP set, this function
- * will fill it in from the passed addr. Otherwise it does nothing and returns.
- */
-void SimpleMessenger::set_addr_unknowns(const entity_addr_t &addr)
-{
-  if (my_inst.addr.is_blank_ip()) {
-    int port = my_inst.addr.get_port();
-    my_inst.addr.u = addr.u;
-    my_inst.addr.set_port(port);
-    init_local_connection();
-  }
-}
-
-void SimpleMessenger::set_addr(const entity_addr_t &addr)
-{
-  entity_addr_t t = addr;
-  t.set_nonce(nonce);
-  set_myaddr(t);
-  init_local_connection();
-}
-
-int SimpleMessenger::get_proto_version(int peer_type, bool connect)
-{
-  int my_type = my_inst.name.type();
-
-  // set reply protocol version
-  if (peer_type == my_type) {
-    // internal
-    return cluster_protocol;
-  } else {
-    // public
-    if (connect) {
-      switch (peer_type) {
-      case CEPH_ENTITY_TYPE_OSD: return CEPH_OSDC_PROTOCOL;
-      case CEPH_ENTITY_TYPE_MDS: return CEPH_MDSC_PROTOCOL;
-      case CEPH_ENTITY_TYPE_MON: return CEPH_MONC_PROTOCOL;
-      }
-    } else {
-      switch (my_type) {
-      case CEPH_ENTITY_TYPE_OSD: return CEPH_OSDC_PROTOCOL;
-      case CEPH_ENTITY_TYPE_MDS: return CEPH_MDSC_PROTOCOL;
-      case CEPH_ENTITY_TYPE_MON: return CEPH_MONC_PROTOCOL;
-      }
-    }
-  }
-  return 0;
-}
-
-
-
-
-
-
-
-/********************************************
- * SimpleMessenger
- */
-#undef dout_prefix
-#define dout_prefix _prefix(_dout, this)
-
-void SimpleMessenger::reaper_entry()
-{
-  ldout(cct,10) << "reaper_entry start" << dendl;
-  lock.Lock();
-  while (!reaper_stop) {
-    reaper();  // may drop and retake the lock
-    if (reaper_stop)
-      break;
-    reaper_cond.Wait(lock);
-  }
-  lock.Unlock();
-  ldout(cct,10) << "reaper_entry done" << dendl;
-}
-
-/*
- * note: assumes lock is held
- */
-void SimpleMessenger::reaper()
-{
-  ldout(cct,10) << "reaper" << dendl;
-  assert(lock.is_locked());
-
-  while (!pipe_reap_queue.empty()) {
-    Pipe *p = pipe_reap_queue.front();
-    pipe_reap_queue.pop_front();
-    ldout(cct,10) << "reaper reaping pipe " << p << " " <<
-      p->get_peer_addr() << dendl;
-    p->pipe_lock.Lock();
-    p->discard_out_queue();
-    if (p->connection_state) {
-      // mark_down, mark_down_all, or fault() should have done this,
-      // or accept() may have switch the Connection to a different
-      // Pipe... but make sure!
-      bool cleared = p->connection_state->clear_pipe(p);
-      assert(!cleared);
-    }
-    p->pipe_lock.Unlock();
-    p->unregister_pipe();
-    assert(pipes.count(p));
-    pipes.erase(p);
-
-    // drop msgr lock while joining thread; the delay through could be
-    // trying to fast dispatch, preventing it from joining without
-    // blocking and deadlocking.
-    lock.Unlock();
-    p->join();
-    lock.Lock();
-
-    if (p->sd >= 0)
-      ::close(p->sd);
-    ldout(cct,10) << "reaper reaped pipe " << p << " " << p->get_peer_addr() << dendl;
-    p->put();
-    ldout(cct,10) << "reaper deleted pipe " << p << dendl;
-  }
-  ldout(cct,10) << "reaper done" << dendl;
-}
-
-void SimpleMessenger::queue_reap(Pipe *pipe)
-{
-  ldout(cct,10) << "queue_reap " << pipe << dendl;
-  lock.Lock();
-  pipe_reap_queue.push_back(pipe);
-  reaper_cond.Signal();
-  lock.Unlock();
-}
-
-bool SimpleMessenger::is_connected(Connection *con)
-{
-  bool r = false;
-  if (con) {
-    Pipe *p = static_cast<Pipe *>(static_cast<PipeConnection*>(con)->get_pipe());
-    if (p) {
-      assert(p->msgr == this);
-      r = p->is_connected();
-      p->put();
-    }
-  }
-  return r;
-}
-
-int SimpleMessenger::bind(const entity_addr_t &bind_addr)
-{
-  lock.Lock();
-  if (started) {
-    ldout(cct,10) << "rank.bind already started" << dendl;
-    lock.Unlock();
-    return -1;
-  }
-  ldout(cct,10) << "rank.bind " << bind_addr << dendl;
-  lock.Unlock();
-
-  // bind to a socket
-  set<int> avoid_ports;
-  int r = accepter.bind(bind_addr, avoid_ports);
-  if (r >= 0)
-    did_bind = true;
-  return r;
-}
-
-int SimpleMessenger::rebind(const set<int>& avoid_ports)
-{
-  ldout(cct,1) << "rebind avoid " << avoid_ports << dendl;
-  assert(did_bind);
-  accepter.stop();
-  mark_down_all();
-  return accepter.rebind(avoid_ports);
-}
-
-
-int SimpleMessenger::client_bind(const entity_addr_t &bind_addr)
-{
-  if (!cct->_conf->ms_bind_before_connect)
-    return 0;
-  Mutex::Locker l(lock);
-  if (did_bind) {
-    assert(my_inst.addr == bind_addr);
-    return 0;
-  }
-  if (started) {
-    ldout(cct,10) << "rank.bind already started" << dendl;
-    return -1;
-  }
-  ldout(cct,10) << "rank.bind " << bind_addr << dendl;
-
-  set_myaddr(bind_addr);
-  return 0;
-}
-
-
-int SimpleMessenger::start()
-{
-  lock.Lock();
-  ldout(cct,1) << "messenger.start" << dendl;
-
-  // register at least one entity, first!
-  assert(my_inst.name.type() >= 0);
-
-  assert(!started);
-  started = true;
-  stopped = false;
-
-  if (!did_bind) {
-    my_inst.addr.nonce = nonce;
-    init_local_connection();
-  }
-
-  lock.Unlock();
-
-  reaper_started = true;
-  reaper_thread.create("ms_reaper");
-  return 0;
-}
-
-Pipe *SimpleMessenger::add_accept_pipe(int sd)
-{
-  lock.Lock();
-  Pipe *p = new Pipe(this, Pipe::STATE_ACCEPTING, NULL);
-  p->sd = sd;
-  p->pipe_lock.Lock();
-  p->start_reader();
-  p->pipe_lock.Unlock();
-  pipes.insert(p);
-  accepting_pipes.insert(p);
-  lock.Unlock();
-  return p;
-}
-
-/* connect_rank
- * NOTE: assumes messenger.lock held.
- */
-Pipe *SimpleMessenger::connect_rank(const entity_addr_t& addr,
-				    int type,
-				    PipeConnection *con,
-				    Message *first)
-{
-  assert(lock.is_locked());
-  assert(addr != my_inst.addr);
-  
-  ldout(cct,10) << "connect_rank to " << addr << ", creating pipe and registering" << dendl;
-  
-  // create pipe
-  Pipe *pipe = new Pipe(this, Pipe::STATE_CONNECTING,
-			static_cast<PipeConnection*>(con));
-  pipe->pipe_lock.Lock();
-  pipe->set_peer_type(type);
-  pipe->set_peer_addr(addr);
-  pipe->policy = get_policy(type);
-  pipe->start_writer();
-  if (first)
-    pipe->_send(first);
-  pipe->pipe_lock.Unlock();
-  pipe->register_pipe();
-  pipes.insert(pipe);
-
-  return pipe;
-}
-
-
-
-
-
-
-AuthAuthorizer *SimpleMessenger::get_authorizer(int peer_type, bool force_new)
-{
-  return ms_deliver_get_authorizer(peer_type, force_new);
-}
-
-bool SimpleMessenger::verify_authorizer(Connection *con, int peer_type,
-					int protocol, bufferlist& authorizer, bufferlist& authorizer_reply,
-					bool& isvalid,CryptoKey& session_key)
-{
-  return ms_deliver_verify_authorizer(con, peer_type, protocol, authorizer, authorizer_reply, isvalid,session_key);
-}
-
-ConnectionRef SimpleMessenger::get_connection(const entity_inst_t& dest)
-{
-  Mutex::Locker l(lock);
-  if (my_inst.addr == dest.addr) {
-    // local
-    return local_connection;
-  }
-
-  // remote
-  while (true) {
-    Pipe *pipe = _lookup_pipe(dest.addr);
-    if (pipe) {
-      ldout(cct, 10) << "get_connection " << dest << " existing " << pipe << dendl;
-    } else {
-      pipe = connect_rank(dest.addr, dest.name.type(), NULL, NULL);
-      ldout(cct, 10) << "get_connection " << dest << " new " << pipe << dendl;
-    }
-    Mutex::Locker l(pipe->pipe_lock);
-    if (pipe->connection_state)
-      return pipe->connection_state;
-    // we failed too quickly!  retry.  FIXME.
-  }
-}
-
-ConnectionRef SimpleMessenger::get_loopback_connection()
-{
-  return local_connection;
-}
-
-void SimpleMessenger::submit_message(Message *m, PipeConnection *con,
-				     const entity_addr_t& dest_addr, int dest_type,
-				     bool already_locked)
-{
-  m->trace.event("simple submitting message");
-  if (cct->_conf->ms_dump_on_send) {
-    m->encode(-1, true);
-    ldout(cct, 0) << "submit_message " << *m << "\n";
-    m->get_payload().hexdump(*_dout);
-    if (m->get_data().length() > 0) {
-      *_dout << " data:\n";
-      m->get_data().hexdump(*_dout);
-    }
-    *_dout << dendl;
-    m->clear_payload();
-  }
-
-  // existing connection?
-  if (con) {
-    Pipe *pipe = NULL;
-    bool ok = static_cast<PipeConnection*>(con)->try_get_pipe(&pipe);
-    if (!ok) {
-      ldout(cct,0) << "submit_message " << *m << " remote, " << dest_addr
-		   << ", failed lossy con, dropping message " << m << dendl;
-      m->put();
-      return;
-    }
-    while (pipe && ok) {
-      // we loop in case of a racing reconnect, either from us or them
-      pipe->pipe_lock.Lock(); // can't use a Locker because of the Pipe ref
-      if (pipe->state != Pipe::STATE_CLOSED) {
-	ldout(cct,20) << "submit_message " << *m << " remote, " << dest_addr << ", have pipe." << dendl;
-	pipe->_send(m);
-	pipe->pipe_lock.Unlock();
-	pipe->put();
-	return;
-      }
-      Pipe *current_pipe;
-      ok = con->try_get_pipe(&current_pipe);
-      pipe->pipe_lock.Unlock();
-      if (current_pipe == pipe) {
-	ldout(cct,20) << "submit_message " << *m << " remote, " << dest_addr
-		      << ", had pipe " << pipe << ", but it closed." << dendl;
-	pipe->put();
-	current_pipe->put();
-	m->put();
-	return;
-      } else {
-	pipe->put();
-	pipe = current_pipe;
-      }
-    }
-  }
-
-  // local?
-  if (my_inst.addr == dest_addr) {
-    // local
-    ldout(cct,20) << "submit_message " << *m << " local" << dendl;
-    m->set_connection(local_connection.get());
-    dispatch_queue.local_delivery(m, m->get_priority());
-    return;
-  }
-
-  // remote, no existing pipe.
-  const Policy& policy = get_policy(dest_type);
-  if (policy.server) {
-    ldout(cct,20) << "submit_message " << *m << " remote, " << dest_addr << ", lossy server for target type "
-		  << ceph_entity_type_name(dest_type) << ", no session, dropping." << dendl;
-    m->put();
-  } else {
-    ldout(cct,20) << "submit_message " << *m << " remote, " << dest_addr << ", new pipe." << dendl;
-    if (!already_locked) {
-      /** We couldn't handle the Message without reference to global data, so
-       *  grab the lock and do it again. If we got here, we know it's a non-lossy
-       *  Connection, so we can use our existing pointer without doing another lookup. */
-      Mutex::Locker l(lock);
-      submit_message(m, con, dest_addr, dest_type, true);
-    } else {
-      connect_rank(dest_addr, dest_type, static_cast<PipeConnection*>(con), m);
-    }
-  }
-}
-
-int SimpleMessenger::send_keepalive(Connection *con)
-{
-  int ret = 0;
-  Pipe *pipe = static_cast<Pipe *>(
-    static_cast<PipeConnection*>(con)->get_pipe());
-  if (pipe) {
-    ldout(cct,20) << "send_keepalive con " << con << ", have pipe." << dendl;
-    assert(pipe->msgr == this);
-    pipe->pipe_lock.Lock();
-    pipe->_send_keepalive();
-    pipe->pipe_lock.Unlock();
-    pipe->put();
-  } else {
-    ldout(cct,0) << "send_keepalive con " << con << ", no pipe." << dendl;
-    ret = -EPIPE;
-  }
-  return ret;
-}
-
-
-
-void SimpleMessenger::wait()
-{
-  lock.Lock();
-  if (!started) {
-    lock.Unlock();
-    return;
-  }
-  if (!stopped)
-    stop_cond.Wait(lock);
-
-  lock.Unlock();
-
-  // done!  clean up.
-  if (did_bind) {
-    ldout(cct,20) << "wait: stopping accepter thread" << dendl;
-    accepter.stop();
-    did_bind = false;
-    ldout(cct,20) << "wait: stopped accepter thread" << dendl;
-  }
-
-  dispatch_queue.shutdown();
-  if (dispatch_queue.is_started()) {
-    ldout(cct,10) << "wait: waiting for dispatch queue" << dendl;
-    dispatch_queue.wait();
-    dispatch_queue.discard_local();
-    ldout(cct,10) << "wait: dispatch queue is stopped" << dendl;
-  }
-
-  if (reaper_started) {
-    ldout(cct,20) << "wait: stopping reaper thread" << dendl;
-    lock.Lock();
-    reaper_cond.Signal();
-    reaper_stop = true;
-    lock.Unlock();
-    reaper_thread.join();
-    reaper_started = false;
-    ldout(cct,20) << "wait: stopped reaper thread" << dendl;
-  }
-
-  // close+reap all pipes
-  lock.Lock();
-  {
-    ldout(cct,10) << "wait: closing pipes" << dendl;
-
-    while (!rank_pipe.empty()) {
-      Pipe *p = rank_pipe.begin()->second;
-      p->unregister_pipe();
-      p->pipe_lock.Lock();
-      p->stop_and_wait();
-      // don't generate an event here; we're shutting down anyway.
-      PipeConnectionRef con = p->connection_state;
-      if (con)
-	con->clear_pipe(p);
-      p->pipe_lock.Unlock();
-    }
-
-    reaper();
-    ldout(cct,10) << "wait: waiting for pipes " << pipes << " to close" << dendl;
-    while (!pipes.empty()) {
-      reaper_cond.Wait(lock);
-      reaper();
-    }
-  }
-  lock.Unlock();
-
-  ldout(cct,10) << "wait: done." << dendl;
-  ldout(cct,1) << "shutdown complete." << dendl;
-  started = false;
-}
-
-
-void SimpleMessenger::mark_down_all()
-{
-  ldout(cct,1) << "mark_down_all" << dendl;
-  lock.Lock();
-  for (set<Pipe*>::iterator q = accepting_pipes.begin(); q != accepting_pipes.end(); ++q) {
-    Pipe *p = *q;
-    ldout(cct,5) << "mark_down_all accepting_pipe " << p << dendl;
-    p->pipe_lock.Lock();
-    p->stop();
-    PipeConnectionRef con = p->connection_state;
-    if (con && con->clear_pipe(p))
-      dispatch_queue.queue_reset(con.get());
-    p->pipe_lock.Unlock();
-  }
-  accepting_pipes.clear();
-
-  while (!rank_pipe.empty()) {
-    ceph::unordered_map<entity_addr_t,Pipe*>::iterator it = rank_pipe.begin();
-    Pipe *p = it->second;
-    ldout(cct,5) << "mark_down_all " << it->first << " " << p << dendl;
-    rank_pipe.erase(it);
-    p->unregister_pipe();
-    p->pipe_lock.Lock();
-    p->stop();
-    PipeConnectionRef con = p->connection_state;
-    if (con && con->clear_pipe(p))
-      dispatch_queue.queue_reset(con.get());
-    p->pipe_lock.Unlock();
-  }
-  lock.Unlock();
-}
-
-void SimpleMessenger::mark_down(const entity_addr_t& addr)
-{
-  lock.Lock();
-  Pipe *p = _lookup_pipe(addr);
-  if (p) {
-    ldout(cct,1) << "mark_down " << addr << " -- " << p << dendl;
-    p->unregister_pipe();
-    p->pipe_lock.Lock();
-    p->stop();
-    if (p->connection_state) {
-      // generate a reset event for the caller in this case, even
-      // though they asked for it, since this is the addr-based (and
-      // not Connection* based) interface
-      PipeConnectionRef con = p->connection_state;
-      if (con && con->clear_pipe(p))
-	dispatch_queue.queue_reset(con.get());
-    }
-    p->pipe_lock.Unlock();
-  } else {
-    ldout(cct,1) << "mark_down " << addr << " -- pipe dne" << dendl;
-  }
-  lock.Unlock();
-}
-
-void SimpleMessenger::mark_down(Connection *con)
-{
-  if (con == NULL)
-    return;
-  lock.Lock();
-  Pipe *p = static_cast<Pipe *>(static_cast<PipeConnection*>(con)->get_pipe());
-  if (p) {
-    ldout(cct,1) << "mark_down " << con << " -- " << p << dendl;
-    assert(p->msgr == this);
-    p->unregister_pipe();
-    p->pipe_lock.Lock();
-    p->stop();
-    if (p->connection_state) {
-      // do not generate a reset event for the caller in this case,
-      // since they asked for it.
-      p->connection_state->clear_pipe(p);
-    }
-    p->pipe_lock.Unlock();
-    p->put();
-  } else {
-    ldout(cct,1) << "mark_down " << con << " -- pipe dne" << dendl;
-  }
-  lock.Unlock();
-}
-
-void SimpleMessenger::mark_disposable(Connection *con)
-{
-  lock.Lock();
-  Pipe *p = static_cast<Pipe *>(static_cast<PipeConnection*>(con)->get_pipe());
-  if (p) {
-    ldout(cct,1) << "mark_disposable " << con << " -- " << p << dendl;
-    assert(p->msgr == this);
-    p->pipe_lock.Lock();
-    p->policy.lossy = true;
-    p->pipe_lock.Unlock();
-    p->put();
-  } else {
-    ldout(cct,1) << "mark_disposable " << con << " -- pipe dne" << dendl;
-  }
-  lock.Unlock();
-}
-
-void SimpleMessenger::learned_addr(const entity_addr_t &peer_addr_for_me)
-{
-  // be careful here: multiple threads may block here, and readers of
-  // my_inst.addr do NOT hold any lock.
-
-  // this always goes from true -> false under the protection of the
-  // mutex.  if it is already false, we need not retake the mutex at
-  // all.
-  if (!need_addr)
-    return;
-
-  lock.Lock();
-  if (need_addr) {
-    entity_addr_t t = peer_addr_for_me;
-    t.set_port(my_inst.addr.get_port());
-    t.set_nonce(my_inst.addr.get_nonce());
-    ANNOTATE_BENIGN_RACE_SIZED(&my_inst.addr, sizeof(my_inst.addr),
-                               "SimpleMessenger learned addr");
-    my_inst.addr = t;
-    ldout(cct,1) << "learned my addr " << my_inst.addr << dendl;
-    need_addr = false;
-    init_local_connection();
-  }
-  lock.Unlock();
-}
-
-void SimpleMessenger::init_local_connection()
-{
-  local_connection->peer_addr = my_inst.addr;
-  local_connection->peer_type = my_inst.name.type();
-  local_connection->set_features(CEPH_FEATURES_ALL);
-  ms_deliver_handle_fast_connect(local_connection.get());
-}
diff --git a/src/msg/simple/SimpleMessenger.h b/src/msg/simple/SimpleMessenger.h
deleted file mode 100644
index 0a0512382eb..00000000000
--- a/src/msg/simple/SimpleMessenger.h
+++ /dev/null
@@ -1,413 +0,0 @@
-// -*- mode:C++; tab-width:8; c-basic-offset:2; indent-tabs-mode:t -*- 
-// vim: ts=8 sw=2 smarttab
-/*
- * Ceph - scalable distributed file system
- *
- * Copyright (C) 2004-2006 Sage Weil <sage@newdream.net>
- *
- * This is free software; you can redistribute it and/or
- * modify it under the terms of the GNU Lesser General Public
- * License version 2.1, as published by the Free Software 
- * Foundation.  See file COPYING.
- * 
- */
-
-#ifndef CEPH_SIMPLEMESSENGER_H
-#define CEPH_SIMPLEMESSENGER_H
-
-#include "include/types.h"
-#include "include/xlist.h"
-
-#include <list>
-#include <map>
-using namespace std;
-#include "include/unordered_map.h"
-#include "include/unordered_set.h"
-
-#include "common/Mutex.h"
-#include "include/Spinlock.h"
-#include "common/Cond.h"
-#include "common/Thread.h"
-#include "common/Throttle.h"
-
-#include "msg/SimplePolicyMessenger.h"
-#include "msg/Message.h"
-#include "include/assert.h"
-
-#include "msg/DispatchQueue.h"
-#include "Pipe.h"
-#include "Accepter.h"
-
-/*
- * This class handles transmission and reception of messages. Generally
- * speaking, there are several major components:
- *
- * - Connection
- *    Each logical session is associated with a Connection.
- * - Pipe
- *    Each network connection is handled through a pipe, which handles
- *    the input and output of each message.  There is normally a 1:1
- *    relationship between Pipe and Connection, but logical sessions may
- *    get handed off between Pipes when sockets reconnect or during
- *    connection races.
- * - IncomingQueue
- *    Incoming messages are associated with an IncomingQueue, and there
- *    is one such queue associated with each Pipe.
- * - DispatchQueue
- *    IncomingQueues get queued in the DIspatchQueue, which is responsible
- *    for doing a round-robin sweep and processing them via a worker thread.
- * - SimpleMessenger
- *    It's the exterior class passed to the external message handler and
- *    most of the API details.
- *
- * Lock ordering:
- *
- *   SimpleMessenger::lock
- *       Pipe::pipe_lock
- *           DispatchQueue::lock
- *               IncomingQueue::lock
- */
-
-class SimpleMessenger : public SimplePolicyMessenger {
-  // First we have the public Messenger interface implementation...
-public:
-  /**
-   * Initialize the SimpleMessenger!
-   *
-   * @param cct The CephContext to use
-   * @param name The name to assign ourselves
-   * _nonce A unique ID to use for this SimpleMessenger. It should not
-   * be a value that will be repeated if the daemon restarts.
-   * features The local features bits for the local_connection
-   */
-  SimpleMessenger(CephContext *cct, entity_name_t name,
-		  string mname, uint64_t _nonce);
-
-  /**
-   * Destroy the SimpleMessenger. Pretty simple since all the work is done
-   * elsewhere.
-   */
-  ~SimpleMessenger() override;
-
-  /** @defgroup Accessors
-   * @{
-   */
-  void set_addr_unknowns(const entity_addr_t& addr) override;
-  void set_addr(const entity_addr_t &addr) override;
-
-  int get_dispatch_queue_len() override {
-    return dispatch_queue.get_queue_len();
-  }
-
-  double get_dispatch_queue_max_age(utime_t now) override {
-    return dispatch_queue.get_max_age(now);
-  }
-  /** @} Accessors */
-
-  /**
-   * @defgroup Configuration functions
-   * @{
-   */
-  void set_cluster_protocol(int p) override {
-    assert(!started && !did_bind);
-    cluster_protocol = p;
-  }
-
-  int bind(const entity_addr_t& bind_addr) override;
-  int rebind(const set<int>& avoid_ports) override;
-  int client_bind(const entity_addr_t& bind_addr) override;
-
-  /** @} Configuration functions */
-
-  /**
-   * @defgroup Startup/Shutdown
-   * @{
-   */
-  int start() override;
-  void wait() override;
-  int shutdown() override;
-
-  /** @} // Startup/Shutdown */
-
-  /**
-   * @defgroup Messaging
-   * @{
-   */
-  int send_message(Message *m, const entity_inst_t& dest) override {
-    return _send_message(m, dest);
-  }
-
-  int send_message(Message *m, Connection *con) {
-    return _send_message(m, con);
-  }
-
-  /** @} // Messaging */
-
-  /**
-   * @defgroup Connection Management
-   * @{
-   */
-  ConnectionRef get_connection(const entity_inst_t& dest) override;
-  ConnectionRef get_loopback_connection() override;
-  int send_keepalive(Connection *con);
-  void mark_down(const entity_addr_t& addr) override;
-  void mark_down(Connection *con);
-  void mark_disposable(Connection *con);
-  void mark_down_all() override;
-  /** @} // Connection Management */
-protected:
-  /**
-   * @defgroup Messenger Interfaces
-   * @{
-   */
-  /**
-   * Start up the DispatchQueue thread once we have somebody to dispatch to.
-   */
-  void ready() override;
-  /** @} // Messenger Interfaces */
-private:
-  /**
-   * @defgroup Inner classes
-   * @{
-   */
-
-public:
-  Accepter accepter;
-  DispatchQueue dispatch_queue;
-
-  friend class Accepter;
-
-  /**
-   * Register a new pipe for accept
-   *
-   * @param sd socket
-   */
-  Pipe *add_accept_pipe(int sd);
-
-private:
-
-  /**
-   * A thread used to tear down Pipes when they're complete.
-   */
-  class ReaperThread : public Thread {
-    SimpleMessenger *msgr;
-  public:
-    explicit ReaperThread(SimpleMessenger *m) : msgr(m) {}
-    void *entry() override {
-      msgr->reaper_entry();
-      return 0;
-    }
-  } reaper_thread;
-
-  /**
-   * @} // Inner classes
-   */
-
-  /**
-   * @defgroup Utility functions
-   * @{
-   */
-
-  /**
-   * Create a Pipe associated with the given entity (of the given type).
-   * Initiate the connection. (This function returning does not guarantee
-   * connection success.)
-   *
-   * @param addr The address of the entity to connect to.
-   * @param type The peer type of the entity at the address.
-   * @param con An existing Connection to associate with the new Pipe. If
-   * NULL, it creates a new Connection.
-   * @param first an initial message to queue on the new pipe
-   *
-   * @return a pointer to the newly-created Pipe. Caller does not own a
-   * reference; take one if you need it.
-   */
-  Pipe *connect_rank(const entity_addr_t& addr, int type, PipeConnection *con,
-		     Message *first);
-  /**
-   * Send a message, lazily or not.
-   * This just glues send_message together and passes
-   * the input on to submit_message.
-   */
-  int _send_message(Message *m, const entity_inst_t& dest);
-  /**
-   * Same as above, but for the Connection-based variants.
-   */
-  int _send_message(Message *m, Connection *con);
-  /**
-   * Queue up a Message for delivery to the entity specified
-   * by addr and dest_type.
-   * submit_message() is responsible for creating
-   * new Pipes (and closing old ones) as necessary.
-   *
-   * @param m The Message to queue up. This function eats a reference.
-   * @param con The existing Connection to use, or NULL if you don't know of one.
-   * @param addr The address to send the Message to.
-   * @param dest_type The peer type of the address we're sending to
-   * just drop silently under failure.
-   * @param already_locked If false, submit_message() will acquire the
-   * SimpleMessenger lock before accessing shared data structures; otherwise
-   * it will assume the lock is held. NOTE: if you are making a request
-   * without locking, you MUST have filled in the con with a valid pointer.
-   */
-  void submit_message(Message *m, PipeConnection *con,
-		      const entity_addr_t& addr, int dest_type,
-		      bool already_locked);
-  /**
-   * Look through the pipes in the pipe_reap_queue and tear them down.
-   */
-  void reaper();
-  /**
-   * @} // Utility functions
-   */
-
-  // SimpleMessenger stuff
-  /// approximately unique ID set by the Constructor for use in entity_addr_t
-  uint64_t nonce;
-  /// overall lock used for SimpleMessenger data structures
-  Mutex lock;
-  /// true, specifying we haven't learned our addr; set false when we find it.
-  // maybe this should be protected by the lock?
-  bool need_addr;
-
-public:
-  bool get_need_addr() const { return need_addr; }
-
-private:
-  /**
-   *  false; set to true if the SimpleMessenger bound to a specific address;
-   *  and set false again by Accepter::stop(). This isn't lock-protected
-   *  since you shouldn't be able to race the only writers.
-   */
-  bool did_bind;
-  /// counter for the global seq our connection protocol uses
-  __u32 global_seq;
-  /// lock to protect the global_seq
-  ceph_spinlock_t global_seq_lock;
-
-  /**
-   * hash map of addresses to Pipes
-   *
-   * NOTE: a Pipe* with state CLOSED may still be in the map but is considered
-   * invalid and can be replaced by anyone holding the msgr lock
-   */
-  ceph::unordered_map<entity_addr_t, Pipe*> rank_pipe;
-  /**
-   * list of pipes are in teh process of accepting
-   *
-   * These are not yet in the rank_pipe map.
-   */
-  set<Pipe*> accepting_pipes;
-  /// a set of all the Pipes we have which are somehow active
-  set<Pipe*>      pipes;
-  /// a list of Pipes we want to tear down
-  list<Pipe*>     pipe_reap_queue;
-
-  /// internal cluster protocol version, if any, for talking to entities of the same type.
-  int cluster_protocol;
-
-  Cond  stop_cond;
-  bool stopped = true;
-
-  bool reaper_started, reaper_stop;
-  Cond reaper_cond;
-
-  /// This Cond is slept on by wait() and signaled by dispatch_entry()
-  Cond  wait_cond;
-
-  friend class Pipe;
-
-  Pipe *_lookup_pipe(const entity_addr_t& k) {
-    ceph::unordered_map<entity_addr_t, Pipe*>::iterator p = rank_pipe.find(k);
-    if (p == rank_pipe.end())
-      return NULL;
-    // see lock cribbing in Pipe::fault()
-    if (p->second->state_closed)
-      return NULL;
-    return p->second;
-  }
-
-public:
-
-  int timeout;
-
-  /// con used for sending messages to ourselves
-  ConnectionRef local_connection;
-
-  /**
-   * @defgroup SimpleMessenger internals
-   * @{
-   */
-
-  /**
-   * This wraps ms_deliver_get_authorizer. We use it for Pipe.
-   */
-  AuthAuthorizer *get_authorizer(int peer_type, bool force_new);
-  /**
-   * This wraps ms_deliver_verify_authorizer; we use it for Pipe.
-   */
-  bool verify_authorizer(Connection *con, int peer_type, int protocol, bufferlist& auth, bufferlist& auth_reply,
-                         bool& isvalid,CryptoKey& session_key);
-  /**
-   * Increment the global sequence for this SimpleMessenger and return it.
-   * This is for the connect protocol, although it doesn't hurt if somebody
-   * else calls it.
-   *
-   * @return a global sequence ID that nobody else has seen.
-   */
-  __u32 get_global_seq(__u32 old=0) {
-    ceph_spin_lock(&global_seq_lock);
-    if (old > global_seq)
-      global_seq = old;
-    __u32 ret = ++global_seq;
-    ceph_spin_unlock(&global_seq_lock);
-    return ret;
-  }
-  /**
-   * Get the protocol version we support for the given peer type: either
-   * a peer protocol (if it matches our own), the protocol version for the
-   * peer (if we're connecting), or our protocol version (if we're accepting).
-   */
-  int get_proto_version(int peer_type, bool connect);
-
-  /**
-   * Fill in the features, address and peer type for the local connection, which
-   * is used for delivering messages back to ourself.
-   */
-  void init_local_connection();
-  /**
-   * Tell the SimpleMessenger its full IP address.
-   *
-   * This is used by Pipes when connecting to other endpoints, and
-   * probably shouldn't be called by anybody else.
-   */
-  void learned_addr(const entity_addr_t& peer_addr_for_me);
-
-  /**
-   * This function is used by the reaper thread. As long as nobody
-   * has set reaper_stop, it calls the reaper function, then
-   * waits to be signaled when it needs to reap again (or when it needs
-   * to stop).
-   */
-  void reaper_entry();
-  /**
-   * Add a pipe to the pipe_reap_queue, to be torn down on
-   * the next call to reaper().
-   * It should really only be the Pipe calling this, in our current
-   * implementation.
-   *
-   * @param pipe A Pipe which has stopped its threads and is
-   * ready to be torn down.
-   */
-  void queue_reap(Pipe *pipe);
-
-  /**
-   * Used to get whether this connection ready to send
-   */
-  bool is_connected(Connection *con);
-  /**
-   * @} // SimpleMessenger Internals
-   */
-} ;
-
-#endif /* CEPH_SIMPLEMESSENGER_H */
diff --git a/src/msg/xio/XioConnection.cc b/src/msg/xio/XioConnection.cc
deleted file mode 100644
index 36b946de08f..00000000000
--- a/src/msg/xio/XioConnection.cc
+++ /dev/null
@@ -1,860 +0,0 @@
-// -*- mode:C++; tab-width:8; c-basic-offset:2; indent-tabs-mode:t -*-
-// vim: ts=8 sw=2 smarttab
-/*
- * Ceph - scalable distributed file system
- *
- * Copyright (C) 2004-2006 Sage Weil <sage@newdream.net>
- * Portions Copyright (C) 2013 CohortFS, LLC
- *
- * This is free software; you can redistribute it and/or
- * modify it under the terms of the GNU Lesser General Public
- * License version 2.1, as published by the Free Software
- * Foundation.  See file COPYING.
- *
- */
-
-#include "XioMsg.h"
-#include "XioConnection.h"
-#include "XioMessenger.h"
-#include "messages/MDataPing.h"
-#include "msg/msg_types.h"
-#include "auth/none/AuthNoneProtocol.h" // XXX
-
-#include "include/assert.h"
-#include "common/dout.h"
-
-extern struct xio_mempool *xio_msgr_mpool;
-extern struct xio_mempool *xio_msgr_noreg_mpool;
-
-#define dout_subsys ceph_subsys_xio
-
-void print_xio_msg_hdr(CephContext *cct, const char *tag,
-		       const XioMsgHdr &hdr, const struct xio_msg *msg)
-{
-  if (msg) {
-    ldout(cct,4) << tag <<
-      " xio msg:" <<
-      " sn: " << msg->sn <<
-      " timestamp: " << msg->timestamp <<
-      dendl;
-  }
-
-  ldout(cct,4) << tag <<
-    " ceph header: " <<
-    " front_len: " << hdr.hdr->front_len <<
-    " seq: " << hdr.hdr->seq <<
-    " tid: " << hdr.hdr->tid <<
-    " type: " << hdr.hdr->type <<
-    " prio: " << hdr.hdr->priority <<
-    " name type: " << (int) hdr.hdr->src.type <<
-    " name num: " << (int) hdr.hdr->src.num <<
-    " version: " << hdr.hdr->version <<
-    " compat_version: " << hdr.hdr->compat_version <<
-    " front_len: " << hdr.hdr->front_len <<
-    " middle_len: " << hdr.hdr->middle_len <<
-    " data_len: " << hdr.hdr->data_len <<
-    " xio header: " <<
-    " msg_cnt: " << hdr.msg_cnt <<
-    dendl;
-
-  ldout(cct,4) << tag <<
-    " ceph footer: " <<
-    " front_crc: " << hdr.ftr->front_crc <<
-    " middle_crc: " << hdr.ftr->middle_crc <<
-    " data_crc: " << hdr.ftr->data_crc <<
-    " sig: " << hdr.ftr->sig <<
-    " flags: " << (uint32_t) hdr.ftr->flags <<
-    dendl;
-}
-
-void print_ceph_msg(CephContext *cct, const char *tag, Message *m)
-{
-  if (m->get_magic() & (MSG_MAGIC_XIO & MSG_MAGIC_TRACE_DTOR)) {
-    ceph_msg_header& header = m->get_header();
-    ldout(cct,4) << tag << " header version " << header.version <<
-      " compat version " << header.compat_version <<
-      dendl;
-  }
-}
-
-#undef dout_prefix
-#define dout_prefix conn_prefix(_dout)
-ostream& XioConnection::conn_prefix(std::ostream *_dout) {
-  return *_dout << "-- " << get_messenger()->get_myinst().addr << " >> " << peer_addr
-                << " peer=" << peer.name.type_str()
-                << " conn=" << conn << " sess=" << session << " ";
-}
-
-XioConnection::XioConnection(XioMessenger *m, XioConnection::type _type,
-			     const entity_inst_t& _peer) :
-  Connection(m->cct, m),
-  xio_conn_type(_type),
-  portal(m->get_portal()),
-  connected(false),
-  peer(_peer),
-  session(NULL),
-  conn(NULL),
-  magic(m->get_magic()),
-  scount(0),
-  send_ctr(0),
-  in_seq(),
-  cstate(this)
-{
-  pthread_spin_init(&sp, PTHREAD_PROCESS_PRIVATE);
-  set_peer_type(peer.name.type());
-  set_peer_addr(peer.addr);
-
-  Messenger::Policy policy;
-  int64_t max_msgs = 0, max_bytes = 0, bytes_opt = 0;
-  int xopt;
-
-  policy = m->get_policy(peer_type);
-
-  if (policy.throttler_messages) {
-    max_msgs = policy.throttler_messages->get_max();
-    ldout(m->cct,4) << "XioMessenger throttle_msgs: " << max_msgs << dendl;
-  }
-
-  xopt = m->cct->_conf->xio_queue_depth;
-  if (max_msgs > xopt)
-    xopt = max_msgs;
-
-  /* set high mark for send, reserved 20% for credits */
-  q_high_mark = xopt * 4 / 5;
-  q_low_mark = q_high_mark/2;
-
-  /* set send & receive msgs queue depth */
-  xio_set_opt(NULL, XIO_OPTLEVEL_ACCELIO, XIO_OPTNAME_SND_QUEUE_DEPTH_MSGS,
-             &xopt, sizeof(xopt));
-  xio_set_opt(NULL, XIO_OPTLEVEL_ACCELIO, XIO_OPTNAME_RCV_QUEUE_DEPTH_MSGS,
-             &xopt, sizeof(xopt));
-
-  if (policy.throttler_bytes) {
-    max_bytes = policy.throttler_bytes->get_max();
-    ldout(m->cct,4) << "XioMessenger throttle_bytes: " << max_bytes << dendl;
-  }
-
-  bytes_opt = (2 << 28); /* default: 512 MB */
-  if (max_bytes > bytes_opt)
-    bytes_opt = max_bytes;
-
-  /* set send & receive total bytes throttle */
-  xio_set_opt(NULL, XIO_OPTLEVEL_ACCELIO, XIO_OPTNAME_SND_QUEUE_DEPTH_BYTES,
-             &bytes_opt, sizeof(bytes_opt));
-  xio_set_opt(NULL, XIO_OPTLEVEL_ACCELIO, XIO_OPTNAME_RCV_QUEUE_DEPTH_BYTES,
-             &bytes_opt, sizeof(bytes_opt));
-
-  ldout(m->cct,4) << "throttle_msgs: " << xopt << " throttle_bytes: " << bytes_opt << dendl;
-
-  /* XXXX fake features, aieee! */
-  set_features(XIO_ALL_FEATURES);
-}
-
-int XioConnection::send_message(Message *m)
-{
-  XioMessenger *ms = static_cast<XioMessenger*>(get_messenger());
-  return ms->_send_message(m, this);
-}
-
-void XioConnection::send_keepalive_or_ack(bool ack, const utime_t *tp)
-{
-  /* If con is not in READY state, we need to queue the request */
-  if (cstate.session_state.read() != XioConnection::UP) {
-    pthread_spin_lock(&sp);
-    if (cstate.session_state.read() != XioConnection::UP) {
-      if (ack) {
-	outgoing.ack = true;
-	outgoing.ack_time = *tp;
-      }
-      else {
-	outgoing.keepalive = true;
-      }
-      pthread_spin_unlock(&sp);
-      return;
-    }
-    pthread_spin_unlock(&sp);
-  }
-
-  send_keepalive_or_ack_internal(ack, tp);
-}
-
-void XioConnection::send_keepalive_or_ack_internal(bool ack, const utime_t *tp)
-{
-  XioCommand *xcmd = pool_alloc_xio_command(this);
-  if (! xcmd) {
-    /* could happen if Accelio has been shutdown */
-    return;
-  }
-
-  struct ceph_timespec ts;
-  if (ack) {
-    assert(tp);
-    tp->encode_timeval(&ts);
-    xcmd->get_bl_ref().append(CEPH_MSGR_TAG_KEEPALIVE2_ACK);
-    xcmd->get_bl_ref().append((char*)&ts, sizeof(ts));
-  } else if (has_feature(CEPH_FEATURE_MSGR_KEEPALIVE2)) {
-    utime_t t = ceph_clock_now();
-    t.encode_timeval(&ts);
-    xcmd->get_bl_ref().append(CEPH_MSGR_TAG_KEEPALIVE2);
-    xcmd->get_bl_ref().append((char*)&ts, sizeof(ts));
-  } else {
-    xcmd->get_bl_ref().append(CEPH_MSGR_TAG_KEEPALIVE);
-  }
-
-  const std::list<buffer::ptr>& header = xcmd->get_bl_ref().buffers();
-  assert(header.size() == 1);  /* accelio header must be without scatter gather */
-  list<bufferptr>::const_iterator pb = header.begin();
-  assert(pb->length() < XioMsgHdr::get_max_encoded_length());
-  struct xio_msg * msg = xcmd->get_xio_msg();
-  msg->out.header.iov_base = (char*) pb->c_str();
-  msg->out.header.iov_len = pb->length();
-
-  ldout(msgr->cct,8) << __func__ << " sending command with tag " << (int)(*(char*)msg->out.header.iov_base)
-       << " len " << msg->out.header.iov_len << dendl;
-
-  portal->enqueue(this, xcmd);
-}
-
-
-int XioConnection::passive_setup()
-{
-  /* XXX passive setup is a placeholder for (potentially active-side
-     initiated) feature and auth* negotiation */
-  static bufferlist authorizer_reply; /* static because fake */
-  static CryptoKey session_key; /* ditto */
-  bool authorizer_valid;
-
-  XioMessenger *msgr = static_cast<XioMessenger*>(get_messenger());
-
-  // fake an auth buffer
-  EntityName name;
-  name.set_type(peer.name.type());
-
-  AuthNoneAuthorizer auth;
-  auth.build_authorizer(name, peer.name.num());
-
-  /* XXX fake authorizer! */
-  msgr->ms_deliver_verify_authorizer(
-    this, peer_type, CEPH_AUTH_NONE,
-    auth.bl,
-    authorizer_reply,
-    authorizer_valid,
-    session_key);
-
-  /* notify hook */
-  msgr->ms_deliver_handle_accept(this);
-  msgr->ms_deliver_handle_fast_accept(this);
-
-  /* try to insert in conns_entity_map */
-  msgr->try_insert(this);
-  return (0);
-}
-
-static inline XioDispatchHook* pool_alloc_xio_dispatch_hook(
-  XioConnection *xcon, Message *m, XioInSeq& msg_seq)
-{
-  struct xio_reg_mem mp_mem;
-  int e = xpool_alloc(xio_msgr_noreg_mpool,
-		      sizeof(XioDispatchHook), &mp_mem);
-  if (!!e)
-    return NULL;
-  XioDispatchHook *xhook = static_cast<XioDispatchHook*>(mp_mem.addr);
-  new (xhook) XioDispatchHook(xcon, m, msg_seq, mp_mem);
-  return xhook;
-}
-
-int XioConnection::handle_data_msg(struct xio_session *session,
-			      struct xio_msg *msg,
-			      int more_in_batch,
-			      void *cb_user_context)
-{
-  struct xio_msg *tmsg = msg;
-
-  /* XXX Accelio guarantees message ordering at
-   * xio_session */
-
-  if (! in_seq.p()) {
-    if (!tmsg->in.header.iov_len) {
-	ldout(msgr->cct,0) << __func__ << " empty header: packet out of sequence?" << dendl;
-	xio_release_msg(msg);
-	return 0;
-    }
-    const size_t sizeof_tag = 1;
-    XioMsgCnt msg_cnt(
-      buffer::create_static(tmsg->in.header.iov_len-sizeof_tag,
-			    ((char*) tmsg->in.header.iov_base)+sizeof_tag));
-    ldout(msgr->cct,10) << __func__ << " receive msg " << "tmsg " << tmsg
-      << " msg_cnt " << msg_cnt.msg_cnt
-      << " iov_base " << tmsg->in.header.iov_base
-      << " iov_len " << (int) tmsg->in.header.iov_len
-      << " nents " << tmsg->in.pdata_iov.nents
-      << " sn " << tmsg->sn << dendl;
-    assert(session == this->session);
-    in_seq.set_count(msg_cnt.msg_cnt);
-  } else {
-    /* XXX major sequence error */
-    assert(! tmsg->in.header.iov_len);
-  }
-
-  in_seq.append(msg);
-  if (in_seq.count() > 0) {
-    return 0;
-  }
-
-  XioMessenger *msgr = static_cast<XioMessenger*>(get_messenger());
-  XioDispatchHook *m_hook =
-    pool_alloc_xio_dispatch_hook(this, NULL /* msg */, in_seq);
-  XioInSeq& msg_seq = m_hook->msg_seq;
-  in_seq.clear();
-
-  ceph_msg_header header;
-  ceph_msg_footer footer;
-  buffer::list payload, middle, data;
-
-  const utime_t recv_stamp = ceph_clock_now();
-
-  ldout(msgr->cct,4) << __func__ << " " << "msg_seq.size()="  << msg_seq.size() <<
-    dendl;
-
-  struct xio_msg* msg_iter = msg_seq.begin();
-  tmsg = msg_iter;
-  XioMsgHdr hdr(header, footer,
-		buffer::create_static(tmsg->in.header.iov_len,
-				      (char*) tmsg->in.header.iov_base));
-
-  if (magic & (MSG_MAGIC_TRACE_XCON)) {
-    if (hdr.hdr->type == 43) {
-      print_xio_msg_hdr(msgr->cct, "on_msg", hdr, NULL);
-    }
-  }
-
-  unsigned int ix, blen, iov_len;
-  struct xio_iovec_ex *msg_iov, *iovs;
-  uint32_t take_len, left_len = 0;
-  char *left_base = NULL;
-
-  ix = 0;
-  blen = header.front_len;
-
-  while (blen && (msg_iter != msg_seq.end())) {
-    tmsg = msg_iter;
-    iov_len = vmsg_sglist_nents(&tmsg->in);
-    iovs = vmsg_sglist(&tmsg->in);
-    for (; blen && (ix < iov_len); ++ix) {
-      msg_iov = &iovs[ix];
-
-      /* XXX need to detect any buffer which needs to be
-       * split due to coalescing of a segment (front, middle,
-       * data) boundary */
-
-      take_len = MIN(blen, msg_iov->iov_len);
-      payload.append(
-	buffer::create_msg(
-	  take_len, (char*) msg_iov->iov_base, m_hook));
-      blen -= take_len;
-      if (! blen) {
-	left_len = msg_iov->iov_len - take_len;
-	if (left_len) {
-	  left_base = ((char*) msg_iov->iov_base) + take_len;
-	}
-      }
-    }
-    /* XXX as above, if a buffer is split, then we needed to track
-     * the new start (carry) and not advance */
-    if (ix == iov_len) {
-      msg_seq.next(&msg_iter);
-      ix = 0;
-    }
-  }
-
-  if (magic & (MSG_MAGIC_TRACE_XCON)) {
-    if (hdr.hdr->type == 43) {
-      ldout(msgr->cct,4) << "front (payload) dump:";
-      payload.hexdump( *_dout );
-      *_dout << dendl;
-    }
-  }
-
-  blen = header.middle_len;
-
-  if (blen && left_len) {
-    middle.append(
-      buffer::create_msg(left_len, left_base, m_hook));
-    left_len = 0;
-  }
-
-  while (blen && (msg_iter != msg_seq.end())) {
-    tmsg = msg_iter;
-    iov_len = vmsg_sglist_nents(&tmsg->in);
-    iovs = vmsg_sglist(&tmsg->in);
-    for (; blen && (ix < iov_len); ++ix) {
-      msg_iov = &iovs[ix];
-      take_len = MIN(blen, msg_iov->iov_len);
-      middle.append(
-	buffer::create_msg(
-	  take_len, (char*) msg_iov->iov_base, m_hook));
-      blen -= take_len;
-      if (! blen) {
-	left_len = msg_iov->iov_len - take_len;
-	if (left_len) {
-	  left_base = ((char*) msg_iov->iov_base) + take_len;
-	}
-      }
-    }
-    if (ix == iov_len) {
-      msg_seq.next(&msg_iter);
-      ix = 0;
-    }
-  }
-
-  blen = header.data_len;
-
-  if (blen && left_len) {
-    data.append(
-      buffer::create_msg(left_len, left_base, m_hook));
-    left_len = 0;
-  }
-
-  while (blen && (msg_iter != msg_seq.end())) {
-    tmsg = msg_iter;
-    iov_len = vmsg_sglist_nents(&tmsg->in);
-    iovs = vmsg_sglist(&tmsg->in);
-    for (; blen && (ix < iov_len); ++ix) {
-      msg_iov = &iovs[ix];
-      data.append(
-	buffer::create_msg(
-	  msg_iov->iov_len, (char*) msg_iov->iov_base, m_hook));
-      blen -= msg_iov->iov_len;
-    }
-    if (ix == iov_len) {
-      msg_seq.next(&msg_iter);
-      ix = 0;
-    }
-  }
-
-  /* update connection timestamp */
-  recv = tmsg->timestamp;
-
-  Message *m = decode_message(msgr->cct, msgr->crcflags, header, footer,
-                              payload, middle, data, this);
-
-  if (m) {
-    /* completion */
-    m->set_connection(this);
-
-    /* reply hook */
-    m_hook->set_message(m);
-    m->set_completion_hook(m_hook);
-
-    /* trace flag */
-    m->set_magic(magic);
-
-    /* update timestamps */
-    m->set_recv_stamp(recv_stamp);
-    m->set_recv_complete_stamp(ceph_clock_now());
-    m->set_seq(header.seq);
-
-    /* MP-SAFE */
-    state.set_in_seq(header.seq);
-
-    /* XXXX validate peer type */
-    if (peer_type != (int) hdr.peer_type) { /* XXX isn't peer_type -1? */
-      peer_type = hdr.peer_type;
-      peer_addr = hdr.addr;
-      peer.addr = peer_addr;
-      peer.name = entity_name_t(hdr.hdr->src);
-      if (xio_conn_type == XioConnection::PASSIVE) {
-	/* XXX kick off feature/authn/authz negotiation
-	 * nb:  very possibly the active side should initiate this, but
-	 * for now, call a passive hook so OSD and friends can create
-	 * sessions without actually negotiating
-	 */
-	passive_setup();
-      }
-    }
-
-    if (magic & (MSG_MAGIC_TRACE_XCON)) {
-      ldout(msgr->cct,4) << "decode m is " << m->get_type() << dendl;
-    }
-
-    /* dispatch it */
-    msgr->ds_dispatch(m);
-  } else {
-    /* responds for undecoded messages and frees hook */
-    ldout(msgr->cct,4) << "decode m failed" << dendl;
-    m_hook->on_err_finalize(this);
-  }
-
-  return 0;
-}
-
-int XioConnection::on_msg(struct xio_session *session,
-			      struct xio_msg *msg,
-			      int more_in_batch,
-			      void *cb_user_context)
-{
-  char tag = CEPH_MSGR_TAG_MSG;
-  if (msg->in.header.iov_len)
-    tag = *(char*)msg->in.header.iov_base;
-
-  ldout(msgr->cct,8) << __func__ << " receive msg with iov_len "
-    << (int) msg->in.header.iov_len << " tag " << (int)tag << dendl;
-
-  //header_len_without_tag is only meaningful in case we have tag
-  size_t header_len_without_tag = msg->in.header.iov_len - sizeof(tag);
-
-  switch(tag) {
-  case CEPH_MSGR_TAG_MSG:
-    ldout(msgr->cct, 20) << __func__ << " got data message" << dendl;
-    return handle_data_msg(session, msg, more_in_batch, cb_user_context);
-
-  case CEPH_MSGR_TAG_KEEPALIVE:
-    ldout(msgr->cct, 20) << __func__ << " got KEEPALIVE" << dendl;
-    set_last_keepalive(ceph_clock_now());
-    break;
-
-  case CEPH_MSGR_TAG_KEEPALIVE2:
-    if (header_len_without_tag < sizeof(ceph_timespec)) {
-      lderr(msgr->cct) << __func__ << " too few data for KEEPALIVE2: got " << header_len_without_tag <<
-         " bytes instead of " << sizeof(ceph_timespec) << " bytes" << dendl;
-    }
-    else {
-      ceph_timespec *t = (ceph_timespec *) ((char*)msg->in.header.iov_base + sizeof(tag));
-      utime_t kp_t = utime_t(*t);
-      ldout(msgr->cct, 20) << __func__ << " got KEEPALIVE2 with timestamp" << kp_t << dendl;
-      send_keepalive_or_ack(true, &kp_t);
-      set_last_keepalive(ceph_clock_now());
-    }
-
-    break;
-
-  case CEPH_MSGR_TAG_KEEPALIVE2_ACK:
-    if (header_len_without_tag < sizeof(ceph_timespec)) {
-      lderr(msgr->cct) << __func__ << " too few data for KEEPALIVE2_ACK: got " << header_len_without_tag <<
-         " bytes instead of " << sizeof(ceph_timespec) << " bytes" << dendl;
-    }
-    else {
-      ceph_timespec *t = (ceph_timespec *) ((char*)msg->in.header.iov_base + sizeof(tag));
-      utime_t kp_t(*t);
-      ldout(msgr->cct, 20) << __func__ << " got KEEPALIVE2_ACK with timestamp" << kp_t << dendl;
-      set_last_keepalive_ack(kp_t);
-    }
-    break;
-
-  default:
-    lderr(msgr->cct) << __func__ << " unsupported message tag " << (int) tag << dendl;
-    assert(! "unsupported message tag");
-  }
-
-  xio_release_msg(msg);
-  return 0;
-}
-
-
-int XioConnection::on_ow_msg_send_complete(struct xio_session *session,
-					   struct xio_msg *req,
-					   void *conn_user_context)
-{
-  /* requester send complete (one-way) */
-  uint64_t rc = ++scount;
-
-  XioSend* xsend = static_cast<XioSend*>(req->user_context);
-  if (unlikely(magic & MSG_MAGIC_TRACE_CTR)) {
-    if (unlikely((rc % 1000000) == 0)) {
-      std::cout << "xio finished " << rc << " " << time(0) << std::endl;
-    }
-  } /* trace ctr */
-
-  ldout(msgr->cct,11) << "on_msg_delivered xcon: " << xsend->xcon <<
-    " msg: " << req << " sn: " << req->sn << dendl;
-
-  XioMsg *xmsg = dynamic_cast<XioMsg*>(xsend);
-  if (xmsg) {
-    ldout(msgr->cct,11) << "on_msg_delivered xcon: " <<
-      " type: " << xmsg->m->get_type() << " tid: " << xmsg->m->get_tid() <<
-      " seq: " << xmsg->m->get_seq() << dendl;
-  }
-
-  --send_ctr; /* atomic, because portal thread */
-
-  /* unblock flow-controlled connections, avoid oscillation */
-  if (unlikely(cstate.session_state.read() ==
-	       XioConnection::FLOW_CONTROLLED)) {
-    if ((send_ctr <= uint32_t(xio_qdepth_low_mark())) &&
-	(1 /* XXX memory <= memory low-water mark */))  {
-      cstate.state_up_ready(XioConnection::CState::OP_FLAG_NONE);
-      ldout(msgr->cct,2) << "on_msg_delivered xcon: " << xsend->xcon
-        << " up_ready from flow_controlled" << dendl;
-    }
-  }
-
-  xsend->put();
-
-  return 0;
-}  /* on_msg_delivered */
-
-void XioConnection::msg_send_fail(XioSend *xsend, int code)
-{
-  ldout(msgr->cct,2) << "xio_send_msg FAILED xcon: " << this <<
-    " msg: " << xsend->get_xio_msg() << " code=" << code <<
-    " (" << xio_strerror(code) << ")" << dendl;
-  /* return refs taken for each xio_msg */
-  xsend->put_msg_refs();
-} /* msg_send_fail */
-
-void XioConnection::msg_release_fail(struct xio_msg *msg, int code)
-{
-  ldout(msgr->cct,2) << "xio_release_msg FAILED xcon: " << this <<
-    " msg: " << msg <<  "code=" << code <<
-    " (" << xio_strerror(code) << ")" << dendl;
-} /* msg_release_fail */
-
-int XioConnection::flush_out_queues(uint32_t flags) {
-  XioMessenger* msgr = static_cast<XioMessenger*>(get_messenger());
-  if (! (flags & CState::OP_FLAG_LOCKED))
-    pthread_spin_lock(&sp);
-
-  if (outgoing.keepalive) {
-    outgoing.keepalive = false;
-    send_keepalive_or_ack_internal();
-  }
-
-  if (outgoing.ack) {
-    outgoing.ack = false;
-    send_keepalive_or_ack_internal(true, &outgoing.ack_time);
-  }
-
-  // send deferred 1 (direct backpresssure)
-  if (outgoing.requeue.size() > 0)
-    portal->requeue(this, outgoing.requeue);
-
-  // send deferred 2 (sent while deferred)
-  int ix, q_size = outgoing.mqueue.size();
-  for (ix = 0; ix < q_size; ++ix) {
-    Message::Queue::iterator q_iter = outgoing.mqueue.begin();
-    Message* m = &(*q_iter);
-    outgoing.mqueue.erase(q_iter);
-    msgr->_send_message_impl(m, this);
-  }
-  if (! (flags & CState::OP_FLAG_LOCKED))
-    pthread_spin_unlock(&sp);
-  return 0;
-}
-
-int XioConnection::discard_out_queues(uint32_t flags)
-{
-  Message::Queue disc_q;
-  XioSubmit::Queue deferred_q;
-
-  if (! (flags & CState::OP_FLAG_LOCKED))
-    pthread_spin_lock(&sp);
-
-  /* the two send queues contain different objects:
-   * - anything on the mqueue is a Message
-   * - anything on the requeue is an XioSend
-   */
-  Message::Queue::const_iterator i1 = disc_q.end();
-  disc_q.splice(i1, outgoing.mqueue);
-
-  XioSubmit::Queue::const_iterator i2 = deferred_q.end();
-  deferred_q.splice(i2, outgoing.requeue);
-
-  outgoing.keepalive = outgoing.ack = false;
-
-  if (! (flags & CState::OP_FLAG_LOCKED))
-    pthread_spin_unlock(&sp);
-
-  // mqueue
-  while (!disc_q.empty()) {
-    Message::Queue::iterator q_iter = disc_q.begin();
-    Message* m = &(*q_iter);
-    disc_q.erase(q_iter);
-    m->put();
-  }
-
-  // requeue
-  while (!deferred_q.empty()) {
-    XioSubmit::Queue::iterator q_iter = deferred_q.begin();
-    XioSubmit* xs = &(*q_iter);
-    XioSend* xsend;
-    switch (xs->type) {
-      case XioSubmit::OUTGOING_MSG:
-	xsend = static_cast<XioSend*>(xs);
-	deferred_q.erase(q_iter);
-	// release once for each chained xio_msg
-	xsend->put(xsend->get_msg_count());
-	break;
-      case XioSubmit::INCOMING_MSG_RELEASE:
-	deferred_q.erase(q_iter);
-	portal->release_xio_msg(static_cast<XioCompletion*>(xs));
-	break;
-      default:
-	ldout(msgr->cct,0) << __func__ << ": Unknown Msg type " << xs->type << dendl;
-	break;
-    }
-  }
-
-  return 0;
-}
-
-int XioConnection::adjust_clru(uint32_t flags)
-{
-  if (flags & CState::OP_FLAG_LOCKED)
-    pthread_spin_unlock(&sp);
-
-  XioMessenger* msgr = static_cast<XioMessenger*>(get_messenger());
-  msgr->conns_sp.lock();
-  pthread_spin_lock(&sp);
-
-  if (cstate.flags & CState::FLAG_MAPPED) {
-    XioConnection::ConnList::iterator citer =
-      XioConnection::ConnList::s_iterator_to(*this);
-    msgr->conns_list.erase(citer);
-    msgr->conns_list.push_front(*this); // LRU
-  }
-
-  msgr->conns_sp.unlock();
-
-  if (! (flags & CState::OP_FLAG_LOCKED))
-    pthread_spin_unlock(&sp);
-
-  return 0;
-}
-
-int XioConnection::on_msg_error(struct xio_session *session,
-				enum xio_status error,
-				struct xio_msg  *msg,
-				void *conn_user_context)
-{
-  XioSend *xsend = static_cast<XioSend*>(msg->user_context);
-  if (xsend)
-    xsend->put();
-
-  --send_ctr; /* atomic, because portal thread */
-  return 0;
-} /* on_msg_error */
-
-void XioConnection::mark_down()
-{
-  _mark_down(XioConnection::CState::OP_FLAG_NONE);
-}
-
-int XioConnection::_mark_down(uint32_t flags)
-{
-  if (! (flags & CState::OP_FLAG_LOCKED))
-    pthread_spin_lock(&sp);
-
-  // per interface comment, we only stage a remote reset if the
-  // current policy required it
-  if (cstate.policy.resetcheck)
-    cstate.flags |= CState::FLAG_RESET;
-
-  disconnect();
-
-  /* XXX this will almost certainly be called again from
-   * on_disconnect_event() */
-  discard_out_queues(flags|CState::OP_FLAG_LOCKED);
-
-  if (! (flags & CState::OP_FLAG_LOCKED))
-    pthread_spin_unlock(&sp);
-
-  return 0;
-}
-
-void XioConnection::mark_disposable()
-{
-  _mark_disposable(XioConnection::CState::OP_FLAG_NONE);
-}
-
-int XioConnection::_mark_disposable(uint32_t flags)
-{
-  if (! (flags & CState::OP_FLAG_LOCKED))
-    pthread_spin_lock(&sp);
-
-  cstate.policy.lossy = true;
-
-  if (! (flags & CState::OP_FLAG_LOCKED))
-    pthread_spin_unlock(&sp);
-
-  return 0;
-}
-
-int XioConnection::CState::state_up_ready(uint32_t flags)
-{
-  if (! (flags & CState::OP_FLAG_LOCKED))
-    pthread_spin_lock(&xcon->sp);
-
-  xcon->flush_out_queues(flags|CState::OP_FLAG_LOCKED);
-
-  session_state = session_states::UP;
-  startup_state = session_startup_states::READY;
-
-  if (! (flags & CState::OP_FLAG_LOCKED))
-    pthread_spin_unlock(&xcon->sp);
-
-  return (0);
-}
-
-int XioConnection::CState::state_discon()
-{
-  session_state = session_states::DISCONNECTED;
-  startup_state = session_startup_states::IDLE;
-
-  return 0;
-}
-
-int XioConnection::CState::state_flow_controlled(uint32_t flags)
-{
-  if (! (flags & OP_FLAG_LOCKED))
-    pthread_spin_lock(&xcon->sp);
-
-  session_state = session_states::FLOW_CONTROLLED;
-
-  if (! (flags & OP_FLAG_LOCKED))
-    pthread_spin_unlock(&xcon->sp);
-
-  return (0);
-}
-
-int XioConnection::CState::state_fail(Message* m, uint32_t flags)
-{
-  if (! (flags & OP_FLAG_LOCKED))
-    pthread_spin_lock(&xcon->sp);
-
-  // advance to state FAIL, drop queued, msgs, adjust LRU
-  session_state = session_states::DISCONNECTED);
-  startup_state = session_startup_states::FAIL);
-
-  xcon->discard_out_queues(flags|OP_FLAG_LOCKED);
-  xcon->adjust_clru(flags|OP_FLAG_LOCKED|OP_FLAG_LRU);
-
-  xcon->disconnect();
-
-  if (! (flags & OP_FLAG_LOCKED))
-    pthread_spin_unlock(&xcon->sp);
-
-  // notify ULP
-  XioMessenger* msgr = static_cast<XioMessenger*>(xcon->get_messenger());
-  msgr->ms_deliver_handle_reset(xcon);
-  m->put();
-
-  return 0;
-}
-
-
-int XioLoopbackConnection::send_message(Message *m)
-{
-  XioMessenger *ms = static_cast<XioMessenger*>(get_messenger());
-  m->set_connection(this);
-  m->set_seq(next_seq());
-  m->set_src(ms->get_myinst().name);
-  ms->ds_dispatch(m);
-  return 0;
-}
-
-void XioLoopbackConnection::send_keepalive()
-{
-  utime_t t = ceph_clock_now();
-  set_last_keepalive(t);
-  set_last_keepalive_ack(t);
-}
diff --git a/src/msg/xio/XioConnection.h b/src/msg/xio/XioConnection.h
deleted file mode 100644
index 1ed88b995d8..00000000000
--- a/src/msg/xio/XioConnection.h
+++ /dev/null
@@ -1,375 +0,0 @@
-// -*- mode:C++; tab-width:8; c-basic-offset:2; indent-tabs-mode:t -*-
-// vim: ts=8 sw=2 smarttab
-/*
- * Ceph - scalable distributed file system
- *
- * Copyright (C) 2004-2006 Sage Weil <sage@newdream.net>
- * Portions Copyright (C) 2013 CohortFS, LLC
- *
- * This is free software; you can redistribute it and/or
- * modify it under the terms of the GNU Lesser General Public
- * License version 2.1, as published by the Free Software
- * Foundation.  See file COPYING.
- *
- */
-
-#ifndef XIO_CONNECTION_H
-#define XIO_CONNECTION_H
-
-#include <atomic>
-
-#include <boost/intrusive/avl_set.hpp>
-#include <boost/intrusive/list.hpp>
-
-extern "C" {
-#include "libxio.h"
-}
-
-#include "XioInSeq.h"
-#include "XioSubmit.h"
-#include "msg/Connection.h"
-#include "msg/Messenger.h"
-#include "auth/AuthSessionHandler.h"
-
-#define XIO_ALL_FEATURES (CEPH_FEATURES_ALL)
-
-
-#define XIO_NOP_TAG_MARKDOWN 0x0001
-
-namespace bi = boost::intrusive;
-
-class XioPortal;
-class XioMessenger;
-class XioSend;
-
-class XioConnection : public Connection
-{
-public:
-  enum type { ACTIVE, PASSIVE };
-
-  enum class session_states : unsigned {
-    INIT = 0,
-    START,
-    UP,
-    FLOW_CONTROLLED,
-    DISCONNECTED,
-    DELETED,
-    BARRIER
-  };
-
-  enum class session_startup_states : unsigned {
-    IDLE = 0,
-    CONNECTING,
-    ACCEPTING,
-    READY,
-    FAIL
-  };
-
-private:
-  XioConnection::type xio_conn_type;
-  XioPortal *portal;
-  std::atomic<bool> connected = { false };
-  entity_inst_t peer;
-  struct xio_session *session;
-  struct xio_connection	*conn;
-  pthread_spinlock_t sp;
-  std::atomic<int64_t> send = { 0 };
-  std::atomic<int64_t> recv = { 0 };
-  uint32_t n_reqs; // Accelio-initiated reqs in progress (!counting partials)
-  uint32_t magic;
-  uint32_t special_handling;
-  uint64_t scount;
-  uint32_t send_ctr;
-  int q_high_mark;
-  int q_low_mark;
-
-  struct lifecycle {
-    // different from Pipe states?
-    enum lf_state {
-      INIT,
-      LOCAL_DISCON,
-      REMOTE_DISCON,
-      RECONNECTING,
-      UP,
-      DEAD } state;
-
-    /* XXX */
-    uint32_t reconnects;
-    uint32_t connect_seq, peer_global_seq;
-    uint64_t in_seq, out_seq_acked; // atomic<uint64_t>, got receipt
-    std::atomic<int64_t> out_seq = { 0 }; 
-
-    lifecycle() : state(lifecycle::INIT), reconnects(0), connect_seq(0),
-		  peer_global_seq(0), in_seq(0), out_seq_acked(0)
-		  {}
-
-    void set_in_seq(uint64_t seq) {
-      in_seq = seq;
-    }
-
-    uint64_t next_out_seq() {
-      return ++out_seq;
-    }
-
-  } state;
-
-  /* batching */
-  XioInSeq in_seq;
-
-  class CState
-  {
-  public:
-    static const int FLAG_NONE = 0x0000;
-    static const int FLAG_BAD_AUTH = 0x0001;
-    static const int FLAG_MAPPED = 0x0002;
-    static const int FLAG_RESET = 0x0004;
-
-    static const int OP_FLAG_NONE = 0x0000;
-    static const int OP_FLAG_LOCKED = 0x0001;
-    static const int OP_FLAG_LRU = 0x0002;
-
-    uint64_t features;
-    Messenger::Policy policy;
-
-    CryptoKey session_key;
-    ceph::shared_ptr<AuthSessionHandler> session_security;
-    AuthAuthorizer *authorizer;
-    XioConnection *xcon;
-    uint32_t protocol_version;
-
-    std::atomic<session_states> session_state = { 0 };
-    std::atomic<session_startup_state> startup_state = { 0 };
-
-    uint32_t reconnects;
-    uint32_t connect_seq, global_seq, peer_global_seq;
-    uint64_t in_seq, out_seq_acked; // atomic<uint64_t>, got receipt
-    std::atomic<uint64_t> out_seq = { 0 }; 
-
-    uint32_t flags;
-
-    explicit CState(XioConnection* _xcon)
-      : features(0),
-	authorizer(NULL),
-	xcon(_xcon),
-	protocol_version(0),
-	session_state(INIT),
-	startup_state(IDLE),
-	reconnects(0),
-	connect_seq(0),
-	global_seq(0),
-	peer_global_seq(0),
-	in_seq(0),
-	out_seq_acked(0),
-	flags(FLAG_NONE) {}
-
-    uint64_t get_session_state() {
-      return session_state;
-    }
-
-    uint64_t get_startup_state() {
-      return startup_state;
-    }
-
-    void set_in_seq(uint64_t seq) {
-      in_seq = seq;
-    }
-
-    uint64_t next_out_seq() {
-      return ++out_seq;
-    };
-
-    // state machine
-    int init_state();
-    int next_state(Message* m);
-#if 0 // future (session startup)
-    int msg_connect(MConnect *m);
-    int msg_connect_reply(MConnectReply *m);
-    int msg_connect_reply(MConnectAuthReply *m);
-    int msg_connect_auth(MConnectAuth *m);
-    int msg_connect_auth_reply(MConnectAuthReply *m);
-#endif
-    int state_up_ready(uint32_t flags);
-    int state_flow_controlled(uint32_t flags);
-    int state_discon();
-    int state_fail(Message* m, uint32_t flags);
-
-  } cstate; /* CState */
-
-  // message submission queue
-  struct SendQ {
-    bool keepalive;
-    bool ack;
-    utime_t ack_time;
-    Message::Queue mqueue; // deferred
-    XioSubmit::Queue requeue;
-
-    SendQ():keepalive(false), ack(false){}
-  } outgoing;
-
-  // conns_entity_map comparison functor
-  struct EntityComp
-  {
-    // for internal ordering
-    bool operator()(const XioConnection &lhs,  const XioConnection &rhs) const
-      {  return lhs.get_peer() < rhs.get_peer(); }
-
-    // for external search by entity_inst_t(peer)
-    bool operator()(const entity_inst_t &peer, const XioConnection &c) const
-      {  return peer < c.get_peer(); }
-
-    bool operator()(const XioConnection &c, const entity_inst_t &peer) const
-      {  return c.get_peer() < peer;  }
-  };
-
-  bi::list_member_hook<> conns_hook;
-  bi::avl_set_member_hook<> conns_entity_map_hook;
-
-  typedef bi::list< XioConnection,
-		    bi::member_hook<XioConnection, bi::list_member_hook<>,
-				    &XioConnection::conns_hook > > ConnList;
-
-  typedef bi::member_hook<XioConnection, bi::avl_set_member_hook<>,
-			  &XioConnection::conns_entity_map_hook> EntityHook;
-
-  typedef bi::avl_set< XioConnection, EntityHook,
-		       bi::compare<EntityComp> > EntitySet;
-
-  friend class XioPortal;
-  friend class XioMessenger;
-  friend class XioDispatchHook;
-  friend class XioMarkDownHook;
-  friend class XioSend;
-
-  int on_disconnect_event() {
-    connected = false;
-    pthread_spin_lock(&sp);
-    discard_out_queues(CState::OP_FLAG_LOCKED);
-    pthread_spin_unlock(&sp);
-    return 0;
-  }
-
-  int on_teardown_event() {
-    pthread_spin_lock(&sp);
-    if (conn)
-      xio_connection_destroy(conn);
-    conn = NULL;
-    pthread_spin_unlock(&sp);
-    this->put();
-    return 0;
-  }
-
-  int xio_qdepth_high_mark() {
-    return q_high_mark;
-  }
-
-  int xio_qdepth_low_mark() {
-    return q_low_mark;
-  }
-
-public:
-  XioConnection(XioMessenger *m, XioConnection::type _type,
-		const entity_inst_t& peer);
-
-  ~XioConnection() {
-    if (conn)
-      xio_connection_destroy(conn);
-  }
-  ostream& conn_prefix(std::ostream *_dout);
-
-  bool is_connected() override { return connected; }
-
-  int send_message(Message *m) override;
-  void send_keepalive() override {send_keepalive_or_ack();}
-  void send_keepalive_or_ack(bool ack = false, const utime_t *tp = nullptr);
-  void mark_down() override;
-  int _mark_down(uint32_t flags);
-  void mark_disposable() override;
-  int _mark_disposable(uint32_t flags);
-
-  const entity_inst_t& get_peer() const { return peer; }
-
-  XioConnection* get() {
-#if 0
-    cout << "XioConnection::get " << this << " " << nref.load() << std::endl;
-#endif
-    RefCountedObject::get();
-    return this;
-  }
-
-  void put() {
-    RefCountedObject::put();
-#if 0
-    cout << "XioConnection::put " << this << " " << nref.load() << std::endl;
-#endif
-  }
-
-  void disconnect() {
-    if (is_connected()) {
-      connected = false;
-      xio_disconnect(conn); // normal teardown will clean up conn
-    }
-  }
-
-  uint32_t get_magic() { return magic; }
-  void set_magic(int _magic) { magic = _magic; }
-  uint32_t get_special_handling() { return special_handling; }
-  void set_special_handling(int n) { special_handling = n; }
-  uint64_t get_scount() { return scount; }
-
-  int passive_setup(); /* XXX */
-
-  int handle_data_msg(struct xio_session *session, struct xio_msg *msg,
-		 int more_in_batch, void *cb_user_context);
-  int on_msg(struct xio_session *session, struct xio_msg *msg,
-		 int more_in_batch, void *cb_user_context);
-  int on_ow_msg_send_complete(struct xio_session *session, struct xio_msg *msg,
-			      void *conn_user_context);
-  int on_msg_error(struct xio_session *session, enum xio_status error,
-		   struct xio_msg  *msg, void *conn_user_context);
-  void msg_send_fail(XioSend *xsend, int code);
-  void msg_release_fail(struct xio_msg *msg, int code);
-private:
-  void send_keepalive_or_ack_internal(bool ack = false, const utime_t *tp = nullptr);
-  int flush_out_queues(uint32_t flags);
-  int discard_out_queues(uint32_t flags);
-  int adjust_clru(uint32_t flags);
-};
-
-typedef boost::intrusive_ptr<XioConnection> XioConnectionRef;
-
-class XioLoopbackConnection : public Connection
-{
-private:
-  std::atomic<uint64_t> seq = { 0 };
-public:
-  explicit XioLoopbackConnection(Messenger *m) : Connection(m->cct, m)
-    {
-      const entity_inst_t& m_inst = m->get_myinst();
-      peer_addr = m_inst.addr;
-      peer_type = m_inst.name.type();
-      set_features(XIO_ALL_FEATURES); /* XXXX set to ours */
-    }
-
-  XioLoopbackConnection* get() {
-    return static_cast<XioLoopbackConnection*>(RefCountedObject::get());
-  }
-
-  bool is_connected() override { return true; }
-
-  int send_message(Message *m) override;
-  void send_keepalive() override;
-  void mark_down() override {}
-  void mark_disposable() override {}
-
-  uint64_t get_seq() {
-    return seq;
-  }
-
-  uint64_t next_seq() {
-    return ++seq;
-  }
-};
-
-typedef boost::intrusive_ptr<XioLoopbackConnection> XioLoopbackConnectionRef;
-
-#endif /* XIO_CONNECTION_H */
diff --git a/src/msg/xio/XioInSeq.h b/src/msg/xio/XioInSeq.h
deleted file mode 100644
index 7863a8f66c3..00000000000
--- a/src/msg/xio/XioInSeq.h
+++ /dev/null
@@ -1,84 +0,0 @@
-// -*- mode:C++; tab-width:8; c-basic-offset:2; indent-tabs-mode:t -*-
-// vim: ts=8 sw=2 smarttab
-/*
- * Ceph - scalable distributed file system
- *
- * Copyright (C) 2004-2006 Sage Weil <sage@newdream.net>
- * Portions Copyright (C) 2013 CohortFS, LLC
- *
- * This is free software; you can redistribute it and/or
- * modify it under the terms of the GNU Lesser General Public
- * License version 2.1, as published by the Free Software
- * Foundation.  See file COPYING.
- *
- */
-
-#ifndef XIO_IN_SEQ_H
-#define XIO_IN_SEQ_H
-
-#include <boost/intrusive/list.hpp>
-#include "msg/SimplePolicyMessenger.h"
-extern "C" {
-#include "libxio.h"
-}
-
-/* For inbound messages (Accelio-owned) ONLY, use the message's
- * user_context as an SLIST */
-class XioInSeq {
-private:
-  int cnt;
-  int sz;
-  struct xio_msg* head;
-  struct xio_msg* tail;
-
-public:
-  XioInSeq() : cnt(0), sz(0), head(NULL), tail(NULL) {}
-  XioInSeq(const XioInSeq& seq) {
-    cnt = seq.cnt;
-    sz = seq.sz;
-    head = seq.head;
-    tail = seq.tail;
-  }
-
-  int count() { return cnt; }
-
-  int size() { return sz; }
-
-  bool p() { return !!head; }
-
-  void set_count(int _cnt) { cnt = _cnt; }
-
-  void append(struct xio_msg* msg) {
-    msg->user_context = NULL;
-    if (!head) {
-      head = tail = msg;
-    } else {
-      tail->user_context = msg;
-      tail = msg;
-    }
-    ++sz;
-    --cnt;
-  }
-
-  struct xio_msg* begin() { return head; }
-
-  struct xio_msg* end() { return NULL; }
-
-  void next(struct xio_msg** msg) {
-    *msg = static_cast<struct xio_msg *>((*msg)->user_context);
-  }
-
-  struct xio_msg* dequeue() {
-    struct xio_msg* msgs = head;
-    clear();
-    return msgs;
-  }
-
-  void clear() {
-    head = tail = NULL;
-    cnt = 0;
-    sz = 0;
-  }
-};
-
-#endif /* XIO_IN_SEQ_H */
diff --git a/src/msg/xio/XioMessenger.cc b/src/msg/xio/XioMessenger.cc
deleted file mode 100644
index 6bf4d52c726..00000000000
--- a/src/msg/xio/XioMessenger.cc
+++ /dev/null
@@ -1,1137 +0,0 @@
-// -*- mode:C++; tab-width:8; c-basic-offset:2; indent-tabs-mode:t -*-
-// vim: ts=8 sw=2 smarttab
-/*
- * Ceph - scalable distributed file system
- *
- * Copyright (C) 2004-2006 Sage Weil <sage@newdream.net>
- * Portions Copyright (C) 2013 CohortFS, LLC
- *
- * This is free software; you can redistribute it and/or
- * modify it under the terms of the GNU Lesser General Public
- * License version 2.1, as published by the Free Software
- * Foundation.  See file COPYING.
- *
- */
-
-#include <arpa/inet.h>
-#include <boost/lexical_cast.hpp>
-#include <set>
-#include <stdlib.h>
-#include <memory>
-
-#include "XioMsg.h"
-#include "XioMessenger.h"
-#include "common/address_helper.h"
-#include "common/code_environment.h"
-#include "messages/MNop.h"
-
-#define dout_subsys ceph_subsys_xio
-#undef dout_prefix
-#define dout_prefix *_dout << "xio."
-
-Mutex mtx("XioMessenger Package Lock");
-std::atomic<bool> initialized = { false };
-
-std::atomic<unsigned> XioMessenger::nInstances = { 0 };
-
-struct xio_mempool *xio_msgr_noreg_mpool;
-
-static struct xio_session_ops xio_msgr_ops;
-
-/* Accelio API callouts */
-
-namespace xio_log
-{
-typedef pair<const char*, int> level_pair;
-static const level_pair LEVELS[] = {
-  make_pair("fatal", 0),
-  make_pair("error", 0),
-  make_pair("warn", 1),
-  make_pair("info", 1),
-  make_pair("debug", 2),
-  make_pair("trace", 20)
-};
-
-static CephContext *context;
-
-int get_level()
-{
-  int level = 0;
-  for (size_t i = 0; i < sizeof(LEVELS); i++) {
-    if (!ldlog_p1(context, dout_subsys, LEVELS[i].second))
-      break;
-    level++;
-  }
-  return level;
-}
-
-void log_dout(const char *file, unsigned line,
-	      const char *function, unsigned level,
-	      const char *fmt, ...)
-{
-  char buffer[2048];
-  va_list args;
-  va_start(args, fmt);
-  int n = vsnprintf(buffer, sizeof(buffer), fmt, args);
-  va_end(args);
-
-  if (n > 0) {
-    const char *short_file = strrchr(file, '/');
-    short_file = (short_file == NULL) ? file : short_file + 1;
-
-    const level_pair &lvl = LEVELS[level];
-    ldout(context, lvl.second) << '[' << lvl.first << "] "
-      << short_file << ':' << line << ' '
-      << function << " - " << buffer << dendl;
-  }
-}
-}
-
-static int on_session_event(struct xio_session *session,
-			    struct xio_session_event_data *event_data,
-			    void *cb_user_context)
-{
-  XioMessenger *msgr = static_cast<XioMessenger*>(cb_user_context);
-  CephContext *cct = msgr->cct;
-
-  ldout(cct,4) << "session event: " << xio_session_event_str(event_data->event)
-    << ". reason: " << xio_strerror(event_data->reason) << dendl;
-
-  return msgr->session_event(session, event_data, cb_user_context);
-}
-
-static int on_new_session(struct xio_session *session,
-			  struct xio_new_session_req *req,
-			  void *cb_user_context)
-{
-  XioMessenger *msgr = static_cast<XioMessenger*>(cb_user_context);
-  CephContext *cct = msgr->cct;
-
-  ldout(cct,4) << "new session " << session
-    << " user_context " << cb_user_context << dendl;
-
-  return (msgr->new_session(session, req, cb_user_context));
-}
-
-static int on_msg(struct xio_session *session,
-		  struct xio_msg *req,
-		  int more_in_batch,
-		  void *cb_user_context)
-{
-  XioConnection* xcon __attribute__((unused)) =
-    static_cast<XioConnection*>(cb_user_context);
-  CephContext *cct = xcon->get_messenger()->cct;
-
-  ldout(cct,25) << "on_msg session " << session << " xcon " << xcon << dendl;
-
-  if (unlikely(XioPool::trace_mempool)) {
-    static uint32_t nreqs;
-    if (unlikely((++nreqs % 65536) == 0)) {
-      xp_stats.dump(__func__, nreqs);
-    }
-  }
-
-  return xcon->on_msg(session, req, more_in_batch,
-			  cb_user_context);
-}
-
-static int on_ow_msg_send_complete(struct xio_session *session,
-				   struct xio_msg *msg,
-				   void *conn_user_context)
-{
-  XioConnection *xcon =
-    static_cast<XioConnection*>(conn_user_context);
-  CephContext *cct = xcon->get_messenger()->cct;
-
-  ldout(cct,25) << "msg delivered session: " << session
-		<< " msg: " << msg << " conn_user_context "
-		<< conn_user_context << dendl;
-
-  return xcon->on_ow_msg_send_complete(session, msg, conn_user_context);
-}
-
-static int on_msg_error(struct xio_session *session,
-			enum xio_status error,
-			enum xio_msg_direction dir,
-			struct xio_msg  *msg,
-			void *conn_user_context)
-{
-  /* XIO promises to flush back undelivered messages */
-  XioConnection *xcon =
-    static_cast<XioConnection*>(conn_user_context);
-  CephContext *cct = xcon->get_messenger()->cct;
-
-  ldout(cct,4) << "msg error session: " << session
-    << " error: " << xio_strerror(error) << " msg: " << msg
-    << " conn_user_context " << conn_user_context << dendl;
-
-  return xcon->on_msg_error(session, error, msg, conn_user_context);
-}
-
-static int on_cancel(struct xio_session *session,
-		     struct xio_msg  *msg,
-		     enum xio_status result,
-		     void *conn_user_context)
-{
-  XioConnection* xcon __attribute__((unused)) =
-    static_cast<XioConnection*>(conn_user_context);
-  CephContext *cct = xcon->get_messenger()->cct;
-
-  ldout(cct,25) << "on cancel: session: " << session << " msg: " << msg
-    << " conn_user_context " << conn_user_context << dendl;
-
-  return 0;
-}
-
-static int on_cancel_request(struct xio_session *session,
-			     struct xio_msg  *msg,
-			     void *conn_user_context)
-{
-  XioConnection* xcon __attribute__((unused)) =
-    static_cast<XioConnection*>(conn_user_context);
-  CephContext *cct = xcon->get_messenger()->cct;
-
-  ldout(cct,25) << "on cancel request: session: " << session << " msg: " << msg
-    << " conn_user_context " << conn_user_context << dendl;
-
-  return 0;
-}
-
-/* free functions */
-static string xio_uri_from_entity(const string &type,
-				  const entity_addr_t& addr, bool want_port)
-{
-  const char *host = NULL;
-  char addr_buf[129];
-  string xio_uri;
-
-  switch(addr.get_family()) {
-  case AF_INET:
-    host = inet_ntop(AF_INET, &addr.in4_addr().sin_addr, addr_buf,
-		     INET_ADDRSTRLEN);
-    break;
-  case AF_INET6:
-    host = inet_ntop(AF_INET6, &addr.in6_addr().sin6_addr, addr_buf,
-		     INET6_ADDRSTRLEN);
-    break;
-  default:
-    abort();
-    break;
-  };
-
-  if (type == "rdma" || type == "tcp")
-      xio_uri = type + "://";
-  else
-      xio_uri = "rdma://";
-
-  /* The following can only succeed if the host is rdma-capable */
-  xio_uri += host;
-  if (want_port) {
-    xio_uri += ":";
-    xio_uri += boost::lexical_cast<std::string>(addr.get_port());
-  }
-
-  return xio_uri;
-} /* xio_uri_from_entity */
-
-void XioInit::package_init(CephContext *cct) {
-   if (! initialized) {
-
-     mtx.Lock();
-     if (! initialized) {
-
-       xio_init();
-
-       // claim a reference to the first context we see
-       xio_log::context = cct->get();
-
-       int xopt;
-       xopt = xio_log::get_level();
-       xio_set_opt(NULL, XIO_OPTLEVEL_ACCELIO, XIO_OPTNAME_LOG_LEVEL,
- 		  &xopt, sizeof(xopt));
-       xio_set_opt(NULL, XIO_OPTLEVEL_ACCELIO, XIO_OPTNAME_LOG_FN,
- 		  (const void*)xio_log::log_dout, sizeof(xio_log_fn));
-
-       xopt = 1;
-       xio_set_opt(NULL, XIO_OPTLEVEL_ACCELIO, XIO_OPTNAME_DISABLE_HUGETBL,
- 		  &xopt, sizeof(xopt));
-
-       if (g_code_env == CODE_ENVIRONMENT_DAEMON) {
-         xopt = 1;
-         xio_set_opt(NULL, XIO_OPTLEVEL_RDMA, XIO_OPTNAME_ENABLE_FORK_INIT,
- 		    &xopt, sizeof(xopt));
-       }
-
-       xopt = XIO_MSGR_IOVLEN;
-       xio_set_opt(NULL, XIO_OPTLEVEL_ACCELIO, XIO_OPTNAME_MAX_IN_IOVLEN,
- 		  &xopt, sizeof(xopt));
-       xio_set_opt(NULL, XIO_OPTLEVEL_ACCELIO, XIO_OPTNAME_MAX_OUT_IOVLEN,
- 		  &xopt, sizeof(xopt));
-
-       /* enable flow-control */
-       xopt = 1;
-       xio_set_opt(NULL, XIO_OPTLEVEL_ACCELIO, XIO_OPTNAME_ENABLE_FLOW_CONTROL,
-                  &xopt, sizeof(xopt));
-
-       /* and set threshold for buffer callouts */
-       xopt = max(cct->_conf->xio_max_send_inline, 512);
-       xio_set_opt(NULL, XIO_OPTLEVEL_ACCELIO, XIO_OPTNAME_MAX_INLINE_XIO_DATA,
-                  &xopt, sizeof(xopt));
-
-       xopt = XioMsgHdr::get_max_encoded_length();
-       ldout(cct,2) << "setting accelio max header size " << xopt << dendl;
-       xio_set_opt(NULL, XIO_OPTLEVEL_ACCELIO, XIO_OPTNAME_MAX_INLINE_XIO_HEADER,
-                  &xopt, sizeof(xopt));
-
-       size_t queue_depth = cct->_conf->xio_queue_depth;
-       struct xio_mempool_config mempool_config = {
-         6,
-         {
-           {1024,  0,  queue_depth,  262144},
-           {4096,  0,  queue_depth,  262144},
-           {16384, 0,  queue_depth,  262144},
-           {65536, 0,  128,  65536},
-           {262144, 0,  32,  16384},
-           {1048576, 0, 8,  8192}
-         }
-       };
-       xio_set_opt(NULL,
-                   XIO_OPTLEVEL_ACCELIO, XIO_OPTNAME_CONFIG_MEMPOOL,
-                   &mempool_config, sizeof(mempool_config));
-
-       /* and unregisterd one */
- #define XMSG_MEMPOOL_QUANTUM 4096
-
-       xio_msgr_noreg_mpool =
- 	xio_mempool_create(-1 /* nodeid */,
- 			   XIO_MEMPOOL_FLAG_REGULAR_PAGES_ALLOC);
-
-       (void) xio_mempool_add_slab(xio_msgr_noreg_mpool, 64,
- 				       cct->_conf->xio_mp_min,
- 				       cct->_conf->xio_mp_max_64,
- 				       XMSG_MEMPOOL_QUANTUM, 0);
-       (void) xio_mempool_add_slab(xio_msgr_noreg_mpool, 256,
- 				       cct->_conf->xio_mp_min,
- 				       cct->_conf->xio_mp_max_256,
- 				       XMSG_MEMPOOL_QUANTUM, 0);
-       (void) xio_mempool_add_slab(xio_msgr_noreg_mpool, 1024,
- 				       cct->_conf->xio_mp_min,
- 				       cct->_conf->xio_mp_max_1k,
- 				       XMSG_MEMPOOL_QUANTUM, 0);
-       (void) xio_mempool_add_slab(xio_msgr_noreg_mpool, getpagesize(),
- 				       cct->_conf->xio_mp_min,
- 				       cct->_conf->xio_mp_max_page,
- 				       XMSG_MEMPOOL_QUANTUM, 0);
-
-       /* initialize ops singleton */
-       xio_msgr_ops.on_session_event = on_session_event;
-       xio_msgr_ops.on_new_session = on_new_session;
-       xio_msgr_ops.on_session_established = NULL;
-       xio_msgr_ops.on_msg = on_msg;
-       xio_msgr_ops.on_ow_msg_send_complete = on_ow_msg_send_complete;
-       xio_msgr_ops.on_msg_error = on_msg_error;
-       xio_msgr_ops.on_cancel = on_cancel;
-       xio_msgr_ops.on_cancel_request = on_cancel_request;
-
-       /* mark initialized */
-       initialized = true;
-     }
-     mtx.Unlock();
-   }
- }
-
-/* XioMessenger */
-#undef dout_prefix
-#define dout_prefix _prefix(_dout, this)
-static ostream& _prefix(std::ostream *_dout, XioMessenger *msgr) {
-  return *_dout << "-- " << msgr->get_myaddr() << " ";
-}
-
-XioMessenger::XioMessenger(CephContext *cct, entity_name_t name,
-			   string mname, uint64_t _nonce,
-			   uint64_t cflags, DispatchStrategy *ds)
-  : SimplePolicyMessenger(cct, name, mname, _nonce),
-    XioInit(cct),
-    portals(this, get_nportals(cflags), get_nconns_per_portal(cflags)),
-    dispatch_strategy(ds),
-    loop_con(new XioLoopbackConnection(this)),
-    special_handling(0),
-    sh_mtx("XioMessenger session mutex"),
-    sh_cond(),
-    need_addr(true),
-    did_bind(false),
-    nonce(_nonce)
-{
-
-  if (cct->_conf->xio_trace_xcon)
-    magic |= MSG_MAGIC_TRACE_XCON;
-
-  XioPool::trace_mempool = (cct->_conf->xio_trace_mempool);
-  XioPool::trace_msgcnt = (cct->_conf->xio_trace_msgcnt);
-
-  dispatch_strategy->set_messenger(this);
-
-  /* update class instance count */
-  nInstances++;
-
-  loop_con->set_features(CEPH_FEATURES_ALL);
-
-  ldout(cct,2) << "Create msgr: " << this << " instance: "
-    << nInstances << " type: " << name.type_str()
-    << " subtype: " << mname << " nportals: " << get_nportals(cflags)
-    << " nconns_per_portal: " << get_nconns_per_portal(cflags)
-    << dendl;
-
-} /* ctor */
-
-int XioMessenger::pool_hint(uint32_t dsize) {
-  if (dsize > 1024*1024)
-    return 0;
-
-  /* if dsize is already present, returns -EEXIST */
-  return xio_mempool_add_slab(xio_msgr_noreg_mpool, dsize, 0,
-				   cct->_conf->xio_mp_max_hint,
-				   XMSG_MEMPOOL_QUANTUM, 0);
-}
-
-int XioMessenger::get_nconns_per_portal(uint64_t cflags)
-{
-  const int XIO_DEFAULT_NUM_CONNS_PER_PORTAL = 8;
-  int nconns = XIO_DEFAULT_NUM_CONNS_PER_PORTAL;
-
-  if (cflags & Messenger::HAS_MANY_CONNECTIONS)
-    nconns = max(cct->_conf->xio_max_conns_per_portal, XIO_DEFAULT_NUM_CONNS_PER_PORTAL);
-  else if (cflags & Messenger::HEARTBEAT)
-    nconns = max(cct->_conf->osd_heartbeat_min_peers * 4, XIO_DEFAULT_NUM_CONNS_PER_PORTAL);
-
-  return nconns;
-}
-
-int XioMessenger::get_nportals(uint64_t cflags)
-{
-  int nportals = 1;
-
-  if (cflags & Messenger::HAS_HEAVY_TRAFFIC)
-    nportals = max(cct->_conf->xio_portal_threads, 1);
-
-  return nportals;
-}
-
-void XioMessenger::learned_addr(const entity_addr_t &peer_addr_for_me)
-{
-  // be careful here: multiple threads may block here, and readers of
-  // my_inst.addr do NOT hold any lock.
-
-  // this always goes from true -> false under the protection of the
-  // mutex.  if it is already false, we need not retake the mutex at
-  // all.
-  if (!need_addr)
-    return;
-
-  sh_mtx.Lock();
-  if (need_addr) {
-    entity_addr_t t = peer_addr_for_me;
-    t.set_port(my_inst.addr.get_port());
-    my_inst.addr.set_sockaddr(t.get_sockaddr());
-    ldout(cct,2) << "learned my addr " << my_inst.addr << dendl;
-    need_addr = false;
-    // init_local_connection();
-  }
-  sh_mtx.Unlock();
-
-}
-
-int XioMessenger::new_session(struct xio_session *session,
-			      struct xio_new_session_req *req,
-			      void *cb_user_context)
-{
-  if (shutdown_called) {
-    return xio_reject(
-      session, XIO_E_SESSION_REFUSED, NULL /* udata */, 0 /* udata len */);
-  }
-  int code = portals.accept(session, req, cb_user_context);
-  if (! code)
-    nsessions++;
-  return code;
-} /* new_session */
-
-int XioMessenger::session_event(struct xio_session *session,
-				struct xio_session_event_data *event_data,
-				void *cb_user_context)
-{
-  XioConnection *xcon;
-
-  switch (event_data->event) {
-  case XIO_SESSION_CONNECTION_ESTABLISHED_EVENT:
-  {
-    struct xio_connection *conn = event_data->conn;
-    struct xio_connection_attr xcona;
-    entity_addr_t peer_addr_for_me, paddr;
-
-    xcon = static_cast<XioConnection*>(event_data->conn_user_context);
-
-    ldout(cct,2) << "connection established " << event_data->conn
-      << " session " << session << " xcon " << xcon << dendl;
-
-    (void) xio_query_connection(conn, &xcona,
-				XIO_CONNECTION_ATTR_LOCAL_ADDR|
-				XIO_CONNECTION_ATTR_PEER_ADDR);
-    peer_addr_for_me.set_sockaddr((struct sockaddr *)&xcona.local_addr);
-    paddr.set_sockaddr((struct sockaddr *)&xcona.peer_addr);
-    //set_myaddr(peer_addr_for_me);
-    learned_addr(peer_addr_for_me);
-    ldout(cct,2) << "client: connected from " << peer_addr_for_me << " to " << paddr << dendl;
-
-    /* notify hook */
-    this->ms_deliver_handle_connect(xcon);
-    this->ms_deliver_handle_fast_connect(xcon);
-  }
-  break;
-
-  case XIO_SESSION_NEW_CONNECTION_EVENT:
-  {
-    struct xio_connection *conn = event_data->conn;
-    struct xio_connection_attr xcona;
-    entity_inst_t s_inst;
-    entity_addr_t peer_addr_for_me;
-
-    (void) xio_query_connection(conn, &xcona,
-				XIO_CONNECTION_ATTR_CTX|
-				XIO_CONNECTION_ATTR_PEER_ADDR|
-				XIO_CONNECTION_ATTR_LOCAL_ADDR);
-    /* XXX assumes RDMA */
-    s_inst.addr.set_sockaddr((struct sockaddr *)&xcona.peer_addr);
-    peer_addr_for_me.set_sockaddr((struct sockaddr *)&xcona.local_addr);
-
-    xcon = new XioConnection(this, XioConnection::PASSIVE, s_inst);
-    xcon->session = session;
-
-    struct xio_context_attr xctxa;
-    (void) xio_query_context(xcona.ctx, &xctxa, XIO_CONTEXT_ATTR_USER_CTX);
-
-    xcon->conn = conn;
-    xcon->portal = static_cast<XioPortal*>(xctxa.user_context);
-    assert(xcon->portal);
-
-    xcona.user_context = xcon;
-    (void) xio_modify_connection(conn, &xcona, XIO_CONNECTION_ATTR_USER_CTX);
-
-    xcon->connected = true;
-
-    /* sentinel ref */
-    xcon->get(); /* xcon->nref == 1 */
-    conns_sp.lock();
-    conns_list.push_back(*xcon);
-    /* XXX we can't put xcon in conns_entity_map becase we don't yet know
-     * it's peer address */
-    conns_sp.unlock();
-
-    /* XXXX pre-merge of session startup negotiation ONLY! */
-    xcon->cstate.state_up_ready(XioConnection::CState::OP_FLAG_NONE);
-
-    ldout(cct,2) << "New connection session " << session
-      << " xcon " << xcon << " on msgr: " << this << " portal: " << xcon->portal << dendl;
-    ldout(cct,2) << "Server: connected from " << s_inst.addr << " to " << peer_addr_for_me << dendl;
-  }
-  break;
-  case XIO_SESSION_CONNECTION_ERROR_EVENT:
-  case XIO_SESSION_CONNECTION_CLOSED_EVENT: /* orderly discon */
-  case XIO_SESSION_CONNECTION_DISCONNECTED_EVENT: /* unexpected discon */
-  case XIO_SESSION_CONNECTION_REFUSED_EVENT:
-    xcon = static_cast<XioConnection*>(event_data->conn_user_context);
-    ldout(cct,2) << xio_session_event_str(event_data->event)
-      << " xcon " << xcon << " session " << session  << dendl;
-    if (likely(!!xcon)) {
-      unregister_xcon(xcon);
-      xcon->on_disconnect_event();
-    }
-    break;
-  case XIO_SESSION_CONNECTION_TEARDOWN_EVENT:
-    xcon = static_cast<XioConnection*>(event_data->conn_user_context);
-    ldout(cct,2) << xio_session_event_str(event_data->event)
-      << " xcon " << xcon << " session " << session << dendl;
-    /*
-     * There are flows where Accelio sends teardown event without going
-     * through disconnect event. so we make sure we cleaned the connection.
-     */
-    unregister_xcon(xcon);
-    xcon->on_teardown_event();
-    break;
-  case XIO_SESSION_TEARDOWN_EVENT:
-    ldout(cct,2) << xio_session_event_str(event_data->event)
-      << " session " << session << dendl;
-    if (unlikely(XioPool::trace_mempool)) {
-      xp_stats.dump("xio session dtor", reinterpret_cast<uint64_t>(session));
-    }
-    xio_session_destroy(session);
-    if (--nsessions == 0) {
-      Mutex::Locker lck(sh_mtx);
-      if (nsessions == 0)
-	sh_cond.Signal();
-    }
-    break;
-  default:
-    break;
-  };
-
-  return 0;
-}
-
-enum bl_type
-{
-  BUFFER_PAYLOAD,
-  BUFFER_MIDDLE,
-  BUFFER_DATA
-};
-
-#define MAX_XIO_BUF_SIZE 1044480
-
-static inline int
-xio_count_buffers(const buffer::list& bl, int& req_size, int& msg_off, int& req_off)
-{
-
-  const std::list<buffer::ptr>& buffers = bl.buffers();
-  list<bufferptr>::const_iterator pb;
-  size_t size, off;
-  int result;
-  int first = 1;
-
-  off = size = 0;
-  result = 0;
-  for (;;) {
-    if (off >= size) {
-      if (first) pb = buffers.begin(); else ++pb;
-      if (pb == buffers.end()) {
-	break;
-      }
-      off = 0;
-      size = pb->length();
-      first = 0;
-    }
-    size_t count = size - off;
-    if (!count) continue;
-    if (req_size + count > MAX_XIO_BUF_SIZE) {
-	count = MAX_XIO_BUF_SIZE - req_size;
-    }
-
-    ++result;
-
-    /* advance iov and perhaps request */
-
-    off += count;
-    req_size += count;
-    ++msg_off;
-    if (unlikely(msg_off >= XIO_MSGR_IOVLEN || req_size >= MAX_XIO_BUF_SIZE)) {
-      ++req_off;
-      msg_off = 0;
-      req_size = 0;
-    }
-  }
-
-  return result;
-}
-
-static inline void
-xio_place_buffers(const buffer::list& bl, XioMsg *xmsg, struct xio_msg*& req,
-		  struct xio_iovec_ex*& msg_iov, int& req_size,
-		  int ex_cnt, int& msg_off, int& req_off, bl_type type)
-{
-
-  const std::list<buffer::ptr>& buffers = bl.buffers();
-  list<bufferptr>::const_iterator pb;
-  struct xio_iovec_ex* iov;
-  size_t size, off;
-  const char *data = NULL;
-  int first = 1;
-
-  off = size = 0;
-  for (;;) {
-    if (off >= size) {
-      if (first) pb = buffers.begin(); else ++pb;
-      if (pb == buffers.end()) {
-	break;
-      }
-      off = 0;
-      size = pb->length();
-      data = pb->c_str();	 // is c_str() efficient?
-      first = 0;
-    }
-    size_t count = size - off;
-    if (!count) continue;
-    if (req_size + count > MAX_XIO_BUF_SIZE) {
-	count = MAX_XIO_BUF_SIZE - req_size;
-    }
-
-    /* assign buffer */
-    iov = &msg_iov[msg_off];
-    iov->iov_base = (void *) (&data[off]);
-    iov->iov_len = count;
-
-    switch (type) {
-    case BUFFER_DATA:
-      //break;
-    default:
-    {
-      struct xio_reg_mem *mp = get_xio_mp(*pb);
-      iov->mr = (mp) ? mp->mr : NULL;
-    }
-      break;
-    }
-
-    /* advance iov(s) */
-
-    off += count;
-    req_size += count;
-    ++msg_off;
-
-    /* next request if necessary */
-
-    if (unlikely(msg_off >= XIO_MSGR_IOVLEN || req_size >= MAX_XIO_BUF_SIZE)) {
-      /* finish this request */
-      req->out.pdata_iov.nents = msg_off;
-      /* advance to next, and write in it if it's not the last one. */
-      if (++req_off >= ex_cnt) {
-	req = 0;	/* poison.  trap if we try to use it. */
-	msg_iov = NULL;
-      } else {
-	req = &xmsg->req_arr[req_off].msg;
-	msg_iov = req->out.pdata_iov.sglist;
-      }
-      msg_off = 0;
-      req_size = 0;
-    }
-  }
-}
-
-int XioMessenger::bind(const entity_addr_t& addr)
-{
-  if (addr.is_blank_ip()) {
-      lderr(cct) << "ERROR: need rdma ip for remote use! " << dendl;
-      cout << "Error: xio bind failed. public/cluster ip not specified" << std::endl;
-      return -1;
-  }
-
-  entity_addr_t shift_addr = addr;
-  string base_uri = xio_uri_from_entity(cct->_conf->xio_transport_type,
-					shift_addr, false /* want_port */);
-  ldout(cct,4) << "XioMessenger " << this << " bind: xio_uri "
-    << base_uri << ':' << shift_addr.get_port() << dendl;
-
-  uint16_t port0;
-  int r = portals.bind(&xio_msgr_ops, base_uri, shift_addr.get_port(), &port0);
-  if (r == 0) {
-    shift_addr.set_port(port0);
-    shift_addr.nonce = nonce;
-    set_myaddr(shift_addr);
-    need_addr = false;
-    did_bind = true;
-  }
-  return r;
-} /* bind */
-
-int XioMessenger::rebind(const set<int>& avoid_ports)
-{
-  ldout(cct,4) << "XioMessenger " << this << " rebind attempt" << dendl;
-  return 0;
-} /* rebind */
-
-int XioMessenger::start()
-{
-  portals.start();
-  dispatch_strategy->start();
-  if (!did_bind) {
-	  my_inst.addr.nonce = nonce;
-  }
-  started = true;
-  return 0;
-}
-
-void XioMessenger::wait()
-{
-  portals.join();
-  dispatch_strategy->wait();
-} /* wait */
-
-int XioMessenger::_send_message(Message *m, const entity_inst_t& dest)
-{
-  ConnectionRef conn = get_connection(dest);
-  if (conn)
-    return _send_message(m, &(*conn));
-  else
-    return EINVAL;
-} /* send_message(Message *, const entity_inst_t&) */
-
-static inline XioMsg* pool_alloc_xio_msg(Message *m, XioConnection *xcon,
-  int ex_cnt)
-{
-  struct xio_reg_mem mp_mem;
-  int e = xpool_alloc(xio_msgr_noreg_mpool, sizeof(XioMsg), &mp_mem);
-  if (!!e)
-    return NULL;
-  XioMsg *xmsg = reinterpret_cast<XioMsg*>(mp_mem.addr);
-  assert(!!xmsg);
-  new (xmsg) XioMsg(m, xcon, mp_mem, ex_cnt, CEPH_FEATURES_ALL);
-  return xmsg;
-}
-
-XioCommand* pool_alloc_xio_command(XioConnection *xcon)
-{
-  struct xio_reg_mem mp_mem;
-  int e = xpool_alloc(xio_msgr_noreg_mpool, sizeof(XioCommand), &mp_mem);
-  if (!!e)
-    return NULL;
-  XioCommand *xcmd = reinterpret_cast<XioCommand*>(mp_mem.addr);
-  assert(!!xcmd);
-  new (xcmd) XioCommand(xcon, mp_mem);
-  return xcmd;
-}
-
-int XioMessenger::_send_message(Message *m, Connection *con)
-{
-  if (con == loop_con.get() /* intrusive_ptr get() */) {
-    m->set_connection(con);
-    m->set_src(get_myinst().name);
-    m->set_seq(loop_con->next_seq());
-    ds_dispatch(m);
-    return 0;
-  }
-
-  XioConnection *xcon = static_cast<XioConnection*>(con);
-
-  /* If con is not in READY state, we have to enforce policy */
-  if (xcon->cstate.session_state.read() != XioConnection::UP) {
-    pthread_spin_lock(&xcon->sp);
-    if (xcon->cstate.session_state.read() != XioConnection::UP) {
-      xcon->outgoing.mqueue.push_back(*m);
-      pthread_spin_unlock(&xcon->sp);
-      return 0;
-    }
-    pthread_spin_unlock(&xcon->sp);
-  }
-
-  return _send_message_impl(m, xcon);
-} /* send_message(Message* m, Connection *con) */
-
-int XioMessenger::_send_message_impl(Message* m, XioConnection* xcon)
-{
-  int code = 0;
-
-  Mutex::Locker l(xcon->lock);
-  if (unlikely(XioPool::trace_mempool)) {
-    static uint32_t nreqs;
-    if (unlikely((++nreqs % 65536) == 0)) {
-      xp_stats.dump(__func__, nreqs);
-    }
-  }
-
-  m->set_seq(xcon->state.next_out_seq());
-  m->set_magic(magic); // trace flags and special handling
-
-  m->encode(xcon->get_features(), this->crcflags);
-
-  buffer::list &payload = m->get_payload();
-  buffer::list &middle = m->get_middle();
-  buffer::list &data = m->get_data();
-
-  int msg_off = 0;
-  int req_off = 0;
-  int req_size = 0;
-  int nbuffers =
-    xio_count_buffers(payload, req_size, msg_off, req_off) +
-    xio_count_buffers(middle, req_size, msg_off, req_off) +
-    xio_count_buffers(data, req_size, msg_off, req_off);
-
-  int ex_cnt = req_off;
-  if (msg_off == 0 && ex_cnt > 0) {
-    // no buffers for last msg
-    ldout(cct,10) << "msg_off 0, ex_cnt " << ex_cnt << " -> " << ex_cnt-1 << dendl;
-    ex_cnt--;
-  }
-
-  /* get an XioMsg frame */
-  XioMsg *xmsg = pool_alloc_xio_msg(m, xcon, ex_cnt);
-  if (! xmsg) {
-    /* could happen if Accelio has been shutdown */
-    return ENOMEM;
-  }
-
-  ldout(cct,4) << __func__ << " " << m << " new XioMsg " << xmsg
-       << " tag " << (int)xmsg->hdr.tag
-       << " req_0 " << xmsg->get_xio_msg() << " msg type " << m->get_type()
-       << " features: " << xcon->get_features()
-       << " conn " << xcon->conn << " sess " << xcon->session << dendl;
-
-  if (magic & (MSG_MAGIC_XIO)) {
-
-    /* XXXX verify */
-    switch (m->get_type()) {
-    case 43:
-    // case 15:
-      ldout(cct,4) << __func__ << "stop 43 " << m->get_type() << " " << *m << dendl;
-      buffer::list &payload = m->get_payload();
-      ldout(cct,4) << __func__ << "payload dump:" << dendl;
-      payload.hexdump(cout);
-    }
-  }
-
-  struct xio_msg *req = xmsg->get_xio_msg();
-  struct xio_iovec_ex *msg_iov = req->out.pdata_iov.sglist;
-
-  if (magic & (MSG_MAGIC_XIO)) {
-    ldout(cct,4) << "payload: " << payload.buffers().size() <<
-      " middle: " << middle.buffers().size() <<
-      " data: " << data.buffers().size() <<
-      dendl;
-  }
-
-  if (unlikely(ex_cnt > 0)) {
-    ldout(cct,4) << __func__ << " buffer cnt > XIO_MSGR_IOVLEN (" <<
-      ((XIO_MSGR_IOVLEN-1) + nbuffers) << ")" << dendl;
-  }
-
-  /* do the invariant part */
-  msg_off = 0;
-  req_off = -1; /* most often, not used */
-  req_size = 0;
-
-  xio_place_buffers(payload, xmsg, req, msg_iov, req_size, ex_cnt, msg_off,
-		    req_off, BUFFER_PAYLOAD);
-
-  xio_place_buffers(middle, xmsg, req, msg_iov, req_size, ex_cnt, msg_off,
-		    req_off, BUFFER_MIDDLE);
-
-  xio_place_buffers(data, xmsg, req, msg_iov, req_size, ex_cnt, msg_off,
-		    req_off, BUFFER_DATA);
-  ldout(cct,10) << "ex_cnt " << ex_cnt << ", req_off " << req_off
-    << ", msg_cnt " << xmsg->get_msg_count() << dendl;
-
-  /* finalize request */
-  if (msg_off)
-    req->out.pdata_iov.nents = msg_off;
-
-  /* fixup first msg */
-  req = xmsg->get_xio_msg();
-
-  const std::list<buffer::ptr>& header = xmsg->hdr.get_bl().buffers();
-  assert(header.size() == 1); /* XXX */
-  list<bufferptr>::const_iterator pb = header.begin();
-  req->out.header.iov_base = (char*) pb->c_str();
-  req->out.header.iov_len = pb->length();
-
-  /* deliver via xio, preserve ordering */
-  if (xmsg->get_msg_count() > 1) {
-    struct xio_msg *head = xmsg->get_xio_msg();
-    struct xio_msg *tail = head;
-    for (req_off = 0; ((unsigned) req_off) < xmsg->get_msg_count()-1; ++req_off) {
-      req = &xmsg->req_arr[req_off].msg;
-assert(!req->in.pdata_iov.nents);
-assert(req->out.pdata_iov.nents || !nbuffers);
-      tail->next = req;
-      tail = req;
-     }
-    tail->next = NULL;
-  }
-  xmsg->trace = m->trace;
-  m->trace.event("xio portal enqueue for send");
-  m->trace.keyval("xio message segments", xmsg->hdr.msg_cnt);
-  xcon->portal->enqueue_for_send(xcon, xmsg);
-
-  return code;
-} /* send_message(Message *, Connection *) */
-
-int XioMessenger::shutdown()
-{
-  shutdown_called = true;
-  conns_sp.lock();
-  XioConnection::ConnList::iterator iter;
-  iter = conns_list.begin();
-  for (iter = conns_list.begin(); iter != conns_list.end(); ++iter) {
-    (void) iter->disconnect(); // XXX mark down?
-  }
-  conns_sp.unlock();
-  while(nsessions > 0) {
-    Mutex::Locker lck(sh_mtx);
-    if (nsessions > 0)
-      sh_cond.Wait(sh_mtx);
-  }
-  portals.shutdown();
-  dispatch_strategy->shutdown();
-  did_bind = false;
-  started = false;
-  return 0;
-} /* shutdown */
-
-ConnectionRef XioMessenger::get_connection(const entity_inst_t& dest)
-{
-  if (shutdown_called)
-    return NULL;
-
-  const entity_inst_t& self_inst = get_myinst();
-  if ((&dest == &self_inst) ||
-      (dest == self_inst)) {
-    return get_loopback_connection();
-  }
-
-  conns_sp.lock();
-  XioConnection::EntitySet::iterator conn_iter =
-    conns_entity_map.find(dest, XioConnection::EntityComp());
-  if (conn_iter != conns_entity_map.end()) {
-    ConnectionRef cref = &(*conn_iter);
-    conns_sp.unlock();
-    return cref;
-  }
-  else {
-    conns_sp.unlock();
-    string xio_uri = xio_uri_from_entity(cct->_conf->xio_transport_type,
-					 dest.addr, true /* want_port */);
-
-    ldout(cct,4) << "XioMessenger " << this << " get_connection: xio_uri "
-      << xio_uri << dendl;
-
-    /* XXX client session creation parameters */
-    struct xio_session_params params = {};
-    params.type         = XIO_SESSION_CLIENT;
-    params.ses_ops      = &xio_msgr_ops;
-    params.user_context = this;
-    params.uri          = xio_uri.c_str();
-
-    XioConnection *xcon = new XioConnection(this, XioConnection::ACTIVE,
-					    dest);
-
-    xcon->session = xio_session_create(&params);
-    if (! xcon->session) {
-      delete xcon;
-      return NULL;
-    }
-
-    /* this should cause callbacks with user context of conn, but
-     * we can always set it explicitly */
-    struct xio_connection_params xcp = {};
-    xcp.session           = xcon->session;
-    xcp.ctx               = xcon->portal->ctx;
-    xcp.conn_user_context = xcon;
-
-    xcon->conn = xio_connect(&xcp);
-    if (!xcon->conn) {
-      xio_session_destroy(xcon->session);
-      delete xcon;
-      return NULL;
-    }
-
-    nsessions++;
-    xcon->connected = true;
-
-    /* sentinel ref */
-    xcon->get(); /* xcon->nref == 1 */
-    conns_sp.lock();
-    conns_list.push_back(*xcon);
-    conns_entity_map.insert(*xcon);
-    conns_sp.unlock();
-
-    /* XXXX pre-merge of session startup negotiation ONLY! */
-    xcon->cstate.state_up_ready(XioConnection::CState::OP_FLAG_NONE);
-
-    ldout(cct,2) << "New connection xcon: " << xcon <<
-      " up_ready on session " << xcon->session <<
-      " on msgr: " << this << " portal: " << xcon->portal << dendl;
-
-    return xcon->get(); /* nref +1 */
-  }
-} /* get_connection */
-
-ConnectionRef XioMessenger::get_loopback_connection()
-{
-  return (loop_con.get());
-} /* get_loopback_connection */
-
-void XioMessenger::unregister_xcon(XioConnection *xcon)
-{
-  Spinlock::Locker lckr(conns_sp);
-
-  XioConnection::EntitySet::iterator conn_iter =
-	conns_entity_map.find(xcon->peer, XioConnection::EntityComp());
-  if (conn_iter != conns_entity_map.end()) {
-	XioConnection *xcon2 = &(*conn_iter);
-	if (xcon == xcon2) {
-	  conns_entity_map.erase(conn_iter);
-	}
-  }
-
-  /* check if citer on conn_list */
-  if (xcon->conns_hook.is_linked()) {
-    /* now find xcon on conns_list and erase */
-    XioConnection::ConnList::iterator citer =
-        XioConnection::ConnList::s_iterator_to(*xcon);
-    conns_list.erase(citer);
-  }
-}
-
-void XioMessenger::mark_down(const entity_addr_t& addr)
-{
-  entity_inst_t inst(entity_name_t(), addr);
-  Spinlock::Locker lckr(conns_sp);
-  XioConnection::EntitySet::iterator conn_iter =
-    conns_entity_map.find(inst, XioConnection::EntityComp());
-  if (conn_iter != conns_entity_map.end()) {
-      (*conn_iter)._mark_down(XioConnection::CState::OP_FLAG_NONE);
-    }
-} /* mark_down(const entity_addr_t& */
-
-void XioMessenger::mark_down(Connection* con)
-{
-  XioConnection *xcon = static_cast<XioConnection*>(con);
-  xcon->_mark_down(XioConnection::CState::OP_FLAG_NONE);
-} /* mark_down(Connection*) */
-
-void XioMessenger::mark_down_all()
-{
-  Spinlock::Locker lckr(conns_sp);
-  XioConnection::EntitySet::iterator conn_iter;
-  for (conn_iter = conns_entity_map.begin(); conn_iter !=
-	 conns_entity_map.begin(); ++conn_iter) {
-    (*conn_iter)._mark_down(XioConnection::CState::OP_FLAG_NONE);
-  }
-} /* mark_down_all */
-
-static inline XioMarkDownHook* pool_alloc_markdown_hook(
-  XioConnection *xcon, Message *m)
-{
-  struct xio_reg_mem mp_mem;
-  int e = xio_mempool_alloc(xio_msgr_noreg_mpool,
-			    sizeof(XioMarkDownHook), &mp_mem);
-  if (!!e)
-    return NULL;
-  XioMarkDownHook *hook = static_cast<XioMarkDownHook*>(mp_mem.addr);
-  new (hook) XioMarkDownHook(xcon, m, mp_mem);
-  return hook;
-}
-
-void XioMessenger::mark_down_on_empty(Connection* con)
-{
-  XioConnection *xcon = static_cast<XioConnection*>(con);
-  MNop* m = new MNop();
-  m->tag = XIO_NOP_TAG_MARKDOWN;
-  m->set_completion_hook(pool_alloc_markdown_hook(xcon, m));
-  // stall new messages
-  xcon->cstate.session_state = XioConnection::session_states::BARRIER;
-  (void) _send_message_impl(m, xcon);
-}
-
-void XioMessenger::mark_disposable(Connection *con)
-{
-  XioConnection *xcon = static_cast<XioConnection*>(con);
-  xcon->_mark_disposable(XioConnection::CState::OP_FLAG_NONE);
-}
-
-void XioMessenger::try_insert(XioConnection *xcon)
-{
-  Spinlock::Locker lckr(conns_sp);
-  /* already resident in conns_list */
-  conns_entity_map.insert(*xcon);
-}
-
-XioMessenger::~XioMessenger()
-{
-  delete dispatch_strategy;
-  nInstances--;
-} /* dtor */
diff --git a/src/msg/xio/XioMessenger.h b/src/msg/xio/XioMessenger.h
deleted file mode 100644
index ea20d36bd8c..00000000000
--- a/src/msg/xio/XioMessenger.h
+++ /dev/null
@@ -1,169 +0,0 @@
-// -*- mode:C++; tab-width:8; c-basic-offset:2; indent-tabs-mode:t -*-
-// vim: ts=8 sw=2 smarttab
-/*
- * Ceph - scalable distributed file system
- *
- * Copyright (C) 2004-2006 Sage Weil <sage@newdream.net>
- * Portions Copyright (C) 2013 CohortFS, LLC
- *
- * This is free software; you can redistribute it and/or
- * modify it under the terms of the GNU Lesser General Public
- * License version 2.1, as published by the Free Software
- * Foundation.  See file COPYING.
- *
- */
-
-#ifndef XIO_MESSENGER_H
-#define XIO_MESSENGER_H
-
-#include "msg/SimplePolicyMessenger.h"
-
-#include <atomic>
-
-extern "C" {
-#include "libxio.h"
-}
-
-#include "XioConnection.h"
-#include "XioPortal.h"
-#include "QueueStrategy.h"
-#include "common/Thread.h"
-#include "common/Mutex.h"
-#include "include/Spinlock.h"
-
-class XioInit {
-  /* safe to be called multiple times */
-  void package_init(CephContext *cct);
-
-protected:
-  XioInit(CephContext *cct) {
-    this->package_init(cct);
-  }
-};
-
-class XioMessenger : public SimplePolicyMessenger, XioInit
-{
-private:
-  static std::atomic<uint64_t> nInstances = { 0 };
-  std::atomic<uint64_t> nsessions = { 0 };
-  std::atomic<bool> shutdown_called = { false };
-  Spinlock conns_sp;
-  XioConnection::ConnList conns_list;
-  XioConnection::EntitySet conns_entity_map;
-  XioPortals portals;
-  DispatchStrategy* dispatch_strategy;
-  XioLoopbackConnectionRef loop_con;
-  uint32_t special_handling;
-  Mutex sh_mtx;
-  Cond sh_cond;
-  bool need_addr;
-  bool did_bind;
-
-  /// approximately unique ID set by the Constructor for use in entity_addr_t
-  uint64_t nonce;
-
-  friend class XioConnection;
-
-public:
-  XioMessenger(CephContext *cct, entity_name_t name,
-	       string mname, uint64_t nonce,
-	       uint64_t cflags = 0,
-	       DispatchStrategy* ds = new QueueStrategy(1));
-
-  virtual ~XioMessenger();
-
-  XioPortal* get_portal() { return portals.get_next_portal(); }
-
-  virtual void set_myaddr(const entity_addr_t& a) {
-    Messenger::set_myaddr(a);
-    loop_con->set_peer_addr(a);
-  }
-
-  int _send_message(Message *m, const entity_inst_t &dest);
-  int _send_message(Message *m, Connection *con);
-  int _send_message_impl(Message *m, XioConnection *xcon);
-
-  uint32_t get_special_handling() { return special_handling; }
-  void set_special_handling(int n) { special_handling = n; }
-  int pool_hint(uint32_t size);
-  void try_insert(XioConnection *xcon);
-
-  /* xio hooks */
-  int new_session(struct xio_session *session,
-		  struct xio_new_session_req *req,
-		  void *cb_user_context);
-
-  int session_event(struct xio_session *session,
-		    struct xio_session_event_data *event_data,
-		    void *cb_user_context);
-
-  /* Messenger interface */
-  virtual void set_addr_unknowns(const entity_addr_t &addr) override
-    { } /* XXX applicable? */
-  virtual void set_addr(const entity_addr_t &addr) override
-    { } /* XXX applicable? */
-
-  virtual int get_dispatch_queue_len()
-    { return 0; } /* XXX bogus? */
-
-  virtual double get_dispatch_queue_max_age(utime_t now)
-    { return 0; } /* XXX bogus? */
-
-  virtual void set_cluster_protocol(int p)
-    { }
-
-  virtual int bind(const entity_addr_t& addr);
-
-  virtual int rebind(const set<int>& avoid_ports);
-
-  virtual int start();
-
-  virtual void wait();
-
-  virtual int shutdown();
-
-  virtual int send_message(Message *m, const entity_inst_t &dest) {
-    return _send_message(m, dest);
-  }
-
-  virtual int lazy_send_message(Message *m, const entity_inst_t& dest)
-    { return EINVAL; }
-
-  virtual int lazy_send_message(Message *m, Connection *con)
-    { return EINVAL; }
-
-  virtual ConnectionRef get_connection(const entity_inst_t& dest);
-
-  virtual ConnectionRef get_loopback_connection();
-
-  void unregister_xcon(XioConnection *xcon);
-  virtual void mark_down(const entity_addr_t& a);
-  virtual void mark_down(Connection *con);
-  virtual void mark_down_all();
-  virtual void mark_down_on_empty(Connection *con);
-  virtual void mark_disposable(Connection *con);
-
-  void ds_dispatch(Message *m)
-    { dispatch_strategy->ds_dispatch(m); }
-
-  /**
-   * Tell the XioMessenger its full IP address.
-   *
-   * This is used by clients when connecting to other endpoints, and
-   * probably shouldn't be called by anybody else.
-   */
-  void learned_addr(const entity_addr_t& peer_addr_for_me);
-
-private:
-  int get_nconns_per_portal(uint64_t cflags);
-  int get_nportals(uint64_t cflags);
-
-protected:
-  virtual void ready()
-    { }
-};
-
-XioCommand* pool_alloc_xio_command(XioConnection *xcon);
-
-
-#endif /* XIO_MESSENGER_H */
diff --git a/src/msg/xio/XioMsg.cc b/src/msg/xio/XioMsg.cc
deleted file mode 100644
index 8c2d3d8ec06..00000000000
--- a/src/msg/xio/XioMsg.cc
+++ /dev/null
@@ -1,51 +0,0 @@
-// -*- mode:C++; tab-width:8; c-basic-offset:2; indent-tabs-mode:t -*-
-// vim: ts=8 sw=2 smarttab
-/*
- * Ceph - scalable distributed file system
- *
- * Copyright (C) 2004-2006 Sage Weil <sage@newdream.net>
- * Portions Copyright (C) 2013 CohortFS, LLC
- *
- * This is free software; you can redistribute it and/or
- * modify it under the terms of the GNU Lesser General Public
- * License version 2.1, as published by the Free Software
- * Foundation.  See file COPYING.
- *
- */
-
-#include "XioMessenger.h"
-#include "XioConnection.h"
-#include "XioMsg.h"
-
-
-int XioDispatchHook::release_msgs()
-{
-  XioCompletion *xcmp;
-  int r = msg_seq.size();
-  cl_flag = true;
-
-  /* queue for release */
-  xcmp = static_cast<XioCompletion *>(rsp_pool.alloc(sizeof(XioCompletion)));
-  new (xcmp) XioCompletion(xcon, this);
-  xcmp->trace = m->trace;
-
-  /* merge with portal traffic */
-  xcon->portal->enqueue(xcon, xcmp);
-
-  assert(r);
-  return r;
-}
-
-/*static*/ size_t XioMsgHdr::get_max_encoded_length() {
-  ceph_msg_header _ceph_msg_header;
-  ceph_msg_footer _ceph_msg_footer;
-  XioMsgHdr hdr (_ceph_msg_header, _ceph_msg_footer, 0 /* features */);
-  const std::list<buffer::ptr>& hdr_buffers = hdr.get_bl().buffers();
-  assert(hdr_buffers.size() == 1); /* accelio header is small without scatter gather */
-  return hdr_buffers.begin()->length();
-}
-
-void XioMsg::print_debug(CephContext *cct, const char *tag) const {
-  print_xio_msg_hdr(cct, tag, hdr, get_xio_msg());
-  print_ceph_msg(cct, tag, m);
-}
diff --git a/src/msg/xio/XioMsg.h b/src/msg/xio/XioMsg.h
deleted file mode 100644
index f85950ebc40..00000000000
--- a/src/msg/xio/XioMsg.h
+++ /dev/null
@@ -1,442 +0,0 @@
-// -*- mode:C++; tab-width:8; c-basic-offset:2; indent-tabs-mode:t -*-
-// vim: ts=8 sw=2 smarttab
-/*
- * Ceph - scalable distributed file system
- *
- * Copyright (C) 2004-2006 Sage Weil <sage@newdream.net>
- * Portions Copyright (C) 2013 CohortFS, LLC
- *
- * This is free software; you can redistribute it and/or
- * modify it under the terms of the GNU Lesser General Public
- * License version 2.1, as published by the Free Software
- * Foundation.  See file COPYING.
- *
- */
-
-#ifndef XIO_MSG_H
-#define XIO_MSG_H
-
-#include <boost/intrusive/list.hpp>
-#include "msg/SimplePolicyMessenger.h"
-extern "C" {
-#include "libxio.h"
-}
-#include "XioConnection.h"
-#include "XioSubmit.h"
-#include "msg/msg_types.h"
-#include "XioPool.h"
-
-namespace bi = boost::intrusive;
-
-class XioMessenger;
-
-class XioMsgCnt
-{
-public:
-  __le32 msg_cnt;
-  buffer::list bl;
-public:
-  explicit XioMsgCnt(buffer::ptr p)
-    {
-      bl.append(p);
-      buffer::list::iterator bl_iter = bl.begin();
-      ::decode(msg_cnt, bl_iter);
-    }
-};
-
-class XioMsgHdr
-{
-public:
-  char tag;
-  __le32 msg_cnt;
-  __le32 peer_type;
-  entity_addr_t addr; /* XXX hack! */
-  ceph_msg_header* hdr;
-  ceph_msg_footer* ftr;
-  uint64_t features;
-  buffer::list bl;
-public:
-  XioMsgHdr(ceph_msg_header& _hdr, ceph_msg_footer& _ftr, uint64_t _features)
-    : tag(CEPH_MSGR_TAG_MSG), msg_cnt(0), hdr(&_hdr), ftr(&_ftr),
-      features(_features)
-    { }
-
-  XioMsgHdr(ceph_msg_header& _hdr, ceph_msg_footer &_ftr, buffer::ptr p)
-    : hdr(&_hdr), ftr(&_ftr)
-    {
-      bl.append(p);
-      buffer::list::iterator bl_iter = bl.begin();
-      decode(bl_iter);
-    }
-
-  static size_t get_max_encoded_length();
-
-  const buffer::list& get_bl() { encode(bl); return bl; };
-
-  inline void encode_hdr(ceph::buffer::list& bl) const {
-    ::encode(tag, bl);
-    ::encode(msg_cnt, bl);
-    ::encode(peer_type, bl);
-    ::encode(addr, bl, features);
-    ::encode(hdr->seq, bl);
-    ::encode(hdr->tid, bl);
-    ::encode(hdr->type, bl);
-    ::encode(hdr->priority, bl);
-    ::encode(hdr->version, bl);
-    ::encode(hdr->front_len, bl);
-    ::encode(hdr->middle_len, bl);
-    ::encode(hdr->data_len, bl);
-    ::encode(hdr->data_off, bl);
-    ::encode(hdr->src.type, bl);
-    ::encode(hdr->src.num, bl);
-    ::encode(hdr->compat_version, bl);
-    ::encode(hdr->crc, bl);
-  }
-
-  inline void encode_ftr(buffer::list& bl) const {
-    ::encode(ftr->front_crc, bl);
-    ::encode(ftr->middle_crc, bl);
-    ::encode(ftr->data_crc, bl);
-    ::encode(ftr->sig, bl);
-    ::encode(ftr->flags, bl);
-  }
-
-  inline void encode(buffer::list& bl) const {
-    encode_hdr(bl);
-    encode_ftr(bl);
-  }
-
-  inline void decode_hdr(buffer::list::iterator& bl) {
-    ::decode(tag, bl);
-    ::decode(msg_cnt, bl);
-    ::decode(peer_type, bl);
-    ::decode(addr, bl);
-    ::decode(hdr->seq, bl);
-    ::decode(hdr->tid, bl);
-    ::decode(hdr->type, bl);
-    ::decode(hdr->priority, bl);
-    ::decode(hdr->version, bl);
-    ::decode(hdr->front_len, bl);
-    ::decode(hdr->middle_len, bl);
-    ::decode(hdr->data_len, bl);
-    ::decode(hdr->data_off, bl);
-    ::decode(hdr->src.type, bl);
-    ::decode(hdr->src.num, bl);
-    ::decode(hdr->compat_version, bl);
-    ::decode(hdr->crc, bl);
-  }
-
-  inline void decode_ftr(buffer::list::iterator& bl) {
-    ::decode(ftr->front_crc, bl);
-    ::decode(ftr->middle_crc, bl);
-    ::decode(ftr->data_crc, bl);
-    ::decode(ftr->sig, bl);
-    ::decode(ftr->flags, bl);
-  }
-
-  inline void decode(buffer::list::iterator& bl) {
-    decode_hdr(bl);
-    decode_ftr(bl);
-  }
-
-  virtual ~XioMsgHdr()
-    {}
-};
-
-WRITE_CLASS_ENCODER(XioMsgHdr);
-
-extern struct xio_mempool *xio_msgr_noreg_mpool;
-
-#define XIO_MSGR_IOVLEN 16
-
-struct xio_msg_ex
-{
-  struct xio_msg msg;
-  struct xio_iovec_ex iovs[XIO_MSGR_IOVLEN];
-
-  explicit xio_msg_ex(void* user_context) {
-    // go in structure order
-    msg.in.header.iov_len = 0;
-    msg.in.header.iov_base = NULL;  /* XXX Accelio requires this currently */
-
-    msg.in.sgl_type = XIO_SGL_TYPE_IOV_PTR;
-    msg.in.pdata_iov.max_nents = XIO_MSGR_IOVLEN;
-    msg.in.pdata_iov.nents = 0;
-    msg.in.pdata_iov.sglist = iovs;
-
-    // minimal zero "out" side
-    msg.out.header.iov_len = 0;
-    msg.out.header.iov_base = NULL;  /* XXX Accelio requires this currently,
-				      * against spec */
-    // out (some members adjusted later)
-    msg.out.sgl_type = XIO_SGL_TYPE_IOV_PTR;
-    msg.out.pdata_iov.max_nents = XIO_MSGR_IOVLEN;
-    msg.out.pdata_iov.nents = 0;
-    msg.out.pdata_iov.sglist = iovs;
-
-    // minimal initialize an "out" msg
-    msg.request = NULL;
-    msg.type = XIO_MSG_TYPE_ONE_WAY;
-    // for now, we DO NEED receipts for every msg
-    msg.flags = 0;
-    msg.user_context = user_context;
-    msg.next = NULL;
-    // minimal zero "in" side
-  }
-};
-
-class XioSend : public XioSubmit
-{
-public:
-  virtual void print_debug(CephContext *cct, const char *tag) const {};
-  const struct xio_msg * get_xio_msg() const {return &req_0.msg;}
-  struct xio_msg * get_xio_msg() {return &req_0.msg;}
-  virtual size_t get_msg_count() const {return 1;}
-
-  XioSend(XioConnection *_xcon, struct xio_reg_mem& _mp, int _ex_cnt=0) :
-    XioSubmit(XioSubmit::OUTGOING_MSG, _xcon),
-    req_0(this), mp_this(_mp), nrefs(_ex_cnt+1)
-  {
-    xpool_inc_msgcnt();
-    xcon->get();
-  }
-
-  XioSend* get() { nrefs++; return this; };
-
-  void put(int n) {
-    int refs = nrefs -= n;
-    if (refs == 0) {
-      struct xio_reg_mem *mp = &this->mp_this;
-      this->~XioSend();
-      xpool_free(sizeof(XioSend), mp);
-    }
-  }
-
-  void put() {
-    put(1);
-  }
-
-  void put_msg_refs() {
-    put(get_msg_count());
-  }
-
-  virtual ~XioSend() {
-    xpool_dec_msgcnt();
-    xcon->put();
-  }
-
-private:
-  xio_msg_ex req_0;
-  struct xio_reg_mem mp_this;
-  std::atomic<unsigned> nrefs = { 0 };
-};
-
-class XioCommand : public XioSend
-{
-public:
-  XioCommand(XioConnection *_xcon, struct xio_reg_mem& _mp):XioSend(_xcon, _mp) {
-  }
-
-  buffer::list& get_bl_ref() { return bl; };
-
-private:
-  buffer::list bl;
-};
-
-struct XioMsg : public XioSend
-{
-public:
-  Message* m;
-  XioMsgHdr hdr;
-  xio_msg_ex* req_arr;
-
-public:
-  XioMsg(Message *_m, XioConnection *_xcon, struct xio_reg_mem& _mp,
-	 int _ex_cnt, uint64_t _features) :
-    XioSend(_xcon, _mp, _ex_cnt),
-    m(_m), hdr(m->get_header(), m->get_footer(), _features),
-    req_arr(NULL)
-    {
-      const entity_inst_t &inst = xcon->get_messenger()->get_myinst();
-      hdr.peer_type = inst.name.type();
-      hdr.addr = xcon->get_messenger()->get_myaddr();
-      hdr.hdr->src.type = inst.name.type();
-      hdr.hdr->src.num = inst.name.num();
-      hdr.msg_cnt = _ex_cnt+1;
-
-      if (unlikely(_ex_cnt > 0)) {
-	alloc_trailers(_ex_cnt);
-      }
-    }
-
-  void print_debug(CephContext *cct, const char *tag) const override;
-  size_t get_msg_count() const override {
-    return hdr.msg_cnt;
-  }
-
-  void alloc_trailers(int cnt) {
-    req_arr = static_cast<xio_msg_ex*>(malloc(cnt * sizeof(xio_msg_ex)));
-    for (int ix = 0; ix < cnt; ++ix) {
-      xio_msg_ex* xreq = &(req_arr[ix]);
-      new (xreq) xio_msg_ex(this);
-    }
-  }
-
-  Message *get_message() { return m; }
-
-  ~XioMsg()
-    {
-      if (unlikely(!!req_arr)) {
-	for (unsigned int ix = 0; ix < get_msg_count()-1; ++ix) {
-	  xio_msg_ex* xreq = &(req_arr[ix]);
-	  xreq->~xio_msg_ex();
-	}
-	free(req_arr);
-      }
-
-      /* testing only! server's ready, resubmit request (not reached on
-       * PASSIVE/server side) */
-      if (unlikely(m->get_magic() & MSG_MAGIC_REDUPE)) {
-	if (likely(xcon->is_connected())) {
-	  xcon->send_message(m);
-	} else {
-	  /* dispose it */
-	  m->put();
-	}
-      } else {
-	  /* the normal case: done with message */
-	  m->put();
-      }
-    }
-};
-
-class XioDispatchHook : public Message::CompletionHook
-{
-private:
-  XioConnection *xcon;
-  XioInSeq msg_seq;
-  XioPool rsp_pool;
-  std::atomic<unsigned> nrefs { 1 };
-  bool cl_flag;
-  friend class XioConnection;
-  friend class XioMessenger;
-public:
-  struct xio_reg_mem mp_this;
-
-  XioDispatchHook(XioConnection *_xcon, Message *_m, XioInSeq& _msg_seq,
-		    struct xio_reg_mem& _mp) :
-    CompletionHook(_m),
-    xcon(_xcon->get()),
-    msg_seq(_msg_seq),
-    rsp_pool(xio_msgr_noreg_mpool),
-    cl_flag(false),
-    mp_this(_mp)
-    {
-      ++xcon->n_reqs; // atomicity by portal thread
-      xpool_inc_hookcnt();
-    }
-
-  virtual void finish(int r) {
-    this->put();
-  }
-
-  virtual void complete(int r) {
-    finish(r);
-  }
-
-  int release_msgs();
-
-  XioDispatchHook* get() {
-    nrefs++; return this;
-  }
-
-  void put(int n = 1) {
-    int refs = nrefs -= n;
-    if (refs == 0) {
-      /* in Marcus' new system, refs reaches 0 twice:  once in
-       * Message lifecycle, and again after xio_release_msg.
-       */
-      if (!cl_flag && release_msgs())
-	return;
-      struct xio_reg_mem *mp = &this->mp_this;
-      this->~XioDispatchHook();
-      xpool_free(sizeof(XioDispatchHook), mp);
-    }
-  }
-
-  XioInSeq& get_seq() { return msg_seq; }
-
-  XioPool& get_pool() { return rsp_pool; }
-
-  void on_err_finalize(XioConnection *xcon) {
-    /* can't decode message; even with one-way must free
-     * xio_msg structures, and then xiopool
-     */
-    this->finish(-1);
-  }
-
-  ~XioDispatchHook() {
-    --xcon->n_reqs; // atomicity by portal thread
-    xpool_dec_hookcnt();
-    xcon->put();
-  }
-};
-
-/* A sender-side CompletionHook that relies on the on_msg_delivered
- * to complete a pending mark down. */
-class XioMarkDownHook : public Message::CompletionHook
-{
-private:
-  XioConnection* xcon;
-
-public:
-  struct xio_reg_mem mp_this;
-
-  XioMarkDownHook(
-    XioConnection* _xcon, Message *_m, struct xio_reg_mem& _mp) :
-    CompletionHook(_m), xcon(_xcon->get()), mp_this(_mp)
-    { }
-
-  virtual void claim(int r) {}
-
-  virtual void finish(int r) {
-    xcon->put();
-    struct xio_reg_mem *mp = &this->mp_this;
-    this->~XioMarkDownHook();
-    xio_mempool_free(mp);
-  }
-
-  virtual void complete(int r) {
-    xcon->_mark_down(XioConnection::CState::OP_FLAG_NONE);
-    finish(r);
-  }
-};
-
-struct XioCompletion : public XioSubmit
-{
-  XioDispatchHook *xhook;
-public:
-  XioCompletion(XioConnection *_xcon, XioDispatchHook *_xhook)
-    : XioSubmit(XioSubmit::INCOMING_MSG_RELEASE, _xcon /* not xcon! */),
-      xhook(_xhook->get()) {
-      // submit queue ref
-      xcon->get();
-    };
-
-  struct xio_msg* dequeue() {
-    return xhook->get_seq().dequeue();
-  }
-
-  XioDispatchHook* get_xhook() { return xhook; }
-
-  void finalize() {
-    xcon->put();
-    xhook->put();
-  }
-};
-
-void print_xio_msg_hdr(CephContext *cct, const char *tag,
-		       const XioMsgHdr &hdr, const struct xio_msg *msg);
-void print_ceph_msg(CephContext *cct, const char *tag, Message *m);
-
-#endif /* XIO_MSG_H */
diff --git a/src/msg/xio/XioPool.cc b/src/msg/xio/XioPool.cc
deleted file mode 100644
index 5f0d77a27b6..00000000000
--- a/src/msg/xio/XioPool.cc
+++ /dev/null
@@ -1,41 +0,0 @@
-// -*- mode:C++; tab-width:8; c-basic-offset:2; indent-tabs-mode:t -*-
-// vim: ts=8 sw=2 smarttab
-/*
- * Ceph - scalable distributed file system
- *
- * Copyright (C) 2014 CohortFS, LLC
- *
- * This is free software; you can redistribute it and/or
- * modify it under the terms of the GNU Lesser General Public
- * License version 2.1, as published by the Free Software
- * Foundation.  See file COPYING.
- *
- */
-
-#include <iostream>
-#include "XioPool.h"
-
-XioPoolStats xp_stats;
-
-bool XioPool::trace_mempool = 0;
-bool XioPool::trace_msgcnt = 0;
-
-void XioPoolStats::dump(const char* tag, uint64_t serial)
-{
-  std::cout
-    << tag << " #" << serial << ": "
-    << "pool objs: "
-    << "64: " << ctr_set[SLAB_64].read() << " "
-    << "256: " << ctr_set[SLAB_256].read() << " "
-    << "1024: " << ctr_set[SLAB_1024].read() << " "
-    << "page: " << ctr_set[SLAB_PAGE].read() << " "
-    << "max: " << ctr_set[SLAB_MAX].read() << " "
-    << "overflow: " << ctr_set[SLAB_OVERFLOW].read() << " "
-    << std::endl;
-  std::cout
-    << tag << " #" << serial << ": "
-    << " msg objs: "
-    << "in: " << hook_cnt.read() << " "
-    << "out: " << msg_cnt.read() << " "
-    << std::endl;
-}
diff --git a/src/msg/xio/XioPool.h b/src/msg/xio/XioPool.h
deleted file mode 100644
index 6084ce85682..00000000000
--- a/src/msg/xio/XioPool.h
+++ /dev/null
@@ -1,218 +0,0 @@
-// -*- mode:C++; tab-width:8; c-basic-offset:2; indent-tabs-mode:t -*-
-// vim: ts=8 sw=2 smarttab
-/*
- * Ceph - scalable distributed file system
- *
- * Copyright (C) 2014 CohortFS, LLC
- *
- * This is free software; you can redistribute it and/or
- * modify it under the terms of the GNU Lesser General Public
- * License version 2.1, as published by the Free Software
- * Foundation.  See file COPYING.
- *
- */
-#ifndef XIO_POOL_H
-#define XIO_POOL_H
-
-#include <atomic>
-#include <vector>
-#include <cstdlib>
-#include <cstring>
-#include <cstdint>
-
-extern "C" {
-#include "libxio.h"
-}
-
-#include "common/likely.h"
-
-static inline int xpool_alloc(struct xio_mempool *pool, uint64_t size,
-			      struct xio_reg_mem* mp);
-static inline void xpool_free(uint64_t size, struct xio_reg_mem* mp);
-
-class XioPool
-{
-private:
-  struct xio_mempool *handle;
-
-public:
-  static bool trace_mempool;
-  static bool trace_msgcnt;
-  static const int MB = 8;
-
-  struct xio_piece {
-    struct xio_reg_mem mp[1];
-    struct xio_piece *next;
-    int s;
-    char payload[MB];
-  } *first;
-
-  explicit XioPool(struct xio_mempool *_handle) :
-    handle(_handle), first(0)
-    {
-    }
-  ~XioPool()
-    {
-      struct xio_piece *p;
-      while ((p = first)) {
-	first = p->next;
-	if (unlikely(trace_mempool)) {
-	  memset(p->payload, 0xcf, p->s); // guard bytes
-	}
-	xpool_free(sizeof(struct xio_piece)+(p->s)-MB, p->mp);
-      }
-    }
-  void *alloc(size_t _s)
-    {
-	void *r;
-	struct xio_reg_mem mp[1];
-	struct xio_piece *x;
-	int e = xpool_alloc(handle, (sizeof(struct xio_piece)-MB) + _s, mp);
-	if (e) {
-	  r = 0;
-	} else {
-	  x = reinterpret_cast<struct xio_piece *>(mp->addr);
-	  *x->mp = *mp;
-	  x->next = first;
-	  x->s = _s;
-	  first = x;
-	  r = x->payload;
-	}
-	return r;
-    }
-};
-
-class XioPoolStats {
-private:
-  enum pool_sizes {
-    SLAB_64 = 0,
-    SLAB_256,
-    SLAB_1024,
-    SLAB_PAGE,
-    SLAB_MAX,
-    SLAB_OVERFLOW,
-    NUM_SLABS,
-  };
-
-  std::atomic<unsigned> ctr_set[NUM_SLABS] = {};
-  std::atomic<unsigned> msg_cnt = { 0 };  // send msgs
-  std::atomic<unsigned> hook_cnt = { 0 }; // recv msgs
-
-public:
-  void dump(const char* tag, uint64_t serial);
-
-  void inc(uint64_t size) {
-    if (size <= 64) {
-      (ctr_set[SLAB_64])++;
-      return;
-    }
-    if (size <= 256) {
-      (ctr_set[SLAB_256])++;
-      return;
-    }
-    if (size <= 1024) {
-      (ctr_set[SLAB_1024])++;
-      return;
-    }
-    if (size <= 8192) {
-      (ctr_set[SLAB_PAGE])++;
-      return;
-    }
-    (ctr_set[SLAB_MAX])++;
-  }
-
-  void dec(uint64_t size) {
-    if (size <= 64) {
-      (ctr_set[SLAB_64])--;
-      return;
-    }
-    if (size <= 256) {
-      (ctr_set[SLAB_256])--;
-      return;
-    }
-    if (size <= 1024) {
-      (ctr_set[SLAB_1024])--;
-      return;
-    }
-    if (size <= 8192) {
-      (ctr_set[SLAB_PAGE])--;
-      return;
-    }
-    (ctr_set[SLAB_MAX])--;
-  }
-
-  void inc_overflow() { ctr_set[SLAB_OVERFLOW]++; }
-  void dec_overflow() { ctr_set[SLAB_OVERFLOW]--; }
-
-  void inc_msgcnt() {
-    if (unlikely(XioPool::trace_msgcnt)) {
-      msg_cnt++;
-    }
-  }
-
-  void dec_msgcnt() {
-    if (unlikely(XioPool::trace_msgcnt)) {
-      msg_cnt--;
-    }
-  }
-
-  void inc_hookcnt() {
-    if (unlikely(XioPool::trace_msgcnt)) {
-      hook_cnt++;
-    }
-  }
-
-  void dec_hookcnt() {
-    if (unlikely(XioPool::trace_msgcnt)) {
-      hook_cnt--;
-    }
-  }
-};
-
-extern XioPoolStats xp_stats;
-
-static inline int xpool_alloc(struct xio_mempool *pool, uint64_t size,
-			      struct xio_reg_mem* mp)
-{
-  // try to allocate from the xio pool
-  int r = xio_mempool_alloc(pool, size, mp);
-  if (r == 0) {
-    if (unlikely(XioPool::trace_mempool))
-      xp_stats += size;
-    return 0;
-  }
-  // fall back to malloc on errors
-  mp->addr = malloc(size);
-  assert(mp->addr);
-  mp->length = 0;
-  if (unlikely(XioPool::trace_mempool))
-    xp_stats.inc_overflow();
-  return 0;
-}
-
-static inline void xpool_free(uint64_t size, struct xio_reg_mem* mp)
-{
-  if (mp->length) {
-    if (unlikely(XioPool::trace_mempool))
-      xp_stats -= size;
-    xio_mempool_free(mp);
-  } else { // from malloc
-    if (unlikely(XioPool::trace_mempool))
-      xp_stats.dec_overflow();
-    free(mp->addr);
-  }
-}
-
-#define xpool_inc_msgcnt() \
-  do { xp_stats.inc_msgcnt(); } while (0)
-
-#define xpool_dec_msgcnt() \
-  do { xp_stats.dec_msgcnt(); } while (0)
-
-#define xpool_inc_hookcnt() \
-  do { xp_stats.inc_hookcnt(); } while (0)
-
-#define xpool_dec_hookcnt() \
-  do { xp_stats.dec_hookcnt(); } while (0)
-
-#endif /* XIO_POOL_H */
diff --git a/src/msg/xio/XioPortal.cc b/src/msg/xio/XioPortal.cc
deleted file mode 100644
index e2379fb3312..00000000000
--- a/src/msg/xio/XioPortal.cc
+++ /dev/null
@@ -1,98 +0,0 @@
-// -*- mode:C++; tab-width:8; c-basic-offset:2; indent-tabs-mode:t -*-
-// vim: ts=8 sw=2 smarttab
-/*
- * Ceph - scalable distributed file system
- *
- * Copyright (C) 2013 CohortFS, LLC
- *
- * This is free software; you can redistribute it and/or
- * modify it under the terms of the GNU Lesser General Public
- * License version 2.1, as published by the Free Software
- * Foundation.  See file COPYING.
- *
- */
-
-#include "XioPortal.h"
-#include <stdio.h>
-
-#define dout_subsys ceph_subsys_xio
-
-int XioPortal::bind(struct xio_session_ops *ops, const string &base_uri,
-		    uint16_t port, uint16_t *assigned_port)
-{
-  // format uri
-  char buf[40];
-  xio_uri = base_uri;
-  xio_uri += ":";
-  sprintf(buf, "%d", port);
-  xio_uri += buf;
-
-  uint16_t assigned;
-  server = xio_bind(ctx, ops, xio_uri.c_str(), &assigned, 0, msgr);
-  if (server == NULL)
-    return xio_errno();
-
-  // update uri if port changed
-  if (port != assigned) {
-    xio_uri = base_uri;
-    xio_uri += ":";
-    sprintf(buf, "%d", assigned);
-    xio_uri += buf;
-  }
-
-  portal_id = const_cast<char*>(xio_uri.c_str());
-  if (assigned_port)
-    *assigned_port = assigned;
-  ldout(msgr->cct,20) << "xio_bind: portal " << xio_uri
-    << " returned server " << server << dendl;
-  return 0;
-}
-
-int XioPortals::bind(struct xio_session_ops *ops, const string& base_uri,
-		     uint16_t port, uint16_t *port0)
-{
-  /* a server needs at least 1 portal */
-  if (n < 1)
-    return EINVAL;
-  Messenger *msgr = portals[0]->msgr;
-  portals.resize(n);
-
-  uint16_t port_min = msgr->cct->_conf->ms_bind_port_min;
-  const uint16_t port_max = msgr->cct->_conf->ms_bind_port_max;
-
-  /* bind the portals */
-  for (size_t i = 0; i < portals.size(); i++) {
-    uint16_t result_port;
-    if (port != 0) {
-      // bind directly to the given port
-      int r = portals[i]->bind(ops, base_uri, port, &result_port);
-      if (r != 0)
-        return -r;
-    } else {
-      int r = EADDRINUSE;
-      // try ports within the configured range
-      for (; port_min <= port_max; port_min++) {
-        r = portals[i]->bind(ops, base_uri, port_min, &result_port);
-        if (r == 0) {
-          port_min++;
-          break;
-        }
-      }
-      if (r != 0) {
-        lderr(msgr->cct) << "portal.bind unable to bind to " << base_uri
-            << " on any port in range " << msgr->cct->_conf->ms_bind_port_min
-            << "-" << port_max << ": " << xio_strerror(r) << dendl;
-        return -r;
-      }
-    }
-
-    ldout(msgr->cct,5) << "xp::bind: portal " << i << " bind OK: "
-      << portals[i]->xio_uri << dendl;
-
-    if (i == 0 && port0 != NULL)
-      *port0 = result_port;
-    port = 0; // use port 0 for all subsequent portals
-  }
-
-  return 0;
-}
diff --git a/src/msg/xio/XioPortal.h b/src/msg/xio/XioPortal.h
deleted file mode 100644
index b3f21010095..00000000000
--- a/src/msg/xio/XioPortal.h
+++ /dev/null
@@ -1,465 +0,0 @@
-// -*- mode:C++; tab-width:8; c-basic-offset:2; indent-tabs-mode:t -*-
-// vim: ts=8 sw=2 smarttab
-/*
- * Ceph - scalable distributed file system
- *
- * Copyright (C) 2004-2006 Sage Weil <sage@newdream.net>
- * Portions Copyright (C) 2013 CohortFS, LLC
- *s
- * This is free software; you can redistribute it and/or
- * modify it under the terms of the GNU Lesser General Public
- * License version 2.1, as published by the Free Software
- * Foundation.  See file COPYING.
- *
- */
-
-#ifndef XIO_PORTAL_H
-#define XIO_PORTAL_H
-
-#include <string>
-
-extern "C" {
-#include "libxio.h"
-}
-#include "XioInSeq.h"
-#include <boost/lexical_cast.hpp>
-#include "msg/SimplePolicyMessenger.h"
-#include "XioConnection.h"
-#include "XioMsg.h"
-
-#include "include/assert.h"
-#include "common/dout.h"
-
-#ifndef CACHE_LINE_SIZE
-#define CACHE_LINE_SIZE 64 /* XXX arch-specific define */
-#endif
-#define CACHE_PAD(_n) char __pad ## _n [CACHE_LINE_SIZE]
-
-class XioPortal : public Thread
-{
-private:
-
-  struct SubmitQueue
-  {
-    const static int nlanes = 7;
-
-    struct Lane
-    {
-      uint32_t size;
-      XioSubmit::Queue q;
-      pthread_spinlock_t sp;
-      CACHE_PAD(0);
-    };
-
-    Lane qlane[nlanes];
-
-    int ix; /* atomicity by portal thread */
-
-    SubmitQueue() : ix(0)
-      {
-	int ix;
-	Lane* lane;
-
-	for (ix = 0; ix < nlanes; ++ix) {
-	  lane = &qlane[ix];
-	  pthread_spin_init(&lane->sp, PTHREAD_PROCESS_PRIVATE);
-	  lane->size = 0;
-	}
-      }
-
-    inline Lane* get_lane(XioConnection *xcon)
-      {
-	return &qlane[(((uint64_t) xcon) / 16) % nlanes];
-      }
-
-    void enq(XioConnection *xcon, XioSubmit* xs)
-      {
-	Lane* lane = get_lane(xcon);
-	pthread_spin_lock(&lane->sp);
-	lane->q.push_back(*xs);
-	++(lane->size);
-	pthread_spin_unlock(&lane->sp);
-      }
-
-    void enq(XioConnection *xcon, XioSubmit::Queue& requeue_q)
-      {
-	int size = requeue_q.size();
-	Lane* lane = get_lane(xcon);
-	pthread_spin_lock(&lane->sp);
-	XioSubmit::Queue::const_iterator i1 = lane->q.end();
-	lane->q.splice(i1, requeue_q);
-	lane->size += size;
-	pthread_spin_unlock(&lane->sp);
-      }
-
-    void deq(XioSubmit::Queue& send_q)
-      {
-	Lane* lane;
-	int cnt;
-	for (cnt = 0; cnt < nlanes; ++cnt, ++ix, ix = ix % nlanes) {
-	  lane = &qlane[ix];
-	  pthread_spin_lock(&lane->sp);
-	  if (lane->size > 0) {
-	    XioSubmit::Queue::const_iterator i1 = send_q.end();
-	    send_q.splice(i1, lane->q);
-	    lane->size = 0;
-	    ++ix, ix = ix % nlanes;
-	    pthread_spin_unlock(&lane->sp);
-	    break;
-	  }
-	  pthread_spin_unlock(&lane->sp);
-	}
-      }
-
-  }; /* SubmitQueue */
-
-  Messenger *msgr;
-  struct xio_context *ctx;
-  struct xio_server *server;
-  SubmitQueue submit_q;
-  pthread_spinlock_t sp;
-  void *ev_loop;
-  string xio_uri;
-  char *portal_id;
-  bool _shutdown;
-  bool drained;
-  uint32_t magic;
-  uint32_t special_handling;
-
-  friend class XioPortals;
-  friend class XioMessenger;
-
-public:
-  explicit XioPortal(Messenger *_msgr, int max_conns) :
-    msgr(_msgr), ctx(NULL), server(NULL), submit_q(), xio_uri(""),
-    portal_id(NULL), _shutdown(false), drained(false),
-    magic(0),
-    special_handling(0)
-  {
-    pthread_spin_init(&sp, PTHREAD_PROCESS_PRIVATE);
-
-    struct xio_context_params ctx_params;
-    memset(&ctx_params, 0, sizeof(ctx_params));
-    ctx_params.user_context = this;
-    /*
-     * hint to Accelio the total number of connections that will share
-     * this context's resources: internal primary task pool...
-     */
-    ctx_params.max_conns_per_ctx = max_conns;
-
-    /* a portal is an xio_context and event loop */
-    ctx = xio_context_create(&ctx_params, 0 /* poll timeout */, -1 /* cpu hint */);
-    assert(ctx && "Whoops, failed to create portal/ctx");
-  }
-
-  int bind(struct xio_session_ops *ops, const string &base_uri,
-	   uint16_t port, uint16_t *assigned_port);
-
-  inline void release_xio_msg(XioCompletion* xcmp) {
-    struct xio_msg *msg = xcmp->dequeue();
-    struct xio_msg *next_msg = NULL;
-    int code;
-    if (unlikely(!xcmp->xcon->conn)) {
-      // NOTE: msg is not safe to dereference if the connection was torn down
-      xcmp->xcon->msg_release_fail(msg, ENOTCONN);
-    }
-    else while (msg) {
-      next_msg = static_cast<struct xio_msg *>(msg->user_context);
-      code = xio_release_msg(msg);
-      if (unlikely(code)) /* very unlikely, so log it */
-	xcmp->xcon->msg_release_fail(msg, code);
-      msg = next_msg;
-    }
-    xcmp->trace.event("xio_release_msg");
-    xcmp->finalize(); /* unconditional finalize */
-  }
-
-  void enqueue(XioConnection *xcon, XioSubmit *xs)
-    {
-      if (! _shutdown) {
-	submit_q.enq(xcon, xs);
-	xio_context_stop_loop(ctx);
-	return;
-      }
-
-      /* dispose xs */
-      switch(xs->type) {
-      case XioSubmit::OUTGOING_MSG: /* it was an outgoing 1-way */
-      {
-	XioSend* xsend = static_cast<XioSend*>(xs);
-	xs->xcon->msg_send_fail(xsend, -EINVAL);
-      }
-	break;
-      default:
-	/* INCOMING_MSG_RELEASE */
-	release_xio_msg(static_cast<XioCompletion*>(xs));
-      break;
-      };
-    }
-
-  void requeue(XioConnection* xcon, XioSubmit::Queue& send_q) {
-    submit_q.enq(xcon, send_q);
-  }
-
-  void requeue_all_xcon(XioConnection* xcon,
-			XioSubmit::Queue::iterator& q_iter,
-			XioSubmit::Queue& send_q) {
-    // XXX gather all already-dequeued outgoing messages for xcon
-    // and push them in FIFO order to front of the input queue,
-    // and mark the connection as flow-controlled
-    XioSubmit::Queue requeue_q;
-
-    while (q_iter != send_q.end()) {
-      XioSubmit *xs = &(*q_iter);
-      // skip retires and anything for other connections
-      if (xs->xcon != xcon) {
-	q_iter++;
-	continue;
-      }
-      q_iter = send_q.erase(q_iter);
-      requeue_q.push_back(*xs);
-    }
-    pthread_spin_lock(&xcon->sp);
-    XioSubmit::Queue::const_iterator i1 = xcon->outgoing.requeue.begin();
-    xcon->outgoing.requeue.splice(i1, requeue_q);
-    xcon->cstate.state_flow_controlled(XioConnection::CState::OP_FLAG_LOCKED);
-    pthread_spin_unlock(&xcon->sp);
-  }
-
-  void *entry()
-    {
-      int size, code = 0;
-      uint32_t xio_qdepth_high;
-      XioSubmit::Queue send_q;
-      XioSubmit::Queue::iterator q_iter;
-      struct xio_msg *msg = NULL;
-      XioConnection *xcon;
-      XioSubmit *xs;
-      XioSend *xsend;
-
-      do {
-	submit_q.deq(send_q);
-
-	/* shutdown() barrier */
-	pthread_spin_lock(&sp);
-
-      restart:
-	size = send_q.size();
-
-	if (_shutdown) {
-	  // XXX XioSend queues for flow-controlled connections may require
-	  // cleanup
-	  drained = true;
-	}
-
-	if (size > 0) {
-	  q_iter = send_q.begin();
-	  while (q_iter != send_q.end()) {
-	    xs = &(*q_iter);
-	    xcon = xs->xcon;
-
-	    switch (xs->type) {
-	    case XioSubmit::OUTGOING_MSG: /* it was an outgoing 1-way */
-	      xsend = static_cast<XioSend*>(xs);
-	      if (unlikely(!xcon->conn || !xcon->is_connected()))
-		code = ENOTCONN;
-	      else {
-		/* XXX guard Accelio send queue (should be safe to rely
-		 * on Accelio's check on below, but this assures that
-		 * all chained xio_msg are accounted) */
-		xio_qdepth_high = xcon->xio_qdepth_high_mark();
-		if (unlikely((xcon->send_ctr + xsend->get_msg_count()) >
-			     xio_qdepth_high)) {
-		  requeue_all_xcon(xcon, q_iter, send_q);
-		  goto restart;
-		}
-
-		xs->trace.event("xio_send_msg");
-		msg = xsend->get_xio_msg();
-		code = xio_send_msg(xcon->conn, msg);
-		/* header trace moved here to capture xio serial# */
-		if (ldlog_p1(msgr->cct, ceph_subsys_xio, 11)) {
-		  xsend->print_debug(msgr->cct, "xio_send_msg");
-		}
-		/* get the right Accelio's errno code */
-		if (unlikely(code)) {
-		  if ((code == -1) && (xio_errno() == -1)) {
-		    /* In case XIO does not have any credits to send,
-		     * it would still queue up the message(s) for transmission,
-		     * but would return -1 and errno would also be set to -1.
-		     * This needs to be treated as a success.
-		     */
-		    code = 0;
-		  }
-		  else {
-		    code = xio_errno();
-		  }
-		}
-	      } /* !ENOTCONN */
-	      if (unlikely(code)) {
-		switch (code) {
-		case XIO_E_TX_QUEUE_OVERFLOW:
-		{
-		  requeue_all_xcon(xcon, q_iter, send_q);
-		  goto restart;
-		}
-		  break;
-		default:
-		  q_iter = send_q.erase(q_iter);
-		  xcon->msg_send_fail(xsend, code);
-		  continue;
-		  break;
-		};
-	      } else {
-		xcon->send.set(msg->timestamp); // need atomic?
-		xcon->send_ctr += xsend->get_msg_count(); // only inc if cb promised
-	      }
-	      break;
-	    default:
-	      /* INCOMING_MSG_RELEASE */
-	      q_iter = send_q.erase(q_iter);
-	      release_xio_msg(static_cast<XioCompletion*>(xs));
-	      continue;
-	    } /* switch (xs->type) */
-	    q_iter = send_q.erase(q_iter);
-	  } /* while */
-	} /* size > 0 */
-
-	pthread_spin_unlock(&sp);
-	xio_context_run_loop(ctx, 300);
-
-      } while ((!_shutdown) || (!drained));
-
-      /* shutting down */
-      if (server) {
-	xio_unbind(server);
-      }
-      xio_context_destroy(ctx);
-      return NULL;
-    }
-
-  void shutdown()
-    {
-	pthread_spin_lock(&sp);
-	_shutdown = true;
-	pthread_spin_unlock(&sp);
-    }
-};
-
-class XioPortals
-{
-private:
-  vector<XioPortal*> portals;
-  char **p_vec;
-  int n;
-  int last_unused;
-
-public:
-  XioPortals(Messenger *msgr, int _n, int nconns) : p_vec(NULL), last_unused(0)
-  {
-    n = max(_n, 1);
-
-    portals.resize(n);
-    for (int i = 0; i < n; i++) {
-      if (!portals[i]) {
-        portals[i] = new XioPortal(msgr, nconns);
-        assert(portals[i] != nullptr);
-      }
-    }
-  }
-
-  vector<XioPortal*>& get() { return portals; }
-
-  const char **get_vec()
-  {
-    return (const char **) p_vec;
-  }
-
-  int get_portals_len()
-  {
-    return n;
-  }
-
-  int get_last_unused()
-  {
-    int pix = last_unused;
-    if (++last_unused >= get_portals_len())
-      last_unused = 0;
-    return pix;
-  }
-
-  XioPortal* get_next_portal()
-  {
-    int pix = get_last_unused();
-    return portals[pix];
-  }
-
-  int bind(struct xio_session_ops *ops, const string& base_uri,
-	   uint16_t port, uint16_t *port0);
-
-  int accept(struct xio_session *session,
-	     struct xio_new_session_req *req,
-	     void *cb_user_context)
-  {
-    const char **portals_vec = get_vec();
-    int pix = get_last_unused();
-
-    if (pix == 0) {
-      return xio_accept(session, NULL, 0, NULL, 0);
-    } else {
-      return xio_accept(session,
-			(const char **)&(portals_vec[pix]),
-			1, NULL, 0);
-    }
-  }
-
-  void start()
-  {
-    XioPortal *portal;
-    int p_ix, nportals = portals.size();
-
-    p_vec = new char*[nportals];
-    for (p_ix = 0; p_ix < nportals; ++p_ix) {
-      portal = portals[p_ix];
-      p_vec[p_ix] = (char*) /* portal->xio_uri.c_str() */
-			portal->portal_id;
-    }
-
-    for (p_ix = 0; p_ix < nportals; ++p_ix) {
-      string thread_name = "ms_xio_";
-      thread_name.append(std::to_string(p_ix));
-      portal = portals[p_ix];
-      portal->create(thread_name.c_str());
-    }
-  }
-
-  void shutdown()
-  {
-    int nportals = portals.size();
-    for (int p_ix = 0; p_ix < nportals; ++p_ix) {
-      XioPortal *portal = portals[p_ix];
-      portal->shutdown();
-    }
-  }
-
-  void join()
-  {
-    int nportals = portals.size();
-    for (int p_ix = 0; p_ix < nportals; ++p_ix) {
-      XioPortal *portal = portals[p_ix];
-      portal->join();
-    }
-  }
-
-  ~XioPortals()
-  {
-    int nportals = portals.size();
-    for (int ix = 0; ix < nportals; ++ix)
-      delete(portals[ix]);
-    portals.clear();
-    if (p_vec)
-      delete[] p_vec;
-  }
-};
-
-#endif /* XIO_PORTAL_H */
diff --git a/src/msg/xio/XioSubmit.h b/src/msg/xio/XioSubmit.h
deleted file mode 100644
index 9840ad4a449..00000000000
--- a/src/msg/xio/XioSubmit.h
+++ /dev/null
@@ -1,58 +0,0 @@
-// -*- mode:C++; tab-width:8; c-basic-offset:2; indent-tabs-mode:t -*-
-// vim: ts=8 sw=2 smarttab
-/*
- * Ceph - scalable distributed file system
- *
- * Copyright (C) 2004-2006 Sage Weil <sage@newdream.net>
- * Portions Copyright (C) 2013 CohortFS, LLC
- *
- * This is free software; you can redistribute it and/or
- * modify it under the terms of the GNU Lesser General Public
- * License version 2.1, as published by the Free Software
- * Foundation.  See file COPYING.
- *
- */
-
-#ifndef XIO_SUBMIT_H
-#define XIO_SUBMIT_H
-
-#include <boost/intrusive/list.hpp>
-#include "msg/SimplePolicyMessenger.h"
-extern "C" {
-#include "libxio.h"
-}
-#include "XioConnection.h"
-#include "msg/msg_types.h"
-#include "XioPool.h"
-
-namespace bi = boost::intrusive;
-
-class XioConnection;
-
-struct XioSubmit
-{
-public:
-  enum submit_type
-  {
-    OUTGOING_MSG,
-    INCOMING_MSG_RELEASE
-  };
-  enum submit_type type;
-  bi::list_member_hook<> submit_list;
-  XioConnection *xcon;
-  ZTracer::Trace trace;
-
-  XioSubmit(enum submit_type _type, XioConnection *_xcon) :
-    type(_type), xcon(_xcon)
-    {}
-
-  typedef bi::list< XioSubmit,
-		    bi::member_hook< XioSubmit,
-				     bi::list_member_hook<>,
-				     &XioSubmit::submit_list >
-		    > Queue;
-  virtual ~XioSubmit(){
-  }
-};
-
-#endif /* XIO_SUBMIT_H */
