https://github.com/ssbandjl/ceph/tree/v15.2.17
https://github.com/ssbandjl/ceph/tree/main

build:
close mrg,dashboard in build/CMakeCache.txt

基准测试, 文档: messenger.rst
https://blog.csdn.net/bandaoyu/article/details/114292690
使用命令patchelf   修改工具依赖的动态库位置。避免和项目正在使用的库冲突: https://blog.csdn.net/bandaoyu/article/details/113181179
doc/dev/messenger.rst
perf_msgr_client.cc -> ceph_perf_msgr_client
perf_msgr_server.cc -> ceph_perf_msgr_server

perf_msgr_client.cc -> main


编译rdma:
./do_cmake.sh -DCMAKE_INSTALL_PREFIX=/usr -DWITH_RDMA=ON


qa:
启动虚拟集群: https://docs.ceph.com/en/quincy/dev/dev_cluster_deployement/

docker:

git
git remote add upstream https://github.com/Foo/repos.git
git pull upstream v15.2.17
git remote remove upstream
git remote add upstream https://github.com/Foo/repos.git
git remote set-url upstream https://github.com/Foo/repos.git

git push origin ：refs/tags/3.0 这就是明确告诉服务器删除的tag的分支,删除branch分支
git push origin :refs/heads/3.0
git branch -D testtag
删除tag分支的方法：
git tag -d v15.2.17
git push origin v15.2.17
git config --global credential.helper "cache --timeout=604800"


build:
close mrg,dashboard in build/CMakeCache.txt


基准测试:
https://blog.csdn.net/bandaoyu/article/details/114292690
使用命令patchelf   修改工具依赖的动态库位置。避免和项目正在使用的库冲突: https://blog.csdn.net/bandaoyu/article/details/113181179
doc/dev/messenger.rst
perf_msgr_client.cc -> ceph_perf_msgr_client
perf_msgr_server.cc -> ceph_perf_msgr_server


perf_msgr_client.cc -> main


编译rdma:
./do_cmake.sh -DCMAKE_INSTALL_PREFIX=/usr -DWITH_RDMA=ON
cd build
cmake ..
CMakeLists.txt
  
启动虚拟集群: https://docs.ceph.com/en/quincy/dev/dev_cluster_deployement/

CMakeCache.txt
ON/OFF
address sanitizer

docker run -it -d --privileged --cap-add=ALL --name centos7  -p 22223:22 -p 6666:6666 -v /home/xb/project/stor/ceph/xb/docker/ceph:/home/xb/project/stor/ceph/xb/docker/ceph ceph_centos7:v15.2.17
docker exec -u root -it centos7 bash -c 'cd /home/xb/project/stor/ceph/xb/docker/ceph;exec "${SHELL:-sh}"'

gdb:
cd /home/xb/project/stor/ceph/xb/docker/ceph/build/bin
bash gdb_s.sh
b main
r

常用:
获取线程名:
prctl(PR_GET_NAME, buf)

创建线程:
pthread_create(&thread_id, thread_attr, _entry_func, (void*)this)

设置日志文件: set_log_file
打开日志文件: m_fd = ::open(m_log_file.c_str(), O_CREAT|O_WRONLY|O_APPEND|O_CLOEXEC, 0644)
打印日志: cerr << __func__ << " " << __FL__ << " server accept client connect" << std::endl;

默认配置: Option("ms_type"
默认开启RDMA: .set_default("async+rdma")
配置文件: https://docs.ceph.com/en/latest/rados/configuration/ceph-conf/

服务端, rdma堆栈:
#0  NetworkStack::create (c=0x555555641870, t="rdma") at /home/xb/project/stor/ceph/xb/docker/ceph/src/msg/async/Stack.cc:67
#1  0x00007fffeda53354 in StackSingleton::ready (this=0x5555556c6020, type="rdma") at /home/xb/project/stor/ceph/xb/docker/ceph/src/msg/async/AsyncMessenger.cc:257
#2  0x00007fffeda4922a in AsyncMessenger::AsyncMessenger (this=0x5555556c3ef0, cct=0x555555641870, name=..., type="async+rdma", mname="server", _nonce=0)
    at /home/xb/project/stor/ceph/xb/docker/ceph/src/msg/async/AsyncMessenger.cc:294
#3  0x00007fffeda2ef0d in Messenger::create (cct=0x555555641870, type="async+rdma", name=..., lname="", nonce=0, cflags=0)
    at /home/xb/project/stor/ceph/xb/docker/ceph/src/msg/Messenger.cc:46
#4  0x00005555555a48ce in MessengerServer::MessengerServer (this=0x7fffffffdb50, t="async+rdma", addr="175.16.53.61:10001", threads=1, delay=0)
    at /home/xb/project/stor/ceph/xb/docker/ceph/src/test/msgr/perf_msgr_server.cc:119
#5  0x000055555559e8ce in main (argc=4, argv=0x7fffffffdfa8) at /home/xb/project/stor/ceph/xb/docker/ceph/src/test/msgr/perf_msgr_server.cc:173

客户端流程:
#0  AsyncMessenger::connect_to (this=0x5555556d87b0, type=4, addrs=..., anon=false, not_local_dest=false)
    at /home/xb/project/stor/ceph/xb/docker/ceph/src/msg/async/AsyncMessenger.cc:719
#1  0x00005555555abf60 in Messenger::connect_to_osd (this=0x5555556d87b0, dest=..., anon=false, not_local_dest=false)
    at /home/xb/project/stor/ceph/xb/docker/ceph/src/msg/Messenger.h:548
#2  0x00005555555b08d5 in MessengerClient::ready (this=0x7fffffffdc80, c=1, jobs=1, ops=1, msg_len=4096)
    at /home/xb/project/stor/ceph/xb/docker/ceph/src/test/msgr/perf_msgr_client.cc:144
#3  0x00005555555a7c43 in main (argc=7, argv=0x7fffffffdf48) at /home/xb/project/stor/ceph/xb/docker/ceph/src/test/msgr/perf_msgr_client.cc:212


返回自动变量auto:  const auto& _lookup_conn
要求(断言)已上锁: ceph_assert(ceph_mutex_is_locked(lock))

客户端, 建连接:
(gdb) bt
#0  AsyncConnection::connect (this=0x555555768760, addrs=..., type=4, target=...) at /home/xb/project/stor/ceph/xb/docker/ceph/src/msg/async/AsyncConnection.cc:478
#1  0x00007fffeda4ca38 in AsyncMessenger::create_connect (this=0x5555556d87b0, addrs=..., type=4, anon=false)
    at /home/xb/project/stor/ceph/xb/docker/ceph/src/msg/async/AsyncMessenger.cc:612
#2  0x00007fffeda4da7d in AsyncMessenger::connect_to (this=0x5555556d87b0, type=4, addrs=..., anon=false, not_local_dest=false)
    at /home/xb/project/stor/ceph/xb/docker/ceph/src/msg/async/AsyncMessenger.cc:723
#3  0x00005555555abf60 in Messenger::connect_to_osd (this=0x5555556d87b0, dest=..., anon=false, not_local_dest=false)
    at /home/xb/project/stor/ceph/xb/docker/ceph/src/msg/Messenger.h:548
#4  0x00005555555b08d5 in MessengerClient::ready (this=0x7fffffffdc80, c=1, jobs=1, ops=1, msg_len=4096)
    at /home/xb/project/stor/ceph/xb/docker/ceph/src/test/msgr/perf_msgr_client.cc:144
#5  0x00005555555a7c43 in main (argc=7, argv=0x7fffffffdf48) at /home/xb/project/stor/ceph/xb/docker/ceph/src/test/msgr/perf_msgr_client.cc:212

rdma ib初始化: ib->init() -> void Infiniband::init()
gdb 打印ib设备: (gdb) p **((ibv_device **) 0x7fffd4000c30)
cm建连: if (cct->_conf->ms_async_rdma_cm), https://github.com/ssbandjl/ceph/commit/2d4890580f3acdd6387bcdde15f78eba35237589

社区优化, 检查rdma配置和修复逻辑错误: https://github.com/ceph/ceph/pull/28344
1. check rdma configuration is under hardware limitation.
2. fix ibv_port_attr object memory leak by using the object instead of allocating in the heap.
3. fix logic between RDMAV_HUGEPAGES_SAFE and ibv_fork_init.
4. fix error argument to get right qp state
5. separate Device construction when rdma_cm is used.
6. refine/simplify some function implementation.
7. decouple RDMAWorker & RDMAStack, RDMADispatcher & RDMAStack
8. remove redundant code.
9. rename var to improve readability.

cm讨论: https://lists.ceph.io/hyperkitty/list/dev@ceph.io/thread/YUX4DTCFXKLOBCQNSNBEBZGOBBQSYIS4/
您是说 1) 首先创建 RDMA 内存区域 (MR) 2) 在中使用 MR bufferlist 3）将bufferlist作为工作请求发布到RDMA发送队列中直接发送 不使用 tx_copy_chunk？
https://github.com/ceph/ceph/pull/28344/files
使用ceph块设备, rgw, fs: https://www.cnblogs.com/cyh00001/p/16759266.html
https://lists.ceph.io/hyperkitty/search?mlist=dev%40ceph.io&q=rdma
https://lists.ceph.io/hyperkitty/list/dev@ceph.io/message/EHRT7TOSUP7PBJXQOBMQVUBA7JUQZNGF/ 给豪迈的rdma建议
导出实时消息状态数据: sudo ceph daemon osd.0 perf dump AsyncMessenger::RDMAWorker-1
配置文件: ms_async_rdma_device_name = mlx5_0
查询gid; ibv_query_gid(ctxt, port_num, gid_idx, &gid)
roce: https://docs.nvidia.com/networking/pages/viewpage.action?pageId=12013422
支持共享接收队列: https://github.com/ssbandjl/ceph/commit/9fc9f08371d36d0cc38cbe8cbb235fa07ae0a6c0
为 beacon(灯塔) 保留额外的一个 WR，以指示所有 WCE 已被消耗
内存管理: memory_manager = new MemoryManager(cct, device, pd);
提升接收缓存区(内存管理)性能: https://github.com/ssbandjl/ceph/commit/720d044db13886ac9926d689e970381cdf78f8eb
注册内存: int Infiniband::MemoryManager::Cluster::fill(uint32_t num) -> malloc -> ibv_reg_mr
poll 处理接收事件: void RDMADispatcher::handle_rx_event(ibv_wc *cqe, int rx_number)
iwarp或ib(默认)
  if (cct->_conf->ms_async_rdma_type == "iwarp") {
    p = new RDMAIWARPConnectedSocketImpl(cct, ib, dispatcher, this);
  } else {
    p = new RDMAConnectedSocketImpl(cct, ib, dispatcher, this);
  }

连接后: worker->center.create_file_event(tcp_fd, EVENT_READABLE | EVENT_WRITABLE , established_handler)
发送cm元数据: int Infiniband::QueuePair::send_cm_meta

h3c tag: Aug 29, 2017, v12.2.0 https://github.com/ceph/ceph/commit/32ce2a3ae5239ee33d6150705cdb24d43bab910c
社区:
commit b661348f156f148d764b998b65b90451f096cb27 (tag: v12.1.2)
Author: Jenkins <jenkins@ceph.com>
Date:   Tue Aug 1 17:55:40 2017 +0000
12.1.2

rsync_to_h3c_win11(同步二进制到win10):
cd /c/Users/s30893/Downloads/ceph/ceph_perf_msgr
rsync -urpv root@ubuntu22:/root/project/stor/ceph/xb/docker/ceph/build/bin/ceph_perf_msgr_server .
rsync -urpv root@ubuntu22:/root/project/stor/ceph/xb/docker/ceph/build/bin/ceph_perf_msgr_client .
rsync -urpv root@ubuntu22:/root/project/stor/ceph/xb/docker/ceph/build/lib/libceph-common.so.2 .

客户端建连接
conn->connect

编译ceph_msgr_perf工具:
cmake -DWITH_TESTS=1 ../CMakeList.txt
cd build
make common, ceph-common, ceph_perf_msgr_client, ceph_perf_msgr_server,  
make help 查看帮助

高级用法 
修改依赖的库
可以使用命令patchelf   修改工具依赖的动态库位置。避免和项目正在使用的库冲突，修改方法见：https://blog.csdn.net/bandaoyu/article/details/113181179

修改依赖的配置文件
修改依赖的配置文件，避免与正在运行的项目共用配置文件造成相互影响

ceph进程搜索配置文件的路径顺序

Ceph相关进程在读取配置时, 遵循以下的查找顺序

$CEPH_CONF 环境变量所指定的配置
-c path/path 参数所指定的配置
/etc/ceph/ceph.conf
~/.ceph/config (HOME目录下.ceph目录的config文件)
./ceph.conf (当前目录下的ceph.conf)

git:
git diff v12.2.0 main -- src/msg > git_diff_v12_2_0_main_src_msg
git diff v12.2.0 v15.2.17 -- src/msg > git_diff_v12_2_0_15_2_17_src_msg
git diff v15.2.17 main -- src/msg > git_diff_v15_2_17__main_src_msg

sync.sh
hosts='c51 c52'
for host in $hosts;do
	echo -e  "\n\033[32m`date +'%Y/%m/%d %H:%M:%S'` send to $host\033[0m"
	scp libceph-common.so.2 root@${host}:/home/xb/project/stor/ceph/xb/docker/ceph/build/lib/libceph-common.so.2
	scp ceph_perf_msgr_server root@${host}:/home/xb/project/stor/ceph/xb/docker/ceph/build/bin/ceph_perf_msgr_server
	scp ceph_perf_msgr_client root@${host}:/home/xb/project/stor/ceph/xb/docker/ceph/build/bin/ceph_perf_msgr_client
done

set_nonce: 设置随机值


Infiniband::Infiniband
verify_prereq

先设置环境变量, 后 ibv_fork_init() https://github.com/ssbandjl/ceph/commit/b2d3f5e0970a4afbb82676af9e5b9a12f62a7747
ibv_fork_init will check environment variable RDMAV_HUGEPAGES_SAFE to decide whether huge page is usable in system

src/test/msgr/perf_msgr_server.cc
server, 服务端, 建连接
MessengerServer
Messenger *Messenger::create type=async+rdma, lname=server -> Messenger *Messenger::create
  new AsyncMessenger -> AsyncMessenger::AsyncMessenger
    dispatch_queue
      DispatchQueue(CephContext *cct, Messenger *msgr, string &name) 本地快速分发, 节流阀
    lookup_or_create_singleton_object<StackSingleton>
    single->ready(transport_type)
       NetworkStack::create(cct, type) -> std::make_shared<RDMAStack>(c, t)
        RDMAStack::RDMAStack
          NetworkStack(cct, t) 构造, 线程池默认3个线程,
            create_worker(cct, type, worker_id) -> NetworkStack::create_worker -> new RDMAWorker(c, worker_id) -> RDMAWorker::RDMAWorker
              Worker(CephContext *c, unsigned worker_id) Stack是一个网络IO框架，封装了所有必要的基础网络接口，然后它管理线程工作。posix、dpdk甚至RDMA等不同的网络后端都需要继承Stack类来实现必要的接口。 所以这会让其他人轻松网络后端集成到 ceph 中。 否则，每个后端都需要实现整个 Messenger 逻辑，如重新连接、策略处理、会话维持...
            w->center.init -> EventCenter::init
              driver = new EpollDriver(cct)
              driver->init(this, nevent) -> int EpollDriver::init
                events = (struct epoll_event*)calloc
                epfd = epoll_create(1024)
                fcntl(epfd, F_SETFD, FD_CLOEXEC)
              file_events.resize(nevent) 5000
              pipe_cloexec(fds, 0) -> pipe2(pipefd, O_CLOEXEC | flags) 创建管道,均为非阻塞
              notify_receive_fd = fds[0] 接收端,读端
              notify_send_fd = fds[1] 发送端,写端
            workers.push_back(w)
          Infiniband::Infiniband
            device_name(cct->_conf->ms_async_rdma_device_name) 从配置中获取rdma设备名 TODO
            port_num( cct->_conf->ms_async_rdma_port_num) 默认为1 端口也从配置文件中获取
            verify_prereq -> void Infiniband::verify_prereq
              RDMAV_HUGEPAGES_SAFE 设置安全大页
              ibv_fork_init
              getrlimit(RLIMIT_MEMLOCK, &limit) 获取资源限制的配置
          get_num_worker 3
          for
            w->set_dispatcher(rdma_dispatcher)
            w->set_ib(ib)
    stack->start()
      std::function<void ()> thread = add_thread(i) 暂不执行
        w->center.set_owner()
          notify_handler = new C_handle_notify(this, cct)
          create_file_event(notify_receive_fd, EVENT_READABLE, notify_handler) 将之前管道的读端设置epoll监听
            driver->add_event(fd, event->mask, mask)
              epoll_ctl(epfd, op, fd, &ee)
            event->read_cb = ctxt 设置读事件回调
        w->initialize()
        w->init_done()
          init_cond.notify_all() 通知等待的线程,完成初始化
        while (!w->done)
          w->center.process_events 循环处理事件 -> int EventCenter::process_events
            driver->event_wait(fired_events, &tv) -> int EpollDriver::event_wait
              epoll_wait 写端写入c触发执行此处
              fired_events[event_id].fd = e->data.fd
            event = _get_file_event(fired_events[event_id].fd)
            cb = event->read_cb 可读回调
            cb->do_request(fired_events[event_id].fd) 处理事件
              r = read(fd_or_id, c, sizeof(c)) 读管道对端发来的字符,如:c
            cur_process.swap(external_events)
      spawn_worker(i, std::move(thread)) 启动新线程,返回join控制器
      workers[i]->wait_for_init() 等所有工人完成初始化
    local_connection = ceph::make_ref<AsyncConnection> -> AsyncConnection::AsyncConnection
      ms_connection_ready_timeout 建连超时时间
      ms_connection_idle_timeout 不活跃的时间, 如果两端连接空闲超过15分钟(没有活动的读写),则销毁连接
      read_handler = new C_handle_read(this) -> conn->process()
        void AsyncConnection::process()
      write_handler = new C_handle_write(this) -> conn->handle_write()
        void AsyncConnection::handle_write
      write_callback_handler = new C_handle_write_callback(this) -> AsyncConnection::handle_write_callback -> AsyncConnection::write 写的时候传递callback
      wakeup_handler = new C_time_wakeup(this) -> void AsyncConnection::wakeup_from -> void AsyncConnection::process()
      tick_handler = new C_tick_wakeup(this)-> void AsyncConnection::tick 计时器()
        protocol->fault() 处理错误
    init_local_connection
      void ms_deliver_handle_fast_connect
    reap_handler = new C_handle_reap(this)
      void AsyncMessenger::reap_dead 收割死连接
    processors.push_back(new Processor(this, stack->get_worker(i), cct))
      Processor::Processor
        listen_handler(new C_processor_accept(this))
          void Processor::accept() 等待事件触发
msgr->set_default_policy
dummy_auth.auth_registry.refresh_config()
msgr->set_auth_server(&dummy_auth) 初始化函数,在绑定前调用
server.start()
  msgr->bind(addr)
    AsyncMessenger::bind
      bindv -> int r = p->bind
      int Processor::bind
        listen_sockets.resize
        conf->ms_bind_retry_count 3次重试
        worker->center.submit_to lambda []()->void 匿名函数
          c->in_thread()
            pthread_equal(pthread_self(), owner) 本线程
          C_submit_event<func> event(std::move(f), false) f=listen
            void do_request -> f() -> listen -> worker->listen(listen_addr, k, opts, &listen_sockets[k]) -> int RDMAWorker::listen 由事件触发执行
              ib->init() -> void Infiniband::init
                new DeviceList(cct)
                  ibv_get_device_list 4网口
                  if (cct->_conf->ms_async_rdma_cm)
                  new Device(cct, device_list[i]) -> Device::Device
                    ibv_open_device
                    ibv_get_device_name
                    ibv_query_device 参考设备属性: device_attr
                get_device 根据配置的设备名在设备列表中查询
                binding_port -> void Device::binding_port
                  
                new ProtectionDomain(cct, device)
              dispatcher->polling_start()
              new RDMAServerSocketImpl(cct, ib, dispatcher, this, sa, addr_slot)
              int r = p->listen(sa, opt)
              *sock = ServerSocket(std::unique_ptr<ServerSocketImpl>(p))
            cond.notify_all() -> 通知等待的线程
          dispatch_event_external -> void EventCenter::dispatch_event_external
            external_events.push_back(e)
            wakeup()
              write(notify_send_fd, &buf, sizeof(buf)) buf=c -> notify_receive_fd, 唤醒 epoll_wait
          event.wait()
          
    



超时处理: 
new C_handle_reap(this)
  local_worker->create_time_event( ReapDeadConnectionMaxPeriod...
    reap_dead


设备属性: device_attr
(gdb) p device_attr
$17 = {
  fw_ver = "16.33.1048", '\000' <repeats 53 times>, 
  node_guid = 8550064101420093112, 
  sys_image_guid = 8550064101420093112, 
  max_mr_size = 18446744073709551615, 
  page_size_cap = 18446744073709547520, 
  vendor_id = 713, 
  vendor_part_id = 4119, 
  hw_ver = 0, 
  max_qp = 131072, 
  max_qp_wr = 32768, 
---Type <return> to continue, or q <return> to quit---
  device_cap_flags = 3983678518, 
  max_sge = 30, 
  max_sge_rd = 30, 
  max_cq = 16777216, 
  max_cqe = 4194303, 
  max_mr = 16777216, 
  max_pd = 8388608, 
  max_qp_rd_atom = 16, 
  max_ee_rd_atom = 0, 
  max_res_rd_atom = 2097152, 
  max_qp_init_rd_atom = 16, 
---Type <return> to continue, or q <return> to quit---
  max_ee_init_rd_atom = 0, 
  atomic_cap = IBV_ATOMIC_HCA, 
  max_ee = 0, 
  max_rdd = 0, 
  max_mw = 16777216, 
  max_raw_ipv6_qp = 0, 
  max_raw_ethy_qp = 0, 
  max_mcast_grp = 2097152, 
  max_mcast_qp_attach = 240, 
  max_total_mcast_qp_attach = 503316480, 
  max_ah = 2147483647, 
---Type <return> to continue, or q <return> to quit---
  max_fmr = 0, 
  max_map_per_fmr = 0, 
  max_srq = 8388608, 
  max_srq_wr = 32767, 
  max_srq_sge = 31, 
  max_pkeys = 128, 
  local_ca_ack_delay = 16 '\020', 
  phys_port_cnt = 1 '\001'
}

