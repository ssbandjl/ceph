https://github.com/ssbandjl/ceph/tree/v15.2.17
https://github.com/ssbandjl/ceph/tree/main

性能测试,fio: https://www.cnblogs.com/wangmo/p/11309080.html, https://github.com/get-set/fio-bench-disks-ceph, https://blog.csdn.net/get_set/article/details/108092302, https://access.redhat.com/documentation/en-us/red_hat_ceph_storage/1.3/html/administration_guide/performance_counters
ceph管理手册_红帽: https://access.redhat.com/documentation/en-us/red_hat_ceph_storage/1.3/html/administration_guide/index

监控集群: https://access.redhat.com/documentation/en-us/red_hat_ceph_storage/1.3/html/administration_guide/monitoring

ceph客户端源码分析(librados,crush,objecter): https://blog.csdn.net/csnd_pan/category_7315843.html

cache: http://rz2fg6ogr.hn-bkt.clouddn.com/ceph_v15_2_17_with_submodule.tgz

build:
close mrg,dashboard in build/CMakeCache.txt

基准测试, 文档: messenger.rst
https://blog.csdn.net/bandaoyu/article/details/114292690
使用命令patchelf   修改工具依赖的动态库位置。避免和项目正在使用的库冲突: https://blog.csdn.net/bandaoyu/article/details/113181179
doc/dev/messenger.rst
perf_msgr_client.cc -> ceph_perf_msgr_client
_msgr_server.cc -> ceph_perf_msgr_server

perf_msgr_client.cc -> main


编译rdma:
./do_cmake.sh -DCMAKE_INSTALL_PREFIX=/usr -DWITH_RDMA=ON

just build client:
make client > log 2>&1 &

qa:
启动虚拟集群: https://docs.ceph.com/en/quincy/dev/dev_cluster_deployement/

docker:

git
git remote add upstream https://github.com/Foo/repos.git
git pull upstream v15.2.17
git remote remove upstream
git remote add upstream https://github.com/Foo/repos.git
git remote set-url upstream https://github.com/Foo/repos.git

git push origin ：refs/tags/3.0 这就是明确告诉服务器删除的tag的分支,删除branch分支
git push origin :refs/heads/3.0
git branch -D testtag
删除tag分支的方法：
git tag -d v15.2.17
git push origin v15.2.17
git config --global credential.helper "cache --timeout=604800"


build:
close mrg,dashboard in build/CMakeCache.txt


基准测试:
https://blog.csdn.net/bandaoyu/article/details/114292690
使用命令patchelf   修改工具依赖的动态库位置。避免和项目正在使用的库冲突: https://blog.csdn.net/bandaoyu/article/details/113181179
doc/dev/messenger.rst
perf_msgr_client.cc -> ceph_perf_msgr_client
perf_msgr_server.cc -> ceph_perf_msgr_server


perf_msgr_client.cc -> main


编译rdma:
./do_cmake.sh -DCMAKE_INSTALL_PREFIX=/usr -DWITH_RDMA=ON
cd build
cmake ..
CMakeLists.txt
  
启动虚拟集群: https://docs.ceph.com/en/quincy/dev/dev_cluster_deployement/

CMakeCache.txt
ON/OFF
address sanitizer

docker run -it -d --privileged --cap-add=ALL --name centos7  -p 22223:22 -p 6666:6666 -v /home/xb/project/stor/ceph/xb/docker/ceph:/home/xb/project/stor/ceph/xb/docker/ceph ceph_centos7:v15.2.17
docker exec -u root -it ceph bash -c 'cd /home/xb/project/stor/ceph/xb/docker/ceph;exec "${SHELL:-sh}"'

s131:
docker run -it -d --privileged --cap-add=ALL --name ceph_v15  -p 22223:22 -p 6666:6666  -p 42922:42922 -p 44922:44922 -p 46922:46922 -v /home/xb/project/ceph/xb/ceph:/home/xb/project/ceph/xb/ceph ceph_centos7:v15.2.17
docker exec -u root -it ceph_v15 bash -c 'cd /home/xb/project/ceph/xb/ceph;exec "${SHELL:-sh}"'
pip3 install prettytable
yum install -y gdb

env:
export PYTHONPATH=/home/xb/project/ceph/xb/ceph/src/pybind:/home/xb/project/ceph/xb/ceph/build/lib/cython_modules/lib.3:/home/xb/project/ceph/xb/ceph/src/python-common:$PYTHONPATH
export LD_LIBRARY_PATH=/home/xb/project/ceph/xb/ceph/build/lib:$LD_LIBRARY_PATH
alias cephfs-shell=/home/xb/project/ceph/xb/ceph/src/tools/cephfs/cephfs-shell
CEPH_DEV=1


gdb:
cd /home/xb/project/stor/ceph/xb/docker/ceph/build/bin
bash gdb_s.sh
b main
r

常用:
获取线程名:
prctl(PR_GET_NAME, buf)

创建线程:
pthread_create(&thread_id, thread_attr, _entry_func, (void*)this)

设置日志文件: set_log_file
打开日志文件: m_fd = ::open(m_log_file.c_str(), O_CREAT|O_WRONLY|O_APPEND|O_CLOEXEC, 0644)
打印日志: cerr << __func__ << " " << __FL__ << " server accept client connect" << std::endl;
打印日志代码示例: ldout(cct, 10) << __FFL__ << " client connect -> server" << dendl;

日志配置:
debug {subsystem} = {log-level}/{memory-level}
#for example
debug mds log = 1/20

打印每行日志:
void Log::_flush

默认配置: Option("ms_type"
默认开启RDMA: .set_default("async+rdma")
配置文件: https://docs.ceph.com/en/latest/rados/configuration/ceph-conf/


返回自动变量auto:  const auto& _lookup_conn
要求(断言)已上锁: ceph_assert(ceph_mutex_is_locked(lock))

int RDMAWorker::listen -> rdma ib初始化: ib->init() -> void Infiniband::init()

int RDMAWorker::connect -> ib->init() -> void Infiniband::init()
  

gdb 打印ib设备: (gdb) p **((ibv_device **) 0x7fffd4000c30)
cm建连: if (cct->_conf->ms_async_rdma_cm), https://github.com/ssbandjl/ceph/commit/2d4890580f3acdd6387bcdde15f78eba35237589

社区优化, 检查rdma配置和修复逻辑错误: https://github.com/ceph/ceph/pull/28344
1. check rdma configuration is under hardware limitation.
2. fix ibv_port_attr object memory leak by using the object instead of allocating in the heap.
3. fix logic between RDMAV_HUGEPAGES_SAFE and ibv_fork_init.
4. fix error argument to get right qp state
5. separate Device construction when rdma_cm is used.
6. refine/simplify some function implementation.
7. decouple RDMAWorker & RDMAStack, RDMADispatcher & RDMAStack
8. remove redundant code.
9. rename var to improve readability.

cm讨论: https://lists.ceph.io/hyperkitty/list/dev@ceph.io/thread/YUX4DTCFXKLOBCQNSNBEBZGOBBQSYIS4/
您是说 1) 首先创建 RDMA 内存区域 (MR) 2) 在中使用 MR bufferlist 3）将bufferlist作为工作请求发布到RDMA发送队列中直接发送 不使用 tx_copy_chunk？

讨论3个rdma问题: https://lists.ceph.io/hyperkitty/list/dev@ceph.io/message/EHRT7TOSUP7PBJXQOBMQVUBA7JUQZNGF/

https://github.com/ceph/ceph/pull/28344/files
使用ceph块设备, rgw, fs: https://www.cnblogs.com/cyh00001/p/16759266.html
https://lists.ceph.io/hyperkitty/search?mlist=dev%40ceph.io&q=rdma
https://lists.ceph.io/hyperkitty/list/dev@ceph.io/message/EHRT7TOSUP7PBJXQOBMQVUBA7JUQZNGF/ 给豪迈的rdma建议
导出实时消息状态数据: sudo ceph daemon osd.0 perf dump AsyncMessenger::RDMAWorker-1
配置文件: ms_async_rdma_device_name = mlx5_0
查询gid; ibv_query_gid(ctxt, port_num, gid_idx, &gid)
roce: https://docs.nvidia.com/networking/pages/viewpage.action?pageId=12013422

支持共享接收队列: https://github.com/ssbandjl/ceph/commit/9fc9f08371d36d0cc38cbe8cbb235fa07ae0a6c0

为 beacon(灯塔) 保留额外的一个 WR，以指示所有 WCE 已被消耗
内存管理: memory_manager = new MemoryManager(cct, device, pd);
提升接收缓存区(内存管理)性能: https://github.com/ssbandjl/ceph/commit/720d044db13886ac9926d689e970381cdf78f8eb
注册内存: int Infiniband::MemoryManager::Cluster::fill(uint32_t num) -> malloc -> ibv_reg_mr
poll 处理接收事件: void RDMADispatcher::handle_rx_event(ibv_wc *cqe, int rx_number)
iwarp或ib(默认)
  if (cct->_conf->ms_async_rdma_type == "iwarp") {
    p = new RDMAIWARPConnectedSocketImpl(cct, ib, dispatcher, this);
  } else {
    p = new RDMAConnectedSocketImpl(cct, ib, dispatcher, this);
  }

连接后: worker->center.create_file_event(tcp_fd, EVENT_READABLE | EVENT_WRITABLE , established_handler)
发送cm元数据: int Infiniband::QueuePair::send_cm_meta

h3c tag: 2017/8/27, Aug 29, 2017, v12.2.0 https://github.com/ceph/ceph/commit/32ce2a3ae5239ee33d6150705cdb24d43bab910c
社区:
commit b661348f156f148d764b998b65b90451f096cb27 (tag: v12.1.2)
Author: Jenkins <jenkins@ceph.com>
Date:   Tue Aug 1 17:55:40 2017 +0000
12.1.2

rsync_to_h3c_win11(同步二进制到win10):
cd /c/Users/s30893/Downloads/ceph/ceph_perf_msgr
rsync -urpv root@ubuntu22:/root/project/stor/ceph/xb/docker/ceph/build/bin/ceph_perf_msgr_server .
rsync -urpv root@ubuntu22:/root/project/stor/ceph/xb/docker/ceph/build/bin/ceph_perf_msgr_client .
rsync -urpv root@ubuntu22:/root/project/stor/ceph/xb/docker/ceph/build/lib/libceph-common.so.2 .


编译ceph_msgr_perf工具, 单元测试
cmake -DWITH_TESTS=1 ../CMakeList.txt
cd build
make common, ceph-common, ceph_perf_msgr_client, ceph_perf_msgr_server,  
make help 查看帮助

高级用法 
修改依赖的库
可以使用命令patchelf   修改工具依赖的动态库位置。避免和项目正在使用的库冲突，修改方法见：https://blog.csdn.net/bandaoyu/article/details/113181179

修改依赖的配置文件
修改依赖的配置文件，避免与正在运行的项目共用配置文件造成相互影响

ceph进程搜索配置文件的路径顺序

Ceph相关进程在读取配置时, 遵循以下的查找顺序

$CEPH_CONF 环境变量所指定的配置
-c path/path 参数所指定的配置
/etc/ceph/ceph.conf
~/.ceph/config (HOME目录下.ceph目录的config文件)
./ceph.conf (当前目录下的ceph.conf)

git:
git diff v12.2.0 main -- src/msg > git_diff_v12_2_0_main_src_msg
git diff v12.2.0 v15.2.17 -- src/msg > git_diff_v12_2_0_15_2_17_src_msg
git diff v15.2.17 main -- src/msg > git_diff_v15_2_17__main_src_msg

sync.sh
hosts='c51 c52'
for host in $hosts;do
	echo -e  "\n\033[32m`date +'%Y/%m/%d %H:%M:%S'` send to $host\033[0m"
	scp libceph-common.so.2 root@${host}:/home/xb/project/stor/ceph/xb/docker/ceph/build/lib/libceph-common.so.2
	scp ceph_perf_msgr_server root@${host}:/home/xb/project/stor/ceph/xb/docker/ceph/build/bin/ceph_perf_msgr_server
	scp ceph_perf_msgr_client root@${host}:/home/xb/project/stor/ceph/xb/docker/ceph/build/bin/ceph_perf_msgr_client
done

set_nonce: 设置随机值


Infiniband::Infiniband
verify_prereq

先设置环境变量, 后 ibv_fork_init() https://github.com/ceph/ceph/commit/b2d3f5e0970a4afbb82676af9e5b9a12f62a7747
ibv_fork_init will check environment variable RDMAV_HUGEPAGES_SAFE to decide whether huge page is usable in system
新版本增加判断:
if (ibv_is_fork_initialized() == IBV_FORK_UNNEEDED) {
  lderr(cct) << __FFL__ << " no need ibv_fork_init" << dendl;
}

src/test/msgr/perf_msgr_server.cc
server, 服务端, 建连接
MessengerServer
Messenger *Messenger::create type=async+rdma, lname=server -> Messenger *Messenger::create
  new AsyncMessenger -> AsyncMessenger::AsyncMessenger
    dispatch_queue
      DispatchQueue(CephContext *cct, Messenger *msgr, string &name) 本地快速分发, 节流阀
    lookup_or_create_singleton_object<StackSingleton>
    single->ready(transport_type)
       NetworkStack::create(cct, type) -> std::make_shared<RDMAStack>(c, t)
        RDMAStack::RDMAStack
          NetworkStack(cct, t) 构造, 线程池默认3个线程,
            create_worker(cct, type, worker_id) -> NetworkStack::create_worker -> new RDMAWorker(c, worker_id) -> RDMAWorker::RDMAWorker
              Worker(CephContext *c, unsigned worker_id) Stack是一个网络IO框架，封装了所有必要的基础网络接口，然后它管理线程工作。posix、dpdk甚至RDMA等不同的网络后端都需要继承Stack类来实现必要的接口。 所以这会让其他人轻松网络后端集成到 ceph 中。 否则，每个后端都需要实现整个 Messenger 逻辑，如重新连接、策略处理、会话维持...
            w->center.init -> EventCenter::init
              driver = new EpollDriver(cct)
              driver->init(this, nevent) -> int EpollDriver::init
                events = (struct epoll_event*)calloc
                epfd = epoll_create(1024)
                fcntl(epfd, F_SETFD, FD_CLOEXEC)
              file_events.resize(nevent) 5000
              pipe_cloexec(fds, 0) -> pipe2(pipefd, O_CLOEXEC | flags) 创建管道,均为非阻塞
              notify_receive_fd = fds[0] 接收端,读端
              notify_send_fd = fds[1] 发送端,写端
            workers.push_back(w)
          Infiniband::Infiniband
            device_name(cct->_conf->ms_async_rdma_device_name) 从配置中获取rdma设备名 TODO
            port_num( cct->_conf->ms_async_rdma_port_num) 默认为1 端口也从配置文件中获取
            verify_prereq -> void Infiniband::verify_prereq
              RDMAV_HUGEPAGES_SAFE 设置安全大页
              ibv_fork_init
              getrlimit(RLIMIT_MEMLOCK, &limit) 获取资源限制的配置
          get_num_worker 3
          for
            w->set_dispatcher(rdma_dispatcher)
            w->set_ib(ib)
    stack->start()
      std::function<void ()> thread = add_thread(i) 暂不执行
        w->center.set_owner()
          notify_handler = new C_handle_notify(this, cct)
          create_file_event(notify_receive_fd, EVENT_READABLE, notify_handler) 将之前管道的读端设置epoll监听
            driver->add_event(fd, event->mask, mask)
              epoll_ctl(epfd, op, fd, &ee)
            event->read_cb = ctxt 设置读事件回调
        w->initialize()
        w->init_done()
          init_cond.notify_all() 通知等待的线程,完成初始化
        while (!w->done)
          w->center.process_events 循环处理事件 -> int EventCenter::process_events
            driver->event_wait(fired_events, &tv) -> int EpollDriver::event_wait
              epoll_wait 写端写入c触发执行此处
              fired_events[event_id].fd = e->data.fd
            event = _get_file_event(fired_events[event_id].fd)
            cb = event->read_cb 可读回调
            cb->do_request(fired_events[event_id].fd) 处理事件
              r = read(fd_or_id, c, sizeof(c)) 读管道对端发来的字符,如:c
            cur_process.swap(external_events)
      spawn_worker(i, std::move(thread)) 启动新线程,返回join控制器
      workers[i]->wait_for_init() 等所有工人完成初始化
    local_connection = ceph::make_ref<AsyncConnection> -> AsyncConnection::AsyncConnection
      ms_connection_ready_timeout 建连超时时间
      ms_connection_idle_timeout 不活跃的时间, 如果两端连接空闲超过15分钟(没有活动的读写),则销毁连接
      read_handler = new C_handle_read(this) -> conn->process()
        void AsyncConnection::process()
      write_handler = new C_handle_write(this) -> conn->handle_write()
        void AsyncConnection::handle_write
      write_callback_handler = new C_handle_write_callback(this) -> AsyncConnection::handle_write_callback -> AsyncConnection::write 写的时候传递callback
      wakeup_handler = new C_time_wakeup(this) -> void AsyncConnection::wakeup_from -> void AsyncConnection::process()
      tick_handler = new C_tick_wakeup(this)-> void AsyncConnection::tick 计时器()
        protocol->fault() 处理错误
    init_local_connection
      void ms_deliver_handle_fast_connect
    reap_handler = new C_handle_reap(this)
      void AsyncMessenger::reap_dead 收割死连接
    processors.push_back(new Processor(this, stack->get_worker(i), cct))
      Processor::Processor
        listen_handler(new C_processor_accept(this))
          void Processor::accept() 等待事件触发(客户端执行connect后触发)
            listen_sockets -> while (true)
              msgr->get_stack()->get_worker()
              listen_socket.accept(&cli_socket, opts, &addr, w)
              msgr->add_accept
msgr->set_default_policy
dummy_auth.auth_registry.refresh_config()
msgr->set_auth_server(&dummy_auth) 初始化函数,在绑定前调用
server.start()
  msgr->bind(addr)
    AsyncMessenger::bind
      bindv -> int r = p->bind
      int Processor::bind
        listen_sockets.resize
        conf->ms_bind_retry_count 3次重试
        worker->center.submit_to lambda []()->void 匿名函数
          c->in_thread()
            pthread_equal(pthread_self(), owner) 本线程
          C_submit_event<func> event(std::move(f), false) f=listen
            void do_request -> f() -> listen -> worker->listen(listen_addr, k, opts, &listen_sockets[k]) -> int RDMAWorker::listen 由事件触发执行
              ib->init() -> void Infiniband::init
                new DeviceList(cct)
                  ibv_get_device_list 4网口
                  if (cct->_conf->ms_async_rdma_cm)
                  new Device(cct, device_list[i]) -> Device::Device
                    ibv_open_device
                    ibv_get_device_name
                    ibv_query_device 参考设备属性: device_attr
                get_device 根据配置的设备名在设备列表中查询, 默认取第一个, 如: mlx5_0
                binding_port -> void Device::binding_port
                  new Port(cct, ctxt, port_id) 端口ID从1开始 -> Port::Port
                    ibv_query_port(ctxt, port_num, &port_attr)
                    ibv_query_gid(ctxt, port_num, gid_idx, &gid)
                    ib_physical_port = device->active_port->get_port_num() 获取物理端口
                new ProtectionDomain(cct, device) -> Infiniband::ProtectionDomain::ProtectionDomain -> ibv_alloc_pd(device->ctxt)
                support_srq = cct->_conf->ms_async_rdma_support_srq 共享接收队列srq
                rx_queue_len = device->device_attr.max_srq_wr 最终为4096
                tx_queue_len = device->device_attr.max_qp_wr - 1 发送队列为beacon保留1个WR, 如:1024 1_K 重载操作符
                device->device_attr.max_cqe 设备允许 4194303 完成事件
                memory_manager = new MemoryManager(cct, device, pd) -> Infiniband::MemoryManager::MemoryManager 128K -> mem_pool ->  boost::pool
                memory_manager->create_tx_pool(cct->_conf->ms_async_rdma_buffer_size, tx_queue_len) -> void Infiniband::MemoryManager::create_tx_pool
                  send = new Cluster(*this, size)
                  send->fill(tx_num) -> int Infiniband::MemoryManager::Cluster::fill
                    base = (char*)manager.malloc(bytes) -> void* Infiniband::MemoryManager::malloc -> std::malloc(size) 标准分配或分配大页(huge_pages_malloc)
                    ibv_reg_mr 注册内存
                    new(chunk) Chunk
                    free_chunks.push_back(chunk)
                create_shared_receive_queue
                  ibv_create_srq
                post_chunks_to_rq -> int Infiniband::post_chunks_to_rq
                  chunk = get_memory_manager()->get_rx_buffer() -> return reinterpret_cast<Chunk *>(rxbuf_pool.malloc())
                  ibv_post_srq_recv
              dispatcher->polling_start() -> void RDMADispatcher::polling_start
                ib->get_memory_manager()->set_rx_stat_logger(perf_logger) -> void PerfCounters::set
                tx_cc = ib->create_comp_channel(cct) -> Infiniband::CompletionChannel* Infiniband::create_comp_channel -> new Infiniband::CompletionChannel
                tx_cq = ib->create_comp_queue(cct, tx_cc)
                  cq->init() -> int Infiniband::CompletionChannel::init
                    ibv_create_comp_channel 创建完成通道 -> NetHandler(cct).set_nonblock(channel->fd) 设置非阻塞
                t = std::thread(&RDMADispatcher::polling, this) 启动polling线程 rdma-polling -> void RDMADispatcher::polling
                  tx_cq->poll_cq(MAX_COMPLETIONS, wc)
                  handle_tx_event -> tx_chunks.push_back(chunk) -> post_tx_buffer
                    tx -> void RDMAWorker::handle_pending_message()
                  handle_rx_event -> void RDMADispatcher::handle_rx_event
                    conn->post_chunks_to_rq(1) 向接收队列补一个内存块(WR) -> int Infiniband::post_chunks_to_rq
                      ibv_post_srq_recv | ibv_post_recv
                    polled[conn].push_back(*response)
                    qp->remove_rq_wr(chunk)
                    chunk->clear_qp()
                    pass_wc -> void RDMAConnectedSocketImpl::pass_wc(std::vector<ibv_wc> &&v) ->  notify() -> void RDMAConnectedSocketImpl::notify
                      eventfd_write(notify_fd, event_val) -> eventfd_read(notify_fd, &event_val) <- ssize_t RDMAConnectedSocketImpl::read <- process
              new RDMAServerSocketImpl(cct, ib, dispatcher, this, sa, addr_slot)
              int r = p->listen(sa, opt) -> int RDMAServerSocketImpl::listen
                server_setup_socket = net.create_socket(sa.get_family(), true) -> socket_cloexec
                net.set_nonblock
                net.set_socket_options
                ::bind(server_setup_socket, sa.get_sockaddr(), sa.get_sockaddr_len()) 系统调用
                ::listen backlog=512
              *sock = ServerSocket(std::unique_ptr<ServerSocketImpl>(p))
            cond.notify_all() -> 通知等待的线程
          dispatch_event_external -> void EventCenter::dispatch_event_external
            external_events.push_back(e)
            wakeup()
              write(notify_send_fd, &buf, sizeof(buf)) buf=c -> notify_receive_fd, 唤醒 epoll_wait
          event.wait()
  msgr->add_dispatcher_head(&dispatcher)
    ready()
      p->start() -> void Processor::start()
        worker->center.create_file_event listen_handler -> pro->accept() -> void Processor::accept()
  msgr->start() -> int AsyncMessenger::start()
  msgr->wait() -> void AsyncMessenger::wait()  
    

客户端建连接, src/test/msgr/perf_msgr_client.cc, gdb --args ceph_perf_msgr_client 175.16.53.62:10001 10 1 1 0 4096
perf_msgr_client.cc -> main
  MessengerClient client(public_msgr_type, args[0], think_time)
  client.ready
    Messenger *msgr = Messenger::create
    msgr->set_default_policy -> Policy(bool l, bool s ...
    msgr->start() -> int AsyncMessenger::start()
      if (!did_bind) 客户端不需要bind
      set_myaddrs(newaddrs) -> void Messenger::set_endpoint_addr
      _init_local_connection() -> void _init_local_connection()
        ms_deliver_handle_fast_connect(local_connection.get()) -> void ms_deliver_handle_fast_connect将新连接通知每个快速调度程序。 每当启动或重新连接新连接时调用此函数 fast_dispatchers为空?
    ConnectionRef conn = msgr->connect_to_osd(addrs) 连接到OSD -> ConnectionRef connect_to_osd -> ConnectionRef AsyncMessenger::connect_to
      AsyncConnectionRef conn = _lookup_conn(av) 先在连接池查找连接
      conn = create_connect(av, type, false) 没找到,新建连接 -> AsyncConnectionRef AsyncMessenger::create_connect
        Worker *w = stack->get_worker()
        auto conn = ceph::make_ref<AsyncConnection> -> AsyncConnection::AsyncConnection 构造连接
          recv_buf = new char[2*recv_max_prefetch] 使用缓冲区读取来避免小的读取开销
          new ProtocolV2(this) ceph v2协议, 在v1基础上支持地址向量, 在横幅(banner)交换之后，对等体交换他们的地址向量address vectors
        conn->connect(addrs, type, target) -> void AsyncConnection::connect -> _connect -> void AsyncConnection::_connect()
          state = STATE_CONNECTING 初始状态机
          protocol->connect() -> void ProtocolV2::connect() -> state = START_CONNECT
          center->dispatch_event_external(read_handler) -> 触发状态机推进 -> process
        conns[addrs] = conn 保存连接 -> ceph::unordered_map<entity_addrvec_t, AsyncConnectionRef> conns 无序map
    ClientThread *t = new ClientThread(msgr, c, conn, msg_len, ops, think_time_us) -> ClientThread(Messenger *m 新建客户端线程, 构造数据
      m->add_dispatcher_head(&dispatcher)
      bufferptr ptr(msg_len) 申请数据指针 -> buffer::ptr::ptr(unsigned l) : _off(0), _len(l)
         _raw = buffer::create(l).release() -> ceph::unique_leakable_ptr<buffer::raw> buffer::create, 通过返回其值并用空指针替换它来释放其存储指针的所有权。此调用不会破坏托管对象，但 unique_ptr 对象从删除对象的责任中解脱出来。 某些其他实体必须负责在某个时刻删除该对象。要强制销毁指向的对象，请使用成员函数 reset 或对其执行赋值操作
          buffer::create_aligned(len, sizeof(size_t)) -> ceph::unique_leakable_ptr<buffer::raw> buffer::create_aligned
            create_aligned_in_mempool -> mempool::mempool_buffer_anon 宏: f(buffer_anon) -> ceph::unique_leakable_ptr<buffer::raw> buffer::create_aligned_in_mempool, 1M: create_aligned_in_mempool (len=1048576, align=8, mempool=18)
              len >= CEPH_PAGE_SIZE * 2 如果待分配内存长度大于2倍CEPH_PAGE_SIZE(系统页:sysconf(_SC_PAGESIZE))=8K, 则用原生posix对齐分配 -> ceph::unique_leakable_ptr<buffer::raw>(new raw_posix_aligned(len, align)) ->  raw_posix_aligned(unsigned l, unsigned _align) : raw(l)
                r = ::posix_memalign((void**)(void*)&data, align, len);
              return raw_combined::create(len, align, mempool) -> src/common/buffer.cc -> static ceph::unique_leakable_ptr<buffer::raw> -> create(unsigned len,
                align = std::max<unsigned>(align, sizeof(void *)) = 8
                size_t rawlen = round_up_to(sizeof(buffer::raw_combined) 96
                size_t datalen = round_up_to(len, alignof(buffer::raw_combined)) 4096
                int r = ::posix_memalign((void**)(void*)&ptr, align, rawlen + datalen); 96+4096
                new (ptr + datalen) raw_combined(ptr, len, align, mempool))
         _raw->nref.store(1, std::memory_order_release)
      memset(ptr.c_str(), 0, msg_len) 置0
      data.append(ptr) 将data填充全0 -> void buffer::list::append -> void push_back(const ptr& bp)
        _buffers.push_back(*ptr_node::create(bp).release())
        _len += bp.length()
    msgrs.push_back(msgr)
    clients.push_back(t)
  Cycles::init() -> void Cycles::init() 校准时钟频率
  uint64_t start = Cycles::rdtsc()
  client.start() -> void start() -> clients[i]->create("client") -> void Thread::create
    pthread_create(&thread_id, thread_attr, _entry_func, (void*)this) -> void *Thread::_entry_func
    void *entry() override 重写entry
      hobject_t hobj(oid, oloc.key -> struct object_t
        void build_hash_cache() crc32c
      MOSDOp *m = new MOSDOp -> MOSDOp(int inc, long tid, 
      bufferlist msg_data(data) 拷贝构造函数?, 拷贝数据
      m->write(0, msg_len, msg_data) -> void write 通过消息msg写数据到对端, offset=0, len=4096, buffer_list=bl(msg_data)
        add_simple_op(CEPH_OSD_OP_WRITE, off, len) -> ops.push_back(osd_op)
           osd_op.op.extent.offset = off
           osd_op.op.extent.length = len
           ops.push_back(osd_op)
        data.claim(bl)
          clear()
          claim_append(bl) -> void buffer::list::claim_append 要求追加, 免拷贝?
            _buffers.splice_back(bl._buffers) 拼接回来
            bl._buffers.clear_and_dispose()
        header.data_off = off
      conn->send_message(m) -> void ProtocolV2::send_message(Message *m) ssize_t RDMAConnectedSocketImpl::send
        out_queue[m->get_priority()].emplace_back
        connection->center->dispatch_event_external(connection->write_handler) -> void AsyncConnection::handle_write
          const auto out_entry = _get_next_outgoing()
          more = !out_queue.empty() 如果发送队列不为空,则more为true,表示还有更多的待发送的数据
          write_message(out_entry.m, more)
            ssize_t total_send_size = connection->outgoing_bl.length() 4406=310+4096
            connection->_try_send(more) -> cs.send(outgoing_bl, more) -> ssize_t RDMAConnectedSocketImpl::send
              size_t bytes = bl.length() 4KB:4406B=4096+310/1MB:1048886=1048576+310
              pending_bl.claim_append(bl) 换变量, bl留着干啥? 回收?
              ssize_t r = submit(more) ssize_t -> RDMAConnectedSocketImpl::submit
                pending_bl.length() 4406
                auto it = std::cbegin(pending_bl.buffers()) cbegin()和cend()是C++11新增的，它们返回一个const的迭代器，不能用于修改元素, 常量迭代器
                while (it != pending_bl.buffers().end()) 循环, 切片, 分段
                if (ib->is_tx_buffer(it->raw_c_str())) 不进该分支
                msg/async/rdma：使用 shared_ptr 管理 Infiniband obj
                1.不要使用裸指针来管理Infiniband obj
                2.直接访问Infiniband obj而不是从RDMA堆栈。 这可以避免在 RDMAWorker 和 RDMADispatcher 中缓存 RDMAStack obj
                wait_copy_len += it->length() = 32
                tx_buffers.push_back(ib->get_tx_chunk_by_buffer(it->raw_c_str()))
                size_t copied = tx_copy_chunk(tx_buffers, wait_copy_len, copy_start, it);
                total_copied += tx_copy_chunk(tx_buffers, wait_copy_len, copy_start, it) -> size_t RDMAConnectedSocketImpl::tx_copy_chunk
                  int RDMAWorker::get_reged_mem -> 获取已注册的内存 int Infiniband::get_tx_buffers -> get_send_buffers -> Infiniband::MemoryManager::Cluster::get_buffers
                    size_t got = ib->get_memory_manager()->get_tx_buffer_size() * r  131072>4406 获取到的内存满足需求的大小, 1MB, 131072*9=1179648
                  auto chunk_idx = tx_buffers.size() 9个chunk
                  Chunk *current_chunk = tx_buffers[chunk_idx] 
                  size_t real_len = current_chunk->write((char*)addr + slice_write_len, start->length() - slice_write_len) -> uint32_t Infiniband::MemoryManager::Chunk::write
                    memcpy(buffer + offset, buf, write_len) 拷贝内存(循环拷贝)
                  write_len 4406
                pending_bl.clear() 拷贝完释放pb
                post_work_request(tx_buffers)
                  tx_buffers.size() = 1
                  while (current_buffer != tx_buffers.end())
                  ibv_post_send -> ibv_poll_cq 触发发端/收端 -> int Infiniband::CompletionQueue::poll_cq <- void RDMADispatcher::polling()
      msgr->shutdown()
  stop = Cycles::rdtsc()
...
NetworkStack::add_thread
  w->center.process_events -> C_handle_read -> conn->process() -> void AsyncConnection::process()
    worker->connect(target_addr, opts, &cs) -> int RDMAWorker::connect
      ib->init()
      dispatcher->polling_start()
      new RDMAConnectedSocketImpl -> RDMAConnectedSocketImpl::RDMAConnectedSocketImpl
        read_handler(new C_handle_connection_read(this))
        established_handler(new C_handle_connection_established(this))
      p->try_connect(addr, opts) -> int RDMAConnectedSocketImpl::try_connect
        tcp_fd = net.nonblock_connect(peer_addr, opts.connect_bind_addr) -> generic_connect -> int NetHandler::generic_connect
          create_socket
          ::connect(s, addr.get_sockaddr(), addr.get_sockaddr_len()) syscall 客户端连接服务端(socket) -> 服务端触发事件(C_processor_accept) -> void Processor::accept()
          worker->center.create_file_event(tcp_fd, EVENT_READABLE | EVENT_WRITABLE , established_handler) -> established_handler -> int RDMAConnectedSocketImpl::handle_connection_established

      *socket = ConnectedSocket(std::move(csi))
    center->create_file_event(cs.fd(), EVENT_READABLE, read_handler) -> state = STATE_CONNECTING_RE -> void AsyncConnection::process() (回到process)
    ...
    case STATE_CONNECTING_RE
      cs.is_connected()
      center->create_file_event EVENT_WRITABLE read_handler -> process
      logger->tinc -> void PerfCounters::tinc 性能统计(时延统计)
    ...
    protocol->read_event() -> switch (state) -> 判断状态 -> START_ACCEPT -> run_continuation(CONTINUATION(start_server_banner_exchange)) -> 消息状态机 -> class ProtocolV2 : public Protocol 
      CONTINUATION_RUN(continuation)
      CtPtr ProtocolV2::read -> ssize_t AsyncConnection::read -> read_until
        read_bulk ->  nread = cs.read(buf, len) -> ssize_t RDMAConnectedSocketImpl::read -> eventfd_read(notify_fd, &event_val)
          read = read_buffers(buf,len) -> ssize_t RDMAConnectedSocketImpl::read_buffers
            buffer_prefetch() 预读 -> void RDMAConnectedSocketImpl::buffer_prefetch
              ibv_wc* response = &cqe[i]
              chunk->prepare_read(response->byte_len)
              buffers.push_back(chunk)
            tmp = (*pchunk)->read(buf + read_size, len - read_size) -> uint32_t Infiniband::MemoryManager::Chunk::read
              memcpy(buf, buffer + offset, read_len);
            (*pchunk)->reset_read_chunk() 将偏移和边界都置0
            dispatcher->post_chunk_to_pool(*pchunk) -> void RDMADispatcher::post_chunk_to_pool
              ib->post_chunk_to_pool(chunk)
            update_post_backlog -> void RDMAConnectedSocketImpl::update_post_backlog
            
超时处理: 
new C_handle_reap(this)
  local_worker->create_time_event( ReapDeadConnectionMaxPeriod...
    reap_dead


设备属性: device_attr
(gdb) p device_attr
$17 = {
  fw_ver = "16.33.1048", '\000' <repeats 53 times>, 
  node_guid = 8550064101420093112, 
  sys_image_guid = 8550064101420093112, 
  max_mr_size = 18446744073709551615, 
  page_size_cap = 18446744073709547520, 
  vendor_id = 713, 
  vendor_part_id = 4119, 
  hw_ver = 0, 
  max_qp = 131072, 
  max_qp_wr = 32768, 
---Type <return> to continue, or q <return> to quit---
  device_cap_flags = 3983678518, 
  max_sge = 30, 
  max_sge_rd = 30, 
  max_cq = 16777216, 
  max_cqe = 4194303, 
  max_mr = 16777216, 
  max_pd = 8388608, 
  max_qp_rd_atom = 16, 
  max_ee_rd_atom = 0, 
  max_res_rd_atom = 2097152, 
  max_qp_init_rd_atom = 16, 
---Type <return> to continue, or q <return> to quit---
  max_ee_init_rd_atom = 0, 
  atomic_cap = IBV_ATOMIC_HCA, 
  max_ee = 0, 
  max_rdd = 0, 
  max_mw = 16777216, 
  max_raw_ipv6_qp = 0, 
  max_raw_ethy_qp = 0, 
  max_mcast_grp = 2097152, 
  max_mcast_qp_attach = 240, 
  max_total_mcast_qp_attach = 503316480, 
  max_ah = 2147483647, 
---Type <return> to continue, or q <return> to quit---
  max_fmr = 0, 
  max_map_per_fmr = 0, 
  max_srq = 8388608, 
  max_srq_wr = 32767, 
  max_srq_sge = 31, 
  max_pkeys = 128, 
  local_ca_ack_delay = 16 '\020', 
  phys_port_cnt = 1 '\001'
}


qp析构(销毁qp): schedule_qp_destroy
msg/async/rdma：使用特殊的Beacon 检测SQ WRs drained 将QueuePair 切换到error 状态，然后post Beacon WR 发送队列。 所有未完成的 WQE 将被刷新到 CQ。 在 CQ 中，在销毁 QueuePair 之前，检查完成队列元素以检测 SQ WRs 是否已被耗尽。 如果不使用/不支持 SRQ，我们不会将另一个 Beacon WR 发布到 RQ，原因是只有在从 CQ 轮询了所有刷新的 WR 后，才能销毁 QueuePair。请参阅以下规范的第 474 页：InfiniBandTM 架构规范第 1 卷，版本 1.3 规范链接：https://cw.infinibandta.org/document/dl/7859

安装rdma依赖, libibverbs, 
yum install -y libibverbs-devel librdmacm-devel

yum update
yum install epel-release
yum install boost boost-thread boost-devel

centos8: 
rpm -qa|grep libibverbs
libibverbs-35.0-1.el8.x86_64
编译rdma-core: https://runsisi.com/2021/03/07/rdma-core/

版本差异: https://github.com/ceph/ceph/compare/v12.2.0...v15.2.17

TODO:
1. 3.0增加 
modify_qp_to_error
modify_qp_to_rts
modify_qp_to_rtr
内存池: msg/async/rdma: improves RX buffer management: https://github.com/ssbandjl/ceph/commit/720d044db13886ac9926d689e970381cdf78f8eb
共享接收队列 srq: https://github.com/ssbandjl/ceph/commit/282499b77f85fed50ce00c5414af12335371a4b3
修复错误事件中心被 rdma 构造连接传输 ib sync msg C_handle_connection_established: https://github.com/ssbandjl/ceph/commit/8b2a95011ca34ba3880440339693170a174034ab
schedule_qp_destroy: https://github.com/ssbandjl/ceph/commit/e907e18154421885f1b02518496694b0987ab9f9
buffer_prefetch 预读: https://github.com/ssbandjl/ceph/commit/2754d60f6615024c76f09d22d2480a9b69369a12
msg/async/rdma: deal with all RDMA device async event 补全事件处理: https://github.com/ssbandjl/ceph/commit/1c76c1320721cc555a376d7b8660c19538d3f1b4
1.列出RDMA设备的所有异步事件
2.输出致命错误事件以检查RDMA设备状态
加锁获取qp: https://github.com/ssbandjl/ceph/commit/cc08b02046ce1243926c2d716281566bd0a70402
加速 tx handle 以前 Dispatcher 线程将轮询 rx 和 tx 事件，然后调度, 这些事件传递给 RDMAWorker 和 RDMAConnectedSocketImpl: https://github.com/ssbandjl/ceph/commit/bc580b0a6100637ecbfeeecefc84e2b81ff25c34

补充rdma主要改动: 
https://ceph.io/en/news/blog/2019/v14-2-0-nautilus-released/
https://ceph.io/en/news/blog/2020/v15-2-0-octopus-released/
配置gid: https://github.com/ceph/ceph/pull/31517/files
修复内存泄漏: rdma_free_devices https://github.com/ceph/ceph/pull/27574/files
代码优化: 将连接管理数据（LID、GID、QPN、PSN）从 RDMAConnectedSocketImpl 移动到 QueuePair。 目标是 1) 简化 switch QueuePair 状态 2) 简化管理连接管理数据将 QP 切换到 Error 状态以将未完成的 WR 刷新到 CQ 并使用 Beacon WR 检测 SQD 使没有SRQ的RNIC在msg/async/rdma中工作 根据2&3中的变化细化handle_rx/tx_event&handle_async_event 根据其父类简化 RDMAIWARPConnectedSocketImpl, https://github.com/ceph/ceph/pull/29947

v16.2.0 Pacific released 太平洋
centos8: https://ceph.io/en/news/blog/2021/v16-2-0-pacific-released/

AMQP（Advanced Message Queuing Protocol，高级消息队列协议）是一个进程间传递异步消息的网络协议



client 写流程:
conn->send_message(*p)


减少状态机:
 private:
  enum {
    STATE_NONE,
    STATE_CONNECTING,
    STATE_CONNECTING_RE,
    STATE_ACCEPTING,
    STATE_CONNECTION_ESTABLISHED,
    STATE_CLOSED
  };


bench-write
先创建rbd卷: rbd create --size 1024 p1/image1 && rbd ls p1 && rbd info p1/image1
gdb --args rbd bench-write .d2.rbd/500G3 --io-size 4M --io-pattern rand --io-threads 1 --io-total 100M
/home/xb/project/stor/ceph/xb/docker/ceph/src/tools/rbd/rbd.cc main
  do_bench -> start_io aio_write2 ictx->io_work_queue->aio_write 


aio_write | rbd bench --io-type write, 建议用新接口, 参考: https://docs.ceph.com/en/quincy/man/8/rbd/
默认值为：-io-size 4096、-io-threads 16、-io-total 1G、-io-pattern seq、-rw-mix-read 50
cd build
gdb --args ./bin/rbd bench-write p1/image1 --io-size 4M --io-pattern rand --io-threads 1 --io-total 100M
src/tools/rbd/rbd.cc -> main
rbd::Shell shell
return shell.execute(argc, argv) -> int Shell::execute
  global_init
  get_command_spec
  Action *action = find_action(command_spec, &matching_spec, &is_alias)
  add_options
  (*action->execute)(vm, ceph_global_init_args) -> int execute_for_write -> return bench_execute(vm, IO_TYPE_WRITE)
    ...
    do_bench
    ...
    md_ctx.aio_flush()
      aio_write_cond.wait  <- aio_write_cond.notify_all() <- complete_aio_write

...
#0  librados::IoCtxImpl::flush_aio_writes (this=0x55555609ee70) at /home/xb/project/ceph/xb/ceph/src/librados/IoCtxImpl.cc:338
#1  0x00007ffff71edcdb in librados::v14_2_0::IoCtx::aio_flush (this=0x55555605bb28) at /home/xb/project/ceph/xb/ceph/src/librados/librados_cxx.cc:2003
#2  0x00007ffff74aa3c6 in librbd::ImageCtx::~ImageCtx (this=0x5555560594d0, __in_chrg=<optimized out>) at /home/xb/project/ceph/xb/ceph/src/librbd/ImageCtx.cc:172
#3  0x00007ffff74e3784 in librbd::ImageState<librbd::ImageCtx>::close (this=0x55555609d0b0) at /home/xb/project/ceph/xb/ceph/src/librbd/ImageState.cc:279
#4  0x00007ffff7474f2b in librbd::Image::close (this=0x7fffffffdf00) at /home/xb/project/ceph/xb/ceph/src/librbd/librbd.cc:1572
#5  0x00007ffff7474eea in librbd::Image::~Image (this=0x7fffffffdf00, __in_chrg=<optimized out>) at /home/xb/project/ceph/xb/ceph/src/librbd/librbd.cc:1562
#6  0x0000555555a7ac7d in rbd::action::bench::bench_execute (vm=..., bench_io_type=rbd::action::bench::(anonymous namespace)::IO_TYPE_WRITE) at /home/xb/project/ceph/xb/ceph/src/tools/rbd/action/Bench.cc:558
#7  0x0000555555a7af4c in rbd::action::bench::execute_for_write (vm=..., ceph_global_init_args=std::vector of length 0, capacity 0) at /home/xb/project/ceph/xb/ceph/src/tools/rbd/action/Bench.cc:563
#8  0x0000555555a599c8 in rbd::Shell::execute (this=0x7fffffffe3c7, argc=11, argv=0x7fffffffe4b8) at /home/xb/project/ceph/xb/ceph/src/tools/rbd/Shell.cc:204
#9  0x00005555559f39fc in main (argc=11, argv=0x7fffffffe4b8) at /home/xb/project/ceph/xb/ceph/src/tools/rbd/rbd.cc:9




#0  librados::IoCtxImpl::complete_aio_write (this=0x55555609e5f0, c=0x7fffb0011de0) at /home/xb/project/ceph/xb/ceph/src/librados/IoCtxImpl.cc:290
#1  0x00007ffff7225bd2 in librados::IoCtxImpl::C_aio_Complete::finish (this=0x7fffb0001e10, r=0) at /home/xb/project/ceph/xb/ceph/src/librados/IoCtxImpl.cc:1971
#2  0x00007ffff71bfc49 in Context::complete (this=0x7fffb0001e10, r=0) at /home/xb/project/ceph/xb/ceph/src/include/Context.h:77
#3  0x00007ffff726dcf3 in Objecter::handle_osd_op_reply (this=0x555555fa74e0, m=0x7fffdc000d30) at /home/xb/project/ceph/xb/ceph/src/osdc/Objecter.cc:3558
#4  0x00007ffff72586f7 in Objecter::ms_dispatch (this=0x555555fa74e0, m=0x7fffdc000d30) at /home/xb/project/ceph/xb/ceph/src/osdc/Objecter.cc:996
#5  0x00007ffff72906aa in Objecter::ms_fast_dispatch (this=0x555555fa74e0, m=0x7fffdc000d30) at /home/xb/project/ceph/xb/ceph/src/osdc/Objecter.h:2195
#6  0x00007ffff723ec3a in Dispatcher::ms_fast_dispatch2 (this=0x555555fa74e8, m=...) at /home/xb/project/ceph/xb/ceph/src/msg/Dispatcher.h:84
#7  0x00007fffed72018c in Messenger::ms_fast_dispatch (this=0x555555fa22c0, m=...) at /home/xb/project/ceph/xb/ceph/src/msg/Messenger.h:676
#8  0x00007fffed71e13a in DispatchQueue::fast_dispatch (this=0x555555fa2618, m=...) at /home/xb/project/ceph/xb/ceph/src/msg/DispatchQueue.cc:72
#9  0x00007fffed865ab3 in DispatchQueue::fast_dispatch (this=0x555555fa2618, m=0x7fffdc000d30) at /home/xb/project/ceph/xb/ceph/src/msg/DispatchQueue.h:203
#10 0x00007fffed8c4825 in ProtocolV2::handle_message (this=0x7fffb0009060) at /home/xb/project/ceph/xb/ceph/src/msg/async/ProtocolV2.cc:1479
#11 0x00007fffed8c0c09 in ProtocolV2::handle_read_frame_dispatch (this=0x7fffb0009060) at /home/xb/project/ceph/xb/ceph/src/msg/async/ProtocolV2.cc:1137
#12 0x00007fffed8c2c0b in ProtocolV2::_handle_read_frame_epilogue_main (this=0x7fffb0009060) at /home/xb/project/ceph/xb/ceph/src/msg/async/ProtocolV2.cc:1325
#13 0x00007fffed8c2a44 in ProtocolV2::handle_read_frame_epilogue_main(std::unique_ptr<ceph::buffer::v15_2_0::ptr_node, ceph::buffer::v15_2_0::ptr_node::disposer>&&, int) (this=0x7fffb0009060, 
    buffer=<unknown type in /home/xb/project/ceph/xb/ceph/build/lib/libceph-common.so.2, CU 0x2845a97, DIE 0x294b353>, r=0) at /home/xb/project/ceph/xb/ceph/src/msg/async/ProtocolV2.cc:1300
#14 0x00007fffed8ef2ba in CtRxNode<ProtocolV2>::call (this=0x7fffb0009408, foo=0x7fffb0009060) at /home/xb/project/ceph/xb/ceph/src/msg/async/Protocol.h:67
#15 0x00007fffed8b2816 in ProtocolV2::run_continuation (this=0x7fffb0009060, continuation=...) at /home/xb/project/ceph/xb/ceph/src/msg/async/ProtocolV2.cc:47
#16 0x00007fffed8bb265 in operator() (__closure=0x7fffb0006ec0, buffer=0x7fffdc003e40 "\021\002)", r=0) at /home/xb/project/ceph/xb/ceph/src/msg/async/ProtocolV2.cc:755
#17 0x00007fffed8d9d73 in std::__invoke_impl<void, ProtocolV2::read(CONTINUATION_RXBPTR_TYPE<ProtocolV2>&, rx_buffer_t&&)::<lambda(char*, int)>&, char*, long int>(std::__invoke_other, struct {...} &) (__f=...)
    at /opt/rh/devtoolset-11/root/usr/include/c++/11/bits/invoke.h:61
#18 0x00007fffed8d994f in std::__invoke_r<void, ProtocolV2::read(CONTINUATION_RXBPTR_TYPE<ProtocolV2>&, rx_buffer_t&&)::<lambda(char*, int)>&, char*, long int>(struct {...} &) (__fn=...) at /opt/rh/devtoolset-11/root/usr/include/c++/11/bits/invoke.h:111
#19 0x00007fffed8d90cb in std::_Function_handler<void(char*, long int), ProtocolV2::read(CONTINUATION_RXBPTR_TYPE<ProtocolV2>&, rx_buffer_t&&)::<lambda(char*, int)> >::_M_invoke(const std::_Any_data &, <unknown type in /home/xb/project/ceph/xb/ceph/build/lib/libceph-common.so.2, CU 0x2845a97, DIE 0x292f28b>, <unknown type in /home/xb/project/ceph/xb/ceph/build/lib/libceph-common.so.2, CU 0x2845a97, DIE 0x292f29b>) (__functor=..., 
    __args#0=<unknown type in /home/xb/project/ceph/xb/ceph/build/lib/libceph-common.so.2, CU 0x2845a97, DIE 0x292f28b>, __args#1=<unknown type in /home/xb/project/ceph/xb/ceph/build/lib/libceph-common.so.2, CU 0x2845a97, DIE 0x292f29b>)
    at /opt/rh/devtoolset-11/root/usr/include/c++/11/bits/std_function.h:290
#20 0x00007fffed86754f in std::function<void (char*, long)>::operator()(char*, long) const (this=0x7fffb0006ec0, __args#0=0x7fffdc003e40 "\021\002)", __args#1=0) at /opt/rh/devtoolset-11/root/usr/include/c++/11/bits/std_function.h:590
#21 0x00007fffed86140b in AsyncConnection::process (this=0x7fffb0006b40) at /home/xb/project/ceph/xb/ceph/src/msg/async/AsyncConnection.cc:458
#22 0x00007fffed86645e in C_handle_read::do_request (this=0x7fffb0006fb0, fd_or_id=24) at /home/xb/project/ceph/xb/ceph/src/msg/async/AsyncConnection.cc:71
#23 0x00007fffed8f3368 in EventCenter::process_events (this=0x555555fdd8d0, timeout_microseconds=30000000, working_dur=0x7fffe71e8530) at /home/xb/project/ceph/xb/ceph/src/msg/async/Event.cc:406
#24 0x00007fffed900c23 in operator() (__closure=0x555556037978) at /home/xb/project/ceph/xb/ceph/src/msg/async/Stack.cc:53
#25 0x00007fffed90278e in std::__invoke_impl<void, NetworkStack::add_thread(unsigned int)::<lambda()>&>(std::__invoke_other, struct {...} &) (__f=...) at /opt/rh/devtoolset-11/root/usr/include/c++/11/bits/invoke.h:61
#26 0x00007fffed902675 in std::__invoke_r<void, NetworkStack::add_thread(unsigned int)::<lambda()>&>(struct {...} &) (__fn=...) at /opt/rh/devtoolset-11/root/usr/include/c++/11/bits/invoke.h:111
#27 0x00007fffed90255c in std::_Function_handler<void(), NetworkStack::add_thread(unsigned int)::<lambda()> >::_M_invoke(const std::_Any_data &) (__functor=...) at /opt/rh/devtoolset-11/root/usr/include/c++/11/bits/std_function.h:290
#28 0x00007fffed9003f4 in std::function<void ()>::operator()() const (this=0x555556037978) at /opt/rh/devtoolset-11/root/usr/include/c++/11/bits/std_function.h:590
#29 0x00007fffed9003a4 in std::__invoke_impl<void, std::function<void ()>>(std::__invoke_other, std::function<void ()>&&) (__f=<unknown type in /home/xb/project/ceph/xb/ceph/build/lib/libceph-common.so.2, CU 0x2a65ef0, DIE 0x2ae0546>)
    at /opt/rh/devtoolset-11/root/usr/include/c++/11/bits/invoke.h:61
#30 0x00007fffed900359 in std::__invoke<std::function<void ()>>(std::function<void ()>&&) (__fn=<unknown type in /home/xb/project/ceph/xb/ceph/build/lib/libceph-common.so.2, CU 0x2a65ef0, DIE 0x2ae0af2>)
    at /opt/rh/devtoolset-11/root/usr/include/c++/11/bits/invoke.h:96
#31 0x00007fffed900306 in std::thread::_Invoker<std::tuple<std::function<void ()> > >::_M_invoke<0ul>(std::_Index_tuple<0ul>) (this=0x555556037978) at /opt/rh/devtoolset-11/root/usr/include/c++/11/bits/std_thread.h:253
#32 0x00007fffed9002da in std::thread::_Invoker<std::tuple<std::function<void ()> > >::operator()() (this=0x555556037978) at /opt/rh/devtoolset-11/root/usr/include/c++/11/bits/std_thread.h:260
#33 0x00007fffed9002be in std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::function<void ()> > > >::_M_run() (this=0x555556037970) at /opt/rh/devtoolset-11/root/usr/include/c++/11/bits/std_thread.h:211
#34 0x00007fffede2d7c4 in execute_native_thread_routine () from /home/xb/project/ceph/xb/ceph/build/lib/libceph-common.so.2
#35 0x00007fffeba04ea5 in start_thread () from /lib64/libpthread.so.0
#36 0x00007fffea08bb0d in clone () from /lib64/libc.so.6



客户端写, osd写, 参考
https://my.oschina.net/u/2460844/blog/534390
src/osdc/Objecter.cc
void Objecter::_op_submit
  check_for_latest_map = _calc_target(&op->target, nullptr) -> int Objecter::_calc_target

  _get_session(op->target.osd, &s, sul) -> int Objecter::_get_session
    osd_sessions.find(osd)
    OSDSession *s = new OSDSession(cct, osd) 没找到, 准备新建
    s->con = messenger->connect_to_osd(osdmap->get_addrs(osd))
    logger->set(l_osdc_osd_sessions, osd_sessions.size()) 统计会话数量
  _session_op_assign(s, op)
  _send_op(op) -> void Objecter::_send_op(Op *op) -> op->session -> con->send_message(m)


osd落盘, 主副本和从副本同步: https://my.oschina.net/u/2460844/blog/534390
osd读数据
seastar::future<> ProtocolV2::read_message
...
bool OSD::ms_dispatch
void OSD::_dispatch
void OSD::dispatch_op -> void OSD::handle_pg_create
  osdmap->get_primary_shard(on, &pgid)

crimson是crimson-osd的代号，也就是下一代ceph-osd。 它通过利用 DPDK 和 SPDK 等最先进的技术，以快速网络设备、快速存储设备为目标，以获得更好的性能。 并且它将通过 BlueStore 保留对 HDD 和低端 SSD 的支持。 Crimson 将尝试向后兼容经典 OSD

crc问题:
handle_read_frame_dispatch
handle_message
Message *decode_message
  bad crc in front
filter: *.cc, *.h, src/msg, *.sh, *.rst,    *.cc, *.h, *.hpp, *.cpp, *.c, *.py

生成uudid: uuidgen
NetworkStack::add_thread(unsigned int)::{lambda()#1}::operator()() const ()
  EventCenter::process_events(int)
    AsyncConnection::process
      decode_message(CephContext*, int, ceph_msg_header&, ceph_msg_footer&, ceph::buffer::list&, ceph::buffer::list&, ceph::buffer::list&, Connection*)
        bad crc in front

测试:
ceph_test_msgr src/test/test_msgr.cc main


前端:
发现目标
iscsiadm --mode discovery --op update --type sendtargets --portal targetIP
iscsiadm -m discovery -t sendtargets -p 175.19.53.72

创建需要的设备:
iscsiadm --mode node -l all

查看所有活动的会话
iscsiadm --mode session

目标:
查看模块(rbd,bs_rbd.so的动态链接库):
tgtadm --lld iscsi --mode system --op show|grep rbd

创建镜像: rbd create --size {megabytes} {pool-name}/{image-name}
rbd create iscsi-image --size 4096 #4096M=4G
rbd create .disk_pool1.rbd/image_100g --size 102400 && rbd ls -p .disk_pool1.rbd  #查池: rados lspools

查镜像:
rbd ls -p p0 -l --format json --pretty-format

创建tgt:
tgtadm --lld iscsi --mode target --op new --tid 1 --targetname iqn.2013-15.com.example:cephtgt.target0
查看:
tgtadm --lld iscsi --op show --mode target
tgtadm --lld iscsi --op show --mode target --tid 512

创建lun:
tgtadm --lld iscsi --mode logicalunit --op new --tid 1 --lun 1 --backing-store iscsi-image --bstype rbd
tgtadm --lld iscsi --op bind --mode target --tid 1 -I ALL

查看rbd块设备
rbd ls {poolname}

查池:
rados lspools
ceph osd lspools
ceph osd pool ls
ceph osd pool ls detail
ceph osd dump|grep pool
rados df
ceph osd pool get {pool-name} {key}; ceph osd pool get p1 all //查询所有属性
ceph df


创池, 创建池, io, pool: https://docs.ceph.com/en/reef/rados/operations/pools/
ceph osd pool create {pool-name} [{pg-num} [{pgp-num}]] [replicated] [crush-rule-name] [expected-num-objects]
ceph osd pool create {pool-name} [{pg-num} [{pgp-num}]] erasure [erasure-code-profile] [crush-rule-name] [expected_num_objects] [--autoscale-mode=<on,off,warn>]

cd build; ceph osd pool create p1 3 && ceph osd lspools && rados bench -p p1 5 write
ceph osd pool stats p1


python入口:
import pdb; pdb.set_trace()
ceph.in -> def main()
parse_cmdargs
name = 'client.admin'
maybe_daemon_command
run_in_thread -> RadosThread
run_in_thread(cluster_handle.connect
send_command_retry
  def send_command -> cluster.osd_command <- src/pybind/ceph_argparse.py
  rados_osd_command -> int librados::RadosClient::osd_command

...
extern "C" int rados_mon_command -> rados_mgr_command
extern "C" int _rados_mgr_command

osd
case MSG_OSD_PG_CREATE
handle_pg_create(op)


创池, pool_create
examples/librados/hello_world_c.c -> main
...
rados_pool_create(rados, pool_name)
rados_ioctx_create(rados, pool_name, &io_ctx)
rados_write_full(io_ctx, object_name, hello, strlen(hello))
rados_aio_create_completion2(NULL, NULL, &read_completion)
  set_complete_callback
rados_aio_read(io_ctx, object_name, read_completion, read_buf, read_len, 0)
rados_aio_wait_for_complete(read_completion)
rados_aio_get_return_value(read_completion)
rados_aio_release(read_completion)
rados_write_op_write_full(write_op, content, strlen(content)) -> 如果我们想变得非常酷，我们可以在一个原子操作中完成多项事情。 例如，我们可以更新对象的内容并同时设置版本
rados_write_op_setxattr(write_op, "version", version, strlen(version))
rados_write_op_operate(write_op, io_ctx, object_name, NULL, 0)
rados_release_write_op(write_op)
rados_create_write_op() -> 更酷的是，我们可以在写入之前确保对象看起来像我们期望的那样！ 请注意此尝试如何因 xattr 不同而失败



osd 
void OSD::_dispatch(Message *m) -> 
handle_command
  cct->get_admin_socket()->queue_tell_command(m)
...
void AdminSocket::entry()
  ret = poll(fds, 2, -1)
  void AdminSocket::do_tell_queue()
    execute_command -> call_async -> osd->asok_command
      pg->do_command



rbd块设备: https://docs.ceph.com/en/quincy/rbd/rados-rbd-cmds/
该rbd命令使您能够创建、列出、内省和删除块设备映像。您还可以使用它来克隆镜像、创建快照、将镜像回滚到快照、查看快照等。有关使用该命令的详细信息，rbd请参阅RBD – 管理 RADOS 块设备 (RBD) 镜像: https://docs.ceph.com/en/quincy/man/8/rbd/
rbd 是一个用于操作 rados 块设备 (RBD) 映像的实用程序，由 Linux rbd 驱动程序和 QEMU/KVM 的 rbd 存储驱动程序使用。 RBD image是简单的块设备，在对象上进行条带化并存储在 RADOS 对象存储中。 image条带对象的大小必须是 2 的幂

创建卷/image
rbd create --size 1024 p1/image1 && rbd ls p1 && rbd info p1/image1



下发io:
rados bench -p .disk_pool1.rbd 5 write

rados put test-object-1 ../src/vstart.sh --pool=test-blkin
rados -p test-blkin ls
ceph osd map test-blkin test-object-1
rados get test-object-1 ./vstart-copy.sh --pool=test-blkin
md5sum vstart*
rados rm test-object-1 --pool=test-blkin

存储池启用rbd:
ceph osd pool application enable wgsrbd rbd

初始化rbd:
rbd pool init -p wgsrbd
下一步就可创建镜像

客户端映射镜像:
rbd -p wgsrbd map wgs-img1

编译rpm包:
安装工具: yum install rpm-build rpmdevtools -y
make-dist, 该脚本主要用于生成ceph.spec和ceph-*.tar.bz2文件以供后面打rpm包使用。我们主要改变了rpm_version,rpm_release ，是否下载boost库等
bash make-srpm.sh build
bash make-srpm.sh clean

tar --strip-components=1 -C ~/rpmbuild/SPECS/ --no-anchored -xvjf ~/rpmbuild/SOURCES/ceph-10.2.11.tar.bz2 "ceph.spec"
生成目录:
rpmdev-setuptree
ls -alh /root/rpmbuild
tree rpmbuild/SRPMS/
rpmbuild -ba ~/rpmbuild/SPECS/ceph.spec
tree  rpmbuild/RPMS/
%debug_package
%prep

daos参考:
%if (0%{?suse_version} > 0)
%global __debug_package 1
%global _debuginfo_subpackages 0
%debug_package
%endif

%prep
%autosetup

默认目录: /root/rpmbuild
修改目录: –buildroot xxx
或 Specify the topdir parameter in the rpmrc file or rpmmacros file
rpmbuild --root /home/rpmbuild -ta driver.tar.gz

create a file called .rpmmacros in your %HOME% dir, add the following to the file "%_topdir x:/rpmbuild" (w/o the quotes)
.rpmmacros
%packager YourName
%_topdir /home/build/rpmbuild 
%_tmppath /home/build/rpmbuild/tmp

单元测试:
ctest -V -R client

rpm:
https://download.ceph.com/rpm-15.2.17/el7/x86_64/
https://mirrors.cloud.tencent.com/ceph/rpm-15.2.17/el7/SRPMS/ceph-15.2.17-0.el7.src.rpm

编译:
参考命令
安装gcc
git clone [https://github.com/gcc-mirror/gcc.git](https://github.com/gcc-mirror/gcc.git)
mkdir build ; cd build
../configure --prefix=/usr --disable-multilib 
yum install -y gmp-devel libmpc-devel mpfr-devel flex
make -j16 > log 2>&1 && make install >log 2>&1 &

编译前检查:
make check

安装依赖包:
yum -y install python36-Cython

编译, 统计cpu性能
./do-cmake.sh -DCMAKE_CXX_FLAGS="-fno-omit-frame-pointer -O2 -g"
cd build
cmake --build .

rbd接口使用:
examples/librbd/hello_world.cc
make
gdb --args ./hello_world_cpp --conf=/home/xb/project/ceph/xb/ceph/build/ceph.conf
rados.init("admin") -> int librados::Rados::init(const char * const id) -> extern "C" int _rados_create
  ...
rados.conf_parse_argv(argc, argv)




h3c:
Processor::accept


技术检查:
src/script/ceph-debug-docker.sh v15.2.17

cmake .. \
-DPYTHON_INCLUDE_DIR=$(python3 -c "import sysconfig; print(sysconfig.get_path('include'))")  \
-DPYTHON_LIBRARY=$(python3 -c "import sysconfig; print(sysconfig.get_config_var('LIBDIR'))")

message("python3头文件目录：" ${Python3_INCLUDE_DIRS})
message("python3的版本信息：" ${Python3_VERSION})
message("python3的库文件信息：" ${Python3_LIBRARIES})

file(
  DOWNLOAD
  "${url}" "/home/xb/project/stor/ceph/xb/docker/ceph/build/boost/src/boost_1_72_0.tar.bz2"

../src/vstart.sh -d -n -l -e -o "osd_tracing = true"


Timed out waiting for lttng-sessiond (in lttng_ust_init() at lttng-ust-comm.c:1444)

deps install
$SUDO $yumdnf install -y $yumdnf-utils

yum install lttng-ust-devel
yum install epel-release
rm -rf build;clear;./do_cmake.sh
分析代码技术错误,  analyze Teuthology failures
src/script/ceph-debug-docker.sh v15.2.17

虚拟集群:
cd build && OSD=3 MON=3 MGR=3 ../src/vstart.sh -n -x
# check that it's there
bin/ceph health

rm -rf build;clear;./do_cmake.sh && cd build && make -j64
rpm -ivh python3-devel-3.6.8-18.el7.x86_64.rpm
ceph.spec prometheus
ceph.spec.in
yum-builddep -y --setopt=*.skip_if_unavailable=true /tmp/install-deps.3292027/ceph.spec 2>&1 | tee /tmp/install-deps.3292027/yum-builddep.out



echo "182.200.53.62 c62" >> /etc/hosts


lttng:
yum install -y epel-release
yum install -y lttng-tools lttng-ust

qa:
1. 识别大IO, 大块IO(4K, 512K, 1M...)在哪里进行切分? 就在那里做RDMA单边逻辑
2. 

release cpp demo, release.cc, release.cpp
// unique_ptr::release example
#include <iostream>
#include <memory>

int main () {
  std::unique_ptr<int> auto_pointer (new int);
  int * manual_pointer;

  *auto_pointer=10;

  manual_pointer = auto_pointer.release();
  // (auto_pointer is now empty)

  std::cout << "manual_pointer points to " << *manual_pointer << '\n';

  delete manual_pointer;

  return 0;
}

ptr* _carriage(运输): rack bufferptr 我们可以修改（特别是 ::append() 到）。 并非所有 bptrs 缓冲区列表都具有此特性——如果有人 ::push_back(const ptr&)，他希望它不会改变

1MB, WR:
502       if (ibv_post_send(qp->get_qp(), iswr, &bad_tx_work_request)) {
(gdb) p iswr
$59 = {{
    wr_id = 93825015914456, 
    next = 0x0, 
    sg_list = 0x7fffe4af2be0, 
    num_sge = 1, 
    opcode = IBV_WR_SEND, 
    send_flags = 2, 
    {
      imm_data = 0, 
      invalidate_rkey = 0
    }, 
    wr = {
      rdma = {
        remote_addr = 0, 
        rkey = 0
      }, 
      atomic = {
        remote_addr = 0, 
        compare_add = 0, 
        swap = 0, 
        rkey = 0
      }, 
      ud = {
        ah = 0x0, 
        remote_qpn = 0, 
        remote_qkey = 0
      }
    }, 
    qp_type = {
      xrc = {
        remote_srqn = 0
      }
    }, 
    {
      bind_mw = {
        mw = 0x0, 
        rkey = 0, 
        bind_info = {
          mr = 0x0, 
          addr = 0, 
          length = 0, 
          mw_access_flags = 0
        }
      }, 
      tso = {
        hdr = 0x0, 
        hdr_sz = 0, 
        mss = 0
      }
    }
  }}
(gdb) 

(gdb) p iswr[0].sg_list
$79 = (ibv_sge *) 0x7fffe4af2be0
(gdb) p * (ibv_sge *) 0x7fffe4af2be0
$80 = {
  addr = 93825150820352, 
  length = 26, 
  lkey = 33802381
}
(gdb) 





qingyun, brpc,  ucx, libfabric(cart), ceph_rdma, 




rbd_write -> 
参考堆栈:
#0  Objecter::_op_submit (this=this@entry=0x9505c0, op=op@entry=0x997310, sul=..., ptid=ptid@entry=0x9971c0) at /block/gouxu/ceph-L/src/osdc/Objecter.cc:6185
#1  0x00007ffff74a5ed6 in Objecter::_op_submit_with_budget (this=this@entry=0x9505c0, op=op@entry=0x997310, sul=..., ptid=ptid@entry=0x9971c0, ctx_budget=ctx_budget@entry=0x0) at /block/gouxu/ceph-L/src/osdc/Objecter.cc:5878
#2  0x00007ffff74a6c19 in Objecter::op_submit (this=0x9505c0, op=0x997310, ptid=0x9971c0, ctx_budget=0x0) at /block/gouxu/ceph-L/src/osdc/Objecter.cc:5797
#3  0x00007ffff742c229 in librados::IoCtxImpl::aio_operate_read (this=0x98e440, oid=..., o=0x996de0, c=0x9970e0, flags=flags@entry=0, pbl=pbl@entry=0x984708, trace_info=trace_info@entry=0x0, vae_sparse_flag=vae_sparse_flag@entry=false)
    at /block/gouxu/ceph-L/src/librados/IoCtxImpl.cc:993
#4  0x00007ffff73ec78f in librados::IoCtx::aio_operate (this=0x994ed8, oid=..., c=c@entry=0x997290, o=o@entry=0x7fffffffd830, pbl=pbl@entry=0x984708) at /block/gouxu/ceph-L/src/librados/librados.cc:1611
#5  0x00007ffff79212ae in librbd::image::OpenRequest<librbd::ImageCtx>::send_v2_detect_header (this=this@entry=0x9846f0) at /block/gouxu/ceph-L/src/librbd/image/OpenRequest.cc:98
#6  0x00007ffff7921465 in librbd::image::OpenRequest<librbd::ImageCtx>::send (this=this@entry=0x9846f0) at /block/gouxu/ceph-L/src/librbd/image/OpenRequest.cc:44
#7  0x00007ffff787c20f in librbd::ImageState<librbd::ImageCtx>::send_open_unlock (this=0x984100) at /block/gouxu/ceph-L/src/librbd/ImageState.cc:626
#8  0x00007ffff788232f in librbd::ImageState<librbd::ImageCtx>::open (this=this@entry=0x984100, skip_open_parent=skip_open_parent@entry=false, on_finish=on_finish@entry=0x7fffffffdc80, 
    enSpliAgentInit=enSpliAgentInit@entry=librbd::EN_SPLIT_FLAG_TRUE) at /block/gouxu/ceph-L/src/librbd/ImageState.cc:284
#9  0x00007ffff78826a4 in librbd::ImageState<librbd::ImageCtx>::open (this=0x984100, skip_open_parent=skip_open_parent@entry=false, enSpliAgentInit=enSpliAgentInit@entry=librbd::EN_SPLIT_FLAG_TRUE)
    at /block/gouxu/ceph-L/src/librbd/ImageState.cc:252
#10 0x00007ffff78608d3 in rbd_open (p=<optimized out>, name=<optimized out>, image=0x7fffffffdf20, snap_name=<optimized out>) at /block/gouxu/ceph-L/src/librbd/librbd.cc:4604
#11 0x0000000000400cc7 in main (argc=1, argv=0x7fffffffe098) at rbd_write.c:120




---------------------------------------- DL ----------------------------------------
Ceph数据存储2-rbd client 端的数据请求处理 转载
Darren_Wen2020-05-15 18:14:10博主文章分类：ceph研发
文章标签rbd clientIO流程数据请求文章分类软件研发阅读数1507
本节讲述数据写操作的生命开始，介绍写操作的流程处理。代码参考0.94.x版本；
首先看一下我们用python调用librbd 写rbd设备的测试代码：
#!/usr/bin/env python
import sys,rados,rbd

def connectceph():
      cluster = rados.Rados(conffile = '/root/xuyanjiangtest/ceph-0.94.3/src/ceph.conf')
      cluster.connect()
      ioctx = cluster.open_ioctx('mypool')
      rbd_inst = rbd.RBD()
      size = 4*1024**3 #4 GiB
      rbd_inst.create(ioctx,'myimage',size)
      image = rbd.Image(ioctx,'myimage')
      data = 'foo'* 200
      image.write(data,0)
      image.close()
      ioctx.close()
      cluster.shutdown()
 
if __name__ == "__main__":
        connectceph()

一、写操作数据request的孕育过程
在write request 请求开始之前，它需要准备点旅行的用品，往返的机票等。下面先看看前期准备了什么。
首先cluster = rados.Rados(conffile = ‘XXXX/ceph.conf’)，用当前的这个ceph的配置文件去创建一个rados，这里主要是解析ceph.conf中写明的参数。然后将这些参数的值保存在rados中。
cluster.connect() ，这里将会创建一个radosclient的结构，这里会把这个结构主要包含了几个功能模块：消息管理模块Messager，数据处理模块Objector，finisher线程模块。这些模块具体的工作后面讲述。
ioctx = cluster.open_ioctx(‘mypool’)，为一个名字叫做mypool的存储池创建一个ioctx ，ioctx中会指明radosclient与Objector模块，同时也会记录mypool的信息，包括pool的参数等。
rbd_inst.create(ioctx,‘myimage’,size) ，创建一个名字为myimage的rbd设备，之后就是将数据写入这个设备。
image = rbd.Image(ioctx,‘myimage’)，创建image结构，这里该结构将myimage与ioctx 联系起来，后面可以通过image结构直接找到ioctx。这里会将ioctx复制两份，分为为data_ioctx和md_ctx。见明知意，一个用来处理rbd的存储数据，一个用来处理rbd的管理数据。
通过上面的操作就会形成这样的结构(如下图)
Ceph数据存储2-rbd client 端的数据请求处理_IO流程
图1-1 request孕育阶段
过程描述，首先根据配置文件创建一个rados，接下来为这个rados创建一个radosclient，radosclient包含了3个主要模块(finisher,Messager，Objector)。再根据pool创建对应的ioctx，ioctx中能够找到radosclient。再对生成对应rbd的结构image，这个image中复制了两个ioctx，分别成为了md_ioctx与data_ioctx。这时完全可以根据image入口去查找到前期准备的其他数据结构。接下来的数据操作完全从image开始，也是rbd的具体实例。
二、request的出生和成长。
image.write(data,0)，通过image开始了一个写请求的生命的开始。这里指明了request的两个基本要素 buffer=data 和 offset=0。由这里开始进入了ceph的世界，也是c++的世界。
由image.write(data,0) 转化为librbd.cc 文件中的 Image::write 函数，来看看这个函数的主要实现
ssize_t Image::write(uint64_t ofs, size_t len, bufferlist& bl)
  {  
      //…………………
   ImageCtx *ictx = (ImageCtx *)ctx;
    int r = librbd::write(ictx, ofs, len, bl.c_str(), 0); -> int r = ictx->io_work_queue->write(ofs, len, bufferlist{bl}, 0)
    return r;     
  }

该函数中直接进行分发给了 librbd::write 的函数了。跟随下来看看 librbd::write 中的实现。该函数的具体实现在internal.cc文件中。
ssize_t write(ImageCtx *ictx, uint64_t off, size_t len, const char *buf, int op_flags)
 {
      ……………
    Context *ctx = new C_SafeCond(&mylock, &cond, &done, &ret);   //---a
    AioCompletion *c = aio_create_completion_internal(ctx, rbd_ctx_cb);//---b
   r = aio_write(ictx, off, mylen, buf, c, op_flags);  //---c
     ……………     
    while (!done)
           cond.Wait(mylock);  // ---d
      ……………
}

—a.这句要为这个操作申请一个回调操作，所谓的回调就是一些收尾的工作，信号唤醒处理。
—b。这句是要申请一个io完成时 要进行的操作，当io完成时，会调用rbd_ctx_cb函数，该函数会继续调用ctx->complete（）。
—c.该函数aio_write会继续处理这个请求。
—d.当c句将这个io下发到osd的时候，osd还没请求处理完成，则等待在d上，直到底层处理完请求，回调b申请的 AioCompletion, 继续调用a中的ctx->complete（），唤醒这里的等待信号，然后程序继续向下执行。
3.再来看看aio_write 拿到了 请求的offset和buffer会做点什么呢？
int aio_write(ImageCtx *ictx, uint64_t off, size_t len, const char *buf,
           AioCompletion *c, int op_flags)
  {
      ………
      //将请求按着object进行拆分
      vector<ObjectExtent> extents;
     if (len > 0) 
      {
         Striper::file_to_extents(ictx->cct, ictx->format_string,
                        &ictx->layout, off, clip_len, 0, extents);   //---a
      } 
      //处理每一个object上的请求数据
      for (vector<ObjectExtent>::iterator p = extents.begin(); p != extents.end(); ++p) 
      {
               ……..
           C_AioWrite *req_comp = new C_AioWrite(cct, c); //---b
           ……..
           AioWrite *req = new AioWrite(ictx, p->oid.name, p->objectno, p- >offset,bl,….., req_comp);     //---c
           r = req->send();    //---d
           …….
      }
      ……
}

根据请求的大小需要将这个请求按着object进行划分，由函数file_to_extents进行处理，处理完成后按着object进行保存在extents中。file_to_extents()存在很多同名函数注意区分。这些函数的主要内容做了一件事儿，那就对原始请求的拆分。
一个rbd设备是有很多的object组成，也就是将rbd设备进行切块，每一个块叫做object，每个object的大小默认为4M，也可以自己指定。file_to_extents函数将这个大的请求分别映射到object上去，拆成了很多小的请求如下图。最后映射的结果保存在ObjectExtent中。
Ceph数据存储2-rbd client 端的数据请求处理_rbd client_02
原本的offset是指在rbd内的偏移量(写入rbd的位置)，经过file_to_extents后，转化成了一个或者多个object的内部的偏移量offset0。这样转化后处理一批这个object内的请求。
4.再回到 aio_write函数中，需要将拆分后的每一个object请求进行处理。
—b.为写请求申请一个回调处理函数。
—c.根据object内部的请求，创建一个叫做AioWrite的结构。
—d.将这个AioWrite的req进行下发send().
5.这里AioWrite 是继承自 AbstractWrite ，AbstractWrite 继承自AioRequest类，在AbstractWrite 类中定义了send的方法，看下send的具体内容.
int AbstractWrite::send() 
 {  ………………
    if (send_pre())           //---a
      ……………
}
#进入send_pre()函数中
bool AbstractWrite::send_pre()
｛
      m_state = LIBRBD_AIO_WRITE_PRE;   // ----a
      FunctionContext *ctx =    //----b
           new FunctionContext( boost::bind(&AioRequest::complete, this, _1));
      m_ictx->object_map.aio_update（ctx）; //-----c
｝

—a.修改m_state 状态为LIBRBD_AIO_WRITE_PRE。
—b.申请一个回调函数，实际调用AioRequest::complete()
—c.开始下发object_map.aio_update的请求，这是一个状态更新的函数，不是很重要的环节，这里不再多说，当更新的请求完成时会自动回调到b申请的回调函数。
6.进入到AioRequest::complete() 函数中。
void AioRequest::complete(int r)
 {
    if (should_complete(r))   //---a
        …….
}

—a.should_complete函数是一个纯虚函数，需要在继承类AbstractWrite中实现，来7. 看看AbstractWrite:: should_complete()
bool AbstractWrite::should_complete(int r)
{
    switch (m_state) 
  {
          case LIBRBD_AIO_WRITE_PRE:  //----a
      {
                     send_write(); //----b

----a.在send_pre中已经设置m_state的状态为LIBRBD_AIO_WRITE_PRE，所以会走这个分支。
----b. send_write()函数中，会继续进行处理，
7.1.下面来看这个send_write函数
void AbstractWrite::send_write()
｛
      m_state = LIBRBD_AIO_WRITE_FLAT;   //----a
      add_write_ops(&m_write);    // ----b
      int r = m_ictx->data_ctx.aio_operate(m_oid, rados_completion, &m_write);
｝

—a.重新设置m_state的状态为 LIBRBD_AIO_WRITE_FLAT。
—b.填充m_write，将请求转化为m_write。
—c.下发m_write ，使用data_ctx.aio_operate 函数处理。继续调用io_ctx_impl->aio_operate()函数，继续调用objecter->mutate().
//librados.cc
librados::IoCtx::aio_operate(const std::string& oid, AioCompletion *c,
				 librados::ObjectWriteOperation *o,
				 snap_t snap_seq, std::vector<snap_t>& snaps)
{
  return io_ctx_impl->aio_operate(obj, (::ObjectOperation*)o->impl, c->pc,
				  snapc, 0);
}
//IoCtxImpl.cc
librados::IoCtxImpl::aio_operate(......)
{
c->tid = objecter->mutate(oid, oloc, *o, snap_context, ut, flags, onack, oncommit,
		            &c->objver);
}

8.objecter->mutate()
ceph_tid_t mutate(……..) 
  {
    Op *o = prepare_mutate_op(oid, oloc, op, snapc, mtime, flags, onack, oncommit, objver);   //----d
    return op_submit(o);
  }

—d.将请求转化为Op请求，继续使用op_submit下发这个请求。在op_submit中继续调用_op_submit_with_budget处理请求。继续调用_op_submit处理。
8.1 _op_submit 的处理过程。这里值得细看
ceph_tid_t Objecter::_op_submit(Op *op, RWLock::Context& lc)
｛
    check_for_latest_map = _calc_target(&op->target, &op->last_force_resend)； //---a
    int r = _get_session(op->target.osd, &s, lc);  //---b
    _session_op_assign(s, op); //----c
    _send_op(op, m); //----d
｝

----a. _calc_target，通过计算当前object的保存的osd，然后将主osd保存在target中，rbd写数据都是先发送到主osd，主osd再将数据发送到其他的副本osd上。这里对于怎么来选取osd集合与主osd的关系就不再多说，在《ceph的数据存储之路(3)》中已经讲述这个过程的原理了，代码部分不难理解。
----b. _get_session，该函数是用来与主osd建立通信的，建立通信后，可以通过该通道发送给主osd。再来看看这个函数是怎么处理的
9._get_session
int Objecter::_get_session(int osd, OSDSession **session, RWLock::Context& lc)
{
    map<int,OSDSession*>::iterator p = osd_sessions.find(osd);   //----a
    OSDSession *s = new OSDSession(cct, osd); //----b
    osd_sessions[osd] = s;//--c
    s->con = messenger->get_connection(osdmap->get_inst(osd));//-d
    ………
｝

----a.首先在osd_sessions中查找是否已经存在一个连接可以直接使用，第一次通信是没有的。
----b.重新申请一个OSDSession，并且使用osd等信息进行初始化。
—c. 将新申请的OSDSession添加到osd_sessions中保存，以备下次使用。
----d.调用messager的get_connection方法。在该方法中继续想办法与目标osd建立连接。
10.messager 是由子类simpleMessager实现的，下面来看下SimpleMessager中get_connection的实现方法
ConnectionRef SimpleMessenger::get_connection(const entity_inst_t& dest)
{
    Pipe *pipe = _lookup_pipe(dest.addr);     //-----a
    if (pipe) 
    {
        ……
    } 
    else 
    {
      pipe = connect_rank(dest.addr, dest.name.type(), NULL, NULL); //----b
    }
｝

—a.首先要查找这个pipe，第一次通信，自然这个pipe是不存在的。
----b. connect_rank 会根据这个目标osd的addr进行创建。看下connect_rank做了什么。
11.SimpleMessenger::connect_rank
Pipe *SimpleMessenger::connect_rank(const entity_addr_t& addr,  int type, PipeConnection *con,    Message *first)
｛
      Pipe *pipe = new Pipe(this, Pipe::STATE_CONNECTING, static_cast<PipeConnection*>(con));      //----a
      pipe->set_peer_type(type); //----b
      pipe->set_peer_addr(addr); //----c
      pipe->policy = get_policy(type); //----d
      pipe->start_writer();  //----e
      return pipe; //----f
｝

----a.首先需要创建这个pipe，并且pipe同pipecon进行关联。
----b,----c,-----d。都是进行一些参数的设置。
----e.开始启动pipe的写线程，这里pipe的写线程的处理函数pipe->writer(),该函数中会尝试连接osd。并且建立socket连接通道。
目前的资源统计一下，写请求可以根据目标主osd，去查找或者建立一个OSDSession，这个OSDSession中会有一个管理数据通道的Pipe结构，然后这个结构中存在一个发送消息的处理线程writer，这个线程会保持与目标osd的socket通信。
12.建立并且获取到了这些资源，这时再回到_op_submit 函数中
ceph_tid_t Objecter::_op_submit(Op *op, RWLock::Context& lc)
｛
    check_for_latest_map = _calc_target(&op->target, &op->last_force_resend)； //---a
    int r = _get_session(op->target.osd, &s, lc);  //---b
    _session_op_assign(s, op); //----c
    MOSDOp *m = _prepare_osd_op(op); //-----d
    _send_op(op, m); //----e
｝

—c，将当前的op请求与这个session进行绑定，在后面发送请求的时候能知道使用哪一个session进行发送。
–d，将op转化为MOSDop，后面会以MOSDOp为对象进行处理的。
—e，_send_op 会根据之前建立的通信通道，将这个MOSDOp发送出去。_send_op 中调用op->session->con->send_message(m)，这个方法会调用SimpleMessager-> send_message(m), 再调用_send_message(),再调用submit_message().在submit_message会找到之前的pipe，然后调用pipe->send方法，最后通过pipe->writer的线程发送到目标osd。
自此，客户就等待osd处理完成返回结果了。
总结客户端的所有流程和数据结构，下面来看下客户端的所有结构图。
Ceph数据存储2-rbd client 端的数据请求处理_数据请求_03
通过这个全部的结构图来总结客户端的处理过程。
1.看左上角的rados结构，首先创建io环境，创建rados信息，将配置文件中的数据结构化到rados中。
2.根据rados创建一个radosclient的客户端结构，该结构包括了三个重要的模块，finiser 回调处理线程、Messager消息处理结构、Objector数据处理结构。最后的数据都是要封装成消息 通过Messager发送给目标的osd。
3.根据pool的信息与radosclient进行创建一个ioctx，这里面包好了pool相关的信息，然后获得这些信息后在数据处理时会用到。
4.紧接着会复制这个ioctx到imagectx中，变成data_ioctx与md_ioctx数据处理通道，最后将imagectx封装到image结构当中。之后所有的写操作都会通过这个image进行。顺着image的结构可以找到前面创建并且可以使用的数据结构。
5.通过最右上角的image进行读写操作，当读写操作的对象为image时，这个image会开始处理请求，然后这个请求经过处理拆分成object对象的请求。拆分后会交给objector进行处理查找目标osd，当然这里使用的就是crush算法，找到目标osd的集合与主osd。
6.将请求op封装成MOSDOp消息，然后交给SimpleMessager处理，SimpleMessager会尝试在已有的osd_session中查找，如果没有找到对应的session，则会重新创建一个OSDSession，并且为这个OSDSession创建一个数据通道pipe，把数据通道保存在SimpleMessager中，可以下次使用。
7.pipe 会与目标osd建立Socket通信通道，pipe会有专门的写线程writer来负责socket通信。在线程writer中会先连接目标ip，建立通信。消息从SimpleMessager收到后会保存到pipe的outq队列中，writer线程另外的一个用途就是监视这个outq队列，当队列中存在消息等待发送时，会就将消息写入socket，发送给目标OSD。
8.等待OSD将数据消息处理完成之后，就是进行回调，反馈执行结果，然后一步步的将结果告知调用者。
上面是就rbd client处理写请求的过程，那么下面会在分析一个OSD是如何接到请求，并且怎么来处理这个请求的。请期待下一节。
转载： https://my.oschina.net/u/2460844/blog/532755


---------------------------------------- DL ----------------------------------------



rbd_write.c:145 -> main -> rbd_write
  librbd::ImageCtx *ictx = (librbd::ImageCtx *)image
  bl.push_back(create_write_raw(ictx, buf, len, nullptr))
  ictx->io_work_queue->write(ofs, len, std::move(bl), 0)





sphinx-build
yum install python-sphinx


/opt/h3c/lib/librbd.so.1 -> librbd.so.1.12.0
[root@node1 ceph]# ls -alh /opt/h3c/lib/librados.so*
lrwxrwxrwx 1 root root   13 Jul 21 16:23 /opt/h3c/lib/librados.so -> librados.so.2
lrwxrwxrwx 1 root root   17 Jul 21 16:23 /opt/h3c/lib/librados.so.2 -> librados.so.2.0.0
-rwxr-xr-x 1 root root 2.4M Jul 21 16:23 /opt/h3c/lib/librados.so.2.0.0

[root@node1 ceph]# ls -alh /opt/h3c/lib/librbd*
lrwxrwxrwx 1 root root   11 Jul 21 16:23 /opt/h3c/lib/librbd.so -> librbd.so.1
lrwxrwxrwx 1 root root   16 Jul 21 16:23 /opt/h3c/lib/librbd.so.1 -> librbd.so.1.12.0
-rwxr-xr-x 1 root root 4.3M Jul 21 16:23 /opt/h3c/lib/librbd.so.1.12.0
lrwxrwxrwx 1 root root   14 Jul 21 16:23 /opt/h3c/lib/librbd_tp.so -> librbd_tp.so.1
lrwxrwxrwx 1 root root   18 Jul 21 16:23 /opt/h3c/lib/librbd_tp.so.1 -> librbd_tp.so.1.0.0
-rwxr-xr-x 1 root root 468K Jul 21 16:23 /opt/h3c/lib/librbd_tp.so.1.0.0

---------------------------------------- DL ----------------------------------------
参考文档, 深入理解ceph crush(3)—Object至PG映射源码分析: https://www.dovefi.com/post/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3crush3object%E8%87%B3pg%E6%98%A0%E5%B0%84%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/
iopath, cd /home/xb/project/stor/ceph/xb/docker/ceph/test/librados_example; ./gdb_rados_write.sh
rados_write.c:83 -> main
  rados_create2(&cluster, cluster_name, user_name, flags) -> extern "C" int _rados_create2 -> rados_create 的扩展版本，与 rados_create 类似，但不假设 client + id，允许完整指定名称允许指定集群名称标志以供将来扩展
    CephInitParameters iparams(CEPH_ENTITY_TYPE_CLIENT) -> CephInitParameters::CephInitParameters -> name.set(module_type, "admin") -> librados、rados.py：添加rados_create2/init2、librados客户端，特别是ceph工具，需要能够指定完整的“名称”； rados_create 强制执行“client.<param>”，没有解决方法。 新界面。 Python Rados().__init__ 根据是否提供名称或 id 选择适当的创建函数
      name.set(module_type, "admin") -> module_type=CEPH_ENTITY_TYPE_CLIENT(0x8)
    rados_create_cct -> static CephContext *rados_create_cct -> 创建上下文
      common_preinit CODE_ENVIRONMENT_LIBRARY -> CephContext *common_preinit -> 预初始化公共部分, 注意：如果您正在编写 Ceph 守护进程，请忽略此函数并调用 global_init 来代替。 它会为您调用 common_preinit。 common_preinit 创建 CephContext。 该函数为您提供 CephContext 后，您需要设置 Ceph 配置，该配置位于 CephContext 内，名为 md_config_t。 初始设置不是很有用，因为它们没有反映用户的要求。 这通常是通过以下方式完成的：cct->_conf.parse_env(); cct->_conf.apply_changes(); 您的库还可能提供读取配置文件的函数
        注释良性种族大小, helgrind：注释假阳性竞争条件
        new CephContext -> class CephContext -> 实例化ceph上下文
          _log = new ceph::logging::Log(&_conf->subsys) -> 日志实例化
          _log_obs = new LogObs(_log) -> 观察日志记录配置更改，日志记录子系统位于大多数 ceph 代码（包括配置子系统）下方，以保持简单且独立。 将与日志记录相关的配置更改馈送到日志中
          _conf.add_observer(_log_obs) -> common,rbd,rgw,osd：将配置值提取到 ConfigValues 中，此更改引入了三个类：ConfigValues、ConfigProxy 和 ConfigReader。 在OSD的seastar端口中，每个CPU分片将保存自己的配置参考，并且在设置更改时，每个分片将使用异步的新设置进行更新。 所以这迫使我们能够同时保留两组配置。 所以我们需要将md_config_t中可更改的部分提取出来。 因此我们可以根据需要用新的替换旧的，并让不同的分片共享相同的未更改部分，以及选项映射和查找表等其他内容。 这就是我们需要 ConfigValues 的原因。 我们将为此类添加一个策略模板，这样我们就可以专门用于 Seastar 实现，以允许不同的 ConfigProxy 实例将 md_config_impl<> 指向不同的 ConfigValues。 由于观察者接口仍然使用md_config_t，为了尽量减少此更改的影响，handle_conf_change()和handle_subsys_change()没有改变。 但由于它接受一个 `const md_config_t`，它不能用于创建/引用持有它的 ConfigProxy，我们需要引入 ConfigReader 来以更简单的方式从 md_config_t 读取更新的设置，而不暴露内部“values”成员变量
            obs_mgr.add_observer(obs) -> 我们可以将实现放在 .cc 文件中，并且仅显式实例化所使用的模板专业化，但这迫使我们在编译时涉及未使用的标头和库。 例如，为了实例化，要实例化seastar的ObserverMgr，我们需要包含seastar标头以获取必要的类型，但这将迫使我们将非seastar二进制文件链接到seastar库。 因此，为了避免以增加编译时间为代价引入未使用的依赖项，我们将实现放在头文件中
              const char **keys = observer->get_tracked_conf_keys() -> 获取日志所有跟踪的键 -> "log_coarse_timestamps" ...
              observers.emplace(*k, observer) -> 将日志键加入到观察服务器中
            obs_call_gate.emplace(obs, std::make_unique<CallGate>()) -> 开关门机制? -> config: drop config_proxy::lock 当调用配置观察者时为了防止当观察者获取自己的锁（锁定顺序：config_proxy::lock -> foo::lock）并且另一个线程（比如IO路径）尝试获取配置值时发生死锁（ 锁定顺序：foo:lock -> config_proxy::lock)。 调用配置观察者时释放锁的副作用是，当观察者仍在执行时，remove_observer() 可能会潜入，导致释放后使用。 为了缓解这种情况，任何正在进行的观察者调用都需要在删除观察者之前完成。 此外，需要在不持有任何观察者锁的情况下调用remove_observer()，以免陷入死锁
          _cct_obs = new CephContextObs(this) -> enable_experimental_unrecoverable_data_corrupting_features | crush_location | container_image
          _lockdep_obs = new LockdepObs(this) -> lockdep
          _perf_counters_collection = new PerfCountersCollection(this) -> 实例化性能计数器收集器
          _admin_socket = new AdminSocket(this) -> 实例化asok
          _heartbeat_map = new HeartbeatMap(this) -> 心跳
          _plugin_registry = new PluginRegistry(this) -> 注册插件, EC?
          _admin_hook = new CephContextHook(this) -> asok 实例化管理套接字钩子
          _admin_socket->register_command("assert", _admin_hook, "") -> int AdminSocket::register_command
            cmddesc_get_prefix(cmddesc)
            hooks.find(prefix)
            hooks.emplace_hint -> emplace() 和 emplace_hint() 是 C++ 11 标准加入到 set 类模板中的，相比具有同样功能的 insert() 方法，完成同样的任务，emplace() 和 emplace_hint() 的效率会更高
          ...
          _crypto_none = CryptoHandler::create(CEPH_CRYPTO_NONE)
          _crypto_aes = CryptoHandler::create(CEPH_CRYPTO_AES)
          _crypto_random.reset(new CryptoRandom())
          lookup_or_create_singleton_object<MempoolObs>("mempool_obs", false, this) -> 实例化内存池
        conf.set_val_default("log_to_stderr", "false")
      parse_env
      apply_changes -> void apply_changes(std::ostream* oss) -> 展开所有元变量。 进行任何挂起的观察者回调
        _gather_changes -> “call_gate_leave”方法正在访问“obs_call_gate”映射，但未持有所需的锁。 该数据结构可以由观察者回调线程上下文下的另一个线程操作
          map_observer_changes
        call_observers
          obs->handle_conf_change
          call_gate_leave
      TracepointProvider::initialize<tracepoint_traits>(cct) -> 初始化lttng跟踪点
    new librados::RadosClient -> librados::RadosClient::RadosClient
      add_observer -> void add_observer(md_config_obs_t* obs)
        obs_mgr.add_observer(obs) -> 我们可以将实现放在 .cc 文件中，并且仅显式实例化所使用的模板专业化，但这迫使我们在编译时涉及未使用的标头和库。 例如，为了实例化，要实例化seastar的ObserverMgr，我们需要包含seastar标头以获取必要的类型，但这将迫使我们将非seastar二进制文件链接到seastar库。 因此，为了避免以增加编译时间为代价引入未使用的依赖项，我们将实现放在头文件中
          observers.emplace -> rados_mon_op_timeout
        obs_call_gate.emplace -> 安置
    cct->put() -> void CephContext::put() -> 减引用
      if (--nref == 0)
  rados_conf_read_file(cluster, "/home/xb/project/ceph/xb/ceph/build/ceph.conf") -> extern "C" int _rados_conf_read_file
    librados::RadosClient *client = (librados::RadosClient *)cluster -> 集群转rados客户端
    conf.parse_config_files -> int md_config_t::parse_config_files
      const char *c = getenv("CEPH_CONF") -> 没有指定配置文件就从环境变量中获取
      void md_config_t::early_expand_meta
        _expand_meta -> Option::value_t md_config_t::_expand_meta
          ...
        conf_stringify -> to_str -> stringify(v)
      cf.parse_file
      _get_my_sections
      ...
    parse_env
    apply_changes
    complain_about_parse_error
  rados_conf_parse_argv -> extern "C" int _rados_conf_parse_argv
    argv_to_vec
    parse_argv -> int md_config_t::parse_argv -> Ceph 的命令行参数处理, 配置参数, 从命令行参数中读取配置值, https://runsisi.com/2019/02/23/ceph-opt/, 可配置参数: https://zhuanlan.zhihu.com/p/110079635
    conf.apply_changes
  rados_connect -> extern "C" int _rados_connect(rados_t cluster) -> 连接集群, 参考: https://www.jianshu.com/p/58956728dadc
    client->connect() -> int librados::RadosClient::connect()
      state = CONNECTING -> 连接状态机 
      cct->_log->start() -> 启动日志线程
        create("log") -> void Thread::create
      MonClient mc_bootstrap(cct) -> MonClient::MonClient(CephContext *cct_) -> 实例化monitor客户端(启动monc)
        want_monmap(true)
      MonClient::get_monmap_and_config -> mon/MonClient：一次性 mon 连接开始获取配置，这不是特别有效，但它可以工作： - 连接到监视器以获取 monmap 和配置 - 将其全部拆除 - 继续正常启动（这可能涉及 再次重新连接到 mon）。 这允许我们设置可能影响 mon 通信本身的配置选项，例如 ms_type -> 配置网络通信，并开启3个 msgr_worker 线程, 参考: https://blog.csdn.net/DeamonXiao/article/details/120879244
        init_crypto ... -> static void init() 
        build_initial_monmap
          int MonMap::build_initial
            init_with_ips -> ceph.conf -> global -> mon host -> [v2:172.17.0.2:40287,v1:172.17.0.2:40288] [v2:172.17.0.2:40289,v1:172.17.0.2:40290] [v2:172.17.0.2:40291,v1:172.17.0.2:40292]
              init_with_addrs ->
                _add_ambiguous_addr -> no, 不明确的 mon addr 可能是遗留的，也可能是 msgr2——我们不确定何时发生这种情况，我们需要同时尝试它们（除非我们可以从端口号合理地推断出它是
                add(name, addr, 0) -> void add(const std::string &name -> void add(const mon_info_t& m) -> 添加monitor到monmap, 优先级, 权重
                  mon_info[m.name] = m -> noname-a:m
                  calc_legacy_ranks()
                  calc_addr_mons()
                    addr_mons
            calc_legacy_ranks -> mon/MonMap：将排名顺序与entity_addr_t分开 我们当前根据mon地址的排序顺序定义mon排名顺序。 更改它，以便将排名顺序显式编码在 MonMap 的排名字段中。 如果我们加载旧版 MonMap，请计算旧版排序。 如果 monmap 尚不需要 nautilus 功能，请强制使用旧排序。 一旦所有 mons >= nautilus，我们就可以重新排序排名。 请注意，守护进程和客户端 (MonClients) 可能会看到不同的排名顺序。 那应该没问题
            calc_legacy_ranks -> 计算遗留ranks
            monmap.print(*_dout) -> 打印monmap
        messenger = Messenger::create_client_messenger temp_mon_client -> 创建mon临时客户端消息msg对象,启动3个work线程, messenger是MonC的属性
        add_dispatcher_head -> 启动2线程
        messenger->start() -> int AsyncMessenger::start()
        make_scope_guard
        init() -> int MonClient::init() -> MonC初始化
          refresh_config
          new RotatingKeyRing
          set_auth_client
          add_dispatcher_head
          timer.init() -> 启动MonC定时任务
            thread = new SafeTimerThread(this)
            thread->create("safe_timer") -> void SafeTimer::timer_thread() -> ceph中的SafeTimer类详解: https://blog.csdn.net/turou3442/article/details/96441221, https://blog.csdn.net/tiantao2012/article/details/78426276?ydreferer=aHR0cHM6Ly93d3cuZ29vZ2xlLmNvbS8%3D, 由于是thread的子类，从entry的实现就知道这个thread的回调函数是timer_thread, void *entry() override
          finisher.start() -> finisher_thread_entry -> finisher_empty_cond.notify_all() -> 唤醒条件变量,执行回调函数 -> ceph中的finisher类, https://blog.csdn.net/tiantao2012/article/details/79419556?ydreferer=aHR0cHM6Ly93d3cuZ29vZ2xlLmNvbS8%3D, 要结束操作的类会把自己添加到finisher类的queue中，然后finisher类分别调用要结束操作类的complete函数
          schedule_tick() -> void MonClient::schedule_tick() -> 定时扫描发送, 设置了一个定时器，到时间后会被执行(scan_request)，执行完后重新设置新的定时器，所以会一直周期性地扫描 -> void MonClient::tick()
            auto do_tick = make_lambda_context([this](int) { tick(); })
            timer.add_event_after(hunt_interval, do_tick)
        map_cond.wait_for <- 等待monitor发配置给monc, monc处理消息并唤醒(MonClient::handle_config), 然后设置mon启动配置
        cct->_conf.set_mon_vals(cct, config->config, config_cb) -> int set_mon_vals(CephContext *cct
          config.set_mon_vals(cct, values, obs_mgr, kv, config_cb)
          _gather_changes(values.changed, &rev_obs, nullptr)
          call_observers(locker, rev_obs)
      common_init_finish(cct)
        cct->start_service_thread
          _enable_perf_counter
          call_all_observers -> 调用所有订阅者
           _admin_socket->init -> bool AdminSocket::init -> 初始化asok -> 处理 ceph daemon 命令的线程, 和 service 线程一起提供了性能监控服务，service 线程更新各个模块性能参数，admin_socket 线程提供对外查询接口
            create_wakeup_pipe 
            bind_and_listen
      monclient.build_initial_monmap() -> 第二次获取monmap
      Messenger::create_client_messenger(cct, "radosclient")
      objecter = new (std::nothrow) Objecter(cct, messenger, &monclient, &finisher) -> Objecter::Objecter
        ...
        homeless_session(new OSDSession(cct, -1))
        ...
      objecter->set_balanced_budget()
      monclient.set_messenger(messenger) -> monc绑定msg
      mgrclient.set_messenger(messenger)
      objecter->init() -> void Objecter::init() -> ... -> create_rados_client -> 初始化objecter,打点等
        PerfCountersBuilder pcb(cct, "objecter", l_osdc_first, l_osdc_last) -> 性能统计
        RequestStateHook
        AdminSocket* admin_socket = cct->get_admin_socket()
        admin_socket->register_command("objecter_requests"
        update_crush_location
        add_observer
          obs_mgr.add_observer(obs)
            get_tracked_conf_keys -> crush_location -> crush算法(crush_map): https://docs.ceph.com/en/mimic/rados/operations/crush-map/
        initialized = true
      messenger->add_dispatcher_head(&mgrclient)
      messenger->add_dispatcher_tail(objecter)
      messenger->start()
      monclient.set_want_keys -> monc会订阅Monitor的OSDMap、MonMap相关信息, EPH_ENTITY_TYPE_MON | CEPH_ENTITY_TYPE_OSD | CEPH_ENTITY_TYPE_MGR
      monclient.init() -> monc正式初始化(非临时)
      monclient.authenticate -> monc认证
      monclient.sub_want("mgrmap", 0, 0) -> 订阅mgrmap更新的信息, mgr订阅模式: https://blog.csdn.net/tiantao2012/article/details/80109739 -> == "xxx" -> switch (m->get_type()) -> bool Objecter::ms_dispatch(Message *m) -> case MSG_MGR_MAP -> handle_mgr_map
        return sub.want(what, start, flags) -> bool MonSub::want(
      monclient.renew_subs() -> void renew_subs() -> void MonClient::_renew_subs
        _reopen_session
        _send_mon_message(std::move(m)) -> monc -> monitor
          ...
        sub.renewed()
      mgrclient.init()
      objecter->set_client_incarnation -> 客户端化身, 向Monitor订阅 osdmap 请求, 获取 osdmap 数据
      objecter->start()  -> void Objecter::start
        start_tick -> _send_linger_ping -> 心跳
        Objecter::tick()
          osd_sessions.begin
          timer.reschedule_me
      timer.init()
      finisher.start()
      instance_id = monclient.get_global_id()
  rados_ioctx_create(cluster, poolname, &io) -> extern "C" int _rados_ioctx_create -> 基于池和集群创建IO上下文, ioctx 的功能有：读写数据、读写属性、快照 pool、读取快照等
    create_ioctx -> int librados::RadosClient::create_ioctx(
      lookup_pool -> int64_t librados::RadosClient::lookup_pool -> lookup_pg_pool_name -> osdmap中查找池ID?
        wait_for_osdmap
          with_osdmap
      *io = new librados::IoCtxImpl(this, objecter, poolid, CEPH_NOSNAP) -> librados::IoCtxImpl::IoCtxImpl -> 用池ID,对象, 快照(无), 实例化io上下文
    *io = ctx -> io即带有池和集群信息的上下文
  rados_write 同步写 -> CEPH_RADOS_API int rados_write -> LIBRADOS_C_API_BASE_DEFAULT(rados_write) -> extern "C" int _rados_write -> bl.append(buf, len) 追加IO数据(将数据追加到_buffers尾部) -> ctx->write(oid, bl, len, off) -> int librados::IoCtxImpl::write
    ::ObjectOperation op -> 构造OP
    prepare_assert_ops(&op) -> 添加任何适合给定 IoCtx 中统计信息的版本断言操作，无论是目标版本断言还是任何 src 对象断言。 这些会影响单个 ioctx 操作，因此在我们执行操作时清除 ioctx 状态。 如果我们添加了任何事件，则返回指向 ObjectOperation 的指针； 这对于将 extra_ops 参数传递到 Objecter 方法中很方便
    mybl.substr_of(bl, 0, len) -> 先申明mybl, 获取子字符串, 截取子串
    op.write(off, mybl) -> void write(uint64_t off, ceph::buffer::list& bl) -> write(off, bl, 0 /* trunk_size */, 0 /* trunk_seq */)
      add_data(CEPH_OSD_OP_WRITE, off, bl.length(), bl) -> 封装OP, 增加操作码 -> void add_data(int op, uint64_t off, uint64_t len, ceph::buffer::list& bl)
        OSDOp& osd_op = add_op(op) -> 添加操作码
          ops[s].op.op = op
          ...
        osd_op.indata.claim_append(bl) -> 将bl的数据复制_buffers (indata)的尾部/头部，然后接bl的数据清空 -> osd/osd_types：添加每个操作返回字段以记录 [dup] 记录，允许总体正返回值，以及请求中每个操作的每个操作返回值和输出数据
      OSDOp& o = *ops.rbegin() -> 反向迭代, 返回反向迭代器以反向开始, 返回指向向量中最后一个元素（即其反向开头）的反向迭代器。反向迭代器向后迭代：增加它们将它们移向容器的开头。rbegin指向成员end指向的元素之前的元素。 请注意，与成员vector::back返回对同一元素的引用不同，此函数返回一个反向随机访问迭代器
      o.op.extent.truncate_size = truncate_size -> 写操作支持截断参数
    return operate(oid, &op, NULL) -> int librados::IoCtxImpl::operate -> 池IO上下文操作
      Context *oncommit = new C_SafeCond(mylock, cond, &done, &r) -> 通知机制, finish -> cond.notify_all, 回调?
      Objecter::Op *objecter_op = objecter->prepare_mutate_op(oid, oloc, *o, snapc, ut, flags, oncommit, &ver) -> 准备对象OP(把ObjectOperation封装为Op类型) -> Op *prepare_mutate_op
        Op *o = new Op(oid, oloc, op.ops, flags | global_op_flags | CEPH_OSD_FLAG_WRITE, oncommit, objver, nullptr, parent_trace) -> Op(const object_t& o, const object_locator_t& ol -> 构造对象OP, 设置回调, oncommit -> onfinish
          ops.swap(op) -> 向量交换, 交换容器的内容
          ...
          trace.init("op", nullptr, parent_trace) -> 初始化跟踪, 为librados和objecter添加blkin跟踪
      objecter->op_submit(objecter_op) -> 提交OP -> void Objecter::op_submit -> op->trace.event("op submit") 插入性能跟踪点 -> _op_submit_with_budget(op, rl, ptid, ctx_budget) 带预算 -> void Objecter::_op_submit_with_budget
        int op_budget = _take_op_budget(op, sul) -> 流控,减去该Op的预算 -> _op_submit(op, sul, ptid) -> void Objecter::_op_submit
          bool check_for_latest_map = _calc_target(&op->target, nullptr) -> RECALC_OP_TARGET_POOL_DNE -> 计算目标节点(crush) -> false -> target结构参考: (gdb) p op->target -> 关键函数: int Objecter::_calc_target(op_target_t *t, Connection *con, bool any_change) -> crush算法
            通过标记判断是读还是写, 获取epoch(85), 打印基本信息, 通过目标上的基本对象分布在哪个池(t->base_oloc.pool)来获取pg池, osdmap上有pools信息, 默认无分层, 
            get_epoch
            get_pg_pool -> 对象定位器结构说明: 定位器限制对象的放置。 主要是进入哪个池, 3, 
            if ((t->flags & CEPH_OSD_FLAG_IGNORE_OVERLAY) == 0) -> osdc/Objecter：在每次 _calc_target 调用时重新计算 target_*，任何时候我们被要求计算目标时，我们都应该应用池分层参数。 之前仅在未计算目标时才这样做的逻辑没有多大意义，并且破坏了我们为获取目标池的正确 pg_num 所需的 *pi 更新。 对于采用原始 pg 的旧集群来说，这并不重要，但对于 luminous 及其他集群，我们需要精确的 spg_t，这需要正确的 pg_num, 如果设置了该标志(CEPH_OSD_FLAG_IGNORE_OVERLAY)，则将操作发送到指定的池并忽略覆盖。 请注意，这会废弃全局 Objecter 标志
            pi = osdmap->get_pg_pool(t->target_oloc.pool)
            int ret = osdmap->object_locator_to_pg(t->target_oid, t->target_oloc, pgid) -> 将对象映射到放置组
            ceph_stable_mod
            lookup_pg_mapping
            osdmap->pg_to_up_acting_osds
            update_pg_mapping(actual_pgid, std::move(pg_mapping))
        _get_session(op->target.osd, &s, sul) -> 获取会话 -> int Objecter::_get_session
          if (osd < 0) -> osd异常
            *session = homeless_session -> 设置会话为无家可归会话(暂不发)
          map<int,OSDSession*>::iterator p = osd_sessions.find(osd)
          if (p != osd_sessions.end())
            *session = s -> 如果在会话(连接池)中找到了会话, 则直接返回该会话
          OSDSession *s = new OSDSession(cct, osd) -> 新建OSD会话
          s->con = messenger->connect_to_osd(osdmap->get_addrs(osd)) -> 连接目标OSD
          *session = s -> 返回该会话
        if (orig_epoch != osdmap->get_epoch()) -> 比较epoch(任期)
        _send_op_account(op) -> 登记本次操作
          if (op->onfinish) -> 如果有回调,则通过飞行计数器跟踪
            num_in_flight++
          op->target.flags & CEPH_OSD_FLAG_WRITE
            logger->inc(l_osdc_op_w) -> 增加写计数(性能计数器) -> 增加指标: 父指标: PerfCountersBuilder pcb(cct, "objecter", l_osdc_first, l_osdc_last) -> pcb.add_u64_counter(l_osdc_op_w, "op_w", "Write operations", "wr", PerfCountersBuilder::PRIO_CRITICAL)
          case CEPH_OSD_OP_WRITE: code = l_osdc_osdop_write -> 设置code, ceph所有osd操作码: __CEPH_FORALL_OSD_OPS
          logger->inc(code) -> 增加统计计数
        if (op->target.paused) -> 目标osd处于暂停状态(不可用)
          _maybe_request_map() -> 更新map
            monc->sub_want("osdmap"
            monc->renew_subs()
        if (!s->is_homeless()) -> 不是无家可归(osd不是-1)
          need_send = true -> 不是无家可归, 需要发送
        _session_op_assign(s, op)
          get_session(to) -> s->get()
        if (need_send)
          _send_op(op) -> 需要发送, 发送OP -> void Objecter::_send_op(Op *op)
            backoff ? -> 退避, 客户端协议, https://docs.ceph.com/en/latest/dev/rados-client-protocol/
            MOSDOp *m = _prepare_osd_op(op) -> 将对象op转化为MOSDop(多osd操作)，后面会以MOSDOp为对象进行处理的, MOSDOp封装了一些基本的请求。在ops里分装了多个OSDOp操作。每个OSDOp操作里又有一个soid, MOSDOp 封装的操作都是关于oid相关的操作，也就是说，一个MOSDOp只封装针对同一个oid 的操作。但是对于rados_clone_range这样的操作，有一个dest oid， 还有一个src oid，那么src oid 就保存在OSDOp的soid中
              hobject_t hobj = op->target.get_hobj()
              MOSDOp *m = new MOSDOp
              m->set_snapid(op->snapid) -> 设置快照id, 快照序号, 快照等...
              m->set_retry_attempt(op->attempts++) -> 设置重试
              m->set_priority(cct->_conf->osd_client_op_priority) -> 设置op优先级
              logger->inc(l_osdc_op_send)
              logger->inc(l_osdc_op_send_bytes, sum) -> 增加统计计数
            op->session->con->send_message(m) -> 通过操作的会话的连接发送消息 -> int AsyncConnection::send_message(Message *m)
              is_blackhole -> 黑洞, 减少重复代码
              protocol->send_message(m) -> 通过协议层发送 -> void ProtocolV2::send_message
                const bool can_fast_prepare = messenger->ms_can_fast_dispatch(m)
                out_queue[m->get_priority()].emplace_back -> 放入发送队列out_queue
                ...
      cond.wait(l, [&done] { return done;}) -> 等待OP返回, 如: handle_osd_op_reply -> cond.notify_all()
      set_sync_op_version(ver)


回调, 回复, 响应:
Objecter::ms_dispatch -> case CEPH_MSG_OSD_OPREPLY -> void Objecter::handle_osd_op_reply
  op->trace.event("osd op reply")
  m->get_result()
  if (op->onfinish)
    onfinish = op->onfinish
  _finish_op(op, 0) -> void Objecter::_finish_op
    put_op_budget_bytes(op->budget)
    _session_op_remove(op->session, op)
    op->put()
  if (onfinish) 
    onfinish->complete(rc) -> finish(r) -> void finish(int r) override
      *done = true
      cond.notify_all() -> 唤醒等待该条件变量的线程 -> 唤醒 cond.wait(l, [&done] { return done;})
  m->put()





handle_osd_op_reply
#0  C_SafeCond::finish (this=0x586520, r=0) at /home/xb/project/ceph/xb/ceph/src/common/Cond.h:66
#1  0x00007ffff7bc5c49 in Context::complete (this=0x586520, r=0) at /home/xb/project/ceph/xb/ceph/src/include/Context.h:77
#2  0x00007ffff7c73cf3 in Objecter::handle_osd_op_reply (this=0x4e5490, m=0x7fffd4013c60) at /home/xb/project/ceph/xb/ceph/src/osdc/Objecter.cc:3558
#3  0x00007ffff7c5e6f7 in Objecter::ms_dispatch (this=0x4e5490, m=0x7fffd4013c60) at /home/xb/project/ceph/xb/ceph/src/osdc/Objecter.cc:996
#4  0x00007ffff7c966aa in Objecter::ms_fast_dispatch (this=0x4e5490, m=0x7fffd4013c60) at /home/xb/project/ceph/xb/ceph/src/osdc/Objecter.h:2195
#5  0x00007ffff7c44c3a in Dispatcher::ms_fast_dispatch2 (this=0x4e5498, m=...) at /home/xb/project/ceph/xb/ceph/src/msg/Dispatcher.h:84
#6  0x00007fffee7fd18c in Messenger::ms_fast_dispatch (this=0x4de6a0, m=...) at /home/xb/project/ceph/xb/ceph/src/msg/Messenger.h:676
#7  0x00007fffee7fb13a in DispatchQueue::fast_dispatch (this=0x4de9f8, m=...) at /home/xb/project/ceph/xb/ceph/src/msg/DispatchQueue.cc:72
#8  0x00007fffee942ab3 in DispatchQueue::fast_dispatch (this=0x4de9f8, m=0x7fffd4013c60) at /home/xb/project/ceph/xb/ceph/src/msg/DispatchQueue.h:203
#9  0x00007fffee9a1825 in ProtocolV2::handle_message (this=0x585920) at /home/xb/project/ceph/xb/ceph/src/msg/async/ProtocolV2.cc:1479
#10 0x00007fffee99dc09 in ProtocolV2::handle_read_frame_dispatch (this=0x585920) at /home/xb/project/ceph/xb/ceph/src/msg/async/ProtocolV2.cc:1137
#11 0x00007fffee99fc0b in ProtocolV2::_handle_read_frame_epilogue_main (this=0x585920) at /home/xb/project/ceph/xb/ceph/src/msg/async/ProtocolV2.cc:1325
#12 0x00007fffee99fa44 in ProtocolV2::handle_read_frame_epilogue_main(std::unique_ptr<ceph::buffer::v15_2_0::ptr_node, ceph::buffer::v15_2_0::ptr_node::disposer>&&, int) (this=0x585920, 
    buffer=<unknown type in /home/xb/project/ceph/xb/ceph/build/lib/libceph-common.so.2, CU 0x2845a97, DIE 0x294b353>, r=0) at /home/xb/project/ceph/xb/ceph/src/msg/async/ProtocolV2.cc:1300
#13 0x00007fffee9cc2ba in CtRxNode<ProtocolV2>::call (this=0x585cc8, foo=0x585920) at /home/xb/project/ceph/xb/ceph/src/msg/async/Protocol.h:67
#14 0x00007fffee98f816 in ProtocolV2::run_continuation (this=0x585920, continuation=...) at /home/xb/project/ceph/xb/ceph/src/msg/async/ProtocolV2.cc:47
#15 0x00007fffee998265 in operator() (__closure=0x58c610, buffer=0x7fffd400d970 "\021\002)", r=0) at /home/xb/project/ceph/xb/ceph/src/msg/async/ProtocolV2.cc:755
#16 0x00007fffee9b6d73 in std::__invoke_impl<void, ProtocolV2::read(CONTINUATION_RXBPTR_TYPE<ProtocolV2>&, rx_buffer_t&&)::<lambda(char*, int)>&, char*, long int>(std::__invoke_other, struct {...} &) (__f=...)
    at /opt/rh/devtoolset-11/root/usr/include/c++/11/bits/invoke.h:61
#17 0x00007fffee9b694f in std::__invoke_r<void, ProtocolV2::read(CONTINUATION_RXBPTR_TYPE<ProtocolV2>&, rx_buffer_t&&)::<lambda(char*, int)>&, char*, long int>(struct {...} &) (__fn=...) at /opt/rh/devtoolset-11/root/usr/include/c++/11/bits/invoke.h:111
#18 0x00007fffee9b60cb in std::_Function_handler<void(char*, long int), ProtocolV2::read(CONTINUATION_RXBPTR_TYPE<ProtocolV2>&, rx_buffer_t&&)::<lambda(char*, int)> >::_M_invoke(const std::_Any_data &, <unknown type in /home/xb/project/ceph/xb/ceph/build/lib/libceph-common.so.2, CU 0x2845a97, DIE 0x292f28b>, <unknown type in /home/xb/project/ceph/xb/ceph/build/lib/libceph-common.so.2, CU 0x2845a97, DIE 0x292f29b>) (__functor=..., 
    __args#0=<unknown type in /home/xb/project/ceph/xb/ceph/build/lib/libceph-common.so.2, CU 0x2845a97, DIE 0x292f28b>, __args#1=<unknown type in /home/xb/project/ceph/xb/ceph/build/lib/libceph-common.so.2, CU 0x2845a97, DIE 0x292f29b>)
    at /opt/rh/devtoolset-11/root/usr/include/c++/11/bits/std_function.h:290
#19 0x00007fffee94454f in std::function<void (char*, long)>::operator()(char*, long) const (this=0x58c610, __args#0=0x7fffd400d970 "\021\002)", __args#1=0) at /opt/rh/devtoolset-11/root/usr/include/c++/11/bits/std_function.h:590
#20 0x00007fffee93e40b in AsyncConnection::process (this=0x58c290) at /home/xb/project/ceph/xb/ceph/src/msg/async/AsyncConnection.cc:458
#21 0x00007fffee94345e in C_handle_read::do_request (this=0x4b2b30, fd_or_id=21) at /home/xb/project/ceph/xb/ceph/src/msg/async/AsyncConnection.cc:71
#22 0x00007fffee9d0368 in EventCenter::process_events (this=0x519ab0, timeout_microseconds=30000000, working_dur=0x7fffe3ffd230) at /home/xb/project/ceph/xb/ceph/src/msg/async/Event.cc:406
#23 0x00007fffee9ddc23 in operator() (__closure=0x573b48) at /home/xb/project/ceph/xb/ceph/src/msg/async/Stack.cc:53
#24 0x00007fffee9df78e in std::__invoke_impl<void, NetworkStack::add_thread(unsigned int)::<lambda()>&>(std::__invoke_other, struct {...} &) (__f=...) at /opt/rh/devtoolset-11/root/usr/include/c++/11/bits/invoke.h:61
#25 0x00007fffee9df675 in std::__invoke_r<void, NetworkStack::add_thread(unsigned int)::<lambda()>&>(struct {...} &) (__fn=...) at /opt/rh/devtoolset-11/root/usr/include/c++/11/bits/invoke.h:111
#26 0x00007fffee9df55c in std::_Function_handler<void(), NetworkStack::add_thread(unsigned int)::<lambda()> >::_M_invoke(const std::_Any_data &) (__functor=...) at /opt/rh/devtoolset-11/root/usr/include/c++/11/bits/std_function.h:290
#27 0x00007fffee9dd3f4 in std::function<void ()>::operator()() const (this=0x573b48) at /opt/rh/devtoolset-11/root/usr/include/c++/11/bits/std_function.h:590
#28 0x00007fffee9dd3a4 in std::__invoke_impl<void, std::function<void ()>>(std::__invoke_other, std::function<void ()>&&) (__f=<unknown type in /home/xb/project/ceph/xb/ceph/build/lib/libceph-common.so.2, CU 0x2a65ef0, DIE 0x2ae0546>)
    at /opt/rh/devtoolset-11/root/usr/include/c++/11/bits/invoke.h:61
#29 0x00007fffee9dd359 in std::__invoke<std::function<void ()>>(std::function<void ()>&&) (__fn=<unknown type in /home/xb/project/ceph/xb/ceph/build/lib/libceph-common.so.2, CU 0x2a65ef0, DIE 0x2ae0af2>)
    at /opt/rh/devtoolset-11/root/usr/include/c++/11/bits/invoke.h:96
#30 0x00007fffee9dd306 in std::thread::_Invoker<std::tuple<std::function<void ()> > >::_M_invoke<0ul>(std::_Index_tuple<0ul>) (this=0x573b48) at /opt/rh/devtoolset-11/root/usr/include/c++/11/bits/std_thread.h:253
#31 0x00007fffee9dd2da in std::thread::_Invoker<std::tuple<std::function<void ()> > >::operator()() (this=0x573b48) at /opt/rh/devtoolset-11/root/usr/include/c++/11/bits/std_thread.h:260
#32 0x00007fffee9dd2be in std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::function<void ()> > > >::_M_run() (this=0x573b40) at /opt/rh/devtoolset-11/root/usr/include/c++/11/bits/std_thread.h:211
#33 0x00007fffeef0a7c4 in execute_native_thread_routine () from /home/xb/project/ceph/xb/ceph/build/lib/libceph-common.so.2
#34 0x00007fffec27bea5 in start_thread () from /lib64/libpthread.so.0
#35 0x00007ffff78b5b0d in clone () from /lib64/libc.so.6




rados_write 堆栈:
#0  Objecter::ms_dispatch (this=0x4e5490, m=0x7fffd4013c60) at /home/xb/project/ceph/xb/ceph/src/osdc/Objecter.cc:991
#1  0x00007ffff7c966aa in Objecter::ms_fast_dispatch (this=0x4e5490, m=0x7fffd4013c60) at /home/xb/project/ceph/xb/ceph/src/osdc/Objecter.h:2195
#2  0x00007ffff7c44c3a in Dispatcher::ms_fast_dispatch2 (this=0x4e5498, m=...) at /home/xb/project/ceph/xb/ceph/src/msg/Dispatcher.h:84
#3  0x00007fffee7fd18c in Messenger::ms_fast_dispatch (this=0x4de6a0, m=...) at /home/xb/project/ceph/xb/ceph/src/msg/Messenger.h:676
#4  0x00007fffee7fb13a in DispatchQueue::fast_dispatch (this=0x4de9f8, m=...) at /home/xb/project/ceph/xb/ceph/src/msg/DispatchQueue.cc:72
#5  0x00007fffee942ab3 in DispatchQueue::fast_dispatch (this=0x4de9f8, m=0x7fffd4013c60) at /home/xb/project/ceph/xb/ceph/src/msg/DispatchQueue.h:203
#6  0x00007fffee9a1825 in ProtocolV2::handle_message (this=0x585920) at /home/xb/project/ceph/xb/ceph/src/msg/async/ProtocolV2.cc:1479
#7  0x00007fffee99dc09 in ProtocolV2::handle_read_frame_dispatch (this=0x585920) at /home/xb/project/ceph/xb/ceph/src/msg/async/ProtocolV2.cc:1137
#8  0x00007fffee99fc0b in ProtocolV2::_handle_read_frame_epilogue_main (this=0x585920) at /home/xb/project/ceph/xb/ceph/src/msg/async/ProtocolV2.cc:1325
#9  0x00007fffee99fa44 in ProtocolV2::handle_read_frame_epilogue_main(std::unique_ptr<ceph::buffer::v15_2_0::ptr_node, ceph::buffer::v15_2_0::ptr_node::disposer>&&, int) (this=0x585920, 
    buffer=<unknown type in /home/xb/project/ceph/xb/ceph/build/lib/libceph-common.so.2, CU 0x2845a97, DIE 0x294b353>, r=0) at /home/xb/project/ceph/xb/ceph/src/msg/async/ProtocolV2.cc:1300
#10 0x00007fffee9cc2ba in CtRxNode<ProtocolV2>::call (this=0x585cc8, foo=0x585920) at /home/xb/project/ceph/xb/ceph/src/msg/async/Protocol.h:67
#11 0x00007fffee98f816 in ProtocolV2::run_continuation (this=0x585920, continuation=...) at /home/xb/project/ceph/xb/ceph/src/msg/async/ProtocolV2.cc:47
#12 0x00007fffee998265 in operator() (__closure=0x58c610, buffer=0x7fffd400d970 "\021\002)", r=0) at /home/xb/project/ceph/xb/ceph/src/msg/async/ProtocolV2.cc:755
#13 0x00007fffee9b6d73 in std::__invoke_impl<void, ProtocolV2::read(CONTINUATION_RXBPTR_TYPE<ProtocolV2>&, rx_buffer_t&&)::<lambda(char*, int)>&, char*, long int>(std::__invoke_other, struct {...} &) (__f=...)
    at /opt/rh/devtoolset-11/root/usr/include/c++/11/bits/invoke.h:61
#14 0x00007fffee9b694f in std::__invoke_r<void, ProtocolV2::read(CONTINUATION_RXBPTR_TYPE<ProtocolV2>&, rx_buffer_t&&)::<lambda(char*, int)>&, char*, long int>(struct {...} &) (__fn=...) at /opt/rh/devtoolset-11/root/usr/include/c++/11/bits/invoke.h:111
#15 0x00007fffee9b60cb in std::_Function_handler<void(char*, long int), ProtocolV2::read(CONTINUATION_RXBPTR_TYPE<ProtocolV2>&, rx_buffer_t&&)::<lambda(char*, int)> >::_M_invoke(const std::_Any_data &, <unknown type in /home/xb/project/ceph/xb/ceph/build/lib/libceph-common.so.2, CU 0x2845a97, DIE 0x292f28b>, <unknown type in /home/xb/project/ceph/xb/ceph/build/lib/libceph-common.so.2, CU 0x2845a97, DIE 0x292f29b>) (__functor=..., 
    __args#0=<unknown type in /home/xb/project/ceph/xb/ceph/build/lib/libceph-common.so.2, CU 0x2845a97, DIE 0x292f28b>, __args#1=<unknown type in /home/xb/project/ceph/xb/ceph/build/lib/libceph-common.so.2, CU 0x2845a97, DIE 0x292f29b>)
    at /opt/rh/devtoolset-11/root/usr/include/c++/11/bits/std_function.h:290
#16 0x00007fffee94454f in std::function<void (char*, long)>::operator()(char*, long) const (this=0x58c610, __args#0=0x7fffd400d970 "\021\002)", __args#1=0) at /opt/rh/devtoolset-11/root/usr/include/c++/11/bits/std_function.h:590
#17 0x00007fffee93e40b in AsyncConnection::process (this=0x58c290) at /home/xb/project/ceph/xb/ceph/src/msg/async/AsyncConnection.cc:458
#18 0x00007fffee94345e in C_handle_read::do_request (this=0x4b2b30, fd_or_id=21) at /home/xb/project/ceph/xb/ceph/src/msg/async/AsyncConnection.cc:71
#19 0x00007fffee9d0368 in EventCenter::process_events (this=0x519ab0, timeout_microseconds=30000000, working_dur=0x7fffe3ffd230) at /home/xb/project/ceph/xb/ceph/src/msg/async/Event.cc:406
#20 0x00007fffee9ddc23 in operator() (__closure=0x573b48) at /home/xb/project/ceph/xb/ceph/src/msg/async/Stack.cc:53
#21 0x00007fffee9df78e in std::__invoke_impl<void, NetworkStack::add_thread(unsigned int)::<lambda()>&>(std::__invoke_other, struct {...} &) (__f=...) at /opt/rh/devtoolset-11/root/usr/include/c++/11/bits/invoke.h:61
#22 0x00007fffee9df675 in std::__invoke_r<void, NetworkStack::add_thread(unsigned int)::<lambda()>&>(struct {...} &) (__fn=...) at /opt/rh/devtoolset-11/root/usr/include/c++/11/bits/invoke.h:111
#23 0x00007fffee9df55c in std::_Function_handler<void(), NetworkStack::add_thread(unsigned int)::<lambda()> >::_M_invoke(const std::_Any_data &) (__functor=...) at /opt/rh/devtoolset-11/root/usr/include/c++/11/bits/std_function.h:290
#24 0x00007fffee9dd3f4 in std::function<void ()>::operator()() const (this=0x573b48) at /opt/rh/devtoolset-11/root/usr/include/c++/11/bits/std_function.h:590
#25 0x00007fffee9dd3a4 in std::__invoke_impl<void, std::function<void ()>>(std::__invoke_other, std::function<void ()>&&) (__f=<unknown type in /home/xb/project/ceph/xb/ceph/build/lib/libceph-common.so.2, CU 0x2a65ef0, DIE 0x2ae0546>)
    at /opt/rh/devtoolset-11/root/usr/include/c++/11/bits/invoke.h:61
#26 0x00007fffee9dd359 in std::__invoke<std::function<void ()>>(std::function<void ()>&&) (__fn=<unknown type in /home/xb/project/ceph/xb/ceph/build/lib/libceph-common.so.2, CU 0x2a65ef0, DIE 0x2ae0af2>)
    at /opt/rh/devtoolset-11/root/usr/include/c++/11/bits/invoke.h:96
#27 0x00007fffee9dd306 in std::thread::_Invoker<std::tuple<std::function<void ()> > >::_M_invoke<0ul>(std::_Index_tuple<0ul>) (this=0x573b48) at /opt/rh/devtoolset-11/root/usr/include/c++/11/bits/std_thread.h:253
#28 0x00007fffee9dd2da in std::thread::_Invoker<std::tuple<std::function<void ()> > >::operator()() (this=0x573b48) at /opt/rh/devtoolset-11/root/usr/include/c++/11/bits/std_thread.h:260
#29 0x00007fffee9dd2be in std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::function<void ()> > > >::_M_run() (this=0x573b40) at /opt/rh/devtoolset-11/root/usr/include/c++/11/bits/std_thread.h:211
#30 0x00007fffeef0a7c4 in execute_native_thread_routine () from /home/xb/project/ceph/xb/ceph/build/lib/libceph-common.so.2
#31 0x00007fffec27bea5 in start_thread () from /lib64/libpthread.so.0
#32 0x00007ffff78b5b0d in clone () from /lib64/libc.so.6
(gdb) 




(gdb) p op->target
$9 = {
  flags = 32, 
  epoch = 0, 
  base_oid = {
    name = "neo-obj"
  }, 
  base_oloc = {
    pool = 1, 
    key = "", 
    nspace = "", 
    hash = -1
  }, 
  target_oid = {
    name = ""
  }, 
  target_oloc = {
    pool = -1, 
    key = "", 
    nspace = "", 
    hash = -1
  }, 
  precalc_pgid = false, 
  pool_ever_existed = false, 
  base_pgid = {
    m_pool = 0, 
    m_seed = 0, 
    static calc_name_buf_size = 36 '$'
  }, 
  pgid = {
    m_pool = 0, 
    m_seed = 0, 
    static calc_name_buf_size = 36 '$'
  }, 
  actual_pgid = {
    pgid = {
      m_pool = 0, 
      m_seed = 0, 
      static calc_name_buf_size = 36 '$'
    }, 
    shard = {
      id = -1 '\377', 
      static NO_SHARD = {
        id = -1 '\377', 
        static NO_SHARD = <same as static member of an already seen type>
      }
    }, 
    static calc_name_buf_size = 40 '('
  }, 
  pg_num = 0, 
  pg_num_mask = 0, 
  pg_num_pending = 0, 
  up = std::vector of length 0, capacity 0, 
  acting = std::vector of length 0, capacity 0, 
  up_primary = -1, 
  acting_primary = -1, 
  size = -1, 
  min_size = -1, 
  sort_bitwise = false, 
  recovery_deletes = false, 
  used_replica = false, 
  paused = false, 
  osd = -1, 
  last_force_resend = 0
}

        



rbd_write.c:145 -> rbd_write
...
get_handler_way
start_in_flight_io
create_write_request
...




v12
https://github.com/ssbandjl/ceph_v12

v12.2.14, 73, 

sudo vi /etc/sysctl.conf
net.ipv6.conf.all.disable_ipv6 = 1 
net.ipv6.conf.default.disable_ipv6 = 1
net.ipv6.conf.lo.disable_ipv6 = 1
sudo sysctl -p

-i https://pypi.tuna.tsinghua.edu.cn/simple

set(VERSION 12.2.14)
更新yun源
wget https://copr.fedorainfracloud.org/coprs/jsynacek/systemd-backports-for-centos-7/repo/epel-7/jsynacek-systemd-backports-for-centos-7-epel-7.repo -O /etc/yum.repos.d/jsynacek-systemd-centos-7.repo
yum update systemd -y



https://boostorg.jfrog.io/artifactory/main/release/1.66.0/source/boost_1_66_0.tar.bz2

cp cache/boost_1_66_0.tar.bz2 /home/xb/project/stor/ceph/ceph/build/boost/src/boost_1_66_0.tar.bz2


WITH_TESTS
CMakeLists.txt
src/test/CMakeLists.txt

+ echo 'dashboard urls: http://182.200.53.73:41984/'
dashboard urls: http://182.200.53.73:41984/
+ echo '  restful urls: https://182.200.53.73:42984'
  restful urls: https://182.200.53.73:42984
+ echo '  w/ user/pass: admin / 978c009a-1726-45ef-aeac-5be2da32ae56'
  w/ user/pass: admin / 978c009a-1726-45ef-aeac-5be2da32ae56


echo -e "$CEPH_BUILD_ROOT vstart.sh:${LINENO}"

src/ceph_mgr.cc

export PATH=/home/xb/project/stor/ceph/ceph/build/bin:$PATH
export PATH=/home/xb/project/stor/ceph/xb/docker/ceph/build/bin:$PATH
运行 rados lspools | grep rgw
ceph osd pool application enable $POOL rgw

lttng-sessiond --daemoniz
lttng-sessiond -d --no-kernel
../src/vstart.sh -d -n -l -e -o "osd_tracing = true"
../src/vstart.sh -d -n -o "osd_tracing = true"
lttng list --userspace
rados bench -p ec 5 write

ps aux|grep lttng|grep -v grep|awk '{print$2}'|xargs kill -9


[global]
        osd_tracing = true
        bluestore_tracing = true
        event_tracing = true
        osd_function_tracing = true
        osd_objectstore_tracing = true
        rbd_tracing = true
        rados_tracing = true
        rgw_op_tracing = true
        rgw_rados_tracing = true


OSD=3 MON=3 RGW=1 ../src/vstart.sh -n -o "rbd_blkin_trace_all=true" \
  -o "osd_tracing = true" \
  -o "bluestore_tracing = true" \
  -o "event_tracing = true" \
  -o "osd_function_tracing = true" \
  -o "osd_objectstore_tracing = true" \
  -o "rbd_tracing = true" \
  -o "rgw_op_tracing = true" \
  -o "rgw_rados_tracing = true" --mgr_num 0

../stop.sh;OSD=3 MON=3 RGW=1 ../src/vstart.sh -n -X -o "rbd_blkin_trace_all=true" \
  -o "osd_tracing = true" \
  -o "bluestore_tracing = true" \
  -o "event_tracing = true" \
  -o "osd_function_tracing = false" \
  -o "osd_objectstore_tracing = true" \
  -o "rbd_tracing = true" \
  -o "rgw_op_tracing = true" \
  -o "rgw_rados_tracing = true"

clear;./stop.sh;../src/vstart.sh -X -n -o "rbd_blkin_trace_all=true"   -o "osd_tracing = true"   -o "bluestore_tracing = true"   -o "event_tracing = true"   -o "osd_function_tracing = true"   -o "osd_objectstore_tracing = true"   -o "rbd_tracing = true"   -o "rgw_op_tracing = true"   -o "rgw_rados_tracing = true" --mgr_num 0

ps aux|grep ceph|awk '{print$2}'|xargs -x kill
ps aux|grep ceph|awk '{print$2}'|xargs -x -I '{}' kill -9 {}
ps aux|grep ceph|awk '{print$2}'|while read pid;do echo $pid;kill -9 $pid;done;ps aux|grep ceph
systemctl stop dse@n73 ceph-mon@n73 ceph-mgr@n73

dashboard urls: http://182.200.53.73:41920/
  restful urls: https://182.200.53.73:42920
  w/ user/pass: admin / 


export PYTHONPATH=./pybind:/home/xb/project/stor/ceph/ceph/src/pybind:/home/xb/project/stor/ceph/ceph/build/lib/cython_modules/lib.2
export LD_LIBRARY_PATH=/home/xb/project/stor/ceph/ceph/build/lib

export PYTHONPATH=/usr/local/lib64/python3.6/site-packages/babeltrace:$PYTHONPATH
export CEPH_CONF=/home/xb/project/ceph/xb/ceph/build/ceph.conf

sudo vim /etc/ld.so.conf.d/sb.conf
  $ sudo ldconfig 
  $ cat /etc/ld.so.conf.d/sb.conf 
  /usr/local/lib

ceph config-key list
ceph mgr module ls
ceph mgr module disable dashboard
ceph auth list
ceph mgr module disable restful

ps aux |grep lttng-sessiond


PID: 1190156 - Name: /home/xb/project/stor/ceph/ceph/build/bin/ceph-osd
      ust_baddr_statedump:soinfo (loglevel: TRACE_DEBUG_LINE (13)) (type: tracepoint)
      pg:queue_op (loglevel: TRACE_DEBUG_LINE (13)) (type: tracepoint)
      osd:do_osd_op_post (loglevel: TRACE_DEBUG_LINE (13)) (type: tracepoint)
      osd:do_osd_op_pre_unknown (loglevel: TRACE_DEBUG_LINE (13)) (type: tracepoint)
      osd:do_osd_op_pre_copy_from (loglevel: TRACE_DEBUG_LINE (13)) (type: tracepoint)
      osd:do_osd_op_pre_copy_get (loglevel: TRACE_DEBUG_LINE (13)) (type: tracepoint)
      osd:do_osd_op_pre_copy_get_classic (loglevel: TRACE_DEBUG_LINE (13)) (type: tracepoint)
      osd:do_osd_op_pre_omaprmkeys (loglevel: TRACE_DEBUG_LINE (13)) (type: tracepoint)
      osd:do_osd_op_pre_omapclear (loglevel: TRACE_DEBUG_LINE (13)) (type: tracepoint)
      osd:do_osd_op_pre_omapsetheader (loglevel: TRACE_DEBUG_LINE (13)) (type: tracepoint)
      osd:do_osd_op_pre_omapsetvals (loglevel: TRACE_DEBUG_LINE (13)) (type: tracepoint)
      osd:do_osd_op_pre_omap_cmp (loglevel: TRACE_DEBUG_LINE (13)) (type: tracepoint)
      osd:do_osd_op_pre_omapgetvalsbykeys (loglevel: TRACE_DEBUG_LINE (13)) (type: tracepoint)
      osd:do_osd_op_pre_omapgetheader (loglevel: TRACE_DEBUG_LINE (13)) (type: tracepoint)
      osd:do_osd_op_pre_omapgetvals (loglevel: TRACE_DEBUG_LINE (13)) (type: tracepoint)
      osd:do_osd_op_pre_omapgetkeys (loglevel: TRACE_DEBUG_LINE (13)) (type: tracepoint)
      osd:do_osd_op_pre_tmap2omap (loglevel: TRACE_DEBUG_LINE (13)) (type: tracepoint)
      osd:do_osd_op_pre_tmapup (loglevel: TRACE_DEBUG_LINE (13)) (type: tracepoint)
      osd:do_osd_op_pre_tmapput (loglevel: TRACE_DEBUG_LINE (13)) (type: tracepoint)
      osd:do_osd_op_pre_tmapget (loglevel: TRACE_DEBUG_LINE (13)) (type: tracepoint)
      osd:do_osd_op_pre_startsync (loglevel: TRACE_DEBUG_LINE (13)) (type: tracepoint)
      osd:do_osd_op_pre_append (loglevel: TRACE_DEBUG_LINE (13)) (type: tracepoint)
      osd:do_osd_op_pre_rmxattr (loglevel: TRACE_DEBUG_LINE (13)) (type: tracepoint)
      osd:do_osd_op_pre_setxattr (loglevel: TRACE_DEBUG_LINE (13)) (type: tracepoint)
      osd:do_osd_op_pre_cache_unpin (loglevel: TRACE_DEBUG_LINE (13)) (type: tracepoint)
      osd:do_osd_op_pre_cache_pin (loglevel: TRACE_DEBUG_LINE (13)) (type: tracepoint)
      osd:do_osd_op_pre_watch (loglevel: TRACE_DEBUG_LINE (13)) (type: tracepoint)
      osd:do_osd_op_pre_clonerange (loglevel: TRACE_DEBUG_LINE (13)) (type: tracepoint)
      osd:do_osd_op_pre_delete (loglevel: TRACE_DEBUG_LINE (13)) (type: tracepoint)
      osd:do_osd_op_pre_truncate (loglevel: TRACE_DEBUG_LINE (13)) (type: tracepoint)
      osd:do_osd_op_pre_create (loglevel: TRACE_DEBUG_LINE (13)) (type: tracepoint)
      osd:do_osd_op_pre_zero (loglevel: TRACE_DEBUG_LINE (13)) (type: tracepoint)
      osd:do_osd_op_pre_rollback (loglevel: TRACE_DEBUG_LINE (13)) (type: tracepoint)
      osd:do_osd_op_pre_writesame (loglevel: TRACE_DEBUG_LINE (13)) (type: tracepoint)
      osd:do_osd_op_pre_writefull (loglevel: TRACE_DEBUG_LINE (13)) (type: tracepoint)
      osd:do_osd_op_pre_write (loglevel: TRACE_DEBUG_LINE (13)) (type: tracepoint)
      osd:do_osd_op_pre_setallochint (loglevel: TRACE_DEBUG_LINE (13)) (type: tracepoint)
      osd:do_osd_op_pre_notify_ack (loglevel: TRACE_DEBUG_LINE (13)) (type: tracepoint)
      osd:do_osd_op_pre_notify (loglevel: TRACE_DEBUG_LINE (13)) (type: tracepoint)
      osd:do_osd_op_pre_assert_src_version (loglevel: TRACE_DEBUG_LINE (13)) (type: tracepoint)
      osd:do_osd_op_pre_list_snaps (loglevel: TRACE_DEBUG_LINE (13)) (type: tracepoint)
      osd:do_osd_op_pre_list_watchers (loglevel: TRACE_DEBUG_LINE (13)) (type: tracepoint)
      osd:do_osd_op_pre_assert_ver (loglevel: TRACE_DEBUG_LINE (13)) (type: tracepoint)
      osd:do_osd_op_pre_cmpxattr (loglevel: TRACE_DEBUG_LINE (13)) (type: tracepoint)
      osd:do_osd_op_pre_getxattrs (loglevel: TRACE_DEBUG_LINE (13)) (type: tracepoint)
      osd:do_osd_op_pre_getxattr (loglevel: TRACE_DEBUG_LINE (13)) (type: tracepoint)
      osd:do_osd_op_pre_cache_evict (loglevel: TRACE_DEBUG_LINE (13)) (type: tracepoint)
      osd:do_osd_op_pre_cache_flush (loglevel: TRACE_DEBUG_LINE (13)) (type: tracepoint)
      osd:do_osd_op_pre_try_flush (loglevel: TRACE_DEBUG_LINE (13)) (type: tracepoint)
      osd:do_osd_op_pre_undirty (loglevel: TRACE_DEBUG_LINE (13)) (type: tracepoint)
      osd:do_osd_op_pre_isdirty (loglevel: TRACE_DEBUG_LINE (13)) (type: tracepoint)
      osd:do_osd_op_pre_stat (loglevel: TRACE_DEBUG_LINE (13)) (type: tracepoint)
      osd:do_osd_op_pre_call (loglevel: TRACE_DEBUG_LINE (13)) (type: tracepoint)
      osd:do_osd_op_pre_sparse_read (loglevel: TRACE_DEBUG_LINE (13)) (type: tracepoint)
      osd:do_osd_op_pre_mapext (loglevel: TRACE_DEBUG_LINE (13)) (type: tracepoint)
      osd:do_osd_op_pre_checksum (loglevel: TRACE_DEBUG_LINE (13)) (type: tracepoint)
      osd:do_osd_op_pre_read (loglevel: TRACE_DEBUG_LINE (13)) (type: tracepoint)
      osd:do_osd_op_pre_extent_cmp (loglevel: TRACE_DEBUG_LINE (13)) (type: tracepoint)
      osd:do_osd_op_pre (loglevel: TRACE_DEBUG_LINE (13)) (type: tracepoint)
      osd:opwq_process_finish (loglevel: TRACE_DEBUG_LINE (13)) (type: tracepoint)
      osd:opwq_process_start (loglevel: TRACE_DEBUG_LINE (13)) (type: tracepoint)
      osd:ms_fast_dispatch (loglevel: TRACE_DEBUG_LINE (13)) (type: tracepoint)
      osd:prepare_tx_exit (loglevel: TRACE_DEBUG_LINE (13)) (type: tracepoint)
      osd:prepare_tx_enter (loglevel: TRACE_DEBUG_LINE (13)) (type: tracepoint)
      oprequest:mark_flag_point (loglevel: TRACE_DEBUG_LINE (13)) (type: tracepoint)
      oprequest:set_rmw_flags (loglevel: TRACE_DEBUG_LINE (13)) (type: tracepoint)
      zipkin:timestamp (loglevel: TRACE_WARNING (4)) (type: tracepoint)
      zipkin:keyval_integer (loglevel: TRACE_WARNING (4)) (type: tracepoint)
      zipkin:keyval_string (loglevel: TRACE_WARNING (4)) (type: tracepoint)


../src/vstart.sh -help

src/tools/ceph_conf.cc


restart
./stop.sh;clear;sh -x ../src/vstart.sh -d -n -o "osd_tracing = true"

replace in dir
<< dendl;
<< __FFL__ << dendl;

std::string build_time = "2023/04/27 10:10:10";
priv_ss << "set uid:gid to " << uid << ":" << gid << " (" << uid_string << ":" << gid_string << ")" << "build_time:" << build_time;

tee .gitignore <<EOF
install-deps-python2.7_tmp
install-deps-python3_tmp
install-deps-python3_tmp_bak
<<EOF


2023-05-07 10:22:38.372811 7fffeb0d5700  1 mgr init Loading python module 'balancer'init PyModuleRegistry.cc:165
2023-05-07 10:22:38.534696 7fffeb0d5700  1 mgr init Loading python module 'dashboard'init PyModuleRegistry.cc:165
LTTng-UST: Error (-17) while registering tracepoint probe. Duplicate registration of tracepoint probes having the same name is not allowed.

Program received signal SIGABRT, Aborted.
[Switching to Thread 0x7fffeb0d5700 (LWP 2814992)]
0x00007ffff491a387 in raise () from /lib64/libc.so.6
Missing separate debuginfos, use: debuginfo-install glibc-2.17-326.el7_9.x86_64 gperftools-libs-2.6.1-1.el7.x86_64 keyutils-libs-1.5.8-3.el7.x86_64 krb5-libs-1.15.1-55.el7_9.x86_64 libblkid-2.29-2.el7.centos.x86_64 libcom_err-1.42.9-19.el7.x86_64 libgcc-4.8.5-44.el7.x86_64 libibverbs-52mlnx1-1.53100.x86_64 libnl3-3.2.28-4.el7.x86_64 libselinux-2.5-15.el7.x86_64 libstdc++-4.8.5-44.el7.x86_64 libuuid-2.29-2.el7.centos.x86_64 lttng-ust-2.4.1-4.el7.x86_64 nspr-4.34.0-3.1.el7_9.x86_64 nss-3.79.0-5.el7_9.x86_64 nss-softokn-3.79.0-4.el7_9.x86_64 nss-softokn-freebl-3.79.0-4.el7_9.x86_64 nss-util-3.79.0-1.el7_9.x86_64 openssl-libs-1.0.2k-26.el7_9.x86_64 pcre-8.32-17.el7.x86_64 python-libs-2.7.5-92.el7_9.x86_64 python-markupsafe-0.11-10.el7.x86_64 sqlite-3.35.1-hl1.el7.x86_64 userspace-rcu-0.7.16-1.el7.x86_64 zlib-1.2.7-21.el7_9.x86_64
(gdb) bt
#0  0x00007ffff491a387 in raise () from /lib64/libc.so.6
#1  0x00007ffff491ba78 in abort () from /lib64/libc.so.6
#2  0x00007fffd236ff6b in __lttng_events_init__zipkin () at /usr/include/lttng/ust-tracepoint-event.h:782
#3  0x00007ffff7dea9c3 in _dl_init_internal () from /lib64/ld-linux-x86-64.so.2
#4  0x00007ffff7def59e in dl_open_worker () from /lib64/ld-linux-x86-64.so.2
#5  0x00007ffff7dea7d4 in _dl_catch_error () from /lib64/ld-linux-x86-64.so.2
#6  0x00007ffff7deeb8b in _dl_open () from /lib64/ld-linux-x86-64.so.2
#7  0x00007ffff75c5fab in dlopen_doit () from /lib64/libdl.so.2
#8  0x00007ffff7dea7d4 in _dl_catch_error () from /lib64/ld-linux-x86-64.so.2
#9  0x00007ffff75c65ad in _dlerror_run () from /lib64/libdl.so.2
#10 0x00007ffff75c6041 in dlopen@@GLIBC_2.2.5 () from /lib64/libdl.so.2
#11 0x00007ffff7b20a4f in _PyImport_GetDynLoadFunc () from /lib64/libpython2.7.so.1.0
#12 0x00007ffff7b0934e in _PyImport_LoadDynamicModule () from /lib64/libpython2.7.so.1.0
#13 0x00007ffff7b07451 in import_submodule () from /lib64/libpython2.7.so.1.0
#14 0x00007ffff7b07736 in load_next () from /lib64/libpython2.7.so.1.0
#15 0x00007ffff7b0807e in PyImport_ImportModuleLevel () from /lib64/libpython2.7.so.1.0
#16 0x00007ffff7aeb32f in builtin___import__ () from /lib64/libpython2.7.so.1.0
#17 0x00007ffff7a5b073 in PyObject_Call () from /lib64/libpython2.7.so.1.0
#18 0x00007ffff7aecf07 in PyEval_CallObjectWithKeywords () from /lib64/libpython2.7.so.1.0
#19 0x00007ffff7af1bc5 in PyEval_EvalFrameEx () from /lib64/libpython2.7.so.1.0
#20 0x00007ffff7af664d in PyEval_EvalCodeEx () from /lib64/libpython2.7.so.1.0
#21 0x00007ffff7af6752 in PyEval_EvalCode () from /lib64/libpython2.7.so.1.0
#22 0x00007ffff7b0653c in PyImport_ExecCodeModuleEx () from /lib64/libpython2.7.so.1.0
#23 0x00007ffff7b067b8 in load_source_module () from /lib64/libpython2.7.so.1.0
#24 0x00007ffff7b07451 in import_submodule () from /lib64/libpython2.7.so.1.0
#25 0x00007ffff7b0769d in load_next () from /lib64/libpython2.7.so.1.0
#26 0x00007ffff7b0807e in PyImport_ImportModuleLevel () from /lib64/libpython2.7.so.1.0
#27 0x00007ffff7aeb32f in builtin___import__ () from /lib64/libpython2.7.so.1.0
#28 0x00007ffff7a5b073 in PyObject_Call () from /lib64/libpython2.7.so.1.0
#29 0x00007ffff7aecf07 in PyEval_CallObjectWithKeywords () from /lib64/libpython2.7.so.1.0
#30 0x00007ffff7af1bc5 in PyEval_EvalFrameEx () from /lib64/libpython2.7.so.1.0
#31 0x00007ffff7af664d in PyEval_EvalCodeEx () from /lib64/libpython2.7.so.1.0
#32 0x00007ffff7af6752 in PyEval_EvalCode () from /lib64/libpython2.7.so.1.0
#33 0x00007ffff7b0653c in PyImport_ExecCodeModuleEx () from /lib64/libpython2.7.so.1.0
#34 0x00007ffff7b067b8 in load_source_module () from /lib64/libpython2.7.so.1.0
#35 0x00007ffff7b07c4a in load_package () from /lib64/libpython2.7.so.1.0
#36 0x00007ffff7b07451 in import_submodule () from /lib64/libpython2.7.so.1.0
#37 0x00007ffff7b0769d in load_next () from /lib64/libpython2.7.so.1.0
#38 0x00007ffff7b0807e in PyImport_ImportModuleLevel () from /lib64/libpython2.7.so.1.0
#39 0x00007ffff7aeb32f in builtin___import__ () from /lib64/libpython2.7.so.1.0
#40 0x00007ffff7a5b073 in PyObject_Call () from /lib64/libpython2.7.so.1.0
#41 0x00007ffff7a5b155 in call_function_tail () from /lib64/libpython2.7.so.1.0
#42 0x00007ffff7a5b23e in PyObject_CallFunction () from /lib64/libpython2.7.so.1.0
#43 0x00007ffff7b08b22 in PyImport_Import () from /lib64/libpython2.7.so.1.0
#44 0x000055555584472d in PyModule::load (this=this@entry=0x55555ee079b0, pMainThreadState=<optimized out>) at /home/xb/project/stor/ceph/ceph/src/mgr/PyModuleRegistry.cc:261
#45 0x00005555558459d9 in PyModuleRegistry::init (this=this@entry=0x7fffffffe118, map=...) at /home/xb/project/stor/ceph/ceph/src/mgr/PyModuleRegistry.cc:167
#46 0x00005555558598ec in MgrStandby::handle_mgr_map (this=this@entry=0x7fffffffc320, mmap=mmap@entry=0x55555ebb5200) at /home/xb/project/stor/ceph/ceph/src/mgr/MgrStandby.cc:316
#47 0x000055555585aca0 in MgrStandby::ms_dispatch (this=0x7fffffffc320, m=0x55555ebb5200) at /home/xb/project/stor/ceph/ceph/src/mgr/MgrStandby.cc:376
#48 0x0000555555c7e982 in ms_deliver_dispatch (m=0x55555ebb5200, this=0x55555eb6d500) at /home/xb/project/stor/ceph/ceph/src/msg/Messenger.h:668
#49 DispatchQueue::entry (this=0x55555eb6d678) at /home/xb/project/stor/ceph/ceph/src/msg/DispatchQueue.cc:197
#50 0x0000555555a4736d in DispatchQueue::DispatchThread::entry (this=<optimized out>) at /home/xb/project/stor/ceph/ceph/src/msg/DispatchQueue.h:101
#51 0x00007ffff590eea5 in start_thread () from /lib64/libpthread.so.0
#52 0x00007ffff49e2b0d in clone () from /lib64/libc.so.6
(gdb) info locals
No symbol table info

gdb --args /home/xb/project/stor/ceph/ceph/build/bin/ceph-mgr -i x -c /home/xb/project/stor/ceph/ceph/build/ceph.conf -d
mgr_module_path

void MgrStandby::send_beacon() 发送信号
_list_modules
path, /home/xb/project/stor/ceph/ceph/src/pybind/mgr, 
mgr initial modules = restful status dashboard balancer

trace, 
FUNCTRACE(), src/msg/async/AsyncMessenger.cc -> #define FUNCTRACE() EventTrace _t1
event, https://github.com/ssbandjl/ceph_v12/commit/b3b20449dabbdfae6fb035d3e7efca52c21e9869

Tracing your own user application, https://lttng.org/docs/v2.5/#doc-viewing-and-analyzing-your-traces
跟踪就像在源代码的特定位置调用 printf()，尽管 LTTng 比 printf() 更快更灵活。 在 LTTng 领域，tracepoint() 类似于 printf()。
但是，与 printf() 不同的是，tracepoint() 不使用格式字符串来了解其参数的类型：所有跟踪点的格式必须在使用它们之前定义。 所以在编写我们的 Hello world 程序之前，我们需要定义跟踪点的格式。 这是通过编写一个模板文件来完成的，该文件的名称通常以 .tp 扩展名（用于跟踪点）结尾，lttng-gen-tp 工具（与 LTTng-UST 一起提供）将使用它来生成目标文件（以及 .c 文件）和要包含在我们的应用程序源代码中的标头
通过在您自己的应用程序中包含 hello-tp.h，您可以通过在调用 tracepoint() 时正确引用它来使用上面定义的跟踪点
tracepoint(hello_world, my_first_tracepoint, 23, "hi there!")
第一个：提供商名称（始终）
第二：跟踪点名称（始终）
第三个：my_integer_arg（第一个用户定义的参数）
第 4 个：my_string_arg（第二个用户定义的参数）
请注意，提供者和跟踪点名称不是字符串； 它们实际上是由 hello-tp.h 中的宏创建的变量的一部分。

color, "\033[32m`date +'%Y/%m/%d %H:%M:%S'` xxx \033[0m"


---------------------------------------- DL ----------------------------------------
在以上背景下，ceph 官方开发了 ceph-mgr，主要目标实现 ceph 集群的管理，为外界提供统一的入口。要深入了解 ceph-mgr，就得了解 ceph-mgr 是如何跑起来的。

由 官方文档 可知，ceph-mgr 是通过可执行文件 ceph-mgr 跑起来的，在源码src/CMakeLists.txt 搜索 ceph-mgr 可以搜到 add_executable(ceph-mgr ${mgr_srcs}...，从中可以看出 ceph-mgr 主要由 src/mgr 里的文件编译出来（猜也猜的出来），main 函数在 src/ceph_mgr.cc。以上就是相关文件，有需要深入的人可以去读，这里介绍整理之后的 ceph-mgr 工作原理。

ceph-mgr 工作的模式是事件驱动型的，意思就是等待事件，事件来了则处理事件返回结果，又继续等待。其主要运行的线程包括：

messenger 线程。这是事件驱动主线程，监听某一端口，由外界给输入事件，messenger 收到事件后分派给各个处理者。通过向 monitor 订阅某一个 topic 的消息，例如 mgrmap, osdmap，monitor 会在这些数据发生变化时把事件通知到 messenger 监听的端口。事件处理器包括：
MgrStandby。Mgr 通过 standby 实现高可用，每一个运行的 ceph-mgr 都包含一个 MgrStandby，MgrStandby 并没有运行的线程，它存在于 messenger 收到消息时的回调，以及通过定时器线程运行的定时任务，并且管理着其他实体。其处理的唯一消息是 mgrmap，就是当主挂掉时要顶上来，当自己不是主时要退回去。什么时候切主由 monitor 管理，所以 MgrStandby 里切主逻辑比较简单，有一个 Mgr 实例，当收到 mgrmap 时生成该实例，存到 MgrStandby 属性里，就完了。因为在收到消息时，MgrStandby 如果看到有 Mgr 实例，就会把消息发到它那处理，在定时函数里，也会调用 mgr 的定时函数，这样，实际上，MgrStandby 就担起了主的任务。
Mgr。如上段所述，Mgr 依附于 MgrStandby 存在，也没有单独线程。它通过处理 mon_map，fs_map，osd_map等事件，在内存中维护了集群成员信息，它管理 ceph-mgr 插件，为插件提供了所有数据的来源，也在特定事件发生时通知给 ceph-mgr 的插件，例如插件的 notify 函数，就是被 Mgr 回调的。
DaemonServer。独立线程，和主 messenger 监听同一端口(待确认)。是 cluster 指标数据的主要维护者，并且负载执行对集群的操作，例如吩咐 OSD 进行 pg scrub等。
plugin 线程。plugin 是 Python 写的，每个 plugin 都跑在单独线程里，线程调用的函数是 python 类的 serve。plugin 可以在 serve 里跑个 http server 来提供对外服务，ceph-mgr 为 plugin 提供了 get，get_server 等函数，这些函数返回关于集群的指标等数据。例如 prometheus 插件，就把 ceph 内部指标通过 http 协议以 prometheus 格式暴露出来，使得监控 ceph 集群变得较为简单。ceph 是 c++ 写的，ceph 会调用 python plugin 定义的方法（例如 serve），python plugin 可以调用 c++ 定义的函数（例如 get)，python/c++ 的互调是 python 提供的机制，其基本原理是：
c++ 调 python。python 的实体在 c++ 里类型都是 PyObject，模块，函数、类、数据都是。cpython 提供了 PyImport_Import 用于通过名字得到 m模块对象对应的 PyObject，类可以通过 PyObject_GetAttrString 取模块的属性得到，以此类推，cpython 还提供了由 c 类型的值生成对应 python 类型的值的PyObject 的方法，例如 PyObject* PyString_FromString(char *)。有函数对象，有参数对象，就可以通过 PyObject * PyObject_CallObject() 调用函数，将得到的 PyObject* 再转回 c++ 类型就 OK 了。
python 调用 c++。在 c++ 里定义 PyObject* ceph_state_get(PyObject *self, PyObject *args)，在函数里里面通过 PyArg_ParseTuple(args, "ss:ceph_state_get", &handle, &what) 把参数解析为 c++ 类型，就实现了一个 Python 函数。通过 PyMethodDef CephStateMethods[] = get 把 Python 函数加入到一个注册表里。通过 Py_InitModule("ceph_state", CephStateMethods)，将注册表里的函数定义为 ceph_state 模块的属性，并把该模块注入到 python sys.path 里，python 就可以通过 ceph_state.ceph_state_get 调用该函数了


虚拟集群默认参数:
[ -z "$CEPH_NUM_MON" ] && CEPH_NUM_MON=3
[ -z "$CEPH_NUM_OSD" ] && CEPH_NUM_OSD=3
[ -z "$CEPH_NUM_MDS" ] && CEPH_NUM_MDS=3
[ -z "$CEPH_NUM_MGR" ] && CEPH_NUM_MGR=1
[ -z "$CEPH_NUM_FS"  ] && CEPH_NUM_FS=1
[ -z "$CEPH_MAX_MDS" ] && CEPH_MAX_MDS=1
[ -z "$CEPH_NUM_RGW" ] && CEPH_NUM_RGW=0
[ -z "$GANESHA_DAEMON_NUM" ] && GANESHA_DAEMON_NUM=0

[ -z "$CEPH_DIR" ] && CEPH_DIR="$PWD"
[ -z "$CEPH_DEV_DIR" ] && CEPH_DEV_DIR="$CEPH_DIR/dev"
[ -z "$CEPH_OUT_DIR" ] && CEPH_OUT_DIR="$CEPH_DIR/out"
[ -z "$CEPH_RGW_PORT" ] && CEPH_RGW_PORT=8000
[ -z "$CEPH_CONF_PATH" ] && CEPH_CONF_PATH=$CEPH_DIR


常用命令:
ceph mon dump
ceph osd dump
ceph pg dump
ceph osd crush dump
map
上面说过，monitor组件负责监视整个集群的运行状况，如各节点之间的状态、集群配置信息，这些信息由维护集群成员的守护程序来提供，如何存放这些信息呢，答案就是map，ceph monitor map 主要包括如下这几个
Monitor map：包括有关monitor 节点端到端的信息，其中包括 Ceph 集群ID，监控主机名和IP以及端口。并且存储当前版本信息以及最新更改信息，通过 “ ceph mon dump ” 查看 monitor map
OSD map：包括一些常用的信息，如集群ID、创建OSD map的 版本信息和最后修改信息，以及pool相关信息，主要包括pool 名字、pool的ID、类型，副本数目以及PGP等，还包括数量、状态、权重、最新的清洁间隔和OSD主机信息。通过命令 “ceph osd dump” 查看
PG map：包括当前PG版本、时间戳、最新的OSD Map的版本信息、空间使用比例，以及接近占满比例信息，同事，也包括每个PG ID、对象数目、状态、OSD 的状态以及深度清理的详细信息。通过命令 “ceph pg dump” 可以查看相关状态
CRUSH map： CRUSH map 包括集群存储设备信息，故障域层次结构和存储数据时定义失败域规则信息。通过 命令 “ceph osd crush dump 查看
MDS map：MDS Map 包括存储当前 MDS map 的版本信息、创建当前的Map的信息、修改时间、数据和元数据POOL ID、集群MDS数目和MDS状态，可通过"ceph mds dump"查看
副本
副本是ceph存放数据的份数，可以理解为对一个文件备份的份数，ceph默认的副本数是3，即一个主（primary ），一个次（secondary），一个次次（tertiary）,只有primary osd的副本才解释客户端请求，它将数据写入其他osd
如下,可以看到这个叫做testpool的pool中有一个叫做object1的object，他的map信息获取后可以看到
这个对象在osd1上面是主，在osd0和osd2上是次和次次，也就是说在副本数为3的情况下，每个osd存储一个副本
[root@ceph-1 ~]# ceph osd map testpool object1


Ceph RBD 的实现原理与常规操作: https://www.cnblogs.com/jmilkfan-fanguiju/p/11825071.html
rbd showmapped


性能测试:
rados bench -p <pool_name> <seconds> <write|seq|rand>

写入和查看临时数据
rados bench -p p1 10 write --no-cleanup && rados ls -p p1

测试顺序读性能
rados bench -p p1 10 seq

随机读:  rados bench -p p1 10 rand

fio 参考配置,  yum install -y fio "*librbd*", 
cat write.fio
[global]
description="write test with block size of 4M"
direct=1
ioengine=rbd
clustername=ceph
clientname=admin
pool=rbd_pool
rbdname=volume01
iodepth=32
runtime=300
rw=randrw
numjobs=1
bs=8k

[logging]
write_iops_log=write_iops_log
write_bw_log=write_bw_log
write_lat_log=write_lat_log

rbd引擎:
FIO_STATIC struct ioengine_ops ioengine = {
	.name			= "rbd",
	.version		= FIO_IOOPS_VERSION,
	.setup			= fio_rbd_setup,
	.init			= fio_rbd_init,
	.queue			= fio_rbd_queue,
	.getevents		= fio_rbd_getevents,
	.event			= fio_rbd_event,
	.cleanup		= fio_rbd_cleanup,
	.open_file		= fio_rbd_open,
	.invalidate		= fio_rbd_invalidate,
	.options		= options,
	.io_u_init		= fio_rbd_io_u_init,
	.io_u_free		= fio_rbd_io_u_free,
	.option_struct_size	= sizeof(struct rbd_options),
};

rados引擎:
FIO_STATIC struct ioengine_ops ioengine = {
	.name = "rados",
	.version		= FIO_IOOPS_VERSION,
	.flags			= FIO_DISKLESSIO,
	.setup			= fio_rados_setup,
	.queue			= fio_rados_queue,
	.getevents		= fio_rados_getevents,
	.event			= fio_rados_event,
	.cleanup		= fio_rados_cleanup,
	.open_file		= fio_rados_open,
	.invalidate		= fio_rados_invalidate,
	.options		= options,
	.io_u_init		= fio_rados_io_u_init,
	.io_u_free		= fio_rados_io_u_free,
	.option_struct_size	= sizeof(struct rados_options),
};


fio_rados参考配置:
######################################################################
# Example test for the RADOS engine.
#
# Runs a 4k random write test against a RADOS via librados
#
# NOTE: Make sure you have either Ceph pool named 'rados' or change
#       the pool parameter.
######################################################################
[global]
#logging
#write_iops_log=write_iops_log
#write_bw_log=write_bw_log
#write_lat_log=write_lat_log
ioengine=rados
clientname=admin
pool=rados
conf=/etc/ceph/ceph.conf
busy_poll=0
rw=randwrite
bs=4k

[rbd_iodepth32]
iodepth=32
size=128m
nr_files=32





gdb --args rados bench -p p1 10 write --no-cleanup
src/tools/rados/rados.cc -> main
argv_to_vec
ceph_argparse_need_usage
global_init CEPH_ENTITY_TYPE_CLIENT CODE_ENVIRONMENT_UTILITY -> global_init (defaults=0x0, args=std::vector of length 6, capacity 6 = {...}, module_type=8, code_env=CODE_ENVIRONMENT_UTILITY, flags=0, data_dir_option=0x0, run_pre_init=true)  _. global_init.cc:169
  global_pre_init
common_init_finish -> common_init.cc:87
rados_tool_common
  ret = rados.init_with_context(g_ceph_context) -> open rados
  rados.connect()
  rados.ioctx_create




rm -f rados_write && gcc rados_write.c  -lrados -lrados  -g3 -Og -Wall  -Wl,-rpath=/home/xb/project/stor/ceph/xb/docker/ceph/build/lib -lrados -L/home/xb/project/stor/ceph/xb/docker/ceph/build/lib/ -o rados_write && ./rados_write --debug_objecter=30 --debug_ms=30



写参考: https://ibz.bz/2017/01/12/8ab0314b456d4b1fa6e063dace9c9d8a.html
设置image id
ioctx->exec(oid, "rbd", "set_id", in, out)
       >io_ctx_impl->exec(obj, cls, method, inbl, outbl)
           >(::ObjectOperation)rd.call(cls, method, inbl) //将该操作封装成OSDOp，放入ObjectOperation对象的vector集合中
               >add_call(CEPH_OSD_OP_CALL, cname, method, indata, NULL, NULL, NULL)
           >operate_read(oid, &rd, &outbl) //发起读请求
               >Objecter::Op *objecter_op = objecter->prepare_read_op(oid, oloc,*o, snap_seq, pbl, flags,onack, &ver) //创建Op的实例 数据结构变成Op
               >objecter->op_submit(objecter_op) //提交到objecter层 操作对象为Objecter::Op
                   >_op_submit_with_budget(op, lc, ctx_budget)
                       >int op_budget = _take_op_budget(op) //减去该Op的预算for throttle;
                           >int op_budget = calc_op_budget(op) //预算值是该Op的字节大小
                           > _throttle_op(op, op_budget) //这里是Objecter的Throttle层，如果keep_balanced_budget=true，能实现对速度的限制（op_throttle_bytes&op_throttle_ops）
                       >_op_submit(op, lc)
                           >_calc_target(&op->target, &op->last_force_resend) //计算该op的操作对象（用到CRUSH算法）
                           >_get_session(op->target.osd, &s, lc) //为该Op构建与osd对应的OSDSession
                           >_send_op_account(op) //登记该次op操作
                           > m = _prepare_osd_op(op) //使用Op中的信息，初始化MOSDOp的实例
                           >_session_op_assign(s, op) //将Op与OSDSession相关联。
                           > _send_op(op, m)
                               >op->session->con->send_message(m) //进入Massenger层，操作对象MOSDOp
                                   >static_cast<SimpleMessenger*>(msgr)->send_message(m, this) //使用使用massenger层的SimpleMessenger的实例发生消息
                                       >_send_message(m, con)
                                           >submit_message(m, static_cast<PipeConnection*>(con),con->get_peer_addr(), con->get_peer_type(), false) //提交信息
                                               >static_cast<PipeConnection*>(con)->try_get_pipe(&pipe) //获取该PipConnection对应的Pipe的实例
                                                   >pipe->_send(m) //通过Pipe发送消息，即：把消息放入到Pipe::out_q队列中，并通知Pipe中的写线程来做实际的发生操作。
                                                       >out_q[m->get_priority()].push_back(m);
                                                   >dispatch_queue.local_delivery(m, m->get_priority()) //如果发送端与接收端是同一个，则直接将消息投递到DispathcQueue::local_messages中


m_ictx->data_ctx.aio_operate(m_oid, rados_completion, &m_write,m_snap_seq, m_snaps)
   >io_ctx_impl->aio_operate(obj, (::ObjectOperation*)o->impl, c->pc,snapc, 0)
       >objecter->mutate(oid, oloc, *o, snap_context, ut, flags, onack, oncommit,&c->objver) //进入Objecter层
           >prepare_mutate_op(oid, oloc, op, snapc, mtime, flags, onack, oncommit, objver) //封装成Op
           >objecter->op_submit(objecter_op) //提交到objecter层 操作对象为Objecter::Op
               >_op_submit_with_budget(op, lc, ctx_budget)
                   >int op_budget = _take_op_budget(op) //减去该Op的预算for throttle;
                       >int op_budget = calc_op_budget(op) //预算值是该Op的字节大小
                       > _throttle_op(op, op_budget) //这里是Objecter的Throttle层，如果keep_balanced_budget=true，能实现对速度的限制（op_throttle_bytes&op_throttle_ops）
                   >_op_submit(op, lc)
                       >_calc_target(&op->target, &op->last_force_resend) //计算该op的操作对象（用到CRUSH算法）
                       >_get_session(op->target.osd, &s, lc) //为该Op构建与osd对应的OSDSession
                       >_send_op_account(op) //登记该次op操作
                       > m = _prepare_osd_op(op) //使用Op中的信息，初始化MOSDOp的实例
                       >_session_op_assign(s, op) //将Op与OSDSession相关联。
                       > _send_op(op, m)
                           >op->session->con->send_message(m) //进入Massenger层，操作对象MOSDOp
                               >static_cast<SimpleMessenger*>(msgr)->send_message(m, this) //使用使用massenger层的SimpleMessenger的实例发生消息
                                   >_send_message(m, con)
                                       >submit_message(m, static_cast<PipeConnection*>(con),con->get_peer_addr(), con->get_peer_type(), false) //提交信息
                                           >static_cast<PipeConnection*>(con)->try_get_pipe(&pipe) //获取该PipConnection对应的Pipe的实例
                                               >pipe->_send(m) //通过Pipe发送消息，即：把消息放入到Pipe::out_q队列中，并通知Pipe中的写线程来做实际的发生操作。
                                                   >out_q[m->get_priority()].push_back(m);
                                               >dispatch_queue.local_delivery(m, m->get_priority()) //如果发送端与接收端是同一个，则直接将消息投递到DispathcQueue::local_messages中


                                      


编译, do_cmake.sh

cmake -DCMAKE_C_FLAGS="-O0 -g3 -gdwarf-4" -DCMAKE_CXX_FLAGS="-O0 -g3 -gdwarf-4" -DBOOST_J=$(nproc) $ARGS "$@" ..    
cmake -DWITH_EVENTTRACE=OFF -DWITH_LTTNG=OFF -DWITH_BLKIN=OFF '-DCMAKE_C_FLAGS=-O0 -g3 -gdwarf-4' '-DCMAKE_CXX_FLAGS=-O0 -g3 -gdwarf-4' ..





crush算法, 深入理解ceph crush(3)—Object至PG映射源码分析: https://www.dovefi.com/post/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3crush3object%E8%87%B3pg%E6%98%A0%E5%B0%84%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/
Created I/O context.
// 在断点处卡住
Breakpoint 1, crush_do_rule (map=0x7fffcc010040, ruleno=0, x=-2119696504, result=0x7fffffffd5d8, result_max=3, weight=0x7fffcc00fea0,
    weight_max=3, cwin=0x7fffffffd508, choose_args=0x0) at /home/user/ceph/src/crush/mapper.c:887
887    {
Missing separate debuginfos, use: debuginfo-install libibverbs-1.1.8mlnx1-OFED.3.3.0.0.9.33100.x86_64
(gdb) bt                // 查看当前的函数堆栈
#0  crush_do_rule (map=0x7fffcc010040, ruleno=0, x=-2119696504, result=0x7fffffffd5d8, result_max=3, weight=0x7fffcc00fea0,
    weight_max=3, cwin=0x7fffffffd508, choose_args=0x0) at /home/user/ceph/src/crush/mapper.c:887
#1  0x00007fffeee59274 in do_rule<std::vector<unsigned int, mempool::pool_allocator<(mempool::pool_index_t)15, unsigned int> > > (
    choose_args_index=<optimized out>, weight=std::vector of length 3, capacity 4 = {...}, maxout=3,
    out=std::vector of length 0, capacity 0, x=-2119696504, rule=<optimized out>, this=<optimized out>)
    at /home/user/ceph/src/crush/CrushWrapper.h:1502
#2  OSDMap::_pg_to_raw_osds (this=this@entry=0x78b960, pool=..., pg=..., osds=osds@entry=0x7fffffffd6d0, ppps=ppps@entry=0x7fffffffd6c4)
    at /home/user/ceph/src/osd/OSDMap.cc:2061
#3  0x00007fffeee5b036 in OSDMap::_pg_to_up_acting_osds (this=0x78b960, pg=..., up=up@entry=0x7fffffffd890,
    up_primary=up_primary@entry=0x7fffffffd834, acting=acting@entry=0x7fffffffd8b0, acting_primary=acting_primary@entry=0x7fffffffd838,
    raw_pg_to_pg=raw_pg_to_pg@entry=true) at /home/user/ceph/src/osd/OSDMap.cc:2295
#4  0x00007ffff7b42377 in pg_to_up_acting_osds (acting_primary=0x7fffffffd838, acting=0x7fffffffd8b0, up_primary=0x7fffffffd834,
    up=0x7fffffffd890, pg=..., this=<optimized out>) at /home/user/ceph/src/osd/OSDMap.h:1159
#5  Objecter::_calc_target (this=this@entry=0x78b2b0, t=t@entry=0x7acf18, con=con@entry=0x0, any_change=any_change@entry=false)
    at /home/user/ceph/src/osdc/Objecter.cc:2846
#6  0x00007ffff7b4ff89 in Objecter::_op_submit (this=this@entry=0x78b2b0, op=op@entry=0x7acef0, sul=..., ptid=ptid@entry=0x7fffffffdcc8)
    at /home/user/ceph/src/osdc/Objecter.cc:2393
#7  0x00007ffff7b5d128 in Objecter::_op_submit_with_budget (this=this@entry=0x78b2b0, op=op@entry=0x7acef0, sul=...,
    ptid=ptid@entry=0x7fffffffdcc8, ctx_budget=ctx_budget@entry=0x0) at /home/user/ceph/src/osdc/Objecter.cc:2307
#8  0x00007ffff7b5d3aa in Objecter::op_submit (this=0x78b2b0, op=0x7acef0, ptid=0x7fffffffdcc8, ctx_budget=0x0)
    at /home/user/ceph/src/osdc/Objecter.cc:2274
#9  0x00007ffff7b0c4f0 in librados::IoCtxImpl::operate (this=this@entry=0x7a1ec0, oid=..., o=o@entry=0x7fffffffe020,
    pmtime=pmtime@entry=0x0, flags=flags@entry=0) at /home/user/ceph/src/librados/IoCtxImpl.cc:715
#10 0x00007ffff7b17f5a in librados::IoCtxImpl::write (this=this@entry=0x7a1ec0, oid=..., bl=..., len=len@entry=16, off=off@entry=0)
    at /home/user/ceph/src/librados/IoCtxImpl.cc:648
#11 0x00007ffff7ae241d in rados_write (io=0x7a1ec0, o=<optimized out>, buf=0x401039 "Hello World!", len=16, off=0)
    at /home/user/ceph/src/librados/librados.cc:3691
#12 0x0000000000400d0c in main (argc=1, argv=0x7fffffffe2e8) at rados_write.c:75


yum install -y jq

onestor:
对象接口: src/include/rados/librados.h, objects, omap


test rados:
src/test/librados/io.cc


pg:
每个osd承载100个pg


op_shardedwq 工作队列,内部包含多个队列, osd_op_tp 线程池


waiting_for_map

execute_ctx


admin socket asok: find / -name "*.asok"
注册命令: register_command
std::string AdminSocket::bind_and_listen




set print pretty on
set history save on

kill gdb: 
kill -9 `ps aux|grep gdb|grep -v grep|awk '{print$2}'`


void PrimaryLogPG::do_command
int CephContext::do_command
get_tracked_conf_keys

void OSD::asok_command

ceph daemon xxx perf_dump -> def admin_socket(asok_path, cmd, format='')
ceph.in


pub/sub, 发布订阅, map订阅和更新
1 与mon建立链接

2 向mon的TgtMapMonitor 注册一个回调函数，
        因为TGT需要自己知道TGTMap，而实际上这个TGTMap是保存在TgtMapMonitor中，
        所以需要当TgtMapMonitor::check_subs函数被调用是，且更新了TgtMap,则顺便将TgtMap同步给TGT进程。
        如何将c++的 map同步给c程序呢？
3 TGT向mon通知节点状态或者信息变更



参考RadosClient、OSD、DSE，都是继承Dispatcher类，类成员保存一个MonClient的实例，
        派生类提供函数完成以下动作：
                MonClient对象实例创建、
                设置messenger给MonClient并将其加入Dispatcher队列监听
                MonClient 初始化、
                向monitor发起认证&&认证成功后建立链接、向monitor注册订阅、
                

FR001_协议注册
        tgtClient类设计
        
        class TgtClient::public Dispatcher
                MonClient对象实例创建，参考cephd_osd、ceph_dse等函数
                        MonClient mc(g_ceph_context)
                        /* 1 初始化monmap*/
                        mc.build_initial_monmap  
                        mc.get_monmap_privately
                /* 2 设置messenger给MonClient，并将其加入Dispatcher队列监听 */        
                messenger = Messenger:;create_client_messenger(cct, "tgtclient");        
                monc.sent_messenger(messenger);        
                messenger->add_dispatcher_head(this)
                
                monc->set_want_keys
                monc->init()         /* 2 MonClient 初始化*/
                monc->set_log_client
                monc->authenticate          /* 3 向monitor发起认证，认证成功后建立链接*/
                while(monc->wait_auth_rotating(30.0 < 0))        
                monc->sub_want                /* 4 向monitor注册订阅 */                
                monc->renew_subs()  /* 5 即刻下发订阅请求 */ -> MonClient::ms_dispatch -> 收到monitor的map更新
                
                ms_dispatch??
                
FR002_协议全局视图通知和维护
        tgtmap类设计：
                class tgtmap {
                        feature  //结构体版本
                        epoch
                        start_time
                        flag  //预留字段
                        map<string, tgt_info_t> //key为tgt_name, value为tgt节点的一些信息
                }        
                接口：需要提供的接口较多，如contains、add、remove、get_***
                struct tgt_info_t {
                        tgt_name
                        manage_network
                        public_network
                        block_service_network
                        block_service_ipaddr
                        down_epoch
                        down_time
                        up_epoch
                        up_time
                        is_up
                        is_master
                }
                接口:encode 、decode 、print
        工作量评估：参考MonMap实现，代码量在500行以上        
                
        TgtmapMonitor设计：
                Monitor::handle_subscribe
                        向单个订阅者发送最新的map
                        TgtmapMonitor::check_sub


                PaxosService::refresh
                        向paxos系统请求最新的map数据，更新后再调用check_subs去通知订阅者
                        TgtmapMonitor::update_from_paxos()
                                遍历所有订阅者，并发送最新的map
                                TgtmapMonitor::check_subs
                                        TgtmapMonitor::check_sub
                PaxosService::_active
                        初始化pending_map,使其与当前全局的tgtmap一直
                        TgtmapMonitor::create_pending()
                        TgtmapMonitor::create_initial()
                        TgtmapMonitor::upgrade_format()
                        TgtmapMonitor::on_active()
                
                PaxosService::dispatch
                        预处理tgtmap查询相关的请求、如查询节点信息、查询master节点，调用mon->reply_command进行回复客户端。
                        TgtmapMonitor::preprocess_query(MonOpRequestRef op)

                        处理tgtmap信息的更新，先对pending_map进行更新，以待集群共识后写入数据库
                        TgtmapMonitor::perpare_update(MonOpRequestRef op)
                        PaxosService::propose_pending
                                将整个pending_map编码成一个bufferlist，再put到transaction
                                TgtmapMonitor::encode_pending

                提案初始化完成并写入本地后，调用do_refresh触发对非ONETIME sub的再次检查
                Paxos::do_refresh
                Monitor::init_paxos
                        Monitor::refresh_from_paxos
                                PaxosService::refresh
                                        TgtmapMonitor::update_from_paxos()

        工作量评估：参考monmapMonitor实现，代码量在1k左右

                                
        MonClient接口设计        
                MonClient::ms_dispatch
                        处理tgtmap的订阅响应,参考handle_monmap实现
                        case CEPH_MSG_TGT_MAP:
                                handle_tgtmap()

Objecter::tick()堆栈
#0  Objecter::tick (this=0x4e3520) at /home/xb/project/ceph/xb/ceph/src/osdc/Objecter.cc:2122
#1  0x00007ffff7cf2eae in std::__invoke_impl<void, void (Objecter::*&)(), Objecter*&> (__f=@0x586b40: (void (Objecter::*)(Objecter * const)) 0x7ffff7c678d2 <Objecter::tick()>, __t=@0x586b50: 0x4e3520)
    at /opt/rh/devtoolset-11/root/usr/include/c++/11/bits/invoke.h:74
#2  0x00007ffff7cf2a81 in std::__invoke<void (Objecter::*&)(), Objecter*&> (__fn=@0x586b40: (void (Objecter::*)(Objecter * const)) 0x7ffff7c678d2 <Objecter::tick()>) at /opt/rh/devtoolset-11/root/usr/include/c++/11/bits/invoke.h:96
#3  0x00007ffff7cf25bb in std::_Bind<void (Objecter::*(Objecter*))()>::__call<void, , 0ul>(std::tuple<>&&, std::_Index_tuple<0ul>) (this=0x586b40, __args=<unknown type in /home/xb/project/ceph/xb/ceph/build/lib/librados.so.2, CU 0x623919, DIE 0x7fe088>)
    at /opt/rh/devtoolset-11/root/usr/include/c++/11/functional:420
#4  0x00007ffff7cf202f in std::_Bind<void (Objecter::*(Objecter*))()>::operator()<, void>() (this=0x586b40) at /opt/rh/devtoolset-11/root/usr/include/c++/11/functional:503
#5  0x00007ffff7cf185d in fu2::abi_310::detail::invocation::invoke<std::_Bind<void (Objecter::*(Objecter*))()>&>(std::_Bind<void (Objecter::*(Objecter*))()>&) (callable=...) at /home/xb/project/ceph/xb/ceph/src/include/function2.hpp:126
#6  0x00007ffff7cf0d53 in fu2::abi_310::detail::type_erasure::invocation_table::function_trait<void ()>::internal_invoker<fu2::abi_310::detail::type_erasure::box<false, std::_Bind<void (Objecter::*(Objecter*))()>, std::allocator<std::_Bind<void (Objecter::*(Objecter*))()> > >, false>::invoke(fu2::abi_310::detail::type_erasure::data_accessor*, unsigned long) (data=0x4c11d0, capacity=16) at /home/xb/project/ceph/xb/ceph/src/include/function2.hpp:518
#7  0x00007ffff7cc9a4e in fu2::abi_310::detail::type_erasure::tables::vtable<fu2::abi_310::detail::property<true, false, void ()> >::invoke<0ul, fu2::abi_310::detail::type_erasure::data_accessor*, unsigned long const&>(fu2::abi_310::detail::type_erasure::data_accessor*&&, unsigned long const&) const (this=0x4c11e0) at /home/xb/project/ceph/xb/ceph/src/include/function2.hpp:921
#8  0x00007ffff7cc9aaf in fu2::abi_310::detail::type_erasure::erasure<true, fu2::abi_310::detail::config<true, false, 16ul>, fu2::abi_310::detail::property<true, false, void ()> >::invoke<0ul, fu2::abi_310::detail::type_erasure::erasure<true, fu2::abi_310::detail::config<true, false, 16ul>, fu2::abi_310::detail::property<true, false, void ()> >&>(fu2::abi_310::detail::type_erasure::erasure<true, fu2::abi_310::detail::config<true, false, 16ul>, fu2::abi_310::detail::property<true, false, void ()> >&) (
    erasure=...) at /home/xb/project/ceph/xb/ceph/src/include/function2.hpp:1123
#9  0x00007ffff7cc9aea in fu2::abi_310::detail::type_erasure::invocation_table::operator_impl<0ul, fu2::abi_310::detail::function<fu2::abi_310::detail::config<true, false, 16ul>, fu2::abi_310::detail::property<true, false, void ()> >, void ()>::operator()()
    (this=0x4c11d0) at /home/xb/project/ceph/xb/ceph/src/include/function2.hpp:702
#10 0x00007ffff7cb7b23 in ceph::timer<ceph::coarse_mono_clock>::timer_thread (this=0x4e36a0) at /home/xb/project/ceph/xb/ceph/src/common/ceph_timer.h:122
#11 0x00007ffff7cf4c22 in std::__invoke_impl<void, void (ceph::timer<ceph::coarse_mono_clock>::*)(), ceph::timer<ceph::coarse_mono_clock>*>(std::__invoke_memfun_deref, void (ceph::timer<ceph::coarse_mono_clock>::*&&)(), ceph::timer<ceph::coarse_mono_clock>*&&) (__f=<unknown type in /home/xb/project/ceph/xb/ceph/build/lib/librados.so.2, CU 0x623919, DIE 0x7f8db5>, __t=<unknown type in /home/xb/project/ceph/xb/ceph/build/lib/librados.so.2, CU 0x623919, DIE 0x7f8dc4>)
    at /opt/rh/devtoolset-11/root/usr/include/c++/11/bits/invoke.h:74
#12 0x00007ffff7cf4b81 in std::__invoke<void (ceph::timer<ceph::coarse_mono_clock>::*)(), ceph::timer<ceph::coarse_mono_clock>*>(void (ceph::timer<ceph::coarse_mono_clock>::*&&)(), ceph::timer<ceph::coarse_mono_clock>*&&) (
    __fn=<unknown type in /home/xb/project/ceph/xb/ceph/build/lib/librados.so.2, CU 0x623919, DIE 0x7f9391>) at /opt/rh/devtoolset-11/root/usr/include/c++/11/bits/invoke.h:96
#13 0x00007ffff7cf4af1 in std::thread::_Invoker<std::tuple<void (ceph::timer<ceph::coarse_mono_clock>::*)(), ceph::timer<ceph::coarse_mono_clock>*> >::_M_invoke<0ul, 1ul> (this=0x4e7718) at /opt/rh/devtoolset-11/root/usr/include/c++/11/bits/std_thread.h:253
#14 0x00007ffff7cf49c4 in std::thread::_Invoker<std::tuple<void (ceph::timer<ceph::coarse_mono_clock>::*)(), ceph::timer<ceph::coarse_mono_clock>*> >::operator() (this=0x4e7718) at /opt/rh/devtoolset-11/root/usr/include/c++/11/bits/std_thread.h:260
#15 0x00007ffff7cf4868 in std::thread::_State_impl<std::thread::_Invoker<std::tuple<void (ceph::timer<ceph::coarse_mono_clock>::*)(), ceph::timer<ceph::coarse_mono_clock>*> > >::_M_run (this=0x4e7710)
    at /opt/rh/devtoolset-11/root/usr/include/c++/11/bits/std_thread.h:211
#16 0x00007fffeef0a7c4 in execute_native_thread_routine () from /home/xb/project/ceph/xb/ceph/build/lib/libceph-common.so.2
#17 0x00007fffec27bea5 in start_thread () from /lib64/libpthread.so.0
#18 0x00007ffff78b5b0d in clone () from /lib64/libc.so.6


layer:
app: rados_aio_write
librados: rados_aio_write -> LIBRADOS_C_API_BASE_DEFAULT(rados_aio_write) -> queue_aio_write -> aio_write_list.push_back(&c->aio_write_list_item)



rados_write_full


bool OSD::ms_dispatch(Message *m)
|---如果是标记osd down的话，直接返回service.got_stop_ack()， return true
|---如果osd已经已经停止服务，则return true
|---do_waiters()      //list<OpRequestRef> finished 只保存pg创建的请求
    |---dispatch_op(next)
        |---handle_pg_create(op)   //处理新的请求的时候，需要等待创建pg的请求处理完成
|---_dispatch(m)
    |---handle_osd_map(static_cast<MOSDMap*>(m))    //CEPH_MSG_OSD_MAP 创建osd map
        |---
    |---handle_scrub(static_cast<MOSDScrub*>(m));  //MSG_OSD_SCRUB 
        |---
    |---handle_command(static_cast<MCommand*>(m))  //MSG_COMMAND
        |---
    |---osdmap    //MSG_OSD_PG_CREATE，如果是创建pg的前提是osdmap存在
        |---如果osdmap不存在，则waiting_for_osdmap.push_back(op)，然后延时执行
    |---dispatch_op(op)    //只处理MSG_OSD_PG_CREATE
        |---handle_pg_create(op)  
            |---

void OSD::handle_pg_create(OpRequestRef op)
|---require_mon_peer //如果不是由MON发过来的消息，则不能执行
|---require_same_or_newer_map  //如果osdmap的epoch比消息中的epoch新，则不能执行
|---op->mark_started()   //op is struct OpRequest : public TrackedOp
    |---OpRequest::mark_flag_point
|---for
    |---如果处于split(分裂)状态，则不用处理
    |---如果已经存在pool，则不用处理
    |---osdmap->pg_to_up_acting_osds(on, &up, &up_primary, &acting, &acting_primary)  //OSDMap.h
        |---void OSDMap::_pg_to_up_acting_osds(
            |---acting_primary 获取主OSD的id
    |---role = osdmap->calc_pg_role   //OSDMap::calc_pg_role
        |---OSDMap::calc_pg_rank    //确定OSD的角色
    |---如果当前osd不是主osd(MON只给主OSD发消息)，则不能处理
    |---history.same_primary_since > m->epoch  如果之前处理的最新的epoch比该消息中的epoch新，则不能处理
    |---enqueue_peering_evt
|---op_shardedwq.queue   //入队，等待process处理



连接堆栈:
Breakpoint 3, ProtocolV2::send_message (this=0x580180, m=0x57e430) at /home/xb/project/ceph/xb/ceph/src/msg/async/ProtocolV2.cc:422
422     void ProtocolV2::send_message(Message *m) {
(gdb) bt
#0  ProtocolV2::send_message (this=0x580180, m=0x57e430) at /home/xb/project/ceph/xb/ceph/src/msg/async/ProtocolV2.cc:422
#1  0x00007fffee93f6a5 in AsyncConnection::send_message (this=0x57fd20, m=0x57e430) at /home/xb/project/ceph/xb/ceph/src/msg/async/AsyncConnection.cc:567
#2  0x00007fffeeab00cd in MonConnection::start (this=0x579aa8, epoch=0, entity_name=...) at /home/xb/project/ceph/xb/ceph/src/mon/MonClient.cc:1770
#3  0x00007fffeeaa29bb in MonClient::_reopen_session (this=0x7fffffffdd40, rank=-1) at /home/xb/project/ceph/xb/ceph/src/mon/MonClient.cc:726
#4  0x00007fffeeaa0eaf in MonClient::authenticate (this=0x7fffffffdd40, timeout=300) at /home/xb/project/ceph/xb/ceph/src/mon/MonClient.cc:567
#5  0x00007fffeea9c3b2 in MonClient::get_monmap_and_config (this=0x7fffffffdd40) at /home/xb/project/ceph/xb/ceph/src/mon/MonClient.cc:153
#6  0x00007ffff7c3af87 in librados::v14_2_0::RadosClient::connect (this=0x4b1000) at /home/xb/project/ceph/xb/ceph/src/librados/RadosClient.cc:237
#7  0x00007ffff7bb4d4b in _rados_connect (cluster=0x4b1000) at /home/xb/project/ceph/xb/ceph/src/librados/librados_c.cc:178
#8  0x00000000004012d1 in main (argc=3, argv=0x7fffffffe628) at rados_write.c:50



read, book, ceph之rados设计原理与实现:
pg中的对象排序
分层目录: hash值逆序, 老PG(祖先)和新PG(子孙), 在一个大目录下, 父包子, 修改文件夹归属即可, 无需移动对象(file), pgp(placement_group_placement): 分裂前的祖先, 
涟漪式集群表传播机制, 可扩展的故障检测机制(类似swim, osd间心跳机制, 精挑小集群), 
crush: ceph osd crush tunables default
monitor: cluster_map串行化, 合并请求, 特定monitor统一发权威日志, leader和peon, mgr做统计, 




rbd: 元数据, #define CEPH_RBD_FEATURES_H, src/include/rbd_types.h, 创建rbd, 参考: https://zhuanlan.zhihu.com/p/643939797, ceph存储专栏: https://www.zhihu.com/column/c_1661407683897901056, image元数据: https://blog.csdn.net/easonwx/article/details/124155878
卷信息目录: rbd_directory, 创池和rbd_image: ceph osd pool create blockpool 3; rbd create blockpool/vol_2 --size 100M
查看rbd_image: rados -p blockpool ls; rados -p blockpool listomapvals rbd_directory; order为0x16，因此rados对象大小为1 << 0x16=4M=65536B
gdb --args ./bin/rbd create --size 1024 p1/image2
src/tools/rbd/rbd.cc -> main -> return shell.execute(argc, argv) -> src\tools\rbd\action\Create.cc -> main -> 
Shell::Action action({"create"}, {}, "Create an empty image.", at::get_long_features_help(),&get_arguments, &execute)
int execute
  utils::get_image_options
  utils::get_image_options
  utils::init(pool_name, namespace_name, &rados, &io_ctx)
    int RGWRados::init_rados
      rados->init_with_context(g_ceph_context) -> return rados_create_with_context
        *cluster = librados_test_stub::get_cluster()->create_rados_client(cct)
      rados.connect()
  do_create -> //librbd层：生成RBD对象
    return rbd.create4(io_ctx, imgname, size, opts) ->  librbd::create(io_ctx, name, "", size, opts, "", "", false) -> int create(IoCtx& io_ctx, const std::string &image_name
      generate_image_id -> 随机产生image id
      detect_format -> 获取image的格式：format2, 检查image是否存在
      mirror_image_mode = RBD_MIRROR_IMAGE_MODE_JOURNAL
      image::CreateRequest<> *req = image::CreateRequest<>::create
      req->send() -> void CreateRequest<I>::send()
        validate_features
        validate_order
        validate_striping(m_cct, m_order, m_stripe_unit, m_stripe_count) -> 校验条带单元 m_stripe_unit(条带单元大小)和条带宽度 m_stripe_count(条带横跨的object个数)
        validate_data_pool()
          RBD_FEATURE_DATA_POOL rados.ioctx_create(m_data_pool.c_str(), m_data_io_ctx)
          set_namespace -> 设置隔离域namespace：一个pool就是一个数据隔离域: |p1-隔离域|p2-隔离域|...隔离域|
          handle_validate_data_pool -> 回调函数
            add_image_to_directory
               cls_client::dir_add_image
                op->exec("rbd", "dir_add_image", bl) -> rbd/cls_rbd_client层：添加image到RBD目录
               create_rados_callback<klass, &klass::handle_add_image_to_directory>(this) -> 创建回调函数 <- librbd::util::detail::rados_callback<librbd::image::CreateRequest<librbd::ImageCtx><- Finisher::finisher_thread_entry < ...
                添加成功后，创建ID对象，并设置回调处理函数 handle_create_id_object() -> create_id_object
                  op.create(true) -> 封装OP写操作：进入librados层，添加一个OSDOp CREATE，创建object_info_t对象 -> void librados::ObjectWriteOperation::create
                    add_op(CEPH_OSD_OP_CREATE)
                  cls_client::set_id(&op, m_image_id);//创建image的ID对象，并写入image id
                  create_rados_callback<klass, &klass::handle_create_id_object>(this)
                  m_io_ctx.aio_operate(m_id_obj, comp, &op) -> int librados::IoCtx::aio_operate -> int librados::IoCtxImpl::aio_operate
                    Context *oncomplete = new C_aio_Complete(c) -> 异步IO公共回调 -> void librados::IoCtxImpl::C_aio_Complete::finish
                    queue_aio_write(c) -> aio入队
                    Objecter::Op *op = objecter->prepare_mutate_op -> CEPH_OSD_FLAG_WRITE -> 把ObjectOperation封装为Op类型
                    objecter->op_submit(op, &c->tid) -> 提交OP
               m_io_ctx.aio_operate(RBD_DIRECTORY, comp, &op) -> OP操作在"rbd_directory"执行
          req->send()
            read_rbd_info() -> 读"rbd_info"
      r = cond.wait()
  thick_write


void CreateRequest<I>::handle_create_id_object
  negotiate_features
    handle_negotiate_features
      create_image -> 创建rbd_image -> void create_image(librados::ObjectWriteOperation *op
        op->exec("rbd", "create", bl) -> int create(cls_method_context_t hctx, bufferlist *in, bufferlist *out)
          ...
          omap_vals["modify_timestamp"] = timestampbl -> 写入image元数据
          ...
          cls_cxx_map_set_vals(hctx, &omap_vals)
      handle_create_image -> 回调
        set_stripe_unit_count
          cls_client::set_stripe_unit_count
          create_rados_callback<klass, &klass::handle_set_stripe_unit_count> -> 回调
          m_io_ctx.aio_operate(m_header_obj, comp, &op)


handle_set_stripe_unit_count
  object_map_resize
    encode(object_count, in);//计算得到对象object的数目
    rados_op->exec("rbd", "object_map_resize", in)

create_journal -> handle_create_journal
          
完成镜像:
void CreateRequest<I>::complete(int r)          

创建rbd_image流程简图: src/librbd/image/CreateRequest.h -> CREATE IMAGE

test: 单元测试, src/test
src/test/librados/c_write_operations.cc



test/librados_example/rados_write_op_create.c
...
rados_create_write_op -> extern "C" rados_write_op_t _rados_create_write_op()
  new (std::nothrow)::ObjectOperation
rados_write_op_create -> extern "C" void _rados_write_op_create
  void create(bool excl)
    OSDOp& o = add_op(CEPH_OSD_OP_CREATE)
rados_write_op_write(op, "four", 4, 0) -> extern "C" void _rados_write_op_write
  add_data(CEPH_OSD_OP_WRITE
rados_write_op_operate(op, io, "test", NULL, 0)




cmake --DEBUG_TEST=ON


创建池(rados_pool_create(rados, pool_name)) -> rados_pool_create(*cluster, pool_name.c_str())
wait_for_osdmap
onfinish
objecter->create_pool(name, onfinish, crush_rule)
  lookup_pg_pool_name
  op->pool_op = POOL_OP_CREATE
  pool_op_submit(op) -> _pool_op_submit
     MPoolOp *m = new MPoolOp
     monc->send_mon_message(m) -> void Monitor::send_mon_message



fio, 配置文件: static struct fio_option options[]
fio --enghelp
fio ./fio_rados.fio
fio.c -> main
fio_server_create_sk_key
parse_options
fio_backend
  stat_init()
  run_threads
    setup_files
      err = td->io_ops->setup(td) -> static int fio_rados_setup
        _fio_setup_rados_data
        _fio_rados_connect
          rados_create(&rados->cluster, o->client_name)
          pool_name -> rados
          rados_ioctx_create
          for td->o.nr_files 32个文件
            rados_write(rados->io_ctx, f->file_name, "", 0, 0) -> filename=rbd_iodepth32.0.0 -> extern "C" int _rados_write
            ...


关键数据结构, 公共模块, src/common/
hash对象 struct hobject_t {

线程池: class ThreadPool
void ThreadPool::start()

共享线程池: class ShardedThreadPool

定时器: class SafeTimer


性能统计:
ceph daemon /var/run/ceph/ceph-client.828073.asok perf dump
find / -name "*.asok"|grep admin|while read i;do echo $i; ceph daemon $i perf dump;done
参考:
/tmp/ceph-asok.4Enc1o/client.admin.219209.asok
{
    "AsyncMessenger::Worker-0": {
        "msgr_recv_messages": 3,
        "msgr_send_messages": 3,
        "msgr_recv_bytes": 1232,
        "msgr_send_bytes": 288,
        "msgr_created_connections": 4,
        "msgr_active_connections": 0,
        "msgr_running_total_time": 0.004081387,
        "msgr_running_send_time": 0.000138548,
        "msgr_running_recv_time": 0.003495461,
        "msgr_running_fast_dispatch_time": 0.000000000,
        "msgr_send_messages_queue_lat": {
            "avgcount": 2,
            "sum": 0.004103733,
            "avgtime": 0.002051866
        },
        "msgr_handle_ack_lat": {
            "avgcount": 0,
            "sum": 0.000000000,
            "avgtime": 0.000000000
        }
    },
    "AsyncMessenger::Worker-1": {
        "msgr_recv_messages": 19,
        "msgr_send_messages": 9,
        "msgr_recv_bytes": 16928,
        "msgr_send_bytes": 1680,
        "msgr_created_connections": 2,
        "msgr_active_connections": 1,
        "msgr_running_total_time": 0.137784121,
        "msgr_running_send_time": 0.028340499,
        "msgr_running_recv_time": 0.055439772,
        "msgr_running_fast_dispatch_time": 0.000000000,
        "msgr_send_messages_queue_lat": {
            "avgcount": 8,
            "sum": 0.004176573,
            "avgtime": 0.000522071
        },
        "msgr_handle_ack_lat": {
            "avgcount": 0,
            "sum": 0.000000000,
            "avgtime": 0.000000000
        }
    },
    "AsyncMessenger::Worker-2": {
        "msgr_recv_messages": 0,
        "msgr_send_messages": 2,
        "msgr_recv_bytes": 0,
        "msgr_send_bytes": 0,
        "msgr_created_connections": 10,
        "msgr_active_connections": 2,
        "msgr_running_total_time": 0.020722024,
        "msgr_running_send_time": 0.000000000,
        "msgr_running_recv_time": 0.018860696,
        "msgr_running_fast_dispatch_time": 0.000000000,
        "msgr_send_messages_queue_lat": {
            "avgcount": 0,
            "sum": 0.000000000,
            "avgtime": 0.000000000
        },
        "msgr_handle_ack_lat": {
            "avgcount": 0,
            "sum": 0.000000000,
            "avgtime": 0.000000000
        }
    },
    "cct": {
        "total_workers": 0,
        "unhealthy_workers": 0
    },
    "finisher-radosclient": {
        "queue_len": 0,
        "complete_latency": {
            "avgcount": 0,
            "sum": 0.000000000,
            "avgtime": 0.000000000
        }
    },
    "mempool": {
        "bloom_filter_bytes": 0,
        "bloom_filter_items": 0,
        "bluestore_alloc_bytes": 0,
        "bluestore_alloc_items": 0,
        "bluestore_cache_data_bytes": 0,
        "bluestore_cache_data_items": 0,
        "bluestore_cache_onode_bytes": 0,
        "bluestore_cache_onode_items": 0,
        "bluestore_cache_meta_bytes": 0,
        "bluestore_cache_meta_items": 0,
        "bluestore_cache_other_bytes": 0,
        "bluestore_cache_other_items": 0,
        "bluestore_Buffer_bytes": 0,
        "bluestore_Buffer_items": 0,
        "bluestore_Extent_bytes": 0,
        "bluestore_Extent_items": 0,
        "bluestore_Blob_bytes": 0,
        "bluestore_Blob_items": 0,
        "bluestore_SharedBlob_bytes": 0,
        "bluestore_SharedBlob_items": 0,
        "bluestore_inline_bl_bytes": 0,
        "bluestore_inline_bl_items": 0,
        "bluestore_fsck_bytes": 0,
        "bluestore_fsck_items": 0,
        "bluestore_txc_bytes": 0,
        "bluestore_txc_items": 0,
        "bluestore_writing_deferred_bytes": 0,
        "bluestore_writing_deferred_items": 0,
        "bluestore_writing_bytes": 0,
        "bluestore_writing_items": 0,
        "bluefs_bytes": 0,
        "bluefs_items": 0,
        "bluefs_file_reader_bytes": 0,
        "bluefs_file_reader_items": 0,
        "bluefs_file_writer_bytes": 0,
        "bluefs_file_writer_items": 0,
        "buffer_anon_bytes": 30680,
        "buffer_anon_items": 22,
        "buffer_meta_bytes": 0,
        "buffer_meta_items": 0,
        "osd_bytes": 0,
        "osd_items": 0,
        "osd_mapbl_bytes": 0,
        "osd_mapbl_items": 0,
        "osd_pglog_bytes": 0,
        "osd_pglog_items": 0,
        "osdmap_bytes": 2496,
        "osdmap_items": 30,
        "osdmap_mapping_bytes": 0,
        "osdmap_mapping_items": 0,
        "pgmap_bytes": 0,
        "pgmap_items": 0,
        "mds_co_bytes": 0,
        "mds_co_items": 0,
        "unittest_1_bytes": 0,
        "unittest_1_items": 0,
        "unittest_2_bytes": 0,
        "unittest_2_items": 0
    },
    "objecter": {
        "op_active": 0,
        "op_laggy": 0,
        "op_send": 0,
        "op_send_bytes": 0,
        "op_resend": 0,
        "op_reply": 0,
        "op": 0,
        "op_r": 0,
        "op_w": 0,
        "op_rmw": 0,
        "op_pg": 0,
        "osdop_stat": 0,
        "osdop_create": 0,
        "osdop_read": 0,
        "osdop_write": 0,
        "osdop_writefull": 0,
        "osdop_writesame": 0,
        "osdop_append": 0,
        "osdop_zero": 0,
        "osdop_truncate": 0,
        "osdop_delete": 0,
        "osdop_mapext": 0,
        "osdop_sparse_read": 0,
        "osdop_clonerange": 0,
        "osdop_getxattr": 0,
        "osdop_setxattr": 0,
        "osdop_cmpxattr": 0,
        "osdop_rmxattr": 0,
        "osdop_resetxattrs": 0,
        "osdop_call": 0,
        "osdop_watch": 0,
        "osdop_notify": 0,
        "osdop_src_cmpxattr": 0,
        "osdop_pgls": 0,
        "osdop_pgls_filter": 0,
        "osdop_other": 0,
        "linger_active": 0,
        "linger_send": 0,
        "linger_resend": 0,
        "linger_ping": 0,
        "poolop_active": 0,
        "poolop_send": 0,
        "poolop_resend": 0,
        "poolstat_active": 0,
        "poolstat_send": 0,
        "poolstat_resend": 0,
        "statfs_active": 0,
        "statfs_send": 0,
        "statfs_resend": 0,
        "command_active": 0,
        "command_send": 0,
        "command_resend": 0,
        "map_epoch": 0,
        "map_full": 0,
        "map_inc": 0,
        "osd_sessions": 0,
        "osd_session_open": 0,
        "osd_session_close": 0,
        "osd_laggy": 0,
        "omap_wr": 0,
        "omap_rd": 0,
        "omap_del": 0
    },
    "throttle-msgr_dispatch_throttler-radosclient": {
        "val": 0,
        "max": 104857600,
        "get_started": 0,
        "get": 19,
        "get_sum": 14374,
        "get_or_fail_fail": 0,
        "get_or_fail_success": 19,
        "take": 0,
        "take_sum": 0,
        "put": 19,
        "put_sum": 14374,
        "wait": {
            "avgcount": 0,
            "sum": 0.000000000,
            "avgtime": 0.000000000
        }
    },
    "throttle-objecter_bytes": {
        "val": 0,
        "max": 104857600,
        "get_started": 0,
        "get": 0,
        "get_sum": 0,
        "get_or_fail_fail": 0,
        "get_or_fail_success": 0,
        "take": 0,
        "take_sum": 0,
        "put": 0,
        "put_sum": 0,
        "wait": {
            "avgcount": 0,
            "sum": 0.000000000,
            "avgtime": 0.000000000
        }
    },
    "throttle-objecter_ops": {
        "val": 0,
        "max": 1024,
        "get_started": 0,
        "get": 0,
        "get_sum": 0,
        "get_or_fail_fail": 0,
        "get_or_fail_success": 0,
        "take": 0,
        "take_sum": 0,
        "put": 0,
        "put_sum": 0,
        "wait": {
            "avgcount": 0,
            "sum": 0.000000000,
            "avgtime": 0.000000000
        }
    }
}


参考写IO流程, 性能统计, https://blog.csdn.net/weixin_42319496/article/details/121122715,
Image::write
->ImageRequestWQ<I>::write
 -> ImageRequestWQ<I>::aio_write
  -> ImageRequest<I>::aio_write
   -> queue() // 把io丢到队列io_work_queue
 
// 工作线程从io_work_queue队列取出io后调用ImageRequest<I>::send(), 从上面的io路径发现一个关键点，也就是io会先丢到队列io_work_queue的，然后工作线程再从队列里面取出io进行处理。这里因为经过队列以及线程的转换，往往可能是瓶颈所在，所以期望查看io丢队列前到工作线程拿到io准备处理这个阶段的耗时。正好他们分别对应着函数ImageRequestWQ::aio_write以及ImageRequest::send()，所以希望从日志里面找到这两个时间点的间隔, 比如: 到这里基本就怀疑是丢队列后线程来不及处理引起的。因为往队列里丢和取了以后进行处理都是一个线程，但是取的线程因为还要调用send函数所以相对会慢很多，进而导致队列堆积, 参考解决: 把rbd_op_thread配置项设为4，fio的随机4k iops可以从三万左右提升到6万以上(三节点，每节点19个sata ssd盘, 如何查看队列积压请求? (加个perf dump信息), 如果社区默认写死线程数, 则可考虑改造为可配置, 把rbd_op_thread的个数做到可配置的
工作线程(io_work_queue = new io::ImageRequestWQ<>)：ImageRequestWQ<I>::process
 
ImageRequest<I>::send()
->AbstractImageWriteRequest<I>::send_request()
 ->AbstractImageWriteRequest<I>::send_object_requests
  --> journal开启
  --> journal未开启
      AbstractObjectWriteRequest<I>::send()
      --> AbstractObjectWriteRequest<I>::pre_write_object_map_update()
        --> AbstractObjectWriteRequest<I>::write_object
          --> image_ctx->data_ctx.aio_operate
            --> librados::IoCtx::aio_operate
              --> librados::IoCtxImpl::aio_operate
                 --> Objecter::op_submit // 向主osd发送io
 
// 收到主osd的io回复
Objecter::ms_dispatch(Message *m)
-> Objecter::handle_osd_op_reply
 -> onfinish->complete(rc) // 丢finish队列，finish处理回调


谷歌测试, TEST, 
google_test, 性能计数示例代码,  -> test/perf_counters.cc -> main -> gdb ./unittest_perf_counters ->  main
admin_socket get_rand_socket_path -> 获取临时随机asok文件
  tdir, (long int)getpid(), time(NULL) -> /tmp/perfcounters_test_socket.222280.1692164672 -> 根据临时目录, 进程id, 空时间生成
global_init -> flag=CINIT_FLAG_NO_DEFAULT_CONFIG_FILE|CINIT_FLAG_NO_CCT_PERF_COUNTERS
  global_pre_init
  block_signals -> 阻塞信号
  install_standard_sighandlers
  register_assert_context
  if (prctl(PR_SET_DUMPABLE, 1) == -1) -> 设置进程或线程属性, 比如这里设置核心转储(coredump), 参考: https://man7.org/linux/man-pages/man2/prctl.2.html
  prctl(PR_SET_THP_DISABLE, 1, 0, 0, 0) -> 设置调用线程的" THP disable"标志的状态。如果arg2具有非零值，则设置标志，否则将其清除。设置此标志提供了一种方法，用于禁用无法修改代码的作业的透明大页面，并且不能将madvise(2)与malloc挂钩一起使用(即静态分配的数据)。 " THP disable"标志的设置由通过fork(2)创建的子代继承，并保留在execve(2)中
  g_conf().apply_changes(nullptr)
  g_conf().call_all_observers()
  g_conf().complain_about_parse_error(g_ceph_context)
  debug_deliberately_leak_memory -> 测试内存泄漏
  g_ceph_context->crush_location.init_on_startup()
  return boost::intrusive_ptr<CephContext>{g_ceph_context, false} -> 返回全局ceph上下文
common_init_finish -> 通用初始化, 启动日志线程, 启动服务线程...
::testing::InitGoogleTest(&argc, argv) -> internal::InitGoogleTestImpl(argc, argv) -> void InitGoogleTestImpl(int* argc, CharType** argv) -> 初始化google单元测试
  GTestIsInitialized
  g_argvs.push_back(StreamableToString(argv[i]))
  ParseGoogleTestFlagsOnly(argc, argv)
  GetUnitTestImpl()->PostFlagParsingInit()
return RUN_ALL_TESTS() -> return ::testing::UnitTest::GetInstance()->Run() -> int UnitTest::Run()
  impl()->set_catch_exceptions(GTEST_FLAG(catch_exceptions))
  return internal::HandleExceptionsInMethodIfSupported
  ...
TEST(PerfCounters, SimpleTest)
  client.do_request -> std::string AdminSocketClient::do_request
    asok_connect(m_path, &socket_fd) -> 绑定socket文件
      socket_cloexec
      connect
      setsockopt
    asok_request
    safe_read_exact
Breakpoint 3, PerfCounters_SimpleTest_Test::TestBody (this=0x555555791100) at /home/xb/project/ceph/xb/ceph/src/test/perf_counters.cc:64
64        AdminSocketClient client(get_rand_socket_path());
(gdb) bt
#0  PerfCounters_SimpleTest_Test::TestBody (this=0x555555791100) at /home/xb/project/ceph/xb/ceph/src/test/perf_counters.cc:64
#1  0x000055555563aeab in testing::internal::HandleSehExceptionsInMethodIfSupported<testing::Test, void> (object=0x555555791100, method=&virtual testing::Test::TestBody(), location=0x55555566e63b "the test body")
    at /home/xb/project/ceph/xb/ceph/src/googletest/googletest/src/gtest.cc:2605
#2  0x00005555556322b5 in testing::internal::HandleExceptionsInMethodIfSupported<testing::Test, void> (object=0x555555791100, method=&virtual testing::Test::TestBody(), location=0x55555566e63b "the test body")
    at /home/xb/project/ceph/xb/ceph/src/googletest/googletest/src/gtest.cc:2641
#3  0x000055555560b448 in testing::Test::Run (this=0x555555791100) at /home/xb/project/ceph/xb/ceph/src/googletest/googletest/src/gtest.cc:2680
#4  0x000055555560bdd9 in testing::TestInfo::Run (this=0x555555708020) at /home/xb/project/ceph/xb/ceph/src/googletest/googletest/src/gtest.cc:2858
#5  0x000055555560c633 in testing::TestSuite::Run (this=0x5555557084e0) at /home/xb/project/ceph/xb/ceph/src/googletest/googletest/src/gtest.cc:3012
#6  0x0000555555618516 in testing::internal::UnitTestImpl::RunAllTests (this=0x5555557080f0) at /home/xb/project/ceph/xb/ceph/src/googletest/googletest/src/gtest.cc:5723
#7  0x000055555563be78 in testing::internal::HandleSehExceptionsInMethodIfSupported<testing::internal::UnitTestImpl, bool> (object=0x5555557080f0, 
    method=(bool (testing::internal::UnitTestImpl::*)(testing::internal::UnitTestImpl * const)) 0x555555618188 <testing::internal::UnitTestImpl::RunAllTests()>, location=0x55555566f060 "auxiliary test code (environments or event listeners)")
    at /home/xb/project/ceph/xb/ceph/src/googletest/googletest/src/gtest.cc:2605
#8  0x00005555556337d2 in testing::internal::HandleExceptionsInMethodIfSupported<testing::internal::UnitTestImpl, bool> (object=0x5555557080f0, 
    method=(bool (testing::internal::UnitTestImpl::*)(testing::internal::UnitTestImpl * const)) 0x555555618188 <testing::internal::UnitTestImpl::RunAllTests()>, location=0x55555566f060 "auxiliary test code (environments or event listeners)")
    at /home/xb/project/ceph/xb/ceph/src/googletest/googletest/src/gtest.cc:2641
#9  0x0000555555616e01 in testing::UnitTest::Run (this=0x5555556b4140 <testing::UnitTest::GetInstance()::instance>) at /home/xb/project/ceph/xb/ceph/src/googletest/googletest/src/gtest.cc:5306
#10 0x00005555555fc618 in RUN_ALL_TESTS () at /home/xb/project/ceph/xb/ceph/src/googletest/googletest/include/gtest/gtest.h:2486
#11 0x00005555555f79c0 in main (argc=1, argv=0x7fffffffe3b8) at /home/xb/project/ceph/xb/ceph/src/test/perf_counters.cc:60

TEST(PerfCounters, SinglePerfCounters)
  PerfCountersCollection *coll = g_ceph_context->get_perfcounters_collection()
  PerfCounters* fake_pf = setup_test_perfcounters1(g_ceph_context)
    PerfCountersBuilder bld(cct, "test_perfcounter_1"




#0  MonClient::handle_config (this=0x7fffffffda90, m=0x7fffd4003d60) at /home/xb/project/ceph/xb/ceph/src/mon/MonClient.cc:454
#1  0x00007fffeea9f472 in MonClient::ms_dispatch (this=0x7fffffffda90, m=0x7fffd4003d60) at /home/xb/project/ceph/xb/ceph/src/mon/MonClient.cc:377 -> case MSG_CONFIG
#2  0x00007fffeeab7db0 in Dispatcher::ms_dispatch2 (this=0x7fffffffda90, m=...) at /home/xb/project/ceph/xb/ceph/src/msg/Dispatcher.h:124
#3  0x00007fffee7fd42b in Messenger::ms_deliver_dispatch (this=0x4ea1e0, m=...) at /home/xb/project/ceph/xb/ceph/src/msg/Messenger.h:703
#4  0x00007fffee7fc0b1 in DispatchQueue::entry (this=0x4ea540) at /home/xb/project/ceph/xb/ceph/src/msg/DispatchQueue.cc:199


void ConfigMonitor::check_sub
  maybe_send_config
    send_config(s)
      new MConfig(s->last_config) -> 消息类型: MSG_CONFIG
      s->con->send_message(m) -> monitor -> monc -> monc 收到消息后 -> MonClient::handle_config
        map_cond.notify_all() -> 唤醒monc阻塞的条件变量
        finisher.queue
          cct->_conf.set_mon_vals
          config_notify_cb()


monc收到monitor的map更新

b Objecter.cc:1159
void DispatchQueue::entry()
  if (dispatcher->ms_dispatch2(m))
    if (ms_dispatch(mr.get())) -> bool Objecter::ms_dispatch(Message *m)
      handle_osd_map(static_cast<MOSDMap*>(m)) -> ...
void Objecter::handle_osd_map(MOSDMap *m) -> 全量/增量: https://blog.51cto.com/liangchaoxi/5068354, 函数分析: https://ouyangtao.gitbooks.io/ceph-/content/handle_osd_map.html, osd restart之后pg如何达到active+clean: https://zhuanlan.zhihu.com/p/423640688
  monc->get_fsid()
  osdmap->test_flag(CEPH_OSDMAP_PAUSERD)
  if (m->get_last() <= osdmap->get_epoch()) -> 如果订阅返回的epoch比当前osdmap更旧,则忽略
  handle_osd_map got epochs -> osdmap有效
  osdmap = std::move(new_osdmap)
  first map.  we want the full thing -> 第一张map
  _scan_requests -> 扫描请求
  osdmap->decode(m->maps[m->get_last()]) -> void OSDMap::decode(ceph::buffer::list& bl) -> 使用完整的osdmap来启动，之前我们请求并解码了增量的完整历史记录，在启动过程中浪费了很大的时间
    void OSDMap::decode -> 有关 osdmap 版本的说明以及引入时间，请参阅 doc/dev/osd_internals/osdmap_versions.txt
      ...
      crush->decode(cblp)
      ...
      post_decode()
        calc_num_osds()
        _calc_up_osd_features()
  prune_pg_mapping -> 修剪放置组映射 -> osdc/Objecter：pg-mapping 缓存，基于 CRUSH 的寻址是某种 CPU 密集型任务，因此应尽可能避免。 该补丁引入了每个对象程序 pg 映射缓存，因此可以节省 10% 以上的 CPU（fio、4k 随机数）读取，24k IOPS）, https://github.com/ssbandjl/ceph/commit/8697db15c4ff59cd0c4dc021652576e72351dbaf
    mapping_array.resize(pg_num)
  _scan_requests(homeless_session -> void Objecter::_scan_requests -> 先扫描后重发, 从homeless会话中拿到op, 然后执行下面的重发
    查找变化的,遍历homeless会话, 重新计算目标, 返回需要重发标记位, 需要重发的OP加入重发队列, 检查map是否更新, 确认需要重发的目标已经拿到最新的map, 重发请求(_send_op(op)), 重发命令, 完成等待map更新的所有上下文, 调用完成方法
    map<ceph_tid_t,LingerOp*>::iterator lp = s->linger_ops.begin()
  _send_linger(op, sul) -> 重发
  _dump_active -> _dump_active(homeless_session)
  monc->sub_got("osdmap", osdmap->get_epoch())




osdmap, 集群map, rados原理与实现: https://e.dangdang.com/pc/reader/index.html?id=1901104772



OP语义, OP事务(class Transaction {):
OP_WRITE -> rados_write 同步写 -> CEPH_RADOS_API int rados_write(rados_ioctx_t io, const char *oid, const char *buf, size_t len, uint64_t off)
...

aio_write -> 异步写 -> CEPH_RADOS_API int rados_aio_write(rados_ioctx_t io, const char *oid, rados_completion_t completion, const char *buf, size_t len, uint64_t off)
...
int librados::IoCtxImpl::aio_write
  Context *oncomplete = new C_aio_Complete(c)
  queue_aio_write(c)
    c->aio_write_seq = ++aio_write_seq
     aio_write_list.push_back(&c->aio_write_list_item) -> 将回调加入队列
  Objecter::Op *o = objecter->prepare_write_op(
    oid, oloc,
    off, len, snapc, bl, ut, 0,
    oncomplete, &c->objver, nullptr, 0, &trace) -> CEPH_OSD_OP_WRITE -> 准备OP(objecter)
  objecter->op_submit(o, &c->tid)
  ...
  flush_aio_writes

OP_WRITEFULL -> write_full -> 异步写入整个对象 该对象填充有提供的数据。 如果该对象存在，则会自动截断该对象，然后写入。 将 write_full 排队并返回。 成功时完成的返回值为 0，失败时为负错误代码 -> CEPH_OSD_OP_WRITEFULL -> CEPH_RADOS_API int rados_aio_write_full(rados_ioctx_t io, const char *oid, rados_completion_t completion, const char *buf, size_t len)

OP_READ -> 读 -> CEPH_RADOS_API int rados_read(rados_ioctx_t io, const char *oid, char *buf, size_t len, uint64_t off)
  CEPH_OSD_OP_READ
  operate_read
    Objecter::Op *objecter_op = objecter->prepare_read_op
    objecter->op_submit(objecter_op)


OP_TRUNCATE -> 截断对象, 丢弃对象中超出指定大小的所有数据, 调整对象大小 如果这放大了对象，则新区域在逻辑上会用零填充。 如果这缩小了对象，多余的数据将被删除
  CEPH_RADOS_API int rados_trunc(rados_ioctx_t io, const char *oid, uint64_t size);
  op.truncate(size)
    add_data(CEPH_OSD_OP_TRUNCATE, off, 0, bl)
    ...
  return operate(oid, &op, NULL)



OP_ZERO -> 将对象清零, 将对象内指定的字节范围清零。 一些ObjectStore实例可能会对此进行优化以释放底层存储空间。 如果零范围超出了对象的末尾，则对象大小会扩展，就像我们正在写入一个充满零的缓冲区一样。 除非长度为 0，在这种情况下（就像 0 长度写入一样）我们不调整对象大小
CEPH_RADOS_API void rados_write_op_zero(rados_write_op_t write_op,
			                uint64_t offset,
			                uint64_t len);
  ((::ObjectOperation *)write_op)->zero(offset, len)
    add_data(CEPH_OSD_OP_ZERO
  

OP_CREATE -> 新建对象 -> CEPH_OSD_OP_CREATE, 应用场景,如:创建RBD image | cls_cxx_create
...
rbd/cls_rbd_client层：添加image到RBD目录
handle_add_image_to_directory
  op.create(true) -> 封装OP写操作
  add_op(CEPH_OSD_OP_CREATE)
  ...

CEPH_RADOS_API void rados_write_op_create(rados_write_op_t write_op,
                                          int exclusive, // 独占, 如果对象已存在，设置为 LIBRADOS_CREATE_EXCLUSIVE 或 LIBRADOS_CREATE_IDEMPOTENT 将出错
                                          const char* category);



OP_DELETE
RGWOp* RGWHandler_REST::get_op -> op_delete -> rgw语义?


OP_STAT -> CEPH_RADOS_API int rados_stat(rados_ioctx_t io, const char *o, uint64_t *psize, time_t *pmtime)
C_ObjectOperation_stat *h = new C_ObjectOperation_stat
add_op(CEPH_OSD_OP_STAT)


OP_CALL
OMAP*
OP_READ_REPAIE(新加语义)
OP_STAT_WRITE (新加语义)


管理命令: https://docs.ceph.com/en/reef/man/8/ceph/
ceph config-key ls



bluestor元数据及缓存, cache, https://www.zhihu.com/people/allincache/posts
OSD::init()
store->set_cache_shards(get_num_cache_shards())


void BlueStore::_txc_state_proc(TransContext *txc)
保序: void BlueStore::_txc_finish_io(TransContext *txc)



osdmap加载和更新流程: 上述osdmap加载过程中涉及到两个内存缓存：map_cache和map_bl_cache（还有一个map_bl_inc_cache是保存增量osdmap的bufferlist的缓存），这两个缓存都是基于LRU算法，在OSDService类的构造函数中初始化的，默认的缓存空间大小（缓存项最大数量）是由配置项osd_map_cache_size决定的，其默认值是500，因此在启动过程中缓存的osdmap数量是足够的（根据实际线程环境osdmap变化速度，有运维操作时版本变化量是150左右，osdmap变化数量跟osd状态变化次数强相关，没有操作时基本不变）
ceph_osd.cc -> main
osdmap = get_map(superblock.current_epoch)
  OSDMapRef OSDService::try_get_map
    return _add_map(map)

osd启动: 
void OSD::start_boot()
void OSD::_preboot(epoch_t oldest, epoch_t newest)
_send_boot
  monc->send_mon_message(mboot) -> 发送osd boot消息给monitor，之后monitor就认为osd已经启动, monitor根据类型做处理: MSG_OSD_BOOT

monitor发送全量或增量
send_latest


void OSD::osdmap_subscribe
  monc->sub_want_increment("osdmap"
  monc->renew_subs()

osd处理map:
void OSD::handle_osd_map(MOSDMap *m)
  处理全量或增量map, 更新superblock中的newest_map版本, 设置OSD为Active状态, 继续更新superblock,  保存superblock到硬盘（leveldb), 更新OSD心跳互检的对端列表, 检查osd是否可以启动（是否能发送MOSDBoot消息
给monitor，使osd变为up状态


---------- 条带 ----------
文件数据线性地址: (object-name, offset, length) -> file_to_extents -> 三维地址空间(objectset, object, stripe)
_write -> objectcacher->file_write -> 
  OSDWrite *wr = prepare_write(snapc, bl, mtime, flags, 0)
  Striper::file_to_extents(cct, oset->ino, layout, offset, len, oset->truncate_size, wr->extents)
    file_to_extents(cct, buf, layout, offset, len, trunc_size, extents) -> 计算条带,负载均衡
      __u32 object_size = layout->object_size
      __u32 su = layout->stripe_unit
      __u32 stripe_count = layout->stripe_count
      upper_bound
  return writex(wr, oset, nullptr) -> ObjectCacher::writex

缓存写
bool ObjectCacherObjectDispatch<I>::write


https://github.com/ssbandjl/ceph/commit/8697db15c4ff59cd0c4dc021652576e72351dbaf
基于 CRUSH 的寻址是某种 CPU 密集型任务，因此应尽可能避免, 该补丁引入了每个对象程序 pg 映射缓存，因此可以节省 10% 以上的 CPU（fio、4k 随机读取、24k IOPS,
The CRUSH-based addressing is some kind of CPU intensive task and hence should be avoid whenever possible.

The patch introduces a per objecter pg-mapping cache, which as
a result can saved us 10%+ CPU (fio, 4k randow read, 24k IOPS):

Was:
$ top -Hp 415864
top - 14:45:39 up 6 days, 5:45, 3 users, load average: 9.67, 8.66, 8.45
Threads: 23 total, 3 running, 20 sleeping, 0 stopped, 0 zombie
%Cpu(s): 21.6 us, 14.5 sy, 0.0 ni, 59.6 id, 1.4 wa, 0.0 hi, 3.0 si, 0.0 st
KiB Mem : 19616576+total, 12773412+free, 50964508 used, 17467128 buff/cache
KiB Swap: 4194300 total, 4194300 free, 0 used. 14352892+avail Mem

PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND
415942 root 20 0 1957800 492820 123880 R 80.0 0.3 0:13.75 tp_librbd

Now:
$ top -Hp 475779
top - 10:22:05 up 1:18, 4 users, load average: 2.65, 1.44, 1.60
Threads: 23 total, 2 running, 21 sleeping, 0 stopped, 0 zombie
%Cpu(s): 6.1 us, 2.2 sy, 0.0 ni, 91.4 id, 0.0 wa, 0.0 hi, 0.2 si, 0.0 st
KiB Mem : 19616576+total, 13919555+free, 27280820 used, 29689392 buff/cache
KiB Swap: 4194300 total, 4194300 free, 0 used. 16798102+avail Mem

PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND
476231 root 20 0 1957808 491712 123836 S 71.1 0.3 0:41.47 tp_librbd

And below is a more detailed report from the perf tool:

Was:
$ perf report -g graph
Children Self Command Shared Object Symbol
+ 29.33% 0.03% tp_librbd librados.so.2.0.0 [.] Objecter::op_submit
+ 28.83% 0.09% tp_librbd librados.so.2.0.0 [.] Objecter::_op_submit_with_budget
+ 27.12% 0.17% tp_librbd librados.so.2.0.0 [.] Objecter::_op_submit
+ 16.47% 0.26% tp_librbd librados.so.2.0.0 [.] Objecter::_calc_target
+ 15.04% 0.10% tp_librbd libceph-common.so.0 [.] OSDMap::_pg_to_up_acting_osds
+ 13.52% 0.16% tp_librbd libceph-common.so.0 [.] OSDMap::_pg_to_raw_osds

Now:
$ perf report -g graph
Children Self Command Shared Object Symbol
+ 17.84% 0.04% tp_librbd librados.so.2.0.0 [.] Objecter::op_submit
+ 17.34% 0.06% tp_librbd librados.so.2.0.0 [.] Objecter::_op_submit_with_budget
+ 15.80% 0.17% tp_librbd librados.so.2.0.0 [.] Objecter::_op_submit
+ 6.11% 2.02% tp_librbd librados.so.2.0.0 [.] Objecter::_calc_target



---------- ec ----------
纠删码是一种将对象持久存储在 Ceph 存储集群中的方法，其中纠删码算法将对象分解为数据块 (k) 和编码块 (m)，并将这些块存储在不同的 OSD 中
ceph_ec源码分析: https://zhuanlan.zhihu.com/p/96834610

关键词: stripe_width 条带宽度=条带大小 * chunk_size, chunk, chunk_size(cell_size), strip_size 条带大小, 
关键数据结构, pg_log: struct pg_log_entry_t
本地回滚条目, 回滚信息: ObjectModDesc mod_desc;
写操作的off和条带宽度对齐: int PrimaryLogPG::do_osd_ops 中
pool.info.requires_aligned_append()
op.extent.offset % pool.info.required_alignment() != 0
EC读写: class PGBackend
EC编码解码: namespace ECUtil
EC事务: ECTransaction
关键函数: int OSDMonitor::prepare_pool_stripe_width
logical_offset_is_stripe_aligned
void pg_pool_t::encode
TestErasureCodeLrc.cc
src/test/erasure-code/TestErasureCode.cc
src/tools/erasure-code/ceph-erasure-code-tool.cc
void ECTransaction::generate_transactions

google单元测试: src/test/unit.cc -> int main(int argc, char **argv) -> RUN_ALL_TESTS
b TestErasureCode.cc:90
ErasureCodeTest erasure_code(k, m, chunk_size)
erasure_code.get_chunk_count() -> k+m
erasure_code.encode(want_to_encode, in, &encoded) -> int ErasureCode::encode
  encode_prepare(in, *encoded) -> int ErasureCode::encode_prepare
    bool buffer::list::rebuild_aligned_size_and_memory
      round_up_to
  encode_chunks(want_to_encode, encoded)


int ECUtil::encode 是将原始数据按条带宽度进行分割，然后对条带数据编码，得到条带的数据块和校验块。把每个条带化数据块和校验块有序的链接，形成数据块和校验块
int ErasureCode::encode
int ErasureCodeJerasure::encode_chunks
  jerasure_encode(&chunks[0], &chunks[k], (*encoded)[0].length()) -> EC编码
  void ErasureCodeJerasureReedSolomonVandermonde::jerasure_encode -> jerasure_matrix_encode(k, m, w, matrix, data, coding, blocksize)
    jerasure_matrix_dotprod -> JErasure库相关介绍: https://blog.csdn.net/weixin_42868849/article/details/109660456, https://durantthorvalds.top/2021/02/11/%E3%80%8C%E6%8E%A2%E7%A7%98%E3%80%8DJerasure%E9%82%A3%E4%BA%9B%E4%BA%8B/, 这将一行编码/解码矩阵乘以数据/幸存者。 源设备的ID（与矢量元素的ID相对应）位于src_id中。 目标设备的ID在目标ID中。 点积操作, 当矩阵中遇到一个1时，将执行正确的XOR /复制操作, Jerasure库提供Reed-Solomon和Cauchy Reed-Solomon两种编码算法的实现, Jerasure库接口简介及性能测试: https://www.cnblogs.com/yunnotes/archive/2013/04/19/3032308.html, 恢复指定块
      galois_region_xor
        galois_w32_region_xor
    ...

解码: int ECUtil::decode
int ErasureCodeJerasure::decode_chunks
  jerasure_decode(erasures, data, coding, blocksize)


ec, 类 ECBackend 实现了EC的读写操作。ECUtil里定义了编码和解码的函数实现。ECTransaction定了EC的事务
int PrimaryLogPG::do_osd_ops(OpContext *ctx, vector<OSDOp>& ops)
  case CEPH_OSD_OP_WRITE
  requires_aligned_append
  t->write(soid, op.extent.offset, op.extent.length, osd_op.indata, op.flags)


默认情况下，Ceph 池是使用“复制”类型创建的。 在复制类型池中，每个对象都被复制到多个磁盘。 这种多重复制是一种称为“复制”的数据保护方法。  相比之下，纠删码池使用与复制不同的数据保护方法。 在纠删码中，数据被分成两种片段：数据块和奇偶校验块。 如果驱动器发生故障或损坏，奇偶校验块将用于重建数据。 在规模上，纠删码相对于复制可以节省空间。  在本文档中，数据块被称为“数据块”，奇偶校验块被称为“编码块”。  纠删码也称为“前向纠错码”。 第一个前向纠错码是由贝尔实验室的 Richard Hamming 于 1950 年开发的。
https://docs.ceph.com/en/reef/rados/operations/erasure-code/

ceph osd pool create ecpool erasure
echo ABCDEFGHI | rados --pool ecpool put NYAN -
rados --pool ecpool get NYAN -

默认纠删码配置文件可以承受两个 OSD 的重叠丢失而不丢失数据。 此纠删码配置文件相当于大小为 3 的复制池，但具有不同的存储要求：它不需要 3TB 来存储 1TB，而是只需要 2TB 来存储 1TB。 可以使用此命令显示默认配置文件
ceph osd erasure-code-profile get default
k=2
m=2
plugin=jerasure
crush-failure-domain=host
technique=reed_sol_van

默认纠删码池有两个数据块 (K) 和两个编码块 (M)。 默认纠删码池的配置文件是“k=2 m=2”。最简单的纠删码池有两个数据块 (K) 和一个编码块 (M)。 最简单的纠删码池的配置文件是“k=2 m=1”

chunk，块, 当调用编码函数时，它返回彼此大小相同的块。 有两种块：（1）数据块，可以连接起来重建原始对象；（2）编码块，可以用来重建丢失的块
K, 对象被划分为的数据块的数量。 例如，如果 K = 2，则将一个 10KB 的对象分为两个各 5KB 的对象
M, 由编码函数计算的编码块的数量。 M 等于集群中可以丢失的 OSD 数量，而集群不会丢失数据。 例如，如果有两个编码块，则可以丢失两个 OSD 而不会丢失数据

在冗余度（冗余度=实际存储空间/有效存储空间）方面，三副本方式下，每1个数据块都需要额外的2个数据块做副本，冗余度为3/1=3，而在RS-3-2的策略下，每3个数据块只需要额外的2个数据块就能够实现可靠性目标，冗余度为5/3=1.67, 最终，我们通过RS-3-2的方式能够在1.67倍冗余的情况下，实现近似三副本的可靠性

class ECBackend : public PGBackend

int OSDMonitor::prepare_command_pool_set
  p.flags |= pg_pool_t::FLAG_EC_OVERWRITES


rados -p ecpool bench 20 write -t 32 -b 4096 --no-cleanup
rbd create rbd/myimage --size 1T --data-pool ec42

设置可覆盖:
ceph osd erasure-code-profile set ec32_nvme \
  plugin=jerasure k=2 m=1 technique=reed_sol_van \
  crush-root=default crush-failure-domain=host crush-device-class=nvme \
  directory=/usr/lib/ceph/erasure-code

ceph osd erasure-code-profile set ec32 \
  plugin=jerasure k=2 m=1 technique=reed_sol_van \
  crush-root=default crush-failure-domain=host \
  directory=/usr/lib/ceph/erasure-code

ceph osd pool create ec_pool 16 erasure ec32
ceph osd pool set ec_pool allow_ec_overwrites true
ceph osd pool application enable ec_pool rbd
rbd create --size 1024 ec_pool/image1



/ErasureCodeClay.cc


---------- ec ----------

-------- fs ----------
cephfs, ceph文件系统
ceph定义的文件操作函数都封装在类Client: class Client : public Dispatcher
Client实例指针作为CephFuse::Handle类的成员变量: class CephFuse::Handle
CephFuse::Handle实例指针又作为CephFuse类的成员变量，这样CephFuse实例可以通过client来调用文件系统操作函数来进行文件操作: class CephFuse

ceph_fs参考: https://zhuanlan.zhihu.com/p/97510864
ceph_fuse.cc -> main
StandaloneClient *client;
client = new StandaloneClient(messenger, mc)
cfuse = new CephFuse(client, forker.get_signal_fd())
client->mount(mountpoint, perms, fuse_require_active_mds)
r = cfuse->start()
...
r = cfuse->loop() -> int CephFuse::Handle::loop()
  fuse_session_loop_mt(se)
...

在fuse_do_work线程中获取到req后，通过 fuse_session_process_buf --> fuse_ll_process_buf，执行fuse_ll_ops[in->opcode].func(req, in->nodeid, inarg)。fuse_ll_ops中定义了函数操作码对应的具体的函数

-------- fs ----------