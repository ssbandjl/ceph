https://github.com/ssbandjl/ceph/tree/v15.2.17
https://github.com/ssbandjl/ceph/tree/main

build:
close mrg,dashboard in build/CMakeCache.txt

基准测试, 文档: messenger.rst
https://blog.csdn.net/bandaoyu/article/details/114292690
使用命令patchelf   修改工具依赖的动态库位置。避免和项目正在使用的库冲突: https://blog.csdn.net/bandaoyu/article/details/113181179
doc/dev/messenger.rst
perf_msgr_client.cc -> ceph_perf_msgr_client
perf_msgr_server.cc -> ceph_perf_msgr_server

perf_msgr_client.cc -> main


编译rdma:
./do_cmake.sh -DCMAKE_INSTALL_PREFIX=/usr -DWITH_RDMA=ON

just build client:
make client > log 2>&1 &

qa:
启动虚拟集群: https://docs.ceph.com/en/quincy/dev/dev_cluster_deployement/

docker:

git
git remote add upstream https://github.com/Foo/repos.git
git pull upstream v15.2.17
git remote remove upstream
git remote add upstream https://github.com/Foo/repos.git
git remote set-url upstream https://github.com/Foo/repos.git

git push origin ：refs/tags/3.0 这就是明确告诉服务器删除的tag的分支,删除branch分支
git push origin :refs/heads/3.0
git branch -D testtag
删除tag分支的方法：
git tag -d v15.2.17
git push origin v15.2.17
git config --global credential.helper "cache --timeout=604800"


build:
close mrg,dashboard in build/CMakeCache.txt


基准测试:
https://blog.csdn.net/bandaoyu/article/details/114292690
使用命令patchelf   修改工具依赖的动态库位置。避免和项目正在使用的库冲突: https://blog.csdn.net/bandaoyu/article/details/113181179
doc/dev/messenger.rst
perf_msgr_client.cc -> ceph_perf_msgr_client
perf_msgr_server.cc -> ceph_perf_msgr_server


perf_msgr_client.cc -> main


编译rdma:
./do_cmake.sh -DCMAKE_INSTALL_PREFIX=/usr -DWITH_RDMA=ON
cd build
cmake ..
CMakeLists.txt
  
启动虚拟集群: https://docs.ceph.com/en/quincy/dev/dev_cluster_deployement/

CMakeCache.txt
ON/OFF
address sanitizer

docker run -it -d --privileged --cap-add=ALL --name centos7  -p 22223:22 -p 6666:6666 -v /home/xb/project/stor/ceph/xb/docker/ceph:/home/xb/project/stor/ceph/xb/docker/ceph ceph_centos7:v15.2.17
docker exec -u root -it centos7 bash -c 'cd /home/xb/project/stor/ceph/xb/docker/ceph;exec "${SHELL:-sh}"'

gdb:
cd /home/xb/project/stor/ceph/xb/docker/ceph/build/bin
bash gdb_s.sh
b main
r

常用:
获取线程名:
prctl(PR_GET_NAME, buf)

创建线程:
pthread_create(&thread_id, thread_attr, _entry_func, (void*)this)

设置日志文件: set_log_file
打开日志文件: m_fd = ::open(m_log_file.c_str(), O_CREAT|O_WRONLY|O_APPEND|O_CLOEXEC, 0644)
打印日志: cerr << __func__ << " " << __FL__ << " server accept client connect" << std::endl;
打印日志代码示例: ldout(cct, 10) << __FFL__ << " client connect -> server" << dendl;

日志配置:
debug {subsystem} = {log-level}/{memory-level}
#for example
debug mds log = 1/20

打印每行日志:
void Log::_flush

默认配置: Option("ms_type"
默认开启RDMA: .set_default("async+rdma")
配置文件: https://docs.ceph.com/en/latest/rados/configuration/ceph-conf/


返回自动变量auto:  const auto& _lookup_conn
要求(断言)已上锁: ceph_assert(ceph_mutex_is_locked(lock))

int RDMAWorker::listen -> rdma ib初始化: ib->init() -> void Infiniband::init()

int RDMAWorker::connect -> ib->init() -> void Infiniband::init()
  

gdb 打印ib设备: (gdb) p **((ibv_device **) 0x7fffd4000c30)
cm建连: if (cct->_conf->ms_async_rdma_cm), https://github.com/ssbandjl/ceph/commit/2d4890580f3acdd6387bcdde15f78eba35237589

社区优化, 检查rdma配置和修复逻辑错误: https://github.com/ceph/ceph/pull/28344
1. check rdma configuration is under hardware limitation.
2. fix ibv_port_attr object memory leak by using the object instead of allocating in the heap.
3. fix logic between RDMAV_HUGEPAGES_SAFE and ibv_fork_init.
4. fix error argument to get right qp state
5. separate Device construction when rdma_cm is used.
6. refine/simplify some function implementation.
7. decouple RDMAWorker & RDMAStack, RDMADispatcher & RDMAStack
8. remove redundant code.
9. rename var to improve readability.

cm讨论: https://lists.ceph.io/hyperkitty/list/dev@ceph.io/thread/YUX4DTCFXKLOBCQNSNBEBZGOBBQSYIS4/
您是说 1) 首先创建 RDMA 内存区域 (MR) 2) 在中使用 MR bufferlist 3）将bufferlist作为工作请求发布到RDMA发送队列中直接发送 不使用 tx_copy_chunk？

讨论3个rdma问题: https://lists.ceph.io/hyperkitty/list/dev@ceph.io/message/EHRT7TOSUP7PBJXQOBMQVUBA7JUQZNGF/

https://github.com/ceph/ceph/pull/28344/files
使用ceph块设备, rgw, fs: https://www.cnblogs.com/cyh00001/p/16759266.html
https://lists.ceph.io/hyperkitty/search?mlist=dev%40ceph.io&q=rdma
https://lists.ceph.io/hyperkitty/list/dev@ceph.io/message/EHRT7TOSUP7PBJXQOBMQVUBA7JUQZNGF/ 给豪迈的rdma建议
导出实时消息状态数据: sudo ceph daemon osd.0 perf dump AsyncMessenger::RDMAWorker-1
配置文件: ms_async_rdma_device_name = mlx5_0
查询gid; ibv_query_gid(ctxt, port_num, gid_idx, &gid)
roce: https://docs.nvidia.com/networking/pages/viewpage.action?pageId=12013422

支持共享接收队列: https://github.com/ssbandjl/ceph/commit/9fc9f08371d36d0cc38cbe8cbb235fa07ae0a6c0

为 beacon(灯塔) 保留额外的一个 WR，以指示所有 WCE 已被消耗
内存管理: memory_manager = new MemoryManager(cct, device, pd);
提升接收缓存区(内存管理)性能: https://github.com/ssbandjl/ceph/commit/720d044db13886ac9926d689e970381cdf78f8eb
注册内存: int Infiniband::MemoryManager::Cluster::fill(uint32_t num) -> malloc -> ibv_reg_mr
poll 处理接收事件: void RDMADispatcher::handle_rx_event(ibv_wc *cqe, int rx_number)
iwarp或ib(默认)
  if (cct->_conf->ms_async_rdma_type == "iwarp") {
    p = new RDMAIWARPConnectedSocketImpl(cct, ib, dispatcher, this);
  } else {
    p = new RDMAConnectedSocketImpl(cct, ib, dispatcher, this);
  }

连接后: worker->center.create_file_event(tcp_fd, EVENT_READABLE | EVENT_WRITABLE , established_handler)
发送cm元数据: int Infiniband::QueuePair::send_cm_meta

h3c tag: 2017/8/27, Aug 29, 2017, v12.2.0 https://github.com/ceph/ceph/commit/32ce2a3ae5239ee33d6150705cdb24d43bab910c
社区:
commit b661348f156f148d764b998b65b90451f096cb27 (tag: v12.1.2)
Author: Jenkins <jenkins@ceph.com>
Date:   Tue Aug 1 17:55:40 2017 +0000
12.1.2

rsync_to_h3c_win11(同步二进制到win10):
cd /c/Users/s30893/Downloads/ceph/ceph_perf_msgr
rsync -urpv root@ubuntu22:/root/project/stor/ceph/xb/docker/ceph/build/bin/ceph_perf_msgr_server .
rsync -urpv root@ubuntu22:/root/project/stor/ceph/xb/docker/ceph/build/bin/ceph_perf_msgr_client .
rsync -urpv root@ubuntu22:/root/project/stor/ceph/xb/docker/ceph/build/lib/libceph-common.so.2 .


编译ceph_msgr_perf工具:
cmake -DWITH_TESTS=1 ../CMakeList.txt
cd build
make common, ceph-common, ceph_perf_msgr_client, ceph_perf_msgr_server,  
make help 查看帮助

高级用法 
修改依赖的库
可以使用命令patchelf   修改工具依赖的动态库位置。避免和项目正在使用的库冲突，修改方法见：https://blog.csdn.net/bandaoyu/article/details/113181179

修改依赖的配置文件
修改依赖的配置文件，避免与正在运行的项目共用配置文件造成相互影响

ceph进程搜索配置文件的路径顺序

Ceph相关进程在读取配置时, 遵循以下的查找顺序

$CEPH_CONF 环境变量所指定的配置
-c path/path 参数所指定的配置
/etc/ceph/ceph.conf
~/.ceph/config (HOME目录下.ceph目录的config文件)
./ceph.conf (当前目录下的ceph.conf)

git:
git diff v12.2.0 main -- src/msg > git_diff_v12_2_0_main_src_msg
git diff v12.2.0 v15.2.17 -- src/msg > git_diff_v12_2_0_15_2_17_src_msg
git diff v15.2.17 main -- src/msg > git_diff_v15_2_17__main_src_msg

sync.sh
hosts='c51 c52'
for host in $hosts;do
	echo -e  "\n\033[32m`date +'%Y/%m/%d %H:%M:%S'` send to $host\033[0m"
	scp libceph-common.so.2 root@${host}:/home/xb/project/stor/ceph/xb/docker/ceph/build/lib/libceph-common.so.2
	scp ceph_perf_msgr_server root@${host}:/home/xb/project/stor/ceph/xb/docker/ceph/build/bin/ceph_perf_msgr_server
	scp ceph_perf_msgr_client root@${host}:/home/xb/project/stor/ceph/xb/docker/ceph/build/bin/ceph_perf_msgr_client
done

set_nonce: 设置随机值


Infiniband::Infiniband
verify_prereq

先设置环境变量, 后 ibv_fork_init() https://github.com/ceph/ceph/commit/b2d3f5e0970a4afbb82676af9e5b9a12f62a7747
ibv_fork_init will check environment variable RDMAV_HUGEPAGES_SAFE to decide whether huge page is usable in system
新版本增加判断:
if (ibv_is_fork_initialized() == IBV_FORK_UNNEEDED) {
  lderr(cct) << __FFL__ << " no need ibv_fork_init" << dendl;
}

src/test/msgr/perf_msgr_server.cc
server, 服务端, 建连接
MessengerServer
Messenger *Messenger::create type=async+rdma, lname=server -> Messenger *Messenger::create
  new AsyncMessenger -> AsyncMessenger::AsyncMessenger
    dispatch_queue
      DispatchQueue(CephContext *cct, Messenger *msgr, string &name) 本地快速分发, 节流阀
    lookup_or_create_singleton_object<StackSingleton>
    single->ready(transport_type)
       NetworkStack::create(cct, type) -> std::make_shared<RDMAStack>(c, t)
        RDMAStack::RDMAStack
          NetworkStack(cct, t) 构造, 线程池默认3个线程,
            create_worker(cct, type, worker_id) -> NetworkStack::create_worker -> new RDMAWorker(c, worker_id) -> RDMAWorker::RDMAWorker
              Worker(CephContext *c, unsigned worker_id) Stack是一个网络IO框架，封装了所有必要的基础网络接口，然后它管理线程工作。posix、dpdk甚至RDMA等不同的网络后端都需要继承Stack类来实现必要的接口。 所以这会让其他人轻松网络后端集成到 ceph 中。 否则，每个后端都需要实现整个 Messenger 逻辑，如重新连接、策略处理、会话维持...
            w->center.init -> EventCenter::init
              driver = new EpollDriver(cct)
              driver->init(this, nevent) -> int EpollDriver::init
                events = (struct epoll_event*)calloc
                epfd = epoll_create(1024)
                fcntl(epfd, F_SETFD, FD_CLOEXEC)
              file_events.resize(nevent) 5000
              pipe_cloexec(fds, 0) -> pipe2(pipefd, O_CLOEXEC | flags) 创建管道,均为非阻塞
              notify_receive_fd = fds[0] 接收端,读端
              notify_send_fd = fds[1] 发送端,写端
            workers.push_back(w)
          Infiniband::Infiniband
            device_name(cct->_conf->ms_async_rdma_device_name) 从配置中获取rdma设备名 TODO
            port_num( cct->_conf->ms_async_rdma_port_num) 默认为1 端口也从配置文件中获取
            verify_prereq -> void Infiniband::verify_prereq
              RDMAV_HUGEPAGES_SAFE 设置安全大页
              ibv_fork_init
              getrlimit(RLIMIT_MEMLOCK, &limit) 获取资源限制的配置
          get_num_worker 3
          for
            w->set_dispatcher(rdma_dispatcher)
            w->set_ib(ib)
    stack->start()
      std::function<void ()> thread = add_thread(i) 暂不执行
        w->center.set_owner()
          notify_handler = new C_handle_notify(this, cct)
          create_file_event(notify_receive_fd, EVENT_READABLE, notify_handler) 将之前管道的读端设置epoll监听
            driver->add_event(fd, event->mask, mask)
              epoll_ctl(epfd, op, fd, &ee)
            event->read_cb = ctxt 设置读事件回调
        w->initialize()
        w->init_done()
          init_cond.notify_all() 通知等待的线程,完成初始化
        while (!w->done)
          w->center.process_events 循环处理事件 -> int EventCenter::process_events
            driver->event_wait(fired_events, &tv) -> int EpollDriver::event_wait
              epoll_wait 写端写入c触发执行此处
              fired_events[event_id].fd = e->data.fd
            event = _get_file_event(fired_events[event_id].fd)
            cb = event->read_cb 可读回调
            cb->do_request(fired_events[event_id].fd) 处理事件
              r = read(fd_or_id, c, sizeof(c)) 读管道对端发来的字符,如:c
            cur_process.swap(external_events)
      spawn_worker(i, std::move(thread)) 启动新线程,返回join控制器
      workers[i]->wait_for_init() 等所有工人完成初始化
    local_connection = ceph::make_ref<AsyncConnection> -> AsyncConnection::AsyncConnection
      ms_connection_ready_timeout 建连超时时间
      ms_connection_idle_timeout 不活跃的时间, 如果两端连接空闲超过15分钟(没有活动的读写),则销毁连接
      read_handler = new C_handle_read(this) -> conn->process()
        void AsyncConnection::process()
      write_handler = new C_handle_write(this) -> conn->handle_write()
        void AsyncConnection::handle_write
      write_callback_handler = new C_handle_write_callback(this) -> AsyncConnection::handle_write_callback -> AsyncConnection::write 写的时候传递callback
      wakeup_handler = new C_time_wakeup(this) -> void AsyncConnection::wakeup_from -> void AsyncConnection::process()
      tick_handler = new C_tick_wakeup(this)-> void AsyncConnection::tick 计时器()
        protocol->fault() 处理错误
    init_local_connection
      void ms_deliver_handle_fast_connect
    reap_handler = new C_handle_reap(this)
      void AsyncMessenger::reap_dead 收割死连接
    processors.push_back(new Processor(this, stack->get_worker(i), cct))
      Processor::Processor
        listen_handler(new C_processor_accept(this))
          void Processor::accept() 等待事件触发(客户端执行connect后触发)
            listen_sockets -> while (true)
              msgr->get_stack()->get_worker()
              listen_socket.accept(&cli_socket, opts, &addr, w)
              msgr->add_accept
msgr->set_default_policy
dummy_auth.auth_registry.refresh_config()
msgr->set_auth_server(&dummy_auth) 初始化函数,在绑定前调用
server.start()
  msgr->bind(addr)
    AsyncMessenger::bind
      bindv -> int r = p->bind
      int Processor::bind
        listen_sockets.resize
        conf->ms_bind_retry_count 3次重试
        worker->center.submit_to lambda []()->void 匿名函数
          c->in_thread()
            pthread_equal(pthread_self(), owner) 本线程
          C_submit_event<func> event(std::move(f), false) f=listen
            void do_request -> f() -> listen -> worker->listen(listen_addr, k, opts, &listen_sockets[k]) -> int RDMAWorker::listen 由事件触发执行
              ib->init() -> void Infiniband::init
                new DeviceList(cct)
                  ibv_get_device_list 4网口
                  if (cct->_conf->ms_async_rdma_cm)
                  new Device(cct, device_list[i]) -> Device::Device
                    ibv_open_device
                    ibv_get_device_name
                    ibv_query_device 参考设备属性: device_attr
                get_device 根据配置的设备名在设备列表中查询, 默认取第一个, 如: mlx5_0
                binding_port -> void Device::binding_port
                  new Port(cct, ctxt, port_id) 端口ID从1开始 -> Port::Port
                    ibv_query_port(ctxt, port_num, &port_attr)
                    ibv_query_gid(ctxt, port_num, gid_idx, &gid)
                    ib_physical_port = device->active_port->get_port_num() 获取物理端口
                new ProtectionDomain(cct, device) -> Infiniband::ProtectionDomain::ProtectionDomain -> ibv_alloc_pd(device->ctxt)
                support_srq = cct->_conf->ms_async_rdma_support_srq 共享接收队列srq
                rx_queue_len = device->device_attr.max_srq_wr 最终为4096
                tx_queue_len = device->device_attr.max_qp_wr - 1 发送队列为beacon保留1个WR, 如:1024 1_K 重载操作符
                device->device_attr.max_cqe 设备允许 4194303 完成事件
                memory_manager = new MemoryManager(cct, device, pd) -> Infiniband::MemoryManager::MemoryManager 128K -> mem_pool ->  boost::pool
                memory_manager->create_tx_pool(cct->_conf->ms_async_rdma_buffer_size, tx_queue_len) -> void Infiniband::MemoryManager::create_tx_pool
                  send = new Cluster(*this, size)
                  send->fill(tx_num) -> int Infiniband::MemoryManager::Cluster::fill
                    base = (char*)manager.malloc(bytes) -> void* Infiniband::MemoryManager::malloc -> std::malloc(size) 标准分配或分配大页(huge_pages_malloc)
                    ibv_reg_mr 注册内存
                    new(chunk) Chunk
                    free_chunks.push_back(chunk)
                create_shared_receive_queue
                  ibv_create_srq
                post_chunks_to_rq -> int Infiniband::post_chunks_to_rq
                  chunk = get_memory_manager()->get_rx_buffer() -> return reinterpret_cast<Chunk *>(rxbuf_pool.malloc())
                  ibv_post_srq_recv
              dispatcher->polling_start() -> void RDMADispatcher::polling_start
                ib->get_memory_manager()->set_rx_stat_logger(perf_logger) -> void PerfCounters::set
                tx_cc = ib->create_comp_channel(cct) -> Infiniband::CompletionChannel* Infiniband::create_comp_channel -> new Infiniband::CompletionChannel
                tx_cq = ib->create_comp_queue(cct, tx_cc)
                  cq->init() -> int Infiniband::CompletionChannel::init
                    ibv_create_comp_channel 创建完成通道 -> NetHandler(cct).set_nonblock(channel->fd) 设置非阻塞
                t = std::thread(&RDMADispatcher::polling, this) 启动polling线程 rdma-polling -> void RDMADispatcher::polling
                  tx_cq->poll_cq(MAX_COMPLETIONS, wc)
                  handle_tx_event -> tx_chunks.push_back(chunk) -> post_tx_buffer
                    tx -> void RDMAWorker::handle_pending_message()
                  handle_rx_event -> void RDMADispatcher::handle_rx_event
                    conn->post_chunks_to_rq(1) 向接收队列补一个内存块(WR) -> int Infiniband::post_chunks_to_rq
                      ibv_post_srq_recv | ibv_post_recv
                    polled[conn].push_back(*response)
                    qp->remove_rq_wr(chunk)
                    chunk->clear_qp()
                    pass_wc -> void RDMAConnectedSocketImpl::pass_wc(std::vector<ibv_wc> &&v) ->  notify() -> void RDMAConnectedSocketImpl::notify
                      eventfd_write(notify_fd, event_val) -> eventfd_read(notify_fd, &event_val) <- ssize_t RDMAConnectedSocketImpl::read <- process
              new RDMAServerSocketImpl(cct, ib, dispatcher, this, sa, addr_slot)
              int r = p->listen(sa, opt) -> int RDMAServerSocketImpl::listen
                server_setup_socket = net.create_socket(sa.get_family(), true) -> socket_cloexec
                net.set_nonblock
                net.set_socket_options
                ::bind(server_setup_socket, sa.get_sockaddr(), sa.get_sockaddr_len()) 系统调用
                ::listen backlog=512
              *sock = ServerSocket(std::unique_ptr<ServerSocketImpl>(p))
            cond.notify_all() -> 通知等待的线程
          dispatch_event_external -> void EventCenter::dispatch_event_external
            external_events.push_back(e)
            wakeup()
              write(notify_send_fd, &buf, sizeof(buf)) buf=c -> notify_receive_fd, 唤醒 epoll_wait
          event.wait()
  msgr->add_dispatcher_head(&dispatcher)
    ready()
      p->start() -> void Processor::start()
        worker->center.create_file_event listen_handler -> pro->accept() -> void Processor::accept()
  msgr->start() -> int AsyncMessenger::start()
  msgr->wait() -> void AsyncMessenger::wait()  
    

客户端建连接, src/test/msgr/perf_msgr_client.cc
perf_msgr_client.cc -> main
  MessengerClient client(public_msgr_type, args[0], think_time)
  client.ready
    Messenger *msgr = Messenger::create
    msgr->set_default_policy -> Policy(bool l, bool s ...
    msgr->start() -> int AsyncMessenger::start()
      if (!did_bind) 客户端不需要bind
      set_myaddrs(newaddrs) -> void Messenger::set_endpoint_addr
      _init_local_connection() -> void _init_local_connection()
        ms_deliver_handle_fast_connect(local_connection.get()) -> void ms_deliver_handle_fast_connect将新连接通知每个快速调度程序。 每当启动或重新连接新连接时调用此函数 fast_dispatchers为空?
    ConnectionRef conn = msgr->connect_to_osd(addrs) 连接到OSD -> ConnectionRef connect_to_osd -> ConnectionRef AsyncMessenger::connect_to
      AsyncConnectionRef conn = _lookup_conn(av) 先在连接池查找连接
      conn = create_connect(av, type, false) 没找到,新建连接 -> AsyncConnectionRef AsyncMessenger::create_connect
        Worker *w = stack->get_worker()
        auto conn = ceph::make_ref<AsyncConnection> -> AsyncConnection::AsyncConnection 构造连接
          recv_buf = new char[2*recv_max_prefetch] 使用缓冲区读取来避免小的读取开销
          new ProtocolV2(this) ceph v2协议, 在v1基础上支持地址向量, 在横幅(banner)交换之后，对等体交换他们的地址向量address vectors
        conn->connect(addrs, type, target) -> void AsyncConnection::connect -> _connect -> void AsyncConnection::_connect()
          state = STATE_CONNECTING 初始状态机
          protocol->connect() -> void ProtocolV2::connect() -> state = START_CONNECT
          center->dispatch_event_external(read_handler) -> 触发状态机推进 -> process
        conns[addrs] = conn 保存连接 -> ceph::unordered_map<entity_addrvec_t, AsyncConnectionRef> conns 无序map
    ClientThread *t = new ClientThread(msgr, c, conn, msg_len, ops, think_time_us) -> ClientThread(Messenger *m 新建客户端线程, 构造数据
      m->add_dispatcher_head(&dispatcher)
      bufferptr ptr(msg_len) 申请数据指针 -> buffer::ptr::ptr(unsigned l) : _off(0), _len(l)
         _raw = buffer::create(l).release() -> ceph::unique_leakable_ptr<buffer::raw> buffer::create, 通过返回其值并用空指针替换它来释放其存储指针的所有权。此调用不会破坏托管对象，但 unique_ptr 对象从删除对象的责任中解脱出来。 某些其他实体必须负责在某个时刻删除该对象。要强制销毁指向的对象，请使用成员函数 reset 或对其执行赋值操作
          buffer::create_aligned(len, sizeof(size_t)) -> ceph::unique_leakable_ptr<buffer::raw> buffer::create_aligned
            create_aligned_in_mempool -> ceph::unique_leakable_ptr<buffer::raw> buffer::create_aligned_in_mempool
              return raw_combined::create(len, align, mempool) -> src/common/buffer.cc -> static ceph::unique_leakable_ptr<buffer::raw> -> create(unsigned len,
                align = std::max<unsigned>(align, sizeof(void *)) = 8
                size_t rawlen = round_up_to(sizeof(buffer::raw_combined) 96
                size_t datalen = round_up_to(len, alignof(buffer::raw_combined)) 4096
                int r = ::posix_memalign((void**)(void*)&ptr, align, rawlen + datalen); 96+4096
                new (ptr + datalen) raw_combined(ptr, len, align, mempool))
         _raw->nref.store(1, std::memory_order_release)
      memset(ptr.c_str(), 0, msg_len) 置0
      data.append(ptr) 将data填充全0 -> void buffer::list::append -> void push_back(const ptr& bp)
        _buffers.push_back(*ptr_node::create(bp).release())
        _len += bp.length()
    msgrs.push_back(msgr)
    clients.push_back(t)
  Cycles::init() -> void Cycles::init() 校准时钟频率
  uint64_t start = Cycles::rdtsc()
  client.start() -> void start() -> clients[i]->create("client") -> void Thread::create
    pthread_create(&thread_id, thread_attr, _entry_func, (void*)this) -> void *Thread::_entry_func
    void *entry() override 重写entry
      hobject_t hobj(oid, oloc.key -> struct object_t
        void build_hash_cache() crc32c
      MOSDOp *m = new MOSDOp -> MOSDOp(int inc, long tid, 
      bufferlist msg_data(data) 拷贝构造函数?, 拷贝数据
      m->write(0, msg_len, msg_data) -> void write 通过消息msg写数据到对端, offset=0, len=4096, buffer_list=bl(msg_data)
        add_simple_op(CEPH_OSD_OP_WRITE, off, len) -> ops.push_back(osd_op)
           osd_op.op.extent.offset = off
           osd_op.op.extent.length = len
           ops.push_back(osd_op)
        data.claim(bl)
          clear()
          claim_append(bl) -> void buffer::list::claim_append 要求追加, 免拷贝?
            _buffers.splice_back(bl._buffers) 拼接回来
            bl._buffers.clear_and_dispose()
        header.data_off = off
      conn->send_message(m) -> void ProtocolV2::send_message(Message *m) ssize_t RDMAConnectedSocketImpl::send
        out_queue[m->get_priority()].emplace_back
        connection->center->dispatch_event_external(connection->write_handler) -> void AsyncConnection::handle_write
          const auto out_entry = _get_next_outgoing()
          more = !out_queue.empty() 如果发送队列不为空,则more为true,表示还有更多的待发送的数据
          write_message(out_entry.m, more)
            ssize_t total_send_size = connection->outgoing_bl.length() 4406=310+4096
            connection->_try_send(more) -> cs.send(outgoing_bl, more) -> ssize_t RDMAConnectedSocketImpl::send
              size_t bytes = bl.length() 4406B
              pending_bl.claim_append(bl) 换变量, bl留着干啥? 回收?
              ssize_t r = submit(more) ssize_t -> RDMAConnectedSocketImpl::submit
                pending_bl.length() 4406
                auto it = std::cbegin(pending_bl.buffers()) cbegin()和cend()是C++11新增的，它们返回一个const的迭代器，不能用于修改元素, 常量迭代器
                while (it != pending_bl.buffers().end()) 循环
                if (ib->is_tx_buffer(it->raw_c_str())) 不进该分支
                msg/async/rdma：使用 shared_ptr 管理 Infiniband obj
                1.不要使用裸指针来管理Infiniband obj
                2.直接访问Infiniband obj而不是从RDMA堆栈。 这可以避免在 RDMAWorker 和 RDMADispatcher 中缓存 RDMAStack obj
                wait_copy_len += it->length() = 32
                tx_buffers.push_back(ib->get_tx_chunk_by_buffer(it->raw_c_str()))
                size_t copied = tx_copy_chunk(tx_buffers, wait_copy_len, copy_start, it);
                total_copied += tx_copy_chunk(tx_buffers, wait_copy_len, copy_start, it) -> size_t RDMAConnectedSocketImpl::tx_copy_chunk
                  int RDMAWorker::get_reged_mem -> 获取已注册的内存 int Infiniband::get_tx_buffers -> get_send_buffers -> Infiniband::MemoryManager::Cluster::get_buffers
                    size_t got = ib->get_memory_manager()->get_tx_buffer_size() * r  131072>4406 获取到的内存满足需求的大小
                  Chunk *current_chunk = tx_buffers[chunk_idx]
                  size_t real_len = current_chunk->write((char*)addr + slice_write_len, start->length() - slice_write_len) -> uint32_t Infiniband::MemoryManager::Chunk::write
                    memcpy(buffer + offset, buf, write_len) 拷贝内存(循环拷贝)
                  write_len 4406
                pending_bl.clear() 拷贝完释放pb
                post_work_request(tx_buffers)
                  while (current_buffer != tx_buffers.end())
                  ibv_post_send -> ibv_poll_cq 触发发端/收端 -> int Infiniband::CompletionQueue::poll_cq <- void RDMADispatcher::polling()
      msgr->shutdown()
  stop = Cycles::rdtsc()
...
NetworkStack::add_thread
  w->center.process_events -> C_handle_read -> conn->process() -> void AsyncConnection::process()
    worker->connect(target_addr, opts, &cs) -> int RDMAWorker::connect
      ib->init()
      dispatcher->polling_start()
      new RDMAConnectedSocketImpl -> RDMAConnectedSocketImpl::RDMAConnectedSocketImpl
        read_handler(new C_handle_connection_read(this))
        established_handler(new C_handle_connection_established(this))
      p->try_connect(addr, opts) -> int RDMAConnectedSocketImpl::try_connect
        tcp_fd = net.nonblock_connect(peer_addr, opts.connect_bind_addr) -> generic_connect -> int NetHandler::generic_connect
          create_socket
          ::connect(s, addr.get_sockaddr(), addr.get_sockaddr_len()) syscall 客户端连接服务端(socket) -> 服务端触发事件(C_processor_accept) -> void Processor::accept()
          worker->center.create_file_event(tcp_fd, EVENT_READABLE | EVENT_WRITABLE , established_handler) -> established_handler -> int RDMAConnectedSocketImpl::handle_connection_established

      *socket = ConnectedSocket(std::move(csi))
    center->create_file_event(cs.fd(), EVENT_READABLE, read_handler) -> state = STATE_CONNECTING_RE -> void AsyncConnection::process() (回到process)
    ...
    case STATE_CONNECTING_RE
      cs.is_connected()
      center->create_file_event EVENT_WRITABLE read_handler -> process
      logger->tinc -> void PerfCounters::tinc 性能统计(时延统计)
    ...
    protocol->read_event() -> START_ACCEPT -> run_continuation(CONTINUATION(start_server_banner_exchange))
      CONTINUATION_RUN(continuation)
      CtPtr ProtocolV2::read -> ssize_t AsyncConnection::read -> read_until
        read_bulk ->  nread = cs.read(buf, len) -> ssize_t RDMAConnectedSocketImpl::read -> eventfd_read(notify_fd, &event_val)
          read = read_buffers(buf,len) -> ssize_t RDMAConnectedSocketImpl::read_buffers
            buffer_prefetch() 预读 -> void RDMAConnectedSocketImpl::buffer_prefetch
              ibv_wc* response = &cqe[i]
              chunk->prepare_read(response->byte_len)
              buffers.push_back(chunk)
            tmp = (*pchunk)->read(buf + read_size, len - read_size) -> uint32_t Infiniband::MemoryManager::Chunk::read
              memcpy(buf, buffer + offset, read_len);
            (*pchunk)->reset_read_chunk() 将偏移和边界都置0
            dispatcher->post_chunk_to_pool(*pchunk) -> void RDMADispatcher::post_chunk_to_pool
              ib->post_chunk_to_pool(chunk)
            update_post_backlog -> void RDMAConnectedSocketImpl::update_post_backlog
            
超时处理: 
new C_handle_reap(this)
  local_worker->create_time_event( ReapDeadConnectionMaxPeriod...
    reap_dead


设备属性: device_attr
(gdb) p device_attr
$17 = {
  fw_ver = "16.33.1048", '\000' <repeats 53 times>, 
  node_guid = 8550064101420093112, 
  sys_image_guid = 8550064101420093112, 
  max_mr_size = 18446744073709551615, 
  page_size_cap = 18446744073709547520, 
  vendor_id = 713, 
  vendor_part_id = 4119, 
  hw_ver = 0, 
  max_qp = 131072, 
  max_qp_wr = 32768, 
---Type <return> to continue, or q <return> to quit---
  device_cap_flags = 3983678518, 
  max_sge = 30, 
  max_sge_rd = 30, 
  max_cq = 16777216, 
  max_cqe = 4194303, 
  max_mr = 16777216, 
  max_pd = 8388608, 
  max_qp_rd_atom = 16, 
  max_ee_rd_atom = 0, 
  max_res_rd_atom = 2097152, 
  max_qp_init_rd_atom = 16, 
---Type <return> to continue, or q <return> to quit---
  max_ee_init_rd_atom = 0, 
  atomic_cap = IBV_ATOMIC_HCA, 
  max_ee = 0, 
  max_rdd = 0, 
  max_mw = 16777216, 
  max_raw_ipv6_qp = 0, 
  max_raw_ethy_qp = 0, 
  max_mcast_grp = 2097152, 
  max_mcast_qp_attach = 240, 
  max_total_mcast_qp_attach = 503316480, 
  max_ah = 2147483647, 
---Type <return> to continue, or q <return> to quit---
  max_fmr = 0, 
  max_map_per_fmr = 0, 
  max_srq = 8388608, 
  max_srq_wr = 32767, 
  max_srq_sge = 31, 
  max_pkeys = 128, 
  local_ca_ack_delay = 16 '\020', 
  phys_port_cnt = 1 '\001'
}


qp析构(销毁qp): schedule_qp_destroy
msg/async/rdma：使用特殊的Beacon 检测SQ WRs drained 将QueuePair 切换到error 状态，然后post Beacon WR 发送队列。 所有未完成的 WQE 将被刷新到 CQ。 在 CQ 中，在销毁 QueuePair 之前，检查完成队列元素以检测 SQ WRs 是否已被耗尽。 如果不使用/不支持 SRQ，我们不会将另一个 Beacon WR 发布到 RQ，原因是只有在从 CQ 轮询了所有刷新的 WR 后，才能销毁 QueuePair。请参阅以下规范的第 474 页：InfiniBandTM 架构规范第 1 卷，版本 1.3 规范链接：https://cw.infinibandta.org/document/dl/7859

安装rdma依赖, libibverbs, 
yum install -y libibverbs-devel librdmacm-devel

yum update
yum install epel-release
yum install boost boost-thread boost-devel

centos8: 
rpm -qa|grep libibverbs
libibverbs-35.0-1.el8.x86_64
编译rdma-core: https://runsisi.com/2021/03/07/rdma-core/

版本差异: https://github.com/ceph/ceph/compare/v12.2.0...v15.2.17

TODO:
1. 3.0增加 
modify_qp_to_error
modify_qp_to_rts
modify_qp_to_rtr
内存池: msg/async/rdma: improves RX buffer management: https://github.com/ssbandjl/ceph/commit/720d044db13886ac9926d689e970381cdf78f8eb
共享接收队列 srq: https://github.com/ssbandjl/ceph/commit/282499b77f85fed50ce00c5414af12335371a4b3
修复错误事件中心被 rdma 构造连接传输 ib sync msg C_handle_connection_established: https://github.com/ssbandjl/ceph/commit/8b2a95011ca34ba3880440339693170a174034ab
schedule_qp_destroy: https://github.com/ssbandjl/ceph/commit/e907e18154421885f1b02518496694b0987ab9f9
buffer_prefetch 预读: https://github.com/ssbandjl/ceph/commit/2754d60f6615024c76f09d22d2480a9b69369a12
msg/async/rdma: deal with all RDMA device async event 补全事件处理: https://github.com/ssbandjl/ceph/commit/1c76c1320721cc555a376d7b8660c19538d3f1b4
1.列出RDMA设备的所有异步事件
2.输出致命错误事件以检查RDMA设备状态
加锁获取qp: https://github.com/ssbandjl/ceph/commit/cc08b02046ce1243926c2d716281566bd0a70402
加速 tx handle 以前 Dispatcher 线程将轮询 rx 和 tx 事件，然后调度, 这些事件传递给 RDMAWorker 和 RDMAConnectedSocketImpl: https://github.com/ssbandjl/ceph/commit/bc580b0a6100637ecbfeeecefc84e2b81ff25c34

补充rdma主要改动: 
https://ceph.io/en/news/blog/2019/v14-2-0-nautilus-released/
https://ceph.io/en/news/blog/2020/v15-2-0-octopus-released/
配置gid: https://github.com/ceph/ceph/pull/31517/files
修复内存泄漏: rdma_free_devices https://github.com/ceph/ceph/pull/27574/files
代码优化: 将连接管理数据（LID、GID、QPN、PSN）从 RDMAConnectedSocketImpl 移动到 QueuePair。 目标是 1) 简化 switch QueuePair 状态 2) 简化管理连接管理数据将 QP 切换到 Error 状态以将未完成的 WR 刷新到 CQ 并使用 Beacon WR 检测 SQD 使没有SRQ的RNIC在msg/async/rdma中工作 根据2&3中的变化细化handle_rx/tx_event&handle_async_event 根据其父类简化 RDMAIWARPConnectedSocketImpl, https://github.com/ceph/ceph/pull/29947

v16.2.0 Pacific released 太平洋
centos8: https://ceph.io/en/news/blog/2021/v16-2-0-pacific-released/

AMQP（Advanced Message Queuing Protocol，高级消息队列协议）是一个进程间传递异步消息的网络协议



client 写流程:
conn->send_message(*p)


减少状态机:
 private:
  enum {
    STATE_NONE,
    STATE_CONNECTING,
    STATE_CONNECTING_RE,
    STATE_ACCEPTING,
    STATE_CONNECTION_ESTABLISHED,
    STATE_CLOSED
  };

bench-write
gdb --args rbd bench-write .d2.rbd/500G3 --io-size 4M --io-pattern rand --io-threads 1 --io-total 100M
/home/xb/project/stor/ceph/xb/docker/ceph/src/tools/rbd/rbd.cc main
  do_bench -> start_io aio_write2 ictx->io_work_queue->aio_write 

客户端写, osd写, 参考
https://my.oschina.net/u/2460844/blog/534390
src/osdc/Objecter.cc
void Objecter::_op_submit
  check_for_latest_map = _calc_target(&op->target, nullptr) -> int Objecter::_calc_target

  _get_session(op->target.osd, &s, sul) -> int Objecter::_get_session
    osd_sessions.find(osd)
    OSDSession *s = new OSDSession(cct, osd) 没找到, 准备新建
    s->con = messenger->connect_to_osd(osdmap->get_addrs(osd))
    logger->set(l_osdc_osd_sessions, osd_sessions.size()) 统计会话数量
  _session_op_assign(s, op)
  _send_op(op) -> void Objecter::_send_op(Op *op) -> op->session -> con->send_message(m)


osd落盘, 主副本和从副本同步: https://my.oschina.net/u/2460844/blog/534390
osd读数据
seastar::future<> ProtocolV2::read_message
...
bool OSD::ms_dispatch
void OSD::_dispatch
void OSD::dispatch_op -> void OSD::handle_pg_create
  osdmap->get_primary_shard(on, &pgid)

crimson是crimson-osd的代号，也就是下一代ceph-osd。 它通过利用 DPDK 和 SPDK 等最先进的技术，以快速网络设备、快速存储设备为目标，以获得更好的性能。 并且它将通过 BlueStore 保留对 HDD 和低端 SSD 的支持。 Crimson 将尝试向后兼容经典 OSD

crc问题:
handle_read_frame_dispatch
handle_message
Message *decode_message
  bad crc in front
filter: *.cc, *.h, src/msg, *.sh, *.rst

生成uudid: uuidgen
NetworkStack::add_thread(unsigned int)::{lambda()#1}::operator()() const ()
  EventCenter::process_events(int)
    AsyncConnection::process
      decode_message(CephContext*, int, ceph_msg_header&, ceph_msg_footer&, ceph::buffer::list&, ceph::buffer::list&, ceph::buffer::list&, Connection*)
        bad crc in front

测试:
ceph_test_msgr src/test/test_msgr.cc main


前端:
发现目标
iscsiadm --mode discovery --op update --type sendtargets --portal targetIP
iscsiadm -m discovery -t sendtargets -p 175.19.53.72

创建需要的设备:
iscsiadm --mode node -l all

查看所有活动的会话
iscsiadm --mode session

目标:
查看模块(rbd,bs_rbd.so的动态链接库):
tgtadm --lld iscsi --mode system --op show|grep rbd

创建镜像:
rbd create iscsi-image --size 4096
查镜像:
rbd ls -p p0 -l --format json --pretty-format

创建tgt:
tgtadm --lld iscsi --mode target --op new --tid 1 --targetname iqn.2013-15.com.example:cephtgt.target0
查看:
tgtadm --lld iscsi --op show --mode target
tgtadm --lld iscsi --op show --mode target --tid 512

创建lun:
tgtadm --lld iscsi --mode logicalunit --op new --tid 1 --lun 1 --backing-store iscsi-image --bstype rbd
tgtadm --lld iscsi --op bind --mode target --tid 1 -I ALL

查看rbd块设备
rbd ls {poolname}

查池:
rados lspools
ceph osd lspools
ceph osd pool ls
ceph osd pool ls detail
ceph osd dump|grep pool
rados df
ceph osd pool get {pool-name} {key}
ceph df


创池:
ceph osd pool create wgsrbd 64 64

存储池启用rbd:
ceph osd pool application enable wgsrbd rbd

初始化rbd:
rbd pool init -p wgsrbd
下一步就可创建镜像

客户端映射镜像:
rbd -p wgsrbd map wgs-img1

编译rpm包:
安装工具: yum install rpm-build rpmdevtools -y
make-dist, 该脚本主要用于生成ceph.spec和ceph-*.tar.bz2文件以供后面打rpm包使用。我们主要改变了rpm_version,rpm_release ，是否下载boost库等
bash make-srpm.sh build
bash make-srpm.sh clean

tar --strip-components=1 -C ~/rpmbuild/SPECS/ --no-anchored -xvjf ~/rpmbuild/SOURCES/ceph-10.2.11.tar.bz2 "ceph.spec"
生成目录:
rpmdev-setuptree
ls -alh /root/rpmbuild
tree rpmbuild/SRPMS/
rpmbuild -ba ~/rpmbuild/SPECS/ceph.spec
tree  rpmbuild/RPMS/
%debug_package
%prep

daos参考:
%if (0%{?suse_version} > 0)
%global __debug_package 1
%global _debuginfo_subpackages 0
%debug_package
%endif

%prep
%autosetup

默认目录: /root/rpmbuild
修改目录: –buildroot xxx
或 Specify the topdir parameter in the rpmrc file or rpmmacros file
rpmbuild --root /home/rpmbuild -ta driver.tar.gz

create a file called .rpmmacros in your %HOME% dir, add the following to the file "%_topdir x:/rpmbuild" (w/o the quotes)
.rpmmacros
%packager YourName
%_topdir /home/build/rpmbuild 
%_tmppath /home/build/rpmbuild/tmp

单元测试:
ctest -V -R client

rpm:
https://download.ceph.com/rpm-15.2.17/el7/x86_64/
https://mirrors.cloud.tencent.com/ceph/rpm-15.2.17/el7/SRPMS/ceph-15.2.17-0.el7.src.rpm

编译:
参考命令
安装gcc
git clone [https://github.com/gcc-mirror/gcc.git](https://github.com/gcc-mirror/gcc.git)
mkdir build ; cd build
../configure --prefix=/usr --disable-multilib 
yum install -y gmp-devel libmpc-devel mpfr-devel flex
make -j16 > log 2>&1 && make install >log 2>&1 &

编译前检查:
make check

安装依赖包:
yum -y install python36-Cython

编译, 统计cpu性能
./do-cmake.sh -DCMAKE_CXX_FLAGS="-fno-omit-frame-pointer -O2 -g"
cd build
cmake --build .

rbd接口使用:
examples/librbd/hello_world.cc



h3c:
Processor::accept


技术检查:
src/script/ceph-debug-docker.sh v15.2.17

cmake .. \
-DPYTHON_INCLUDE_DIR=$(python3 -c "import sysconfig; print(sysconfig.get_path('include'))")  \
-DPYTHON_LIBRARY=$(python3 -c "import sysconfig; print(sysconfig.get_config_var('LIBDIR'))")

message("python3头文件目录：" ${Python3_INCLUDE_DIRS})
message("python3的版本信息：" ${Python3_VERSION})
message("python3的库文件信息：" ${Python3_LIBRARIES})

file(
  DOWNLOAD
  "${url}" "/home/xb/project/stor/ceph/xb/docker/ceph/build/boost/src/boost_1_72_0.tar.bz2"

../src/vstart.sh -d -n -l -e -o "osd_tracing = true"


Timed out waiting for lttng-sessiond (in lttng_ust_init() at lttng-ust-comm.c:1444)

deps install
$SUDO $yumdnf install -y $yumdnf-utils

yum install lttng-ust-devel
yum install epel-release
rm -rf build;clear;./do_cmake.sh
分析代码技术错误,  analyze Teuthology failures
src/script/ceph-debug-docker.sh v15.2.17

虚拟集群:
cd build
OSD=3 MON=3 MGR=3 ../src/vstart.sh -n -x
# check that it's there
bin/ceph health

rm -rf build;clear;./do_cmake.sh && cd build && make -j64
rpm -ivh python3-devel-3.6.8-18.el7.x86_64.rpm
ceph.spec prometheus
ceph.spec.in
yum-builddep -y --setopt=*.skip_if_unavailable=true /tmp/install-deps.3292027/ceph.spec 2>&1 | tee /tmp/install-deps.3292027/yum-builddep.out



echo "182.200.53.62 c62" >> /etc/hosts


lttng:
yum install -y epel-release
yum install -y lttng-tools lttng-ust

qa:
1. 识别大IO, 大块IO(4K, 512K, 1M...)在哪里进行切分? 就在那里做RDMA单边逻辑
2. 

release cpp demo, release.cc, release.cpp
// unique_ptr::release example
#include <iostream>
#include <memory>

int main () {
  std::unique_ptr<int> auto_pointer (new int);
  int * manual_pointer;

  *auto_pointer=10;

  manual_pointer = auto_pointer.release();
  // (auto_pointer is now empty)

  std::cout << "manual_pointer points to " << *manual_pointer << '\n';

  delete manual_pointer;

  return 0;
}

ptr* _carriage(运输): rack bufferptr 我们可以修改（特别是 ::append() 到）。 并非所有 bptrs 缓冲区列表都具有此特性——如果有人 ::push_back(const ptr&)，他希望它不会改变