https://github.com/ssbandjl/ceph/tree/v15.2.17
https://github.com/ssbandjl/ceph/tree/main

build:
close mrg,dashboard in build/CMakeCache.txt

基准测试, 文档: messenger.rst
https://blog.csdn.net/bandaoyu/article/details/114292690
使用命令patchelf   修改工具依赖的动态库位置。避免和项目正在使用的库冲突: https://blog.csdn.net/bandaoyu/article/details/113181179
doc/dev/messenger.rst
perf_msgr_client.cc -> ceph_perf_msgr_client
perf_msgr_server.cc -> ceph_perf_msgr_server

perf_msgr_client.cc -> main


编译rdma:
./do_cmake.sh -DCMAKE_INSTALL_PREFIX=/usr -DWITH_RDMA=ON

just build client:
make client > log 2>&1 &

qa:
启动虚拟集群: https://docs.ceph.com/en/quincy/dev/dev_cluster_deployement/

docker:

git
git remote add upstream https://github.com/Foo/repos.git
git pull upstream v15.2.17
git remote remove upstream
git remote add upstream https://github.com/Foo/repos.git
git remote set-url upstream https://github.com/Foo/repos.git

git push origin ：refs/tags/3.0 这就是明确告诉服务器删除的tag的分支,删除branch分支
git push origin :refs/heads/3.0
git branch -D testtag
删除tag分支的方法：
git tag -d v15.2.17
git push origin v15.2.17
git config --global credential.helper "cache --timeout=604800"


build:
close mrg,dashboard in build/CMakeCache.txt


基准测试:
https://blog.csdn.net/bandaoyu/article/details/114292690
使用命令patchelf   修改工具依赖的动态库位置。避免和项目正在使用的库冲突: https://blog.csdn.net/bandaoyu/article/details/113181179
doc/dev/messenger.rst
perf_msgr_client.cc -> ceph_perf_msgr_client
perf_msgr_server.cc -> ceph_perf_msgr_server


perf_msgr_client.cc -> main


编译rdma:
./do_cmake.sh -DCMAKE_INSTALL_PREFIX=/usr -DWITH_RDMA=ON
cd build
cmake ..
CMakeLists.txt
  
启动虚拟集群: https://docs.ceph.com/en/quincy/dev/dev_cluster_deployement/

CMakeCache.txt
ON/OFF
address sanitizer

docker run -it -d --privileged --cap-add=ALL --name centos7  -p 22223:22 -p 6666:6666 -v /home/xb/project/stor/ceph/xb/docker/ceph:/home/xb/project/stor/ceph/xb/docker/ceph ceph_centos7:v15.2.17
docker exec -u root -it ceph bash -c 'cd /home/xb/project/stor/ceph/xb/docker/ceph;exec "${SHELL:-sh}"'

docker run -it -d --privileged --cap-add=ALL --name ceph  -p 22223:22 -p 6666:6666 -v /home/xb/project/ceph/xb/ceph:/home/xb/project/stor/ceph/xb/docker/ceph ceph_centos7:v15.2.17

gdb:
cd /home/xb/project/stor/ceph/xb/docker/ceph/build/bin
bash gdb_s.sh
b main
r

常用:
获取线程名:
prctl(PR_GET_NAME, buf)

创建线程:
pthread_create(&thread_id, thread_attr, _entry_func, (void*)this)

设置日志文件: set_log_file
打开日志文件: m_fd = ::open(m_log_file.c_str(), O_CREAT|O_WRONLY|O_APPEND|O_CLOEXEC, 0644)
打印日志: cerr << __func__ << " " << __FL__ << " server accept client connect" << std::endl;
打印日志代码示例: ldout(cct, 10) << __FFL__ << " client connect -> server" << dendl;

日志配置:
debug {subsystem} = {log-level}/{memory-level}
#for example
debug mds log = 1/20

打印每行日志:
void Log::_flush

默认配置: Option("ms_type"
默认开启RDMA: .set_default("async+rdma")
配置文件: https://docs.ceph.com/en/latest/rados/configuration/ceph-conf/


返回自动变量auto:  const auto& _lookup_conn
要求(断言)已上锁: ceph_assert(ceph_mutex_is_locked(lock))

int RDMAWorker::listen -> rdma ib初始化: ib->init() -> void Infiniband::init()

int RDMAWorker::connect -> ib->init() -> void Infiniband::init()
  

gdb 打印ib设备: (gdb) p **((ibv_device **) 0x7fffd4000c30)
cm建连: if (cct->_conf->ms_async_rdma_cm), https://github.com/ssbandjl/ceph/commit/2d4890580f3acdd6387bcdde15f78eba35237589

社区优化, 检查rdma配置和修复逻辑错误: https://github.com/ceph/ceph/pull/28344
1. check rdma configuration is under hardware limitation.
2. fix ibv_port_attr object memory leak by using the object instead of allocating in the heap.
3. fix logic between RDMAV_HUGEPAGES_SAFE and ibv_fork_init.
4. fix error argument to get right qp state
5. separate Device construction when rdma_cm is used.
6. refine/simplify some function implementation.
7. decouple RDMAWorker & RDMAStack, RDMADispatcher & RDMAStack
8. remove redundant code.
9. rename var to improve readability.

cm讨论: https://lists.ceph.io/hyperkitty/list/dev@ceph.io/thread/YUX4DTCFXKLOBCQNSNBEBZGOBBQSYIS4/
您是说 1) 首先创建 RDMA 内存区域 (MR) 2) 在中使用 MR bufferlist 3）将bufferlist作为工作请求发布到RDMA发送队列中直接发送 不使用 tx_copy_chunk？

讨论3个rdma问题: https://lists.ceph.io/hyperkitty/list/dev@ceph.io/message/EHRT7TOSUP7PBJXQOBMQVUBA7JUQZNGF/

https://github.com/ceph/ceph/pull/28344/files
使用ceph块设备, rgw, fs: https://www.cnblogs.com/cyh00001/p/16759266.html
https://lists.ceph.io/hyperkitty/search?mlist=dev%40ceph.io&q=rdma
https://lists.ceph.io/hyperkitty/list/dev@ceph.io/message/EHRT7TOSUP7PBJXQOBMQVUBA7JUQZNGF/ 给豪迈的rdma建议
导出实时消息状态数据: sudo ceph daemon osd.0 perf dump AsyncMessenger::RDMAWorker-1
配置文件: ms_async_rdma_device_name = mlx5_0
查询gid; ibv_query_gid(ctxt, port_num, gid_idx, &gid)
roce: https://docs.nvidia.com/networking/pages/viewpage.action?pageId=12013422

支持共享接收队列: https://github.com/ssbandjl/ceph/commit/9fc9f08371d36d0cc38cbe8cbb235fa07ae0a6c0

为 beacon(灯塔) 保留额外的一个 WR，以指示所有 WCE 已被消耗
内存管理: memory_manager = new MemoryManager(cct, device, pd);
提升接收缓存区(内存管理)性能: https://github.com/ssbandjl/ceph/commit/720d044db13886ac9926d689e970381cdf78f8eb
注册内存: int Infiniband::MemoryManager::Cluster::fill(uint32_t num) -> malloc -> ibv_reg_mr
poll 处理接收事件: void RDMADispatcher::handle_rx_event(ibv_wc *cqe, int rx_number)
iwarp或ib(默认)
  if (cct->_conf->ms_async_rdma_type == "iwarp") {
    p = new RDMAIWARPConnectedSocketImpl(cct, ib, dispatcher, this);
  } else {
    p = new RDMAConnectedSocketImpl(cct, ib, dispatcher, this);
  }

连接后: worker->center.create_file_event(tcp_fd, EVENT_READABLE | EVENT_WRITABLE , established_handler)
发送cm元数据: int Infiniband::QueuePair::send_cm_meta

h3c tag: 2017/8/27, Aug 29, 2017, v12.2.0 https://github.com/ceph/ceph/commit/32ce2a3ae5239ee33d6150705cdb24d43bab910c
社区:
commit b661348f156f148d764b998b65b90451f096cb27 (tag: v12.1.2)
Author: Jenkins <jenkins@ceph.com>
Date:   Tue Aug 1 17:55:40 2017 +0000
12.1.2

rsync_to_h3c_win11(同步二进制到win10):
cd /c/Users/s30893/Downloads/ceph/ceph_perf_msgr
rsync -urpv root@ubuntu22:/root/project/stor/ceph/xb/docker/ceph/build/bin/ceph_perf_msgr_server .
rsync -urpv root@ubuntu22:/root/project/stor/ceph/xb/docker/ceph/build/bin/ceph_perf_msgr_client .
rsync -urpv root@ubuntu22:/root/project/stor/ceph/xb/docker/ceph/build/lib/libceph-common.so.2 .


编译ceph_msgr_perf工具:
cmake -DWITH_TESTS=1 ../CMakeList.txt
cd build
make common, ceph-common, ceph_perf_msgr_client, ceph_perf_msgr_server,  
make help 查看帮助

高级用法 
修改依赖的库
可以使用命令patchelf   修改工具依赖的动态库位置。避免和项目正在使用的库冲突，修改方法见：https://blog.csdn.net/bandaoyu/article/details/113181179

修改依赖的配置文件
修改依赖的配置文件，避免与正在运行的项目共用配置文件造成相互影响

ceph进程搜索配置文件的路径顺序

Ceph相关进程在读取配置时, 遵循以下的查找顺序

$CEPH_CONF 环境变量所指定的配置
-c path/path 参数所指定的配置
/etc/ceph/ceph.conf
~/.ceph/config (HOME目录下.ceph目录的config文件)
./ceph.conf (当前目录下的ceph.conf)

git:
git diff v12.2.0 main -- src/msg > git_diff_v12_2_0_main_src_msg
git diff v12.2.0 v15.2.17 -- src/msg > git_diff_v12_2_0_15_2_17_src_msg
git diff v15.2.17 main -- src/msg > git_diff_v15_2_17__main_src_msg

sync.sh
hosts='c51 c52'
for host in $hosts;do
	echo -e  "\n\033[32m`date +'%Y/%m/%d %H:%M:%S'` send to $host\033[0m"
	scp libceph-common.so.2 root@${host}:/home/xb/project/stor/ceph/xb/docker/ceph/build/lib/libceph-common.so.2
	scp ceph_perf_msgr_server root@${host}:/home/xb/project/stor/ceph/xb/docker/ceph/build/bin/ceph_perf_msgr_server
	scp ceph_perf_msgr_client root@${host}:/home/xb/project/stor/ceph/xb/docker/ceph/build/bin/ceph_perf_msgr_client
done

set_nonce: 设置随机值


Infiniband::Infiniband
verify_prereq

先设置环境变量, 后 ibv_fork_init() https://github.com/ceph/ceph/commit/b2d3f5e0970a4afbb82676af9e5b9a12f62a7747
ibv_fork_init will check environment variable RDMAV_HUGEPAGES_SAFE to decide whether huge page is usable in system
新版本增加判断:
if (ibv_is_fork_initialized() == IBV_FORK_UNNEEDED) {
  lderr(cct) << __FFL__ << " no need ibv_fork_init" << dendl;
}

src/test/msgr/perf_msgr_server.cc
server, 服务端, 建连接
MessengerServer
Messenger *Messenger::create type=async+rdma, lname=server -> Messenger *Messenger::create
  new AsyncMessenger -> AsyncMessenger::AsyncMessenger
    dispatch_queue
      DispatchQueue(CephContext *cct, Messenger *msgr, string &name) 本地快速分发, 节流阀
    lookup_or_create_singleton_object<StackSingleton>
    single->ready(transport_type)
       NetworkStack::create(cct, type) -> std::make_shared<RDMAStack>(c, t)
        RDMAStack::RDMAStack
          NetworkStack(cct, t) 构造, 线程池默认3个线程,
            create_worker(cct, type, worker_id) -> NetworkStack::create_worker -> new RDMAWorker(c, worker_id) -> RDMAWorker::RDMAWorker
              Worker(CephContext *c, unsigned worker_id) Stack是一个网络IO框架，封装了所有必要的基础网络接口，然后它管理线程工作。posix、dpdk甚至RDMA等不同的网络后端都需要继承Stack类来实现必要的接口。 所以这会让其他人轻松网络后端集成到 ceph 中。 否则，每个后端都需要实现整个 Messenger 逻辑，如重新连接、策略处理、会话维持...
            w->center.init -> EventCenter::init
              driver = new EpollDriver(cct)
              driver->init(this, nevent) -> int EpollDriver::init
                events = (struct epoll_event*)calloc
                epfd = epoll_create(1024)
                fcntl(epfd, F_SETFD, FD_CLOEXEC)
              file_events.resize(nevent) 5000
              pipe_cloexec(fds, 0) -> pipe2(pipefd, O_CLOEXEC | flags) 创建管道,均为非阻塞
              notify_receive_fd = fds[0] 接收端,读端
              notify_send_fd = fds[1] 发送端,写端
            workers.push_back(w)
          Infiniband::Infiniband
            device_name(cct->_conf->ms_async_rdma_device_name) 从配置中获取rdma设备名 TODO
            port_num( cct->_conf->ms_async_rdma_port_num) 默认为1 端口也从配置文件中获取
            verify_prereq -> void Infiniband::verify_prereq
              RDMAV_HUGEPAGES_SAFE 设置安全大页
              ibv_fork_init
              getrlimit(RLIMIT_MEMLOCK, &limit) 获取资源限制的配置
          get_num_worker 3
          for
            w->set_dispatcher(rdma_dispatcher)
            w->set_ib(ib)
    stack->start()
      std::function<void ()> thread = add_thread(i) 暂不执行
        w->center.set_owner()
          notify_handler = new C_handle_notify(this, cct)
          create_file_event(notify_receive_fd, EVENT_READABLE, notify_handler) 将之前管道的读端设置epoll监听
            driver->add_event(fd, event->mask, mask)
              epoll_ctl(epfd, op, fd, &ee)
            event->read_cb = ctxt 设置读事件回调
        w->initialize()
        w->init_done()
          init_cond.notify_all() 通知等待的线程,完成初始化
        while (!w->done)
          w->center.process_events 循环处理事件 -> int EventCenter::process_events
            driver->event_wait(fired_events, &tv) -> int EpollDriver::event_wait
              epoll_wait 写端写入c触发执行此处
              fired_events[event_id].fd = e->data.fd
            event = _get_file_event(fired_events[event_id].fd)
            cb = event->read_cb 可读回调
            cb->do_request(fired_events[event_id].fd) 处理事件
              r = read(fd_or_id, c, sizeof(c)) 读管道对端发来的字符,如:c
            cur_process.swap(external_events)
      spawn_worker(i, std::move(thread)) 启动新线程,返回join控制器
      workers[i]->wait_for_init() 等所有工人完成初始化
    local_connection = ceph::make_ref<AsyncConnection> -> AsyncConnection::AsyncConnection
      ms_connection_ready_timeout 建连超时时间
      ms_connection_idle_timeout 不活跃的时间, 如果两端连接空闲超过15分钟(没有活动的读写),则销毁连接
      read_handler = new C_handle_read(this) -> conn->process()
        void AsyncConnection::process()
      write_handler = new C_handle_write(this) -> conn->handle_write()
        void AsyncConnection::handle_write
      write_callback_handler = new C_handle_write_callback(this) -> AsyncConnection::handle_write_callback -> AsyncConnection::write 写的时候传递callback
      wakeup_handler = new C_time_wakeup(this) -> void AsyncConnection::wakeup_from -> void AsyncConnection::process()
      tick_handler = new C_tick_wakeup(this)-> void AsyncConnection::tick 计时器()
        protocol->fault() 处理错误
    init_local_connection
      void ms_deliver_handle_fast_connect
    reap_handler = new C_handle_reap(this)
      void AsyncMessenger::reap_dead 收割死连接
    processors.push_back(new Processor(this, stack->get_worker(i), cct))
      Processor::Processor
        listen_handler(new C_processor_accept(this))
          void Processor::accept() 等待事件触发(客户端执行connect后触发)
            listen_sockets -> while (true)
              msgr->get_stack()->get_worker()
              listen_socket.accept(&cli_socket, opts, &addr, w)
              msgr->add_accept
msgr->set_default_policy
dummy_auth.auth_registry.refresh_config()
msgr->set_auth_server(&dummy_auth) 初始化函数,在绑定前调用
server.start()
  msgr->bind(addr)
    AsyncMessenger::bind
      bindv -> int r = p->bind
      int Processor::bind
        listen_sockets.resize
        conf->ms_bind_retry_count 3次重试
        worker->center.submit_to lambda []()->void 匿名函数
          c->in_thread()
            pthread_equal(pthread_self(), owner) 本线程
          C_submit_event<func> event(std::move(f), false) f=listen
            void do_request -> f() -> listen -> worker->listen(listen_addr, k, opts, &listen_sockets[k]) -> int RDMAWorker::listen 由事件触发执行
              ib->init() -> void Infiniband::init
                new DeviceList(cct)
                  ibv_get_device_list 4网口
                  if (cct->_conf->ms_async_rdma_cm)
                  new Device(cct, device_list[i]) -> Device::Device
                    ibv_open_device
                    ibv_get_device_name
                    ibv_query_device 参考设备属性: device_attr
                get_device 根据配置的设备名在设备列表中查询, 默认取第一个, 如: mlx5_0
                binding_port -> void Device::binding_port
                  new Port(cct, ctxt, port_id) 端口ID从1开始 -> Port::Port
                    ibv_query_port(ctxt, port_num, &port_attr)
                    ibv_query_gid(ctxt, port_num, gid_idx, &gid)
                    ib_physical_port = device->active_port->get_port_num() 获取物理端口
                new ProtectionDomain(cct, device) -> Infiniband::ProtectionDomain::ProtectionDomain -> ibv_alloc_pd(device->ctxt)
                support_srq = cct->_conf->ms_async_rdma_support_srq 共享接收队列srq
                rx_queue_len = device->device_attr.max_srq_wr 最终为4096
                tx_queue_len = device->device_attr.max_qp_wr - 1 发送队列为beacon保留1个WR, 如:1024 1_K 重载操作符
                device->device_attr.max_cqe 设备允许 4194303 完成事件
                memory_manager = new MemoryManager(cct, device, pd) -> Infiniband::MemoryManager::MemoryManager 128K -> mem_pool ->  boost::pool
                memory_manager->create_tx_pool(cct->_conf->ms_async_rdma_buffer_size, tx_queue_len) -> void Infiniband::MemoryManager::create_tx_pool
                  send = new Cluster(*this, size)
                  send->fill(tx_num) -> int Infiniband::MemoryManager::Cluster::fill
                    base = (char*)manager.malloc(bytes) -> void* Infiniband::MemoryManager::malloc -> std::malloc(size) 标准分配或分配大页(huge_pages_malloc)
                    ibv_reg_mr 注册内存
                    new(chunk) Chunk
                    free_chunks.push_back(chunk)
                create_shared_receive_queue
                  ibv_create_srq
                post_chunks_to_rq -> int Infiniband::post_chunks_to_rq
                  chunk = get_memory_manager()->get_rx_buffer() -> return reinterpret_cast<Chunk *>(rxbuf_pool.malloc())
                  ibv_post_srq_recv
              dispatcher->polling_start() -> void RDMADispatcher::polling_start
                ib->get_memory_manager()->set_rx_stat_logger(perf_logger) -> void PerfCounters::set
                tx_cc = ib->create_comp_channel(cct) -> Infiniband::CompletionChannel* Infiniband::create_comp_channel -> new Infiniband::CompletionChannel
                tx_cq = ib->create_comp_queue(cct, tx_cc)
                  cq->init() -> int Infiniband::CompletionChannel::init
                    ibv_create_comp_channel 创建完成通道 -> NetHandler(cct).set_nonblock(channel->fd) 设置非阻塞
                t = std::thread(&RDMADispatcher::polling, this) 启动polling线程 rdma-polling -> void RDMADispatcher::polling
                  tx_cq->poll_cq(MAX_COMPLETIONS, wc)
                  handle_tx_event -> tx_chunks.push_back(chunk) -> post_tx_buffer
                    tx -> void RDMAWorker::handle_pending_message()
                  handle_rx_event -> void RDMADispatcher::handle_rx_event
                    conn->post_chunks_to_rq(1) 向接收队列补一个内存块(WR) -> int Infiniband::post_chunks_to_rq
                      ibv_post_srq_recv | ibv_post_recv
                    polled[conn].push_back(*response)
                    qp->remove_rq_wr(chunk)
                    chunk->clear_qp()
                    pass_wc -> void RDMAConnectedSocketImpl::pass_wc(std::vector<ibv_wc> &&v) ->  notify() -> void RDMAConnectedSocketImpl::notify
                      eventfd_write(notify_fd, event_val) -> eventfd_read(notify_fd, &event_val) <- ssize_t RDMAConnectedSocketImpl::read <- process
              new RDMAServerSocketImpl(cct, ib, dispatcher, this, sa, addr_slot)
              int r = p->listen(sa, opt) -> int RDMAServerSocketImpl::listen
                server_setup_socket = net.create_socket(sa.get_family(), true) -> socket_cloexec
                net.set_nonblock
                net.set_socket_options
                ::bind(server_setup_socket, sa.get_sockaddr(), sa.get_sockaddr_len()) 系统调用
                ::listen backlog=512
              *sock = ServerSocket(std::unique_ptr<ServerSocketImpl>(p))
            cond.notify_all() -> 通知等待的线程
          dispatch_event_external -> void EventCenter::dispatch_event_external
            external_events.push_back(e)
            wakeup()
              write(notify_send_fd, &buf, sizeof(buf)) buf=c -> notify_receive_fd, 唤醒 epoll_wait
          event.wait()
  msgr->add_dispatcher_head(&dispatcher)
    ready()
      p->start() -> void Processor::start()
        worker->center.create_file_event listen_handler -> pro->accept() -> void Processor::accept()
  msgr->start() -> int AsyncMessenger::start()
  msgr->wait() -> void AsyncMessenger::wait()  
    

客户端建连接, src/test/msgr/perf_msgr_client.cc, gdb --args ceph_perf_msgr_client 175.16.53.62:10001 10 1 1 0 4096
perf_msgr_client.cc -> main
  MessengerClient client(public_msgr_type, args[0], think_time)
  client.ready
    Messenger *msgr = Messenger::create
    msgr->set_default_policy -> Policy(bool l, bool s ...
    msgr->start() -> int AsyncMessenger::start()
      if (!did_bind) 客户端不需要bind
      set_myaddrs(newaddrs) -> void Messenger::set_endpoint_addr
      _init_local_connection() -> void _init_local_connection()
        ms_deliver_handle_fast_connect(local_connection.get()) -> void ms_deliver_handle_fast_connect将新连接通知每个快速调度程序。 每当启动或重新连接新连接时调用此函数 fast_dispatchers为空?
    ConnectionRef conn = msgr->connect_to_osd(addrs) 连接到OSD -> ConnectionRef connect_to_osd -> ConnectionRef AsyncMessenger::connect_to
      AsyncConnectionRef conn = _lookup_conn(av) 先在连接池查找连接
      conn = create_connect(av, type, false) 没找到,新建连接 -> AsyncConnectionRef AsyncMessenger::create_connect
        Worker *w = stack->get_worker()
        auto conn = ceph::make_ref<AsyncConnection> -> AsyncConnection::AsyncConnection 构造连接
          recv_buf = new char[2*recv_max_prefetch] 使用缓冲区读取来避免小的读取开销
          new ProtocolV2(this) ceph v2协议, 在v1基础上支持地址向量, 在横幅(banner)交换之后，对等体交换他们的地址向量address vectors
        conn->connect(addrs, type, target) -> void AsyncConnection::connect -> _connect -> void AsyncConnection::_connect()
          state = STATE_CONNECTING 初始状态机
          protocol->connect() -> void ProtocolV2::connect() -> state = START_CONNECT
          center->dispatch_event_external(read_handler) -> 触发状态机推进 -> process
        conns[addrs] = conn 保存连接 -> ceph::unordered_map<entity_addrvec_t, AsyncConnectionRef> conns 无序map
    ClientThread *t = new ClientThread(msgr, c, conn, msg_len, ops, think_time_us) -> ClientThread(Messenger *m 新建客户端线程, 构造数据
      m->add_dispatcher_head(&dispatcher)
      bufferptr ptr(msg_len) 申请数据指针 -> buffer::ptr::ptr(unsigned l) : _off(0), _len(l)
         _raw = buffer::create(l).release() -> ceph::unique_leakable_ptr<buffer::raw> buffer::create, 通过返回其值并用空指针替换它来释放其存储指针的所有权。此调用不会破坏托管对象，但 unique_ptr 对象从删除对象的责任中解脱出来。 某些其他实体必须负责在某个时刻删除该对象。要强制销毁指向的对象，请使用成员函数 reset 或对其执行赋值操作
          buffer::create_aligned(len, sizeof(size_t)) -> ceph::unique_leakable_ptr<buffer::raw> buffer::create_aligned
            create_aligned_in_mempool -> mempool::mempool_buffer_anon 宏: f(buffer_anon) -> ceph::unique_leakable_ptr<buffer::raw> buffer::create_aligned_in_mempool, 1M: create_aligned_in_mempool (len=1048576, align=8, mempool=18)
              len >= CEPH_PAGE_SIZE * 2 如果待分配内存长度大于2倍CEPH_PAGE_SIZE(系统页:sysconf(_SC_PAGESIZE))=8K, 则用原生posix对齐分配 -> ceph::unique_leakable_ptr<buffer::raw>(new raw_posix_aligned(len, align)) ->  raw_posix_aligned(unsigned l, unsigned _align) : raw(l)
                r = ::posix_memalign((void**)(void*)&data, align, len);
              return raw_combined::create(len, align, mempool) -> src/common/buffer.cc -> static ceph::unique_leakable_ptr<buffer::raw> -> create(unsigned len,
                align = std::max<unsigned>(align, sizeof(void *)) = 8
                size_t rawlen = round_up_to(sizeof(buffer::raw_combined) 96
                size_t datalen = round_up_to(len, alignof(buffer::raw_combined)) 4096
                int r = ::posix_memalign((void**)(void*)&ptr, align, rawlen + datalen); 96+4096
                new (ptr + datalen) raw_combined(ptr, len, align, mempool))
         _raw->nref.store(1, std::memory_order_release)
      memset(ptr.c_str(), 0, msg_len) 置0
      data.append(ptr) 将data填充全0 -> void buffer::list::append -> void push_back(const ptr& bp)
        _buffers.push_back(*ptr_node::create(bp).release())
        _len += bp.length()
    msgrs.push_back(msgr)
    clients.push_back(t)
  Cycles::init() -> void Cycles::init() 校准时钟频率
  uint64_t start = Cycles::rdtsc()
  client.start() -> void start() -> clients[i]->create("client") -> void Thread::create
    pthread_create(&thread_id, thread_attr, _entry_func, (void*)this) -> void *Thread::_entry_func
    void *entry() override 重写entry
      hobject_t hobj(oid, oloc.key -> struct object_t
        void build_hash_cache() crc32c
      MOSDOp *m = new MOSDOp -> MOSDOp(int inc, long tid, 
      bufferlist msg_data(data) 拷贝构造函数?, 拷贝数据
      m->write(0, msg_len, msg_data) -> void write 通过消息msg写数据到对端, offset=0, len=4096, buffer_list=bl(msg_data)
        add_simple_op(CEPH_OSD_OP_WRITE, off, len) -> ops.push_back(osd_op)
           osd_op.op.extent.offset = off
           osd_op.op.extent.length = len
           ops.push_back(osd_op)
        data.claim(bl)
          clear()
          claim_append(bl) -> void buffer::list::claim_append 要求追加, 免拷贝?
            _buffers.splice_back(bl._buffers) 拼接回来
            bl._buffers.clear_and_dispose()
        header.data_off = off
      conn->send_message(m) -> void ProtocolV2::send_message(Message *m) ssize_t RDMAConnectedSocketImpl::send
        out_queue[m->get_priority()].emplace_back
        connection->center->dispatch_event_external(connection->write_handler) -> void AsyncConnection::handle_write
          const auto out_entry = _get_next_outgoing()
          more = !out_queue.empty() 如果发送队列不为空,则more为true,表示还有更多的待发送的数据
          write_message(out_entry.m, more)
            ssize_t total_send_size = connection->outgoing_bl.length() 4406=310+4096
            connection->_try_send(more) -> cs.send(outgoing_bl, more) -> ssize_t RDMAConnectedSocketImpl::send
              size_t bytes = bl.length() 4KB:4406B=4096+310/1MB:1048886=1048576+310
              pending_bl.claim_append(bl) 换变量, bl留着干啥? 回收?
              ssize_t r = submit(more) ssize_t -> RDMAConnectedSocketImpl::submit
                pending_bl.length() 4406
                auto it = std::cbegin(pending_bl.buffers()) cbegin()和cend()是C++11新增的，它们返回一个const的迭代器，不能用于修改元素, 常量迭代器
                while (it != pending_bl.buffers().end()) 循环, 切片, 分段
                if (ib->is_tx_buffer(it->raw_c_str())) 不进该分支
                msg/async/rdma：使用 shared_ptr 管理 Infiniband obj
                1.不要使用裸指针来管理Infiniband obj
                2.直接访问Infiniband obj而不是从RDMA堆栈。 这可以避免在 RDMAWorker 和 RDMADispatcher 中缓存 RDMAStack obj
                wait_copy_len += it->length() = 32
                tx_buffers.push_back(ib->get_tx_chunk_by_buffer(it->raw_c_str()))
                size_t copied = tx_copy_chunk(tx_buffers, wait_copy_len, copy_start, it);
                total_copied += tx_copy_chunk(tx_buffers, wait_copy_len, copy_start, it) -> size_t RDMAConnectedSocketImpl::tx_copy_chunk
                  int RDMAWorker::get_reged_mem -> 获取已注册的内存 int Infiniband::get_tx_buffers -> get_send_buffers -> Infiniband::MemoryManager::Cluster::get_buffers
                    size_t got = ib->get_memory_manager()->get_tx_buffer_size() * r  131072>4406 获取到的内存满足需求的大小, 1MB, 131072*9=1179648
                  auto chunk_idx = tx_buffers.size() 9个chunk
                  Chunk *current_chunk = tx_buffers[chunk_idx] 
                  size_t real_len = current_chunk->write((char*)addr + slice_write_len, start->length() - slice_write_len) -> uint32_t Infiniband::MemoryManager::Chunk::write
                    memcpy(buffer + offset, buf, write_len) 拷贝内存(循环拷贝)
                  write_len 4406
                pending_bl.clear() 拷贝完释放pb
                post_work_request(tx_buffers)
                  tx_buffers.size() = 1
                  while (current_buffer != tx_buffers.end())
                  ibv_post_send -> ibv_poll_cq 触发发端/收端 -> int Infiniband::CompletionQueue::poll_cq <- void RDMADispatcher::polling()
      msgr->shutdown()
  stop = Cycles::rdtsc()
...
NetworkStack::add_thread
  w->center.process_events -> C_handle_read -> conn->process() -> void AsyncConnection::process()
    worker->connect(target_addr, opts, &cs) -> int RDMAWorker::connect
      ib->init()
      dispatcher->polling_start()
      new RDMAConnectedSocketImpl -> RDMAConnectedSocketImpl::RDMAConnectedSocketImpl
        read_handler(new C_handle_connection_read(this))
        established_handler(new C_handle_connection_established(this))
      p->try_connect(addr, opts) -> int RDMAConnectedSocketImpl::try_connect
        tcp_fd = net.nonblock_connect(peer_addr, opts.connect_bind_addr) -> generic_connect -> int NetHandler::generic_connect
          create_socket
          ::connect(s, addr.get_sockaddr(), addr.get_sockaddr_len()) syscall 客户端连接服务端(socket) -> 服务端触发事件(C_processor_accept) -> void Processor::accept()
          worker->center.create_file_event(tcp_fd, EVENT_READABLE | EVENT_WRITABLE , established_handler) -> established_handler -> int RDMAConnectedSocketImpl::handle_connection_established

      *socket = ConnectedSocket(std::move(csi))
    center->create_file_event(cs.fd(), EVENT_READABLE, read_handler) -> state = STATE_CONNECTING_RE -> void AsyncConnection::process() (回到process)
    ...
    case STATE_CONNECTING_RE
      cs.is_connected()
      center->create_file_event EVENT_WRITABLE read_handler -> process
      logger->tinc -> void PerfCounters::tinc 性能统计(时延统计)
    ...
    protocol->read_event() -> START_ACCEPT -> run_continuation(CONTINUATION(start_server_banner_exchange))
      CONTINUATION_RUN(continuation)
      CtPtr ProtocolV2::read -> ssize_t AsyncConnection::read -> read_until
        read_bulk ->  nread = cs.read(buf, len) -> ssize_t RDMAConnectedSocketImpl::read -> eventfd_read(notify_fd, &event_val)
          read = read_buffers(buf,len) -> ssize_t RDMAConnectedSocketImpl::read_buffers
            buffer_prefetch() 预读 -> void RDMAConnectedSocketImpl::buffer_prefetch
              ibv_wc* response = &cqe[i]
              chunk->prepare_read(response->byte_len)
              buffers.push_back(chunk)
            tmp = (*pchunk)->read(buf + read_size, len - read_size) -> uint32_t Infiniband::MemoryManager::Chunk::read
              memcpy(buf, buffer + offset, read_len);
            (*pchunk)->reset_read_chunk() 将偏移和边界都置0
            dispatcher->post_chunk_to_pool(*pchunk) -> void RDMADispatcher::post_chunk_to_pool
              ib->post_chunk_to_pool(chunk)
            update_post_backlog -> void RDMAConnectedSocketImpl::update_post_backlog
            
超时处理: 
new C_handle_reap(this)
  local_worker->create_time_event( ReapDeadConnectionMaxPeriod...
    reap_dead


设备属性: device_attr
(gdb) p device_attr
$17 = {
  fw_ver = "16.33.1048", '\000' <repeats 53 times>, 
  node_guid = 8550064101420093112, 
  sys_image_guid = 8550064101420093112, 
  max_mr_size = 18446744073709551615, 
  page_size_cap = 18446744073709547520, 
  vendor_id = 713, 
  vendor_part_id = 4119, 
  hw_ver = 0, 
  max_qp = 131072, 
  max_qp_wr = 32768, 
---Type <return> to continue, or q <return> to quit---
  device_cap_flags = 3983678518, 
  max_sge = 30, 
  max_sge_rd = 30, 
  max_cq = 16777216, 
  max_cqe = 4194303, 
  max_mr = 16777216, 
  max_pd = 8388608, 
  max_qp_rd_atom = 16, 
  max_ee_rd_atom = 0, 
  max_res_rd_atom = 2097152, 
  max_qp_init_rd_atom = 16, 
---Type <return> to continue, or q <return> to quit---
  max_ee_init_rd_atom = 0, 
  atomic_cap = IBV_ATOMIC_HCA, 
  max_ee = 0, 
  max_rdd = 0, 
  max_mw = 16777216, 
  max_raw_ipv6_qp = 0, 
  max_raw_ethy_qp = 0, 
  max_mcast_grp = 2097152, 
  max_mcast_qp_attach = 240, 
  max_total_mcast_qp_attach = 503316480, 
  max_ah = 2147483647, 
---Type <return> to continue, or q <return> to quit---
  max_fmr = 0, 
  max_map_per_fmr = 0, 
  max_srq = 8388608, 
  max_srq_wr = 32767, 
  max_srq_sge = 31, 
  max_pkeys = 128, 
  local_ca_ack_delay = 16 '\020', 
  phys_port_cnt = 1 '\001'
}


qp析构(销毁qp): schedule_qp_destroy
msg/async/rdma：使用特殊的Beacon 检测SQ WRs drained 将QueuePair 切换到error 状态，然后post Beacon WR 发送队列。 所有未完成的 WQE 将被刷新到 CQ。 在 CQ 中，在销毁 QueuePair 之前，检查完成队列元素以检测 SQ WRs 是否已被耗尽。 如果不使用/不支持 SRQ，我们不会将另一个 Beacon WR 发布到 RQ，原因是只有在从 CQ 轮询了所有刷新的 WR 后，才能销毁 QueuePair。请参阅以下规范的第 474 页：InfiniBandTM 架构规范第 1 卷，版本 1.3 规范链接：https://cw.infinibandta.org/document/dl/7859

安装rdma依赖, libibverbs, 
yum install -y libibverbs-devel librdmacm-devel

yum update
yum install epel-release
yum install boost boost-thread boost-devel

centos8: 
rpm -qa|grep libibverbs
libibverbs-35.0-1.el8.x86_64
编译rdma-core: https://runsisi.com/2021/03/07/rdma-core/

版本差异: https://github.com/ceph/ceph/compare/v12.2.0...v15.2.17

TODO:
1. 3.0增加 
modify_qp_to_error
modify_qp_to_rts
modify_qp_to_rtr
内存池: msg/async/rdma: improves RX buffer management: https://github.com/ssbandjl/ceph/commit/720d044db13886ac9926d689e970381cdf78f8eb
共享接收队列 srq: https://github.com/ssbandjl/ceph/commit/282499b77f85fed50ce00c5414af12335371a4b3
修复错误事件中心被 rdma 构造连接传输 ib sync msg C_handle_connection_established: https://github.com/ssbandjl/ceph/commit/8b2a95011ca34ba3880440339693170a174034ab
schedule_qp_destroy: https://github.com/ssbandjl/ceph/commit/e907e18154421885f1b02518496694b0987ab9f9
buffer_prefetch 预读: https://github.com/ssbandjl/ceph/commit/2754d60f6615024c76f09d22d2480a9b69369a12
msg/async/rdma: deal with all RDMA device async event 补全事件处理: https://github.com/ssbandjl/ceph/commit/1c76c1320721cc555a376d7b8660c19538d3f1b4
1.列出RDMA设备的所有异步事件
2.输出致命错误事件以检查RDMA设备状态
加锁获取qp: https://github.com/ssbandjl/ceph/commit/cc08b02046ce1243926c2d716281566bd0a70402
加速 tx handle 以前 Dispatcher 线程将轮询 rx 和 tx 事件，然后调度, 这些事件传递给 RDMAWorker 和 RDMAConnectedSocketImpl: https://github.com/ssbandjl/ceph/commit/bc580b0a6100637ecbfeeecefc84e2b81ff25c34

补充rdma主要改动: 
https://ceph.io/en/news/blog/2019/v14-2-0-nautilus-released/
https://ceph.io/en/news/blog/2020/v15-2-0-octopus-released/
配置gid: https://github.com/ceph/ceph/pull/31517/files
修复内存泄漏: rdma_free_devices https://github.com/ceph/ceph/pull/27574/files
代码优化: 将连接管理数据（LID、GID、QPN、PSN）从 RDMAConnectedSocketImpl 移动到 QueuePair。 目标是 1) 简化 switch QueuePair 状态 2) 简化管理连接管理数据将 QP 切换到 Error 状态以将未完成的 WR 刷新到 CQ 并使用 Beacon WR 检测 SQD 使没有SRQ的RNIC在msg/async/rdma中工作 根据2&3中的变化细化handle_rx/tx_event&handle_async_event 根据其父类简化 RDMAIWARPConnectedSocketImpl, https://github.com/ceph/ceph/pull/29947

v16.2.0 Pacific released 太平洋
centos8: https://ceph.io/en/news/blog/2021/v16-2-0-pacific-released/

AMQP（Advanced Message Queuing Protocol，高级消息队列协议）是一个进程间传递异步消息的网络协议



client 写流程:
conn->send_message(*p)


减少状态机:
 private:
  enum {
    STATE_NONE,
    STATE_CONNECTING,
    STATE_CONNECTING_RE,
    STATE_ACCEPTING,
    STATE_CONNECTION_ESTABLISHED,
    STATE_CLOSED
  };

bench-write
gdb --args rbd bench-write .d2.rbd/500G3 --io-size 4M --io-pattern rand --io-threads 1 --io-total 100M
/home/xb/project/stor/ceph/xb/docker/ceph/src/tools/rbd/rbd.cc main
  do_bench -> start_io aio_write2 ictx->io_work_queue->aio_write 

客户端写, osd写, 参考
https://my.oschina.net/u/2460844/blog/534390
src/osdc/Objecter.cc
void Objecter::_op_submit
  check_for_latest_map = _calc_target(&op->target, nullptr) -> int Objecter::_calc_target

  _get_session(op->target.osd, &s, sul) -> int Objecter::_get_session
    osd_sessions.find(osd)
    OSDSession *s = new OSDSession(cct, osd) 没找到, 准备新建
    s->con = messenger->connect_to_osd(osdmap->get_addrs(osd))
    logger->set(l_osdc_osd_sessions, osd_sessions.size()) 统计会话数量
  _session_op_assign(s, op)
  _send_op(op) -> void Objecter::_send_op(Op *op) -> op->session -> con->send_message(m)


osd落盘, 主副本和从副本同步: https://my.oschina.net/u/2460844/blog/534390
osd读数据
seastar::future<> ProtocolV2::read_message
...
bool OSD::ms_dispatch
void OSD::_dispatch
void OSD::dispatch_op -> void OSD::handle_pg_create
  osdmap->get_primary_shard(on, &pgid)

crimson是crimson-osd的代号，也就是下一代ceph-osd。 它通过利用 DPDK 和 SPDK 等最先进的技术，以快速网络设备、快速存储设备为目标，以获得更好的性能。 并且它将通过 BlueStore 保留对 HDD 和低端 SSD 的支持。 Crimson 将尝试向后兼容经典 OSD

crc问题:
handle_read_frame_dispatch
handle_message
Message *decode_message
  bad crc in front
filter: *.cc, *.h, src/msg, *.sh, *.rst

生成uudid: uuidgen
NetworkStack::add_thread(unsigned int)::{lambda()#1}::operator()() const ()
  EventCenter::process_events(int)
    AsyncConnection::process
      decode_message(CephContext*, int, ceph_msg_header&, ceph_msg_footer&, ceph::buffer::list&, ceph::buffer::list&, ceph::buffer::list&, Connection*)
        bad crc in front

测试:
ceph_test_msgr src/test/test_msgr.cc main


前端:
发现目标
iscsiadm --mode discovery --op update --type sendtargets --portal targetIP
iscsiadm -m discovery -t sendtargets -p 175.19.53.72

创建需要的设备:
iscsiadm --mode node -l all

查看所有活动的会话
iscsiadm --mode session

目标:
查看模块(rbd,bs_rbd.so的动态链接库):
tgtadm --lld iscsi --mode system --op show|grep rbd

创建镜像: rbd create --size {megabytes} {pool-name}/{image-name}
rbd create iscsi-image --size 4096 #4096M=4G
rbd create .disk_pool1.rbd/image_100g --size 102400 && rbd ls -p .disk_pool1.rbd  #查池: rados lspools

查镜像:
rbd ls -p p0 -l --format json --pretty-format

创建tgt:
tgtadm --lld iscsi --mode target --op new --tid 1 --targetname iqn.2013-15.com.example:cephtgt.target0
查看:
tgtadm --lld iscsi --op show --mode target
tgtadm --lld iscsi --op show --mode target --tid 512

创建lun:
tgtadm --lld iscsi --mode logicalunit --op new --tid 1 --lun 1 --backing-store iscsi-image --bstype rbd
tgtadm --lld iscsi --op bind --mode target --tid 1 -I ALL

查看rbd块设备
rbd ls {poolname}

查池:
rados lspools
ceph osd lspools
ceph osd pool ls
ceph osd pool ls detail
ceph osd dump|grep pool
rados df
ceph osd pool get {pool-name} {key}
ceph df


创池, 创建池, io, pool: https://docs.ceph.com/en/reef/rados/operations/pools/
ceph osd pool create {pool-name} [{pg-num} [{pgp-num}]] [replicated] [crush-rule-name] [expected-num-objects]
ceph osd pool create {pool-name} [{pg-num} [{pgp-num}]] erasure [erasure-code-profile] [crush-rule-name] [expected_num_objects] [--autoscale-mode=<on,off,warn>]

ceph osd pool create p1 3 && ceph osd lspools && rados bench -p p1 5 write
ceph osd pool stats p1


rbd块设备: https://docs.ceph.com/en/quincy/rbd/rados-rbd-cmds/
该rbd命令使您能够创建、列出、内省和删除块设备映像。您还可以使用它来克隆镜像、创建快照、将镜像回滚到快照、查看快照等。有关使用该命令的详细信息，rbd请参阅RBD – 管理 RADOS 块设备 (RBD) 镜像: https://docs.ceph.com/en/quincy/man/8/rbd/
rbd 是一个用于操作 rados 块设备 (RBD) 映像的实用程序，由 Linux rbd 驱动程序和 QEMU/KVM 的 rbd 存储驱动程序使用。 RBD image是简单的块设备，在对象上进行条带化并存储在 RADOS 对象存储中。 image条带对象的大小必须是 2 的幂

创建卷/image
rbd create --size 1024 p1/image1 && rbd ls p1 && rbd info p1/image1


下发io:
rados bench -p .disk_pool1.rbd 5 write

rados put test-object-1 ../src/vstart.sh --pool=test-blkin
rados -p test-blkin ls
ceph osd map test-blkin test-object-1
rados get test-object-1 ./vstart-copy.sh --pool=test-blkin
md5sum vstart*
rados rm test-object-1 --pool=test-blkin

存储池启用rbd:
ceph osd pool application enable wgsrbd rbd

初始化rbd:
rbd pool init -p wgsrbd
下一步就可创建镜像

客户端映射镜像:
rbd -p wgsrbd map wgs-img1

编译rpm包:
安装工具: yum install rpm-build rpmdevtools -y
make-dist, 该脚本主要用于生成ceph.spec和ceph-*.tar.bz2文件以供后面打rpm包使用。我们主要改变了rpm_version,rpm_release ，是否下载boost库等
bash make-srpm.sh build
bash make-srpm.sh clean

tar --strip-components=1 -C ~/rpmbuild/SPECS/ --no-anchored -xvjf ~/rpmbuild/SOURCES/ceph-10.2.11.tar.bz2 "ceph.spec"
生成目录:
rpmdev-setuptree
ls -alh /root/rpmbuild
tree rpmbuild/SRPMS/
rpmbuild -ba ~/rpmbuild/SPECS/ceph.spec
tree  rpmbuild/RPMS/
%debug_package
%prep

daos参考:
%if (0%{?suse_version} > 0)
%global __debug_package 1
%global _debuginfo_subpackages 0
%debug_package
%endif

%prep
%autosetup

默认目录: /root/rpmbuild
修改目录: –buildroot xxx
或 Specify the topdir parameter in the rpmrc file or rpmmacros file
rpmbuild --root /home/rpmbuild -ta driver.tar.gz

create a file called .rpmmacros in your %HOME% dir, add the following to the file "%_topdir x:/rpmbuild" (w/o the quotes)
.rpmmacros
%packager YourName
%_topdir /home/build/rpmbuild 
%_tmppath /home/build/rpmbuild/tmp

单元测试:
ctest -V -R client

rpm:
https://download.ceph.com/rpm-15.2.17/el7/x86_64/
https://mirrors.cloud.tencent.com/ceph/rpm-15.2.17/el7/SRPMS/ceph-15.2.17-0.el7.src.rpm

编译:
参考命令
安装gcc
git clone [https://github.com/gcc-mirror/gcc.git](https://github.com/gcc-mirror/gcc.git)
mkdir build ; cd build
../configure --prefix=/usr --disable-multilib 
yum install -y gmp-devel libmpc-devel mpfr-devel flex
make -j16 > log 2>&1 && make install >log 2>&1 &

编译前检查:
make check

安装依赖包:
yum -y install python36-Cython

编译, 统计cpu性能
./do-cmake.sh -DCMAKE_CXX_FLAGS="-fno-omit-frame-pointer -O2 -g"
cd build
cmake --build .

rbd接口使用:
examples/librbd/hello_world.cc



h3c:
Processor::accept


技术检查:
src/script/ceph-debug-docker.sh v15.2.17

cmake .. \
-DPYTHON_INCLUDE_DIR=$(python3 -c "import sysconfig; print(sysconfig.get_path('include'))")  \
-DPYTHON_LIBRARY=$(python3 -c "import sysconfig; print(sysconfig.get_config_var('LIBDIR'))")

message("python3头文件目录：" ${Python3_INCLUDE_DIRS})
message("python3的版本信息：" ${Python3_VERSION})
message("python3的库文件信息：" ${Python3_LIBRARIES})

file(
  DOWNLOAD
  "${url}" "/home/xb/project/stor/ceph/xb/docker/ceph/build/boost/src/boost_1_72_0.tar.bz2"

../src/vstart.sh -d -n -l -e -o "osd_tracing = true"


Timed out waiting for lttng-sessiond (in lttng_ust_init() at lttng-ust-comm.c:1444)

deps install
$SUDO $yumdnf install -y $yumdnf-utils

yum install lttng-ust-devel
yum install epel-release
rm -rf build;clear;./do_cmake.sh
分析代码技术错误,  analyze Teuthology failures
src/script/ceph-debug-docker.sh v15.2.17

虚拟集群:
cd build && OSD=3 MON=3 MGR=3 ../src/vstart.sh -n -x
# check that it's there
bin/ceph health

rm -rf build;clear;./do_cmake.sh && cd build && make -j64
rpm -ivh python3-devel-3.6.8-18.el7.x86_64.rpm
ceph.spec prometheus
ceph.spec.in
yum-builddep -y --setopt=*.skip_if_unavailable=true /tmp/install-deps.3292027/ceph.spec 2>&1 | tee /tmp/install-deps.3292027/yum-builddep.out



echo "182.200.53.62 c62" >> /etc/hosts


lttng:
yum install -y epel-release
yum install -y lttng-tools lttng-ust

qa:
1. 识别大IO, 大块IO(4K, 512K, 1M...)在哪里进行切分? 就在那里做RDMA单边逻辑
2. 

release cpp demo, release.cc, release.cpp
// unique_ptr::release example
#include <iostream>
#include <memory>

int main () {
  std::unique_ptr<int> auto_pointer (new int);
  int * manual_pointer;

  *auto_pointer=10;

  manual_pointer = auto_pointer.release();
  // (auto_pointer is now empty)

  std::cout << "manual_pointer points to " << *manual_pointer << '\n';

  delete manual_pointer;

  return 0;
}

ptr* _carriage(运输): rack bufferptr 我们可以修改（特别是 ::append() 到）。 并非所有 bptrs 缓冲区列表都具有此特性——如果有人 ::push_back(const ptr&)，他希望它不会改变

1MB, WR:
502       if (ibv_post_send(qp->get_qp(), iswr, &bad_tx_work_request)) {
(gdb) p iswr
$59 = {{
    wr_id = 93825015914456, 
    next = 0x0, 
    sg_list = 0x7fffe4af2be0, 
    num_sge = 1, 
    opcode = IBV_WR_SEND, 
    send_flags = 2, 
    {
      imm_data = 0, 
      invalidate_rkey = 0
    }, 
    wr = {
      rdma = {
        remote_addr = 0, 
        rkey = 0
      }, 
      atomic = {
        remote_addr = 0, 
        compare_add = 0, 
        swap = 0, 
        rkey = 0
      }, 
      ud = {
        ah = 0x0, 
        remote_qpn = 0, 
        remote_qkey = 0
      }
    }, 
    qp_type = {
      xrc = {
        remote_srqn = 0
      }
    }, 
    {
      bind_mw = {
        mw = 0x0, 
        rkey = 0, 
        bind_info = {
          mr = 0x0, 
          addr = 0, 
          length = 0, 
          mw_access_flags = 0
        }
      }, 
      tso = {
        hdr = 0x0, 
        hdr_sz = 0, 
        mss = 0
      }
    }
  }}
(gdb) 

(gdb) p iswr[0].sg_list
$79 = (ibv_sge *) 0x7fffe4af2be0
(gdb) p * (ibv_sge *) 0x7fffe4af2be0
$80 = {
  addr = 93825150820352, 
  length = 26, 
  lkey = 33802381
}
(gdb) 





qingyun, brpc,  ucx, libfabric(cart), ceph_rdma, 




rbd_write -> 
参考堆栈:
#0  Objecter::_op_submit (this=this@entry=0x9505c0, op=op@entry=0x997310, sul=..., ptid=ptid@entry=0x9971c0) at /block/gouxu/ceph-L/src/osdc/Objecter.cc:6185
#1  0x00007ffff74a5ed6 in Objecter::_op_submit_with_budget (this=this@entry=0x9505c0, op=op@entry=0x997310, sul=..., ptid=ptid@entry=0x9971c0, ctx_budget=ctx_budget@entry=0x0) at /block/gouxu/ceph-L/src/osdc/Objecter.cc:5878
#2  0x00007ffff74a6c19 in Objecter::op_submit (this=0x9505c0, op=0x997310, ptid=0x9971c0, ctx_budget=0x0) at /block/gouxu/ceph-L/src/osdc/Objecter.cc:5797
#3  0x00007ffff742c229 in librados::IoCtxImpl::aio_operate_read (this=0x98e440, oid=..., o=0x996de0, c=0x9970e0, flags=flags@entry=0, pbl=pbl@entry=0x984708, trace_info=trace_info@entry=0x0, vae_sparse_flag=vae_sparse_flag@entry=false)
    at /block/gouxu/ceph-L/src/librados/IoCtxImpl.cc:993
#4  0x00007ffff73ec78f in librados::IoCtx::aio_operate (this=0x994ed8, oid=..., c=c@entry=0x997290, o=o@entry=0x7fffffffd830, pbl=pbl@entry=0x984708) at /block/gouxu/ceph-L/src/librados/librados.cc:1611
#5  0x00007ffff79212ae in librbd::image::OpenRequest<librbd::ImageCtx>::send_v2_detect_header (this=this@entry=0x9846f0) at /block/gouxu/ceph-L/src/librbd/image/OpenRequest.cc:98
#6  0x00007ffff7921465 in librbd::image::OpenRequest<librbd::ImageCtx>::send (this=this@entry=0x9846f0) at /block/gouxu/ceph-L/src/librbd/image/OpenRequest.cc:44
#7  0x00007ffff787c20f in librbd::ImageState<librbd::ImageCtx>::send_open_unlock (this=0x984100) at /block/gouxu/ceph-L/src/librbd/ImageState.cc:626
#8  0x00007ffff788232f in librbd::ImageState<librbd::ImageCtx>::open (this=this@entry=0x984100, skip_open_parent=skip_open_parent@entry=false, on_finish=on_finish@entry=0x7fffffffdc80, 
    enSpliAgentInit=enSpliAgentInit@entry=librbd::EN_SPLIT_FLAG_TRUE) at /block/gouxu/ceph-L/src/librbd/ImageState.cc:284
#9  0x00007ffff78826a4 in librbd::ImageState<librbd::ImageCtx>::open (this=0x984100, skip_open_parent=skip_open_parent@entry=false, enSpliAgentInit=enSpliAgentInit@entry=librbd::EN_SPLIT_FLAG_TRUE)
    at /block/gouxu/ceph-L/src/librbd/ImageState.cc:252
#10 0x00007ffff78608d3 in rbd_open (p=<optimized out>, name=<optimized out>, image=0x7fffffffdf20, snap_name=<optimized out>) at /block/gouxu/ceph-L/src/librbd/librbd.cc:4604
#11 0x0000000000400cc7 in main (argc=1, argv=0x7fffffffe098) at rbd_write.c:120




---------------------------------------- DL ----------------------------------------
Ceph数据存储2-rbd client 端的数据请求处理 转载
Darren_Wen2020-05-15 18:14:10博主文章分类：ceph研发
文章标签rbd clientIO流程数据请求文章分类软件研发阅读数1507
本节讲述数据写操作的生命开始，介绍写操作的流程处理。代码参考0.94.x版本；
首先看一下我们用python调用librbd 写rbd设备的测试代码：
#!/usr/bin/env python
import sys,rados,rbd

def connectceph():
      cluster = rados.Rados(conffile = '/root/xuyanjiangtest/ceph-0.94.3/src/ceph.conf')
      cluster.connect()
      ioctx = cluster.open_ioctx('mypool')
      rbd_inst = rbd.RBD()
      size = 4*1024**3 #4 GiB
      rbd_inst.create(ioctx,'myimage',size)
      image = rbd.Image(ioctx,'myimage')
      data = 'foo'* 200
      image.write(data,0)
      image.close()
      ioctx.close()
      cluster.shutdown()
 
if __name__ == "__main__":
        connectceph()

一、写操作数据request的孕育过程
在write request 请求开始之前，它需要准备点旅行的用品，往返的机票等。下面先看看前期准备了什么。
首先cluster = rados.Rados(conffile = ‘XXXX/ceph.conf’)，用当前的这个ceph的配置文件去创建一个rados，这里主要是解析ceph.conf中写明的参数。然后将这些参数的值保存在rados中。
cluster.connect() ，这里将会创建一个radosclient的结构，这里会把这个结构主要包含了几个功能模块：消息管理模块Messager，数据处理模块Objector，finisher线程模块。这些模块具体的工作后面讲述。
ioctx = cluster.open_ioctx(‘mypool’)，为一个名字叫做mypool的存储池创建一个ioctx ，ioctx中会指明radosclient与Objector模块，同时也会记录mypool的信息，包括pool的参数等。
rbd_inst.create(ioctx,‘myimage’,size) ，创建一个名字为myimage的rbd设备，之后就是将数据写入这个设备。
image = rbd.Image(ioctx,‘myimage’)，创建image结构，这里该结构将myimage与ioctx 联系起来，后面可以通过image结构直接找到ioctx。这里会将ioctx复制两份，分为为data_ioctx和md_ctx。见明知意，一个用来处理rbd的存储数据，一个用来处理rbd的管理数据。
通过上面的操作就会形成这样的结构(如下图)
Ceph数据存储2-rbd client 端的数据请求处理_IO流程
图1-1 request孕育阶段
过程描述，首先根据配置文件创建一个rados，接下来为这个rados创建一个radosclient，radosclient包含了3个主要模块(finisher,Messager，Objector)。再根据pool创建对应的ioctx，ioctx中能够找到radosclient。再对生成对应rbd的结构image，这个image中复制了两个ioctx，分别成为了md_ioctx与data_ioctx。这时完全可以根据image入口去查找到前期准备的其他数据结构。接下来的数据操作完全从image开始，也是rbd的具体实例。
二、request的出生和成长。
image.write(data,0)，通过image开始了一个写请求的生命的开始。这里指明了request的两个基本要素 buffer=data 和 offset=0。由这里开始进入了ceph的世界，也是c++的世界。
由image.write(data,0) 转化为librbd.cc 文件中的 Image::write 函数，来看看这个函数的主要实现
ssize_t Image::write(uint64_t ofs, size_t len, bufferlist& bl)
  {  
      //…………………
   ImageCtx *ictx = (ImageCtx *)ctx;
    int r = librbd::write(ictx, ofs, len, bl.c_str(), 0); -> int r = ictx->io_work_queue->write(ofs, len, bufferlist{bl}, 0)
    return r;     
  }

该函数中直接进行分发给了 librbd::write 的函数了。跟随下来看看 librbd::write 中的实现。该函数的具体实现在internal.cc文件中。
ssize_t write(ImageCtx *ictx, uint64_t off, size_t len, const char *buf, int op_flags)
 {
      ……………
    Context *ctx = new C_SafeCond(&mylock, &cond, &done, &ret);   //---a
    AioCompletion *c = aio_create_completion_internal(ctx, rbd_ctx_cb);//---b
   r = aio_write(ictx, off, mylen, buf, c, op_flags);  //---c
     ……………     
    while (!done)
           cond.Wait(mylock);  // ---d
      ……………
}

—a.这句要为这个操作申请一个回调操作，所谓的回调就是一些收尾的工作，信号唤醒处理。
—b。这句是要申请一个io完成时 要进行的操作，当io完成时，会调用rbd_ctx_cb函数，该函数会继续调用ctx->complete（）。
—c.该函数aio_write会继续处理这个请求。
—d.当c句将这个io下发到osd的时候，osd还没请求处理完成，则等待在d上，直到底层处理完请求，回调b申请的 AioCompletion, 继续调用a中的ctx->complete（），唤醒这里的等待信号，然后程序继续向下执行。
3.再来看看aio_write 拿到了 请求的offset和buffer会做点什么呢？
int aio_write(ImageCtx *ictx, uint64_t off, size_t len, const char *buf,
           AioCompletion *c, int op_flags)
  {
      ………
      //将请求按着object进行拆分
      vector<ObjectExtent> extents;
     if (len > 0) 
      {
         Striper::file_to_extents(ictx->cct, ictx->format_string,
                        &ictx->layout, off, clip_len, 0, extents);   //---a
      } 
      //处理每一个object上的请求数据
      for (vector<ObjectExtent>::iterator p = extents.begin(); p != extents.end(); ++p) 
      {
               ……..
           C_AioWrite *req_comp = new C_AioWrite(cct, c); //---b
           ……..
           AioWrite *req = new AioWrite(ictx, p->oid.name, p->objectno, p- >offset,bl,….., req_comp);     //---c
           r = req->send();    //---d
           …….
      }
      ……
}

根据请求的大小需要将这个请求按着object进行划分，由函数file_to_extents进行处理，处理完成后按着object进行保存在extents中。file_to_extents()存在很多同名函数注意区分。这些函数的主要内容做了一件事儿，那就对原始请求的拆分。
一个rbd设备是有很多的object组成，也就是将rbd设备进行切块，每一个块叫做object，每个object的大小默认为4M，也可以自己指定。file_to_extents函数将这个大的请求分别映射到object上去，拆成了很多小的请求如下图。最后映射的结果保存在ObjectExtent中。
Ceph数据存储2-rbd client 端的数据请求处理_rbd client_02
原本的offset是指在rbd内的偏移量(写入rbd的位置)，经过file_to_extents后，转化成了一个或者多个object的内部的偏移量offset0。这样转化后处理一批这个object内的请求。
4.再回到 aio_write函数中，需要将拆分后的每一个object请求进行处理。
—b.为写请求申请一个回调处理函数。
—c.根据object内部的请求，创建一个叫做AioWrite的结构。
—d.将这个AioWrite的req进行下发send().
5.这里AioWrite 是继承自 AbstractWrite ，AbstractWrite 继承自AioRequest类，在AbstractWrite 类中定义了send的方法，看下send的具体内容.
int AbstractWrite::send() 
 {  ………………
    if (send_pre())           //---a
      ……………
}
#进入send_pre()函数中
bool AbstractWrite::send_pre()
｛
      m_state = LIBRBD_AIO_WRITE_PRE;   // ----a
      FunctionContext *ctx =    //----b
           new FunctionContext( boost::bind(&AioRequest::complete, this, _1));
      m_ictx->object_map.aio_update（ctx）; //-----c
｝

—a.修改m_state 状态为LIBRBD_AIO_WRITE_PRE。
—b.申请一个回调函数，实际调用AioRequest::complete()
—c.开始下发object_map.aio_update的请求，这是一个状态更新的函数，不是很重要的环节，这里不再多说，当更新的请求完成时会自动回调到b申请的回调函数。
6.进入到AioRequest::complete() 函数中。
void AioRequest::complete(int r)
 {
    if (should_complete(r))   //---a
        …….
}

—a.should_complete函数是一个纯虚函数，需要在继承类AbstractWrite中实现，来7. 看看AbstractWrite:: should_complete()
bool AbstractWrite::should_complete(int r)
{
    switch (m_state) 
  {
          case LIBRBD_AIO_WRITE_PRE:  //----a
      {
                     send_write(); //----b

----a.在send_pre中已经设置m_state的状态为LIBRBD_AIO_WRITE_PRE，所以会走这个分支。
----b. send_write()函数中，会继续进行处理，
7.1.下面来看这个send_write函数
void AbstractWrite::send_write()
｛
      m_state = LIBRBD_AIO_WRITE_FLAT;   //----a
      add_write_ops(&m_write);    // ----b
      int r = m_ictx->data_ctx.aio_operate(m_oid, rados_completion, &m_write);
｝

—a.重新设置m_state的状态为 LIBRBD_AIO_WRITE_FLAT。
—b.填充m_write，将请求转化为m_write。
—c.下发m_write ，使用data_ctx.aio_operate 函数处理。继续调用io_ctx_impl->aio_operate()函数，继续调用objecter->mutate().
//librados.cc
librados::IoCtx::aio_operate(const std::string& oid, AioCompletion *c,
				 librados::ObjectWriteOperation *o,
				 snap_t snap_seq, std::vector<snap_t>& snaps)
{
  return io_ctx_impl->aio_operate(obj, (::ObjectOperation*)o->impl, c->pc,
				  snapc, 0);
}
//IoCtxImpl.cc
librados::IoCtxImpl::aio_operate(......)
{
c->tid = objecter->mutate(oid, oloc, *o, snap_context, ut, flags, onack, oncommit,
		            &c->objver);
}

8.objecter->mutate()
ceph_tid_t mutate(……..) 
  {
    Op *o = prepare_mutate_op(oid, oloc, op, snapc, mtime, flags, onack, oncommit, objver);   //----d
    return op_submit(o);
  }

—d.将请求转化为Op请求，继续使用op_submit下发这个请求。在op_submit中继续调用_op_submit_with_budget处理请求。继续调用_op_submit处理。
8.1 _op_submit 的处理过程。这里值得细看
ceph_tid_t Objecter::_op_submit(Op *op, RWLock::Context& lc)
｛
    check_for_latest_map = _calc_target(&op->target, &op->last_force_resend)； //---a
    int r = _get_session(op->target.osd, &s, lc);  //---b
    _session_op_assign(s, op); //----c
    _send_op(op, m); //----d
｝

----a. _calc_target，通过计算当前object的保存的osd，然后将主osd保存在target中，rbd写数据都是先发送到主osd，主osd再将数据发送到其他的副本osd上。这里对于怎么来选取osd集合与主osd的关系就不再多说，在《ceph的数据存储之路(3)》中已经讲述这个过程的原理了，代码部分不难理解。
----b. _get_session，该函数是用来与主osd建立通信的，建立通信后，可以通过该通道发送给主osd。再来看看这个函数是怎么处理的
9._get_session
int Objecter::_get_session(int osd, OSDSession **session, RWLock::Context& lc)
{
    map<int,OSDSession*>::iterator p = osd_sessions.find(osd);   //----a
    OSDSession *s = new OSDSession(cct, osd); //----b
    osd_sessions[osd] = s;//--c
    s->con = messenger->get_connection(osdmap->get_inst(osd));//-d
    ………
｝

----a.首先在osd_sessions中查找是否已经存在一个连接可以直接使用，第一次通信是没有的。
----b.重新申请一个OSDSession，并且使用osd等信息进行初始化。
—c. 将新申请的OSDSession添加到osd_sessions中保存，以备下次使用。
----d.调用messager的get_connection方法。在该方法中继续想办法与目标osd建立连接。
10.messager 是由子类simpleMessager实现的，下面来看下SimpleMessager中get_connection的实现方法
ConnectionRef SimpleMessenger::get_connection(const entity_inst_t& dest)
{
    Pipe *pipe = _lookup_pipe(dest.addr);     //-----a
    if (pipe) 
    {
        ……
    } 
    else 
    {
      pipe = connect_rank(dest.addr, dest.name.type(), NULL, NULL); //----b
    }
｝

—a.首先要查找这个pipe，第一次通信，自然这个pipe是不存在的。
----b. connect_rank 会根据这个目标osd的addr进行创建。看下connect_rank做了什么。
11.SimpleMessenger::connect_rank
Pipe *SimpleMessenger::connect_rank(const entity_addr_t& addr,  int type, PipeConnection *con,    Message *first)
｛
      Pipe *pipe = new Pipe(this, Pipe::STATE_CONNECTING, static_cast<PipeConnection*>(con));      //----a
      pipe->set_peer_type(type); //----b
      pipe->set_peer_addr(addr); //----c
      pipe->policy = get_policy(type); //----d
      pipe->start_writer();  //----e
      return pipe; //----f
｝

----a.首先需要创建这个pipe，并且pipe同pipecon进行关联。
----b,----c,-----d。都是进行一些参数的设置。
----e.开始启动pipe的写线程，这里pipe的写线程的处理函数pipe->writer(),该函数中会尝试连接osd。并且建立socket连接通道。
目前的资源统计一下，写请求可以根据目标主osd，去查找或者建立一个OSDSession，这个OSDSession中会有一个管理数据通道的Pipe结构，然后这个结构中存在一个发送消息的处理线程writer，这个线程会保持与目标osd的socket通信。
12.建立并且获取到了这些资源，这时再回到_op_submit 函数中
ceph_tid_t Objecter::_op_submit(Op *op, RWLock::Context& lc)
｛
    check_for_latest_map = _calc_target(&op->target, &op->last_force_resend)； //---a
    int r = _get_session(op->target.osd, &s, lc);  //---b
    _session_op_assign(s, op); //----c
    MOSDOp *m = _prepare_osd_op(op); //-----d
    _send_op(op, m); //----e
｝

—c，将当前的op请求与这个session进行绑定，在后面发送请求的时候能知道使用哪一个session进行发送。
–d，将op转化为MOSDop，后面会以MOSDOp为对象进行处理的。
—e，_send_op 会根据之前建立的通信通道，将这个MOSDOp发送出去。_send_op 中调用op->session->con->send_message(m)，这个方法会调用SimpleMessager-> send_message(m), 再调用_send_message(),再调用submit_message().在submit_message会找到之前的pipe，然后调用pipe->send方法，最后通过pipe->writer的线程发送到目标osd。
自此，客户就等待osd处理完成返回结果了。
总结客户端的所有流程和数据结构，下面来看下客户端的所有结构图。
Ceph数据存储2-rbd client 端的数据请求处理_数据请求_03
通过这个全部的结构图来总结客户端的处理过程。
1.看左上角的rados结构，首先创建io环境，创建rados信息，将配置文件中的数据结构化到rados中。
2.根据rados创建一个radosclient的客户端结构，该结构包括了三个重要的模块，finiser 回调处理线程、Messager消息处理结构、Objector数据处理结构。最后的数据都是要封装成消息 通过Messager发送给目标的osd。
3.根据pool的信息与radosclient进行创建一个ioctx，这里面包好了pool相关的信息，然后获得这些信息后在数据处理时会用到。
4.紧接着会复制这个ioctx到imagectx中，变成data_ioctx与md_ioctx数据处理通道，最后将imagectx封装到image结构当中。之后所有的写操作都会通过这个image进行。顺着image的结构可以找到前面创建并且可以使用的数据结构。
5.通过最右上角的image进行读写操作，当读写操作的对象为image时，这个image会开始处理请求，然后这个请求经过处理拆分成object对象的请求。拆分后会交给objector进行处理查找目标osd，当然这里使用的就是crush算法，找到目标osd的集合与主osd。
6.将请求op封装成MOSDOp消息，然后交给SimpleMessager处理，SimpleMessager会尝试在已有的osd_session中查找，如果没有找到对应的session，则会重新创建一个OSDSession，并且为这个OSDSession创建一个数据通道pipe，把数据通道保存在SimpleMessager中，可以下次使用。
7.pipe 会与目标osd建立Socket通信通道，pipe会有专门的写线程writer来负责socket通信。在线程writer中会先连接目标ip，建立通信。消息从SimpleMessager收到后会保存到pipe的outq队列中，writer线程另外的一个用途就是监视这个outq队列，当队列中存在消息等待发送时，会就将消息写入socket，发送给目标OSD。
8.等待OSD将数据消息处理完成之后，就是进行回调，反馈执行结果，然后一步步的将结果告知调用者。
上面是就rbd client处理写请求的过程，那么下面会在分析一个OSD是如何接到请求，并且怎么来处理这个请求的。请期待下一节。
转载： https://my.oschina.net/u/2460844/blog/532755


---------------------------------------- DL ----------------------------------------



rbd_write.c:145 -> main -> rbd_write
  librbd::ImageCtx *ictx = (librbd::ImageCtx *)image
  bl.push_back(create_write_raw(ictx, buf, len, nullptr))
  ictx->io_work_queue->write(ofs, len, std::move(bl), 0)





sphinx-build
yum install python-sphinx


/opt/h3c/lib/librbd.so.1 -> librbd.so.1.12.0
[root@node1 ceph]# ls -alh /opt/h3c/lib/librados.so*
lrwxrwxrwx 1 root root   13 Jul 21 16:23 /opt/h3c/lib/librados.so -> librados.so.2
lrwxrwxrwx 1 root root   17 Jul 21 16:23 /opt/h3c/lib/librados.so.2 -> librados.so.2.0.0
-rwxr-xr-x 1 root root 2.4M Jul 21 16:23 /opt/h3c/lib/librados.so.2.0.0

[root@node1 ceph]# ls -alh /opt/h3c/lib/librbd*
lrwxrwxrwx 1 root root   11 Jul 21 16:23 /opt/h3c/lib/librbd.so -> librbd.so.1
lrwxrwxrwx 1 root root   16 Jul 21 16:23 /opt/h3c/lib/librbd.so.1 -> librbd.so.1.12.0
-rwxr-xr-x 1 root root 4.3M Jul 21 16:23 /opt/h3c/lib/librbd.so.1.12.0
lrwxrwxrwx 1 root root   14 Jul 21 16:23 /opt/h3c/lib/librbd_tp.so -> librbd_tp.so.1
lrwxrwxrwx 1 root root   18 Jul 21 16:23 /opt/h3c/lib/librbd_tp.so.1 -> librbd_tp.so.1.0.0
-rwxr-xr-x 1 root root 468K Jul 21 16:23 /opt/h3c/lib/librbd_tp.so.1.0.0



iopath
rados_write.c:83 -> main -> rados_write -> extern "C" int _rados_write
  rados_create2 -> extern "C" int _rados_create2
    CephInitParameters iparams(CEPH_ENTITY_TYPE_CLIENT) -> CephInitParameters::CephInitParameters -> name.set(module_type, "admin")
    rados_create_cct
    new librados::RadosClient
  rados_conf_read_file
  rados_connect -> extern "C" int _rados_connect(rados_t cluster)
    client->connect() -> int librados::RadosClient::connect()
      Messenger::create_client_messenger(cct, "radosclient")
      objecter = new (std::nothrow) Objecter(cct, messenger, &monclient, &finisher)
      objecter->set_balanced_budget()
      objecter->init()
      ...
  rados_ioctx_create -> extern "C" int _rados_ioctx_create
    create_ioctx -> int librados::RadosClient::create_ioctx(
      lookup_pool
      new librados::IoCtxImpl
  ctx->write(oid, bl, len, off) -> extern "C" int _rados_write -> ctx->write(oid, bl, len, off) -> int librados::IoCtxImpl::write
    prepare_assert_ops(&op)
    mybl.substr_of(bl, 0, len) -> 获取子字符串, 截取子串
    op.write(off, mybl) -> void write(uint64_t off, ceph::buffer::list& bl) -> write(off, bl, 0, 0)
      OSDOp& o = *ops.rbegin() -> 反向迭代
      add_data(CEPH_OSD_OP_WRITE, off, bl.length(), bl)
    return operate(oid, &op, NULL) -> int librados::IoCtxImpl::operate
      Context *oncommit = new C_SafeCond(mylock, cond, &done, &r)
      objecter->prepare_mutate_op -> Op *prepare_mutate_op
      objecter->op_submit(objecter_op) -> void Objecter::op_submit -> _op_submit_with_budget(op, rl, ptid, ctx_budget) -> void Objecter::_op_submit_with_budget
        int op_budget = _take_op_budget(op, sul) -> 流控 -> _op_submit(op, sul, ptid) -> void Objecter::_op_submit
          _calc_target -> target
          (gdb) p op->target
$9 = {
  flags = 32, 
  epoch = 0, 
  base_oid = {
    name = "neo-obj"
  }, 
  base_oloc = {
    pool = 1, 
    key = "", 
    nspace = "", 
    hash = -1
  }, 
  target_oid = {
    name = ""
  }, 
  target_oloc = {
    pool = -1, 
    key = "", 
    nspace = "", 
    hash = -1
  }, 
  precalc_pgid = false, 
  pool_ever_existed = false, 
  base_pgid = {
    m_pool = 0, 
    m_seed = 0, 
    static calc_name_buf_size = 36 '$'
  }, 
  pgid = {
    m_pool = 0, 
    m_seed = 0, 
    static calc_name_buf_size = 36 '$'
  }, 
  actual_pgid = {
    pgid = {
      m_pool = 0, 
      m_seed = 0, 
      static calc_name_buf_size = 36 '$'
    }, 
    shard = {
      id = -1 '\377', 
      static NO_SHARD = {
        id = -1 '\377', 
        static NO_SHARD = <same as static member of an already seen type>
      }
    }, 
    static calc_name_buf_size = 40 '('
  }, 
  pg_num = 0, 
  pg_num_mask = 0, 
  pg_num_pending = 0, 
  up = std::vector of length 0, capacity 0, 
  acting = std::vector of length 0, capacity 0, 
  up_primary = -1, 
  acting_primary = -1, 
  size = -1, 
  min_size = -1, 
  sort_bitwise = false, 
  recovery_deletes = false, 
  used_replica = false, 
  paused = false, 
  osd = -1, 
  last_force_resend = 0
}
...
  get_epoch
  get_pg_pool
  if ((t->flags & CEPH_OSD_FLAG_IGNORE_OVERLAY) == 0) -> osdc/Objecter：在每次 _calc_target 调用时重新计算 target_*，任何时候我们被要求计算目标时，我们都应该应用池分层参数。 之前仅在未计算目标时才这样做的逻辑没有多大意义，并且破坏了我们为获取目标池的正确 pg_num 所需的 *pi 更新。 对于采用原始 pg 的旧集群来说，这并不重要，但对于 luminous 及其他集群，我们需要精确的 spg_t，这需要正确的 pg_num
  pi = osdmap->get_pg_pool(t->target_oloc.pool)
  int ret = osdmap->object_locator_to_pg(t->target_oid, t->target_oloc
  ceph_stable_mod
  lookup_pg_mapping
  osdmap->pg_to_up_acting_osds
  update_pg_mapping(actual_pgid, std::move(pg_mapping))
_get_session(op->target.osd, &s, sul)




        



rbd_write.c:145 -> rbd_write
...
get_handler_way
start_in_flight_io
create_write_request
...




v12
https://github.com/ssbandjl/ceph_v12

v12.2.14, 73, 

sudo vi /etc/sysctl.conf
net.ipv6.conf.all.disable_ipv6 = 1 
net.ipv6.conf.default.disable_ipv6 = 1
net.ipv6.conf.lo.disable_ipv6 = 1
sudo sysctl -p

-i https://pypi.tuna.tsinghua.edu.cn/simple

set(VERSION 12.2.14)
更新yun源
wget https://copr.fedorainfracloud.org/coprs/jsynacek/systemd-backports-for-centos-7/repo/epel-7/jsynacek-systemd-backports-for-centos-7-epel-7.repo -O /etc/yum.repos.d/jsynacek-systemd-centos-7.repo
yum update systemd -y



https://boostorg.jfrog.io/artifactory/main/release/1.66.0/source/boost_1_66_0.tar.bz2

cp cache/boost_1_66_0.tar.bz2 /home/xb/project/stor/ceph/ceph/build/boost/src/boost_1_66_0.tar.bz2


WITH_TESTS
CMakeLists.txt
src/test/CMakeLists.txt

+ echo 'dashboard urls: http://182.200.53.73:41984/'
dashboard urls: http://182.200.53.73:41984/
+ echo '  restful urls: https://182.200.53.73:42984'
  restful urls: https://182.200.53.73:42984
+ echo '  w/ user/pass: admin / 978c009a-1726-45ef-aeac-5be2da32ae56'
  w/ user/pass: admin / 978c009a-1726-45ef-aeac-5be2da32ae56


echo -e "$CEPH_BUILD_ROOT vstart.sh:${LINENO}"

src/ceph_mgr.cc

export PATH=/home/xb/project/stor/ceph/ceph/build/bin:$PATH
export PATH=/home/xb/project/stor/ceph/xb/docker/ceph/build/bin:$PATH
运行 rados lspools | grep rgw
ceph osd pool application enable $POOL rgw

lttng-sessiond --daemoniz
lttng-sessiond -d --no-kernel
../src/vstart.sh -d -n -l -e -o "osd_tracing = true"
../src/vstart.sh -d -n -o "osd_tracing = true"
lttng list --userspace
rados bench -p ec 5 write

ps aux|grep lttng|grep -v grep|awk '{print$2}'|xargs kill -9


[global]
        osd_tracing = true
        bluestore_tracing = true
        event_tracing = true
        osd_function_tracing = true
        osd_objectstore_tracing = true
        rbd_tracing = true
        rados_tracing = true
        rgw_op_tracing = true
        rgw_rados_tracing = true


OSD=3 MON=3 RGW=1 ../src/vstart.sh -n -o "rbd_blkin_trace_all=true" \
  -o "osd_tracing = true" \
  -o "bluestore_tracing = true" \
  -o "event_tracing = true" \
  -o "osd_function_tracing = true" \
  -o "osd_objectstore_tracing = true" \
  -o "rbd_tracing = true" \
  -o "rgw_op_tracing = true" \
  -o "rgw_rados_tracing = true" --mgr_num 0

../stop.sh;OSD=3 MON=3 RGW=1 ../src/vstart.sh -n -X -o "rbd_blkin_trace_all=true" \
  -o "osd_tracing = true" \
  -o "bluestore_tracing = true" \
  -o "event_tracing = true" \
  -o "osd_function_tracing = false" \
  -o "osd_objectstore_tracing = true" \
  -o "rbd_tracing = true" \
  -o "rgw_op_tracing = true" \
  -o "rgw_rados_tracing = true"

clear;./stop.sh;../src/vstart.sh -X -n -o "rbd_blkin_trace_all=true"   -o "osd_tracing = true"   -o "bluestore_tracing = true"   -o "event_tracing = true"   -o "osd_function_tracing = true"   -o "osd_objectstore_tracing = true"   -o "rbd_tracing = true"   -o "rgw_op_tracing = true"   -o "rgw_rados_tracing = true" --mgr_num 0

ps aux|grep ceph|awk '{print$2}'|xargs -x kill
ps aux|grep ceph|awk '{print$2}'|xargs -x -I '{}' kill -9 {}
ps aux|grep ceph|awk '{print$2}'|while read pid;do echo $pid;kill -9 $pid;done;ps aux|grep ceph
systemctl stop dse@n73 ceph-mon@n73 ceph-mgr@n73

dashboard urls: http://182.200.53.73:41920/
  restful urls: https://182.200.53.73:42920
  w/ user/pass: admin / 


export PYTHONPATH=./pybind:/home/xb/project/stor/ceph/ceph/src/pybind:/home/xb/project/stor/ceph/ceph/build/lib/cython_modules/lib.2
export LD_LIBRARY_PATH=/home/xb/project/stor/ceph/ceph/build/lib

export PYTHONPATH=/usr/local/lib64/python3.6/site-packages/babeltrace:$PYTHONPATH

sudo vim /etc/ld.so.conf.d/sb.conf
  $ sudo ldconfig 
  $ cat /etc/ld.so.conf.d/sb.conf 
  /usr/local/lib

ceph config-key list
ceph mgr module ls
ceph mgr module disable dashboard
ceph auth list
ceph mgr module disable restful

ps aux |grep lttng-sessiond


PID: 1190156 - Name: /home/xb/project/stor/ceph/ceph/build/bin/ceph-osd
      ust_baddr_statedump:soinfo (loglevel: TRACE_DEBUG_LINE (13)) (type: tracepoint)
      pg:queue_op (loglevel: TRACE_DEBUG_LINE (13)) (type: tracepoint)
      osd:do_osd_op_post (loglevel: TRACE_DEBUG_LINE (13)) (type: tracepoint)
      osd:do_osd_op_pre_unknown (loglevel: TRACE_DEBUG_LINE (13)) (type: tracepoint)
      osd:do_osd_op_pre_copy_from (loglevel: TRACE_DEBUG_LINE (13)) (type: tracepoint)
      osd:do_osd_op_pre_copy_get (loglevel: TRACE_DEBUG_LINE (13)) (type: tracepoint)
      osd:do_osd_op_pre_copy_get_classic (loglevel: TRACE_DEBUG_LINE (13)) (type: tracepoint)
      osd:do_osd_op_pre_omaprmkeys (loglevel: TRACE_DEBUG_LINE (13)) (type: tracepoint)
      osd:do_osd_op_pre_omapclear (loglevel: TRACE_DEBUG_LINE (13)) (type: tracepoint)
      osd:do_osd_op_pre_omapsetheader (loglevel: TRACE_DEBUG_LINE (13)) (type: tracepoint)
      osd:do_osd_op_pre_omapsetvals (loglevel: TRACE_DEBUG_LINE (13)) (type: tracepoint)
      osd:do_osd_op_pre_omap_cmp (loglevel: TRACE_DEBUG_LINE (13)) (type: tracepoint)
      osd:do_osd_op_pre_omapgetvalsbykeys (loglevel: TRACE_DEBUG_LINE (13)) (type: tracepoint)
      osd:do_osd_op_pre_omapgetheader (loglevel: TRACE_DEBUG_LINE (13)) (type: tracepoint)
      osd:do_osd_op_pre_omapgetvals (loglevel: TRACE_DEBUG_LINE (13)) (type: tracepoint)
      osd:do_osd_op_pre_omapgetkeys (loglevel: TRACE_DEBUG_LINE (13)) (type: tracepoint)
      osd:do_osd_op_pre_tmap2omap (loglevel: TRACE_DEBUG_LINE (13)) (type: tracepoint)
      osd:do_osd_op_pre_tmapup (loglevel: TRACE_DEBUG_LINE (13)) (type: tracepoint)
      osd:do_osd_op_pre_tmapput (loglevel: TRACE_DEBUG_LINE (13)) (type: tracepoint)
      osd:do_osd_op_pre_tmapget (loglevel: TRACE_DEBUG_LINE (13)) (type: tracepoint)
      osd:do_osd_op_pre_startsync (loglevel: TRACE_DEBUG_LINE (13)) (type: tracepoint)
      osd:do_osd_op_pre_append (loglevel: TRACE_DEBUG_LINE (13)) (type: tracepoint)
      osd:do_osd_op_pre_rmxattr (loglevel: TRACE_DEBUG_LINE (13)) (type: tracepoint)
      osd:do_osd_op_pre_setxattr (loglevel: TRACE_DEBUG_LINE (13)) (type: tracepoint)
      osd:do_osd_op_pre_cache_unpin (loglevel: TRACE_DEBUG_LINE (13)) (type: tracepoint)
      osd:do_osd_op_pre_cache_pin (loglevel: TRACE_DEBUG_LINE (13)) (type: tracepoint)
      osd:do_osd_op_pre_watch (loglevel: TRACE_DEBUG_LINE (13)) (type: tracepoint)
      osd:do_osd_op_pre_clonerange (loglevel: TRACE_DEBUG_LINE (13)) (type: tracepoint)
      osd:do_osd_op_pre_delete (loglevel: TRACE_DEBUG_LINE (13)) (type: tracepoint)
      osd:do_osd_op_pre_truncate (loglevel: TRACE_DEBUG_LINE (13)) (type: tracepoint)
      osd:do_osd_op_pre_create (loglevel: TRACE_DEBUG_LINE (13)) (type: tracepoint)
      osd:do_osd_op_pre_zero (loglevel: TRACE_DEBUG_LINE (13)) (type: tracepoint)
      osd:do_osd_op_pre_rollback (loglevel: TRACE_DEBUG_LINE (13)) (type: tracepoint)
      osd:do_osd_op_pre_writesame (loglevel: TRACE_DEBUG_LINE (13)) (type: tracepoint)
      osd:do_osd_op_pre_writefull (loglevel: TRACE_DEBUG_LINE (13)) (type: tracepoint)
      osd:do_osd_op_pre_write (loglevel: TRACE_DEBUG_LINE (13)) (type: tracepoint)
      osd:do_osd_op_pre_setallochint (loglevel: TRACE_DEBUG_LINE (13)) (type: tracepoint)
      osd:do_osd_op_pre_notify_ack (loglevel: TRACE_DEBUG_LINE (13)) (type: tracepoint)
      osd:do_osd_op_pre_notify (loglevel: TRACE_DEBUG_LINE (13)) (type: tracepoint)
      osd:do_osd_op_pre_assert_src_version (loglevel: TRACE_DEBUG_LINE (13)) (type: tracepoint)
      osd:do_osd_op_pre_list_snaps (loglevel: TRACE_DEBUG_LINE (13)) (type: tracepoint)
      osd:do_osd_op_pre_list_watchers (loglevel: TRACE_DEBUG_LINE (13)) (type: tracepoint)
      osd:do_osd_op_pre_assert_ver (loglevel: TRACE_DEBUG_LINE (13)) (type: tracepoint)
      osd:do_osd_op_pre_cmpxattr (loglevel: TRACE_DEBUG_LINE (13)) (type: tracepoint)
      osd:do_osd_op_pre_getxattrs (loglevel: TRACE_DEBUG_LINE (13)) (type: tracepoint)
      osd:do_osd_op_pre_getxattr (loglevel: TRACE_DEBUG_LINE (13)) (type: tracepoint)
      osd:do_osd_op_pre_cache_evict (loglevel: TRACE_DEBUG_LINE (13)) (type: tracepoint)
      osd:do_osd_op_pre_cache_flush (loglevel: TRACE_DEBUG_LINE (13)) (type: tracepoint)
      osd:do_osd_op_pre_try_flush (loglevel: TRACE_DEBUG_LINE (13)) (type: tracepoint)
      osd:do_osd_op_pre_undirty (loglevel: TRACE_DEBUG_LINE (13)) (type: tracepoint)
      osd:do_osd_op_pre_isdirty (loglevel: TRACE_DEBUG_LINE (13)) (type: tracepoint)
      osd:do_osd_op_pre_stat (loglevel: TRACE_DEBUG_LINE (13)) (type: tracepoint)
      osd:do_osd_op_pre_call (loglevel: TRACE_DEBUG_LINE (13)) (type: tracepoint)
      osd:do_osd_op_pre_sparse_read (loglevel: TRACE_DEBUG_LINE (13)) (type: tracepoint)
      osd:do_osd_op_pre_mapext (loglevel: TRACE_DEBUG_LINE (13)) (type: tracepoint)
      osd:do_osd_op_pre_checksum (loglevel: TRACE_DEBUG_LINE (13)) (type: tracepoint)
      osd:do_osd_op_pre_read (loglevel: TRACE_DEBUG_LINE (13)) (type: tracepoint)
      osd:do_osd_op_pre_extent_cmp (loglevel: TRACE_DEBUG_LINE (13)) (type: tracepoint)
      osd:do_osd_op_pre (loglevel: TRACE_DEBUG_LINE (13)) (type: tracepoint)
      osd:opwq_process_finish (loglevel: TRACE_DEBUG_LINE (13)) (type: tracepoint)
      osd:opwq_process_start (loglevel: TRACE_DEBUG_LINE (13)) (type: tracepoint)
      osd:ms_fast_dispatch (loglevel: TRACE_DEBUG_LINE (13)) (type: tracepoint)
      osd:prepare_tx_exit (loglevel: TRACE_DEBUG_LINE (13)) (type: tracepoint)
      osd:prepare_tx_enter (loglevel: TRACE_DEBUG_LINE (13)) (type: tracepoint)
      oprequest:mark_flag_point (loglevel: TRACE_DEBUG_LINE (13)) (type: tracepoint)
      oprequest:set_rmw_flags (loglevel: TRACE_DEBUG_LINE (13)) (type: tracepoint)
      zipkin:timestamp (loglevel: TRACE_WARNING (4)) (type: tracepoint)
      zipkin:keyval_integer (loglevel: TRACE_WARNING (4)) (type: tracepoint)
      zipkin:keyval_string (loglevel: TRACE_WARNING (4)) (type: tracepoint)


../src/vstart.sh -help

src/tools/ceph_conf.cc


restart
./stop.sh;clear;sh -x ../src/vstart.sh -d -n -o "osd_tracing = true"

replace in dir
<< dendl;
<< __FFL__ << dendl;

std::string build_time = "2023/04/27 10:10:10";
priv_ss << "set uid:gid to " << uid << ":" << gid << " (" << uid_string << ":" << gid_string << ")" << "build_time:" << build_time;

tee .gitignore <<EOF
install-deps-python2.7_tmp
install-deps-python3_tmp
install-deps-python3_tmp_bak
<<EOF


2023-05-07 10:22:38.372811 7fffeb0d5700  1 mgr init Loading python module 'balancer'init PyModuleRegistry.cc:165
2023-05-07 10:22:38.534696 7fffeb0d5700  1 mgr init Loading python module 'dashboard'init PyModuleRegistry.cc:165
LTTng-UST: Error (-17) while registering tracepoint probe. Duplicate registration of tracepoint probes having the same name is not allowed.

Program received signal SIGABRT, Aborted.
[Switching to Thread 0x7fffeb0d5700 (LWP 2814992)]
0x00007ffff491a387 in raise () from /lib64/libc.so.6
Missing separate debuginfos, use: debuginfo-install glibc-2.17-326.el7_9.x86_64 gperftools-libs-2.6.1-1.el7.x86_64 keyutils-libs-1.5.8-3.el7.x86_64 krb5-libs-1.15.1-55.el7_9.x86_64 libblkid-2.29-2.el7.centos.x86_64 libcom_err-1.42.9-19.el7.x86_64 libgcc-4.8.5-44.el7.x86_64 libibverbs-52mlnx1-1.53100.x86_64 libnl3-3.2.28-4.el7.x86_64 libselinux-2.5-15.el7.x86_64 libstdc++-4.8.5-44.el7.x86_64 libuuid-2.29-2.el7.centos.x86_64 lttng-ust-2.4.1-4.el7.x86_64 nspr-4.34.0-3.1.el7_9.x86_64 nss-3.79.0-5.el7_9.x86_64 nss-softokn-3.79.0-4.el7_9.x86_64 nss-softokn-freebl-3.79.0-4.el7_9.x86_64 nss-util-3.79.0-1.el7_9.x86_64 openssl-libs-1.0.2k-26.el7_9.x86_64 pcre-8.32-17.el7.x86_64 python-libs-2.7.5-92.el7_9.x86_64 python-markupsafe-0.11-10.el7.x86_64 sqlite-3.35.1-hl1.el7.x86_64 userspace-rcu-0.7.16-1.el7.x86_64 zlib-1.2.7-21.el7_9.x86_64
(gdb) bt
#0  0x00007ffff491a387 in raise () from /lib64/libc.so.6
#1  0x00007ffff491ba78 in abort () from /lib64/libc.so.6
#2  0x00007fffd236ff6b in __lttng_events_init__zipkin () at /usr/include/lttng/ust-tracepoint-event.h:782
#3  0x00007ffff7dea9c3 in _dl_init_internal () from /lib64/ld-linux-x86-64.so.2
#4  0x00007ffff7def59e in dl_open_worker () from /lib64/ld-linux-x86-64.so.2
#5  0x00007ffff7dea7d4 in _dl_catch_error () from /lib64/ld-linux-x86-64.so.2
#6  0x00007ffff7deeb8b in _dl_open () from /lib64/ld-linux-x86-64.so.2
#7  0x00007ffff75c5fab in dlopen_doit () from /lib64/libdl.so.2
#8  0x00007ffff7dea7d4 in _dl_catch_error () from /lib64/ld-linux-x86-64.so.2
#9  0x00007ffff75c65ad in _dlerror_run () from /lib64/libdl.so.2
#10 0x00007ffff75c6041 in dlopen@@GLIBC_2.2.5 () from /lib64/libdl.so.2
#11 0x00007ffff7b20a4f in _PyImport_GetDynLoadFunc () from /lib64/libpython2.7.so.1.0
#12 0x00007ffff7b0934e in _PyImport_LoadDynamicModule () from /lib64/libpython2.7.so.1.0
#13 0x00007ffff7b07451 in import_submodule () from /lib64/libpython2.7.so.1.0
#14 0x00007ffff7b07736 in load_next () from /lib64/libpython2.7.so.1.0
#15 0x00007ffff7b0807e in PyImport_ImportModuleLevel () from /lib64/libpython2.7.so.1.0
#16 0x00007ffff7aeb32f in builtin___import__ () from /lib64/libpython2.7.so.1.0
#17 0x00007ffff7a5b073 in PyObject_Call () from /lib64/libpython2.7.so.1.0
#18 0x00007ffff7aecf07 in PyEval_CallObjectWithKeywords () from /lib64/libpython2.7.so.1.0
#19 0x00007ffff7af1bc5 in PyEval_EvalFrameEx () from /lib64/libpython2.7.so.1.0
#20 0x00007ffff7af664d in PyEval_EvalCodeEx () from /lib64/libpython2.7.so.1.0
#21 0x00007ffff7af6752 in PyEval_EvalCode () from /lib64/libpython2.7.so.1.0
#22 0x00007ffff7b0653c in PyImport_ExecCodeModuleEx () from /lib64/libpython2.7.so.1.0
#23 0x00007ffff7b067b8 in load_source_module () from /lib64/libpython2.7.so.1.0
#24 0x00007ffff7b07451 in import_submodule () from /lib64/libpython2.7.so.1.0
#25 0x00007ffff7b0769d in load_next () from /lib64/libpython2.7.so.1.0
#26 0x00007ffff7b0807e in PyImport_ImportModuleLevel () from /lib64/libpython2.7.so.1.0
#27 0x00007ffff7aeb32f in builtin___import__ () from /lib64/libpython2.7.so.1.0
#28 0x00007ffff7a5b073 in PyObject_Call () from /lib64/libpython2.7.so.1.0
#29 0x00007ffff7aecf07 in PyEval_CallObjectWithKeywords () from /lib64/libpython2.7.so.1.0
#30 0x00007ffff7af1bc5 in PyEval_EvalFrameEx () from /lib64/libpython2.7.so.1.0
#31 0x00007ffff7af664d in PyEval_EvalCodeEx () from /lib64/libpython2.7.so.1.0
#32 0x00007ffff7af6752 in PyEval_EvalCode () from /lib64/libpython2.7.so.1.0
#33 0x00007ffff7b0653c in PyImport_ExecCodeModuleEx () from /lib64/libpython2.7.so.1.0
#34 0x00007ffff7b067b8 in load_source_module () from /lib64/libpython2.7.so.1.0
#35 0x00007ffff7b07c4a in load_package () from /lib64/libpython2.7.so.1.0
#36 0x00007ffff7b07451 in import_submodule () from /lib64/libpython2.7.so.1.0
#37 0x00007ffff7b0769d in load_next () from /lib64/libpython2.7.so.1.0
#38 0x00007ffff7b0807e in PyImport_ImportModuleLevel () from /lib64/libpython2.7.so.1.0
#39 0x00007ffff7aeb32f in builtin___import__ () from /lib64/libpython2.7.so.1.0
#40 0x00007ffff7a5b073 in PyObject_Call () from /lib64/libpython2.7.so.1.0
#41 0x00007ffff7a5b155 in call_function_tail () from /lib64/libpython2.7.so.1.0
#42 0x00007ffff7a5b23e in PyObject_CallFunction () from /lib64/libpython2.7.so.1.0
#43 0x00007ffff7b08b22 in PyImport_Import () from /lib64/libpython2.7.so.1.0
#44 0x000055555584472d in PyModule::load (this=this@entry=0x55555ee079b0, pMainThreadState=<optimized out>) at /home/xb/project/stor/ceph/ceph/src/mgr/PyModuleRegistry.cc:261
#45 0x00005555558459d9 in PyModuleRegistry::init (this=this@entry=0x7fffffffe118, map=...) at /home/xb/project/stor/ceph/ceph/src/mgr/PyModuleRegistry.cc:167
#46 0x00005555558598ec in MgrStandby::handle_mgr_map (this=this@entry=0x7fffffffc320, mmap=mmap@entry=0x55555ebb5200) at /home/xb/project/stor/ceph/ceph/src/mgr/MgrStandby.cc:316
#47 0x000055555585aca0 in MgrStandby::ms_dispatch (this=0x7fffffffc320, m=0x55555ebb5200) at /home/xb/project/stor/ceph/ceph/src/mgr/MgrStandby.cc:376
#48 0x0000555555c7e982 in ms_deliver_dispatch (m=0x55555ebb5200, this=0x55555eb6d500) at /home/xb/project/stor/ceph/ceph/src/msg/Messenger.h:668
#49 DispatchQueue::entry (this=0x55555eb6d678) at /home/xb/project/stor/ceph/ceph/src/msg/DispatchQueue.cc:197
#50 0x0000555555a4736d in DispatchQueue::DispatchThread::entry (this=<optimized out>) at /home/xb/project/stor/ceph/ceph/src/msg/DispatchQueue.h:101
#51 0x00007ffff590eea5 in start_thread () from /lib64/libpthread.so.0
#52 0x00007ffff49e2b0d in clone () from /lib64/libc.so.6
(gdb) info locals
No symbol table info

gdb --args /home/xb/project/stor/ceph/ceph/build/bin/ceph-mgr -i x -c /home/xb/project/stor/ceph/ceph/build/ceph.conf -d
mgr_module_path

void MgrStandby::send_beacon() 发送信号
_list_modules
path, /home/xb/project/stor/ceph/ceph/src/pybind/mgr, 
mgr initial modules = restful status dashboard balancer

trace, 
FUNCTRACE(), src/msg/async/AsyncMessenger.cc -> #define FUNCTRACE() EventTrace _t1
event, https://github.com/ssbandjl/ceph_v12/commit/b3b20449dabbdfae6fb035d3e7efca52c21e9869

Tracing your own user application, https://lttng.org/docs/v2.5/#doc-viewing-and-analyzing-your-traces
跟踪就像在源代码的特定位置调用 printf()，尽管 LTTng 比 printf() 更快更灵活。 在 LTTng 领域，tracepoint() 类似于 printf()。
但是，与 printf() 不同的是，tracepoint() 不使用格式字符串来了解其参数的类型：所有跟踪点的格式必须在使用它们之前定义。 所以在编写我们的 Hello world 程序之前，我们需要定义跟踪点的格式。 这是通过编写一个模板文件来完成的，该文件的名称通常以 .tp 扩展名（用于跟踪点）结尾，lttng-gen-tp 工具（与 LTTng-UST 一起提供）将使用它来生成目标文件（以及 .c 文件）和要包含在我们的应用程序源代码中的标头
通过在您自己的应用程序中包含 hello-tp.h，您可以通过在调用 tracepoint() 时正确引用它来使用上面定义的跟踪点
tracepoint(hello_world, my_first_tracepoint, 23, "hi there!")
第一个：提供商名称（始终）
第二：跟踪点名称（始终）
第三个：my_integer_arg（第一个用户定义的参数）
第 4 个：my_string_arg（第二个用户定义的参数）
请注意，提供者和跟踪点名称不是字符串； 它们实际上是由 hello-tp.h 中的宏创建的变量的一部分。

color, "\033[32m`date +'%Y/%m/%d %H:%M:%S'` xxx \033[0m"


---------------------------------------- DL ----------------------------------------
在以上背景下，ceph 官方开发了 ceph-mgr，主要目标实现 ceph 集群的管理，为外界提供统一的入口。要深入了解 ceph-mgr，就得了解 ceph-mgr 是如何跑起来的。

由 官方文档 可知，ceph-mgr 是通过可执行文件 ceph-mgr 跑起来的，在源码src/CMakeLists.txt 搜索 ceph-mgr 可以搜到 add_executable(ceph-mgr ${mgr_srcs}...，从中可以看出 ceph-mgr 主要由 src/mgr 里的文件编译出来（猜也猜的出来），main 函数在 src/ceph_mgr.cc。以上就是相关文件，有需要深入的人可以去读，这里介绍整理之后的 ceph-mgr 工作原理。

ceph-mgr 工作的模式是事件驱动型的，意思就是等待事件，事件来了则处理事件返回结果，又继续等待。其主要运行的线程包括：

messenger 线程。这是事件驱动主线程，监听某一端口，由外界给输入事件，messenger 收到事件后分派给各个处理者。通过向 monitor 订阅某一个 topic 的消息，例如 mgrmap, osdmap，monitor 会在这些数据发生变化时把事件通知到 messenger 监听的端口。事件处理器包括：
MgrStandby。Mgr 通过 standby 实现高可用，每一个运行的 ceph-mgr 都包含一个 MgrStandby，MgrStandby 并没有运行的线程，它存在于 messenger 收到消息时的回调，以及通过定时器线程运行的定时任务，并且管理着其他实体。其处理的唯一消息是 mgrmap，就是当主挂掉时要顶上来，当自己不是主时要退回去。什么时候切主由 monitor 管理，所以 MgrStandby 里切主逻辑比较简单，有一个 Mgr 实例，当收到 mgrmap 时生成该实例，存到 MgrStandby 属性里，就完了。因为在收到消息时，MgrStandby 如果看到有 Mgr 实例，就会把消息发到它那处理，在定时函数里，也会调用 mgr 的定时函数，这样，实际上，MgrStandby 就担起了主的任务。
Mgr。如上段所述，Mgr 依附于 MgrStandby 存在，也没有单独线程。它通过处理 mon_map，fs_map，osd_map等事件，在内存中维护了集群成员信息，它管理 ceph-mgr 插件，为插件提供了所有数据的来源，也在特定事件发生时通知给 ceph-mgr 的插件，例如插件的 notify 函数，就是被 Mgr 回调的。
DaemonServer。独立线程，和主 messenger 监听同一端口(待确认)。是 cluster 指标数据的主要维护者，并且负载执行对集群的操作，例如吩咐 OSD 进行 pg scrub等。
plugin 线程。plugin 是 Python 写的，每个 plugin 都跑在单独线程里，线程调用的函数是 python 类的 serve。plugin 可以在 serve 里跑个 http server 来提供对外服务，ceph-mgr 为 plugin 提供了 get，get_server 等函数，这些函数返回关于集群的指标等数据。例如 prometheus 插件，就把 ceph 内部指标通过 http 协议以 prometheus 格式暴露出来，使得监控 ceph 集群变得较为简单。ceph 是 c++ 写的，ceph 会调用 python plugin 定义的方法（例如 serve），python plugin 可以调用 c++ 定义的函数（例如 get)，python/c++ 的互调是 python 提供的机制，其基本原理是：
c++ 调 python。python 的实体在 c++ 里类型都是 PyObject，模块，函数、类、数据都是。cpython 提供了 PyImport_Import 用于通过名字得到 m模块对象对应的 PyObject，类可以通过 PyObject_GetAttrString 取模块的属性得到，以此类推，cpython 还提供了由 c 类型的值生成对应 python 类型的值的PyObject 的方法，例如 PyObject* PyString_FromString(char *)。有函数对象，有参数对象，就可以通过 PyObject * PyObject_CallObject() 调用函数，将得到的 PyObject* 再转回 c++ 类型就 OK 了。
python 调用 c++。在 c++ 里定义 PyObject* ceph_state_get(PyObject *self, PyObject *args)，在函数里里面通过 PyArg_ParseTuple(args, "ss:ceph_state_get", &handle, &what) 把参数解析为 c++ 类型，就实现了一个 Python 函数。通过 PyMethodDef CephStateMethods[] = get 把 Python 函数加入到一个注册表里。通过 Py_InitModule("ceph_state", CephStateMethods)，将注册表里的函数定义为 ceph_state 模块的属性，并把该模块注入到 python sys.path 里，python 就可以通过 ceph_state.ceph_state_get 调用该函数了


虚拟集群默认参数:
[ -z "$CEPH_NUM_MON" ] && CEPH_NUM_MON=3
[ -z "$CEPH_NUM_OSD" ] && CEPH_NUM_OSD=3
[ -z "$CEPH_NUM_MDS" ] && CEPH_NUM_MDS=3
[ -z "$CEPH_NUM_MGR" ] && CEPH_NUM_MGR=1
[ -z "$CEPH_NUM_FS"  ] && CEPH_NUM_FS=1
[ -z "$CEPH_MAX_MDS" ] && CEPH_MAX_MDS=1
[ -z "$CEPH_NUM_RGW" ] && CEPH_NUM_RGW=0
[ -z "$GANESHA_DAEMON_NUM" ] && GANESHA_DAEMON_NUM=0

[ -z "$CEPH_DIR" ] && CEPH_DIR="$PWD"
[ -z "$CEPH_DEV_DIR" ] && CEPH_DEV_DIR="$CEPH_DIR/dev"
[ -z "$CEPH_OUT_DIR" ] && CEPH_OUT_DIR="$CEPH_DIR/out"
[ -z "$CEPH_RGW_PORT" ] && CEPH_RGW_PORT=8000
[ -z "$CEPH_CONF_PATH" ] && CEPH_CONF_PATH=$CEPH_DIR



常用命令:
ceph mon dump
ceph osd dump
ceph pg dump
ceph osd crush dump
map
上面说过，monitor组件负责监视整个集群的运行状况，如各节点之间的状态、集群配置信息，这些信息由维护集群成员的守护程序来提供，如何存放这些信息呢，答案就是map，ceph monitor map 主要包括如下这几个
Monitor map：包括有关monitor 节点端到端的信息，其中包括 Ceph 集群ID，监控主机名和IP以及端口。并且存储当前版本信息以及最新更改信息，通过 “ ceph mon dump ” 查看 monitor map
OSD map：包括一些常用的信息，如集群ID、创建OSD map的 版本信息和最后修改信息，以及pool相关信息，主要包括pool 名字、pool的ID、类型，副本数目以及PGP等，还包括数量、状态、权重、最新的清洁间隔和OSD主机信息。通过命令 “ceph osd dump” 查看
PG map：包括当前PG版本、时间戳、最新的OSD Map的版本信息、空间使用比例，以及接近占满比例信息，同事，也包括每个PG ID、对象数目、状态、OSD 的状态以及深度清理的详细信息。通过命令 “ceph pg dump” 可以查看相关状态
CRUSH map： CRUSH map 包括集群存储设备信息，故障域层次结构和存储数据时定义失败域规则信息。通过 命令 “ceph osd crush dump 查看
MDS map：MDS Map 包括存储当前 MDS map 的版本信息、创建当前的Map的信息、修改时间、数据和元数据POOL ID、集群MDS数目和MDS状态，可通过"ceph mds dump"查看
副本
副本是ceph存放数据的份数，可以理解为对一个文件备份的份数，ceph默认的副本数是3，即一个主（primary ），一个次（secondary），一个次次（tertiary）,只有primary osd的副本才解释客户端请求，它将数据写入其他osd
如下,可以看到这个叫做testpool的pool中有一个叫做object1的object，他的map信息获取后可以看到
这个对象在osd1上面是主，在osd0和osd2上是次和次次，也就是说在副本数为3的情况下，每个osd存储一个副本
[root@ceph-1 ~]# ceph osd map testpool object1


Ceph RBD 的实现原理与常规操作: https://www.cnblogs.com/jmilkfan-fanguiju/p/11825071.html
rbd showmapped


性能测试:
rados bench -p <pool_name> <seconds> <write|seq|rand>

写入和查看临时数据
rados bench -p p1 10 write --no-cleanup && rados ls -p p1

测试顺序读性能
rados bench -p p1 10 seq

随机读:  rados bench -p p1 10 rand

fio 参考配置,  yum install -y fio "*librbd*", 
cat write.fio
[global]
description="write test with block size of 4M"
direct=1
ioengine=rbd
clustername=ceph
clientname=admin
pool=rbd_pool
rbdname=volume01
iodepth=32
runtime=300
rw=randrw
numjobs=1
bs=8k

[logging]
write_iops_log=write_iops_log
write_bw_log=write_bw_log
write_lat_log=write_lat_log


gdb --args rados bench -p p1 10 write --no-cleanup
src/tools/rados/rados.cc -> main
argv_to_vec
ceph_argparse_need_usage
global_init CEPH_ENTITY_TYPE_CLIENT CODE_ENVIRONMENT_UTILITY -> global_init (defaults=0x0, args=std::vector of length 6, capacity 6 = {...}, module_type=8, code_env=CODE_ENVIRONMENT_UTILITY, flags=0, data_dir_option=0x0, run_pre_init=true)  _. global_init.cc:169
  global_pre_init
common_init_finish -> common_init.cc:87
rados_tool_common
  ret = rados.init_with_context(g_ceph_context) -> open rados
  rados.connect()
  rados.ioctx_create




rm -f rados_write && gcc rados_write.c  -lrados -lrados  -g3 -Og -Wall  -Wl,-rpath=/home/xb/project/stor/ceph/xb/docker/ceph/build/lib -lrados -L/home/xb/project/stor/ceph/xb/docker/ceph/build/lib/ -o rados_write && ./rados_write --debug_objecter=30 --debug_ms=30



写参考: https://ibz.bz/2017/01/12/8ab0314b456d4b1fa6e063dace9c9d8a.html
设置image id
ioctx->exec(oid, "rbd", "set_id", in, out)
       >io_ctx_impl->exec(obj, cls, method, inbl, outbl)
           >(::ObjectOperation)rd.call(cls, method, inbl) //将该操作封装成OSDOp，放入ObjectOperation对象的vector集合中
               >add_call(CEPH_OSD_OP_CALL, cname, method, indata, NULL, NULL, NULL)
           >operate_read(oid, &rd, &outbl) //发起读请求
               >Objecter::Op *objecter_op = objecter->prepare_read_op(oid, oloc,*o, snap_seq, pbl, flags,onack, &ver) //创建Op的实例 数据结构变成Op
               >objecter->op_submit(objecter_op) //提交到objecter层 操作对象为Objecter::Op
                   >_op_submit_with_budget(op, lc, ctx_budget)
                       >int op_budget = _take_op_budget(op) //减去该Op的预算for throttle;
                           >int op_budget = calc_op_budget(op) //预算值是该Op的字节大小
                           > _throttle_op(op, op_budget) //这里是Objecter的Throttle层，如果keep_balanced_budget=true，能实现对速度的限制（op_throttle_bytes&op_throttle_ops）
                       >_op_submit(op, lc)
                           >_calc_target(&op->target, &op->last_force_resend) //计算该op的操作对象（用到CRUSH算法）
                           >_get_session(op->target.osd, &s, lc) //为该Op构建与osd对应的OSDSession
                           >_send_op_account(op) //登记该次op操作
                           > m = _prepare_osd_op(op) //使用Op中的信息，初始化MOSDOp的实例
                           >_session_op_assign(s, op) //将Op与OSDSession相关联。
                           > _send_op(op, m)
                               >op->session->con->send_message(m) //进入Massenger层，操作对象MOSDOp
                                   >static_cast<SimpleMessenger*>(msgr)->send_message(m, this) //使用使用massenger层的SimpleMessenger的实例发生消息
                                       >_send_message(m, con)
                                           >submit_message(m, static_cast<PipeConnection*>(con),con->get_peer_addr(), con->get_peer_type(), false) //提交信息
                                               >static_cast<PipeConnection*>(con)->try_get_pipe(&pipe) //获取该PipConnection对应的Pipe的实例
                                                   >pipe->_send(m) //通过Pipe发送消息，即：把消息放入到Pipe::out_q队列中，并通知Pipe中的写线程来做实际的发生操作。
                                                       >out_q[m->get_priority()].push_back(m);
                                                   >dispatch_queue.local_delivery(m, m->get_priority()) //如果发送端与接收端是同一个，则直接将消息投递到DispathcQueue::local_messages中


m_ictx->data_ctx.aio_operate(m_oid, rados_completion, &m_write,m_snap_seq, m_snaps)
   >io_ctx_impl->aio_operate(obj, (::ObjectOperation*)o->impl, c->pc,snapc, 0)
       >objecter->mutate(oid, oloc, *o, snap_context, ut, flags, onack, oncommit,&c->objver) //进入Objecter层
           >prepare_mutate_op(oid, oloc, op, snapc, mtime, flags, onack, oncommit, objver) //封装成Op
           >objecter->op_submit(objecter_op) //提交到objecter层 操作对象为Objecter::Op
               >_op_submit_with_budget(op, lc, ctx_budget)
                   >int op_budget = _take_op_budget(op) //减去该Op的预算for throttle;
                       >int op_budget = calc_op_budget(op) //预算值是该Op的字节大小
                       > _throttle_op(op, op_budget) //这里是Objecter的Throttle层，如果keep_balanced_budget=true，能实现对速度的限制（op_throttle_bytes&op_throttle_ops）
                   >_op_submit(op, lc)
                       >_calc_target(&op->target, &op->last_force_resend) //计算该op的操作对象（用到CRUSH算法）
                       >_get_session(op->target.osd, &s, lc) //为该Op构建与osd对应的OSDSession
                       >_send_op_account(op) //登记该次op操作
                       > m = _prepare_osd_op(op) //使用Op中的信息，初始化MOSDOp的实例
                       >_session_op_assign(s, op) //将Op与OSDSession相关联。
                       > _send_op(op, m)
                           >op->session->con->send_message(m) //进入Massenger层，操作对象MOSDOp
                               >static_cast<SimpleMessenger*>(msgr)->send_message(m, this) //使用使用massenger层的SimpleMessenger的实例发生消息
                                   >_send_message(m, con)
                                       >submit_message(m, static_cast<PipeConnection*>(con),con->get_peer_addr(), con->get_peer_type(), false) //提交信息
                                           >static_cast<PipeConnection*>(con)->try_get_pipe(&pipe) //获取该PipConnection对应的Pipe的实例
                                               >pipe->_send(m) //通过Pipe发送消息，即：把消息放入到Pipe::out_q队列中，并通知Pipe中的写线程来做实际的发生操作。
                                                   >out_q[m->get_priority()].push_back(m);
                                               >dispatch_queue.local_delivery(m, m->get_priority()) //如果发送端与接收端是同一个，则直接将消息投递到DispathcQueue::local_messages中


                                      